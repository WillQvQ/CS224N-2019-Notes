1
00:00:05,000 --> 00:00:08,985
So welcome to the Machine [NOISE] Translation lecture,
欢迎来到机器[NOISE]翻译讲座，

2
00:00:08,985 --> 00:00:11,580
which is kind of like a culmination [NOISE] of
这有点像一个高潮[NOISE]

3
00:00:11,580 --> 00:00:15,870
this sequence of three lectures on RNNs and related topics.
关于RNN和相关主题的三个讲座的序列。

4
00:00:15,870 --> 00:00:18,420
So let's have a few announcements first.
所以我们首先要发布一些声明。

5
00:00:18,420 --> 00:00:19,590
Uh, the first thing is,
呃，首先是，

6
00:00:19,590 --> 00:00:20,940
as you probably noticed when you came in,
当你进来时你可能已经注意到

7
00:00:20,940 --> 00:00:22,770
we're taking attendance today.
我们今天正在参加。

8
00:00:22,770 --> 00:00:27,090
Uh, so you need to sign in with the TAs who are outside the auditorium.
呃，所以你需要在礼堂外签名。

9
00:00:27,090 --> 00:00:28,965
Uh, if you missed it,
呃，如果你错过了，

10
00:00:28,965 --> 00:00:30,540
don't get up now, it's fine.
现在不起床，没关系。

11
00:00:30,540 --> 00:00:32,985
There will be time to sign in after the lecture.
讲座后有时间登录。

12
00:00:32,985 --> 00:00:34,880
Uh, and then, if you have any kind of questions about
呃，然后，如果你有任何问题

13
00:00:34,880 --> 00:00:37,130
special cases with the attendance policy, uh,
出勤政策的特殊情况，呃，

14
00:00:37,130 --> 00:00:41,090
you should check out a Piazza post that we put up last night with some clarifications.
你应该查看我们昨晚提出的一个广场帖子，并做了一些澄清。

15
00:00:41,090 --> 00:00:45,230
[NOISE] Uh, you have the reminder that Assignment 4 content is going to be covered today.
[NOISE]呃，你有提醒说今天将分配作业4的内容。

16
00:00:45,230 --> 00:00:47,690
So you're gonna have everything you need to do Assignment 4 at the end of today.
因此，您将拥有在今天结束时完成作业4所需的一切。

17
00:00:47,690 --> 00:00:51,860
[NOISE] And do get started early because the model takes 4 hours to train.
[噪音]并提前开始，因为该模型需要4个小时的训练。

18
00:00:51,860 --> 00:00:54,585
The other announcement is that we're going [NOISE] to be sending out
另一个公告是我们要[发送] [发送]

19
00:00:54,585 --> 00:00:58,580
our mid-quarter feedback survey sometime in the next few days probably,
我们可能会在接下来几天的某个时候进行中期反馈调查，

20
00:00:58,580 --> 00:01:00,110
uh, so please do fill it out.
呃，所以请填写。

21
00:01:00,110 --> 00:01:01,775
You'll get 0,5% credit,
你将获得0,5％的学分，

22
00:01:01,775 --> 00:01:04,520
and you're also gonna help us to make the class better for the rest of the quarter.
而你也将帮助我们在本季剩余时间里让课程变得更好。

23
00:01:04,520 --> 00:01:08,190
[NOISE] Okay.
[NOISE]好的。

24
00:01:08,190 --> 00:01:09,880
So here's the overview of what we're going to do today.
所以这里是我们今天要做的事情的概述。

25
00:01:09,880 --> 00:01:11,535
[NOISE] Uh, today, first,
[NOISE]呃，今天，第一，

26
00:01:11,535 --> 00:01:13,820
we're going to introduce a new task in NLP,
我们将在NLP中介绍一项新任务，

27
00:01:13,820 --> 00:01:16,190
which is machine translation, [NOISE] and then,
这是机器翻译，[NOISE]然后，

28
00:01:16,190 --> 00:01:20,200
we're going to introduce a new neural architecture called sequence-to-sequence.
我们将介绍一种称为序列到序列的新神经结构。

29
00:01:20,200 --> 00:01:22,590
And the connection here is that machine translation
这里的连接就是机器翻译

30
00:01:22,590 --> 00:01:25,020
is a major use case of sequence-to-sequence.
是序列到序列的主要用例。

31
00:01:25,020 --> 00:01:29,525
[NOISE] After that, we're going to introduce a new neural technique called attention,
[NOISE]在那之后，我们将介绍一种名为注意力的新神经技术，

32
00:01:29,525 --> 00:01:33,415
and this is something that improves sequence-to-sequence a lot.
这是一个可以改进序列到序列的东西。

33
00:01:33,415 --> 00:01:37,230
Okay. So Section 1 of this is gonna be about, uh,
好的。因此，第1节将是关于，呃，

34
00:01:37,230 --> 00:01:40,920
a bit of machine translation history, pre-neural machine translation.
一点机器翻译历史，预神经机器翻译。

35
00:01:40,920 --> 00:01:45,180
[NOISE] So machine translation or MT,
[NOISE]所以机器翻译或MT，

36
00:01:45,180 --> 00:01:48,140
uh, is the task of translating a sentence x, uh,
呃，是翻译句子的任务x，呃，

37
00:01:48,140 --> 00:01:50,060
which we call the source language,
我们称之为源语言，

38
00:01:50,060 --> 00:01:52,250
whatever language you're translating from,
无论你翻译什么语言，

39
00:01:52,250 --> 00:01:54,560
into a sentence y, which is in another language,
换句话y，用另一种语言，

40
00:01:54,560 --> 00:01:56,645
which we call the target language.
我们称之为目标语言。

41
00:01:56,645 --> 00:01:59,875
Uh, so here's an example. Let's suppose x is this French sentence.
呃，这是一个例子。我们假设x是这个法语句子。

42
00:01:59,875 --> 00:02:02,100
Um, [NOISE] could anyone in the audience,
嗯，[NOISE]观众中的任何人都可以，

43
00:02:02,100 --> 00:02:04,300
a French speaker, translate this to English for us? [NOISE] [BACKGROUND] Yeah.
一位法语发言人，为我们翻译成英语？ [NOISE] [背景]是的。

44
00:02:04,300 --> 00:02:05,430
Um, the man is born free, and, uh, everywhere, he is in irons.
嗯，这个人天生就是自由的，呃，无处不在，他都是铁杆。

45
00:02:05,430 --> 00:02:12,500
Great.
大。

46
00:02:12,500 --> 00:02:20,760
So that was something like,
这就是这样的，

47
00:02:20,760 --> 00:02:22,785
the man is born free, but everywhere, he's in irons.
这个人天生就是自由的，但无处不在，他都是铁杆。

48
00:02:22,785 --> 00:02:24,395
That was a fairly literal translation.
这是一个相当字面的翻译。

49
00:02:24,395 --> 00:02:27,770
It's usually translated, this quote by Rousseau is usually translated as,
它通常被翻译，卢梭的这句话通常翻译为，

50
00:02:27,770 --> 00:02:29,880
man is born free, but everywhere, he is in chains.
男人天生自由，但无处不在，他是枷锁。

51
00:02:29,880 --> 00:02:32,475
But there's an ambiguity: [NOISE] should fers be,
但是有一种含糊之处：[NOISE]应该是，

52
00:02:32,475 --> 00:02:34,200
um, literally irons or chains?
嗯，字面上是铁杆还是链子？

53
00:02:34,200 --> 00:02:35,880
Also, you could choose to, uh,
另外，你可以选择，呃，

54
00:02:35,880 --> 00:02:38,670
translate l'homme as man or maybe humankind.
将l'homme翻译为男人或人类。

55
00:02:38,670 --> 00:02:41,570
Uh, so this is an example of machine translation,
呃，这是机器翻译的一个例子，

56
00:02:41,570 --> 00:02:42,635
and there's already, you know,
你知道，已经有了

57
00:02:42,635 --> 00:02:43,790
quite a few choices you can make.
你可以做出很多选择。

58
00:02:43,790 --> 00:02:50,505
[NOISE] So the beginning of machine translation as an AI task began in the early 1950s.
[NOISE]因此，作为AI任务的机器翻译的开始始于20世纪50年代初。

59
00:02:50,505 --> 00:02:52,365
So, um, in particular,
所以，嗯，特别是，

60
00:02:52,365 --> 00:02:55,010
there was a lot of work translating Russian to English, uh,
有很多工作将俄语翻译成英语，呃，

61
00:02:55,010 --> 00:02:57,200
because the West was very interested in listening
因为西方对听力很感兴趣

62
00:02:57,200 --> 00:02:59,800
to what the Russians were saying during the Cold War.
俄罗斯人在冷战期间所说的话。

63
00:02:59,800 --> 00:03:01,920
And we've got a fun video here,
我们这里有一个有趣的视频，

64
00:03:01,920 --> 00:03:05,300
[NOISE] which shows the state of machine translation in 1954.
[NOISE]显示1954年的机器翻译状态。

65
00:03:05,300 --> 00:03:08,910
[MUSIC] They haven't reckoned with
[音乐]他们没有考虑到

66
00:03:08,910 --> 00:03:13,185
ambiguity when they set out to use computers to translate languages.
他们开始使用计算机翻译语言时含糊不清。

67
00:03:13,185 --> 00:03:16,395
A $500,000 simple calculator,
500,000美元的简单计算器，

68
00:03:16,395 --> 00:03:18,600
most versatile electronic brain known,
最通用的电子脑已知，

69
00:03:18,600 --> 00:03:20,850
translates Russian into English.
将俄语翻译成英语。

70
00:03:20,850 --> 00:03:22,815
Instead of mathematical wizardry,
而不是数学魔法，

71
00:03:22,815 --> 00:03:24,560
a sentence in Russian is to be fed [OVERLAPPING].
用俄语写一个句子[OVERLAPPING]。

72
00:03:24,560 --> 00:03:27,680
One of the first non-numerical applications of computers,
计算机的第一个非数字应用之一，

73
00:03:27,680 --> 00:03:29,480
[BACKGROUND] it was hyped as the solution to
[背景]它被大肆宣传为解决方案

74
00:03:29,480 --> 00:03:33,380
the Cold War obsession of keeping tabs on what the Russians were doing.
冷战痴迷于密切关注俄罗斯人正在做什么。

75
00:03:33,380 --> 00:03:37,280
Claims were made that the computer would replace most human translators.
声称计算机将取代大多数人工翻译。

76
00:03:37,280 --> 00:03:40,380
[inaudible] you're just in the experimental stage.
[听不清]你刚刚进入实验阶段。

77
00:03:40,380 --> 00:03:42,120
When you go in for full-scale production,
当你进行全面生产时，

78
00:03:42,120 --> 00:03:43,410
what will the capacity be?
容量是多少？

79
00:03:43,410 --> 00:03:46,456
We should be able to do about,
我们应该能够做到，

80
00:03:46,456 --> 00:03:49,860
with the help of a commercial computer, uh, about one to two million words, uh,
在商业电脑的帮助下，呃，大约一两百万字，呃，

81
00:03:49,860 --> 00:03:53,235
an hour, and this will be quite an adequate speed to cope with
一个小时，这将是一个足够的速度来应对

82
00:03:53,235 --> 00:03:57,735
the whole alphabet of the Soviet Union in just a few hours' computer time a week.
苏联的整个字母表，每周只需几个小时的计算机时间。

83
00:03:57,735 --> 00:03:59,970
When do you have to be able to achieve this feat?
你何时必须能够实现这一壮举？

84
00:03:59,970 --> 00:04:01,785
If our experiments go well,
如果我们的实验进展顺利，

85
00:04:01,785 --> 00:04:04,810
then perhaps within, uh, five years or so.
然后也许在，呃，五年左右。

86
00:04:05,210 --> 00:04:08,580
So in this video, I think there's a number of interesting things.
所以在这个视频中，我认为有很多有趣的事情。

87
00:04:08,580 --> 00:04:11,665
Uh, firstly, we can see an example of about how,
呃，首先，我们可以看到一个关于如何的例子，

88
00:04:11,665 --> 00:04:13,540
uh, AI hype is nothing new.
呃，AI炒作并不是什么新鲜事。

89
00:04:13,540 --> 00:04:15,880
Even in 1954, [NOISE] they were talking
甚至在1954年，[NOISE]他们正在谈论

90
00:04:15,880 --> 00:04:19,525
this machine translation system as if it was an electronic brain,
这台机器翻译系统仿佛它是一个电子大脑，

91
00:04:19,525 --> 00:04:22,495
which I think, uh, overstates maybe how general it is.
我认为，呃，夸大其概率。

92
00:04:22,495 --> 00:04:24,130
Uh, they were also, at least some of them,
呃，他们也是，至少其中一些，

93
00:04:24,130 --> 00:04:26,995
fairly optimistic that this [NOISE] machine translation system
相当乐观的是[NOISE]机器翻译系统

94
00:04:26,995 --> 00:04:30,550
was going to be replacing humans, uh, anytime soon.
很快就会取代人类，呃。

95
00:04:30,550 --> 00:04:33,670
Um, so yeah, that's, that's pretty interesting.
嗯，所以是的，那是非常有趣的。

96
00:04:33,670 --> 00:04:38,125
And, um, [NOISE] the thing is that these systems actually were mostly rule-based, uh,
而且，嗯，[NOISE]问题是这些系统实际上主要是基于规则的，呃，

97
00:04:38,125 --> 00:04:40,735
by which I mean that they were mostly using
我的意思是他们大多使用

98
00:04:40,735 --> 00:04:43,270
a bilingual dictionary between Russian and English,
俄语和英语之间的双语词典，

99
00:04:43,270 --> 00:04:46,440
and they were essentially mostly just looking up the Russian words, uh,
他们基本上只是查找俄语单词，呃，

100
00:04:46,440 --> 00:04:47,939
looking up their English counterparts,
抬头看他们的英文同行，

101
00:04:47,939 --> 00:04:51,540
and they were storing these big bilingual dictionaries on these large magnetic tapes.
他们将这些大型双语词典存放在这些大磁带上。

102
00:04:51,540 --> 00:04:55,390
Um, so certainly, it was a [NOISE] huge technical feat at the time, uh,
嗯，当然，那是一个[NOISE]巨大的技术壮举，呃，

103
00:04:55,390 --> 00:04:57,110
but they, uh, some people were probably too
但他们，呃，有些人可能也是

104
00:04:57,110 --> 00:05:00,065
optimistic [NOISE] about how quickly it would replace humans.
关于它将如何快速取代人类的乐观[NOISE]。

105
00:05:00,065 --> 00:05:03,230
So jumping forward several decades in time,
所以几十年前后，

106
00:05:03,230 --> 00:05:06,320
uh, now I want to tell you about statistical machine translation.
呃，现在我想告诉你关于统计机器的翻译。

107
00:05:06,320 --> 00:05:10,070
So the core idea of statistical machine translation is that you're going to
所以统计机器翻译的核心思想就是你要去的

108
00:05:10,070 --> 00:05:14,335
learn a probabilistic model from the data in order to do the translation.
从数据中学习概率模型以进行翻译。

109
00:05:14,335 --> 00:05:16,800
So as an example, uh, as before,
举个例子，呃，和以前一样，

110
00:05:16,800 --> 00:05:19,295
suppose that we're translating from French to English.
假设我们正在从法语翻译成英语。

111
00:05:19,295 --> 00:05:23,225
The idea is that you want to find the best English sentence y,
想法是你想找到最好的英语句子y，

112
00:05:23,225 --> 00:05:24,860
given the French sentence x,
鉴于法语句子x，

113
00:05:24,860 --> 00:05:26,270
[NOISE] and, uh, mathematically,
[NOISE]而且，呃，在数学上，

114
00:05:26,270 --> 00:05:31,640
you can formulate this as finding argmax y of this conditional probability of y, given x,
你可以将此表示为找到y的这个条件概率的argmax y，给定x，

115
00:05:31,640 --> 00:05:33,605
[NOISE] and the model that you're learning is
[NOISE]和你正在学习的模型是

116
00:05:33,605 --> 00:05:38,010
this probability distribution P. [NOISE] So what we usually do is,
这个概率分布P. [NOISE]所以我们通常做的是，

117
00:05:38,010 --> 00:05:40,640
we break down this probability into,
我们把这个概率分解成，

118
00:05:40,640 --> 00:05:43,190
uh, its two components using Bayes' Rule.
呃，它的两个组成部分使用贝叶斯规则。

119
00:05:43,190 --> 00:05:46,995
[NOISE] So this means that finding the y that maximizes,
[NOISE]所以这意味着找到最大化的y，

120
00:05:46,995 --> 00:05:48,585
uh, probability of y, given x,
呃，y的概率，给定x，

121
00:05:48,585 --> 00:05:52,340
is equivalent to finding the y that maximizes the probability of x,
相当于找到最大化x概率的y，

122
00:05:52,340 --> 00:05:55,255
given y, times the probability of y.
给定y乘以y的概率。

123
00:05:55,255 --> 00:05:57,900
So the two components here, on the left,
所以这两个组件在左边，

124
00:05:57,900 --> 00:05:59,475
we have a translation model,
我们有翻译模型，

125
00:05:59,475 --> 00:06:03,605
and this is keeping track of how words and phrases should be translated.
这是跟踪单词和短语应该如何翻译。

126
00:06:03,605 --> 00:06:06,560
Uh, so the idea is that it knows, uh, how, uh,
呃，所以这个想法是它知道，呃，怎么样，呃，

127
00:06:06,560 --> 00:06:10,880
French words and an English word might be translated to each other or maybe small,
法语单词和英语单词可能相互翻译或者可能很小，

128
00:06:10,880 --> 00:06:13,610
small phrases and chunks of words should be translated.
应该翻译小词和大块的单词。

129
00:06:13,610 --> 00:06:15,985
And this is learned from a lot of parallel data,
这是从很多并行数据中学到的，

130
00:06:15,985 --> 00:06:17,780
and I'll be telling you later how we do that.
我稍后会告诉你我们是怎么做到的。

131
00:06:17,780 --> 00:06:20,270
The second compo- component P(y),
第二个组分P（y），

132
00:06:20,270 --> 00:06:22,490
[NOISE] this is just a language model.
[NOISE]这只是一种语言模型。

133
00:06:22,490 --> 00:06:23,990
[NOISE] We learned about this last week.
[噪音]上周我们了解到了这一点。

134
00:06:23,990 --> 00:06:26,890
A language model is a system that can predict the next word,
语言模型是一个可以预测下一个单词的系统，

135
00:06:26,890 --> 00:06:29,360
but it can also be thought of as a system [NOISE] that tells
但它也可以被认为是一个告诉你的系统[NOISE]

136
00:06:29,360 --> 00:06:32,000
you the probability of a sequence of words.
你是一系列单词的概率。

137
00:06:32,000 --> 00:06:34,595
So here, if we're translating from French to English,
所以在这里，如果我们从法语翻译成英语，

138
00:06:34,595 --> 00:06:36,470
P(y) is an English language model.
P（y）是英语语言模型。

139
00:06:36,470 --> 00:06:38,560
[NOISE] So the idea is the,
[NOISE]所以这个想法是，

140
00:06:38,560 --> 00:06:40,355
the reason why we want to break down
我们想要打破的原因

141
00:06:40,355 --> 00:06:44,870
this single conditiona- conditional probability distribution into the,
这条单一条件 - 条件概率分布到，

142
00:06:44,870 --> 00:06:49,490
the pr- product of two different ones is that this is a kind of division of labor.
两种不同的产品是这是一种分工。

143
00:06:49,490 --> 00:06:51,560
The idea is that instead of, uh,
这个想法是，而不是，呃，

144
00:06:51,560 --> 00:06:55,745
a single conditional probability distribution needing to understand how to translate,
单个条件概率分布需要理解如何翻译，

145
00:06:55,745 --> 00:06:57,529
and how to write good English text,
以及如何写出好的英文文本，

146
00:06:57,529 --> 00:06:58,970
and understand sentence structure,
并理解句子结构，

147
00:06:58,970 --> 00:07:02,060
and everything at once, the idea is that you separate it so that [NOISE]
而且一下子，你的想法就是把它分开以便[NOISE]

148
00:07:02,060 --> 00:07:05,735
the translation model on the left in blue mostly just knows about a
蓝色左边的翻译模型大多只知道一个

149
00:07:05,735 --> 00:07:09,170
local translation of small chunks of words and phrases,
小块单词和短语的本地翻译，

150
00:07:09,170 --> 00:07:12,950
whereas the language model on the right more takes care of writing good English,
而右边的语言模型更注重写好英语，

151
00:07:12,950 --> 00:07:15,110
good sentence structure, word order, and so on.
良好的句子结构，词序等。

152
00:07:15,110 --> 00:07:17,465
[NOISE] So you already know
[NOISE]所以你已经知道了

153
00:07:17,465 --> 00:07:20,240
how to learn a language model [NOISE] because we learned about that last time.
如何学习语言模型[NOISE]因为我们最后一次学到了这一点。

154
00:07:20,240 --> 00:07:21,990
You just need lots of monolingual data,
你只需要很多单语数据，

155
00:07:21,990 --> 00:07:23,240
in this case, English data.
在这种情况下，英文数据。

156
00:07:23,240 --> 00:07:25,520
[NOISE] So I'm going to tell you more about how we would learn
[NOISE]所以我要告诉你更多关于我们将如何学习的内容

157
00:07:25,520 --> 00:07:28,610
this translation model that needs to be learned from parallel data.
这个需要从并行数据中学习的翻译模型。

158
00:07:28,610 --> 00:07:30,860
[NOISE]
[噪声]

159
00:07:30,860 --> 00:07:36,565
So we need a large amount of parallel data in order to learn this translation model.
因此，我们需要大量的并行数据才能学习这种翻译模型。

160
00:07:36,565 --> 00:07:39,700
And an early example of a parallel corpus,
并行语料库的早期例子，

161
00:07:39,700 --> 00:07:41,260
is the Rosetta Stone.
是罗塞塔石碑。

162
00:07:41,260 --> 00:07:46,045
So this is a stone that has the same text written in three different languages.
所以这是一块用三种不同语言写成相同文本的石头。

163
00:07:46,045 --> 00:07:48,985
And this is a hugely important artifact
这是一件非常重要的神器

164
00:07:48,985 --> 00:07:53,590
for the people who were trying to understand ancient Egyptian.
对于那些试图了解古埃及人的人。

165
00:07:53,590 --> 00:07:55,060
So in the 19th century,
所以在19世纪，

166
00:07:55,060 --> 00:07:56,680
uh, scholars discovered this stone,
呃，学者发现了这块石头，

167
00:07:56,680 --> 00:07:59,590
and it helped them to figure out ancient Egyptian because there was
它帮助他们找出古埃及人，因为有

168
00:07:59,590 --> 00:08:04,340
this parallel text that had the the same text in other languages that they did know.
这个并行文本在他们知道的其他语言中具有相同的文本。

169
00:08:04,340 --> 00:08:07,605
So this is, this is a really important parallel corpus,
所以这就是，这是一个非常重要的平行语料库，

170
00:08:07,605 --> 00:08:09,210
and if you're ever in London,
如果你曾经在伦敦，

171
00:08:09,210 --> 00:08:10,410
you can go to the British Museum,
你可以去大英博物馆，

172
00:08:10,410 --> 00:08:12,320
and see this in person.
并亲自看到这个。

173
00:08:12,320 --> 00:08:15,235
So the idea is that you get your parallel data.
所以我们的想法是获得并行数据。

174
00:08:15,235 --> 00:08:17,200
Obviously, you need a larger amount that is on the stone,
显然，你需要更多的石头，

175
00:08:17,200 --> 00:08:19,615
and hopefully it shouldn't be written on a stone either.
并且希望它也不应该写在石头上。

176
00:08:19,615 --> 00:08:25,105
But you can use this to learn your statistical machine translation model.
但您可以使用它来学习统计机器翻译模型。

177
00:08:25,105 --> 00:08:27,430
So the idea is that, you are trying to learn
所以我的想法是，你正在努力学习

178
00:08:27,430 --> 00:08:30,550
this conditional probability distribution of x given y.
给定y的x的条件概率分布。

179
00:08:30,550 --> 00:08:33,520
So what we do is we actually break this down even further.
所以我们做的是我们实际上进一步打破了这一点。

180
00:08:33,520 --> 00:08:37,795
We actually want to consider the probability of x and a given y.
我们实际上想要考虑x和给定y的概率。

181
00:08:37,795 --> 00:08:39,685
Where a is the alignment.
其中a是对齐方式。

182
00:08:39,685 --> 00:08:41,125
So the idea of alignment,
所以对齐的想法，

183
00:08:41,125 --> 00:08:43,300
is this is how the words in
这是怎么回事

184
00:08:43,300 --> 00:08:47,110
the English sentence and the French sentence correspond to each other.
英语句子和法语句子相互对应。

185
00:08:47,110 --> 00:08:50,690
So I'm gonna, uh, demonstrate this by an example.
所以我要通过一个例子证明这一点。

186
00:08:50,690 --> 00:08:53,190
So in this example,
所以在这个例子中，

187
00:08:53,190 --> 00:08:57,315
while we're translating the sentence 'Japan shaken by two new quakes' to French.
而我们正在将“日本因两次新地震而动摇”的句子翻译成法语。

188
00:08:57,315 --> 00:09:01,080
Then you can see there is a pretty simple one-to-one alignment here,
然后你可以看到这里有一个非常简单的一对一对齐，

189
00:09:01,080 --> 00:09:02,595
uh, of English words to French words,
呃，英语单词到法语单词，

190
00:09:02,595 --> 00:09:05,150
and also they appear in the exact same order.
它们也以完全相同的顺序出现。

191
00:09:05,150 --> 00:09:10,510
The only thing that doesn't conform to that is the word 'Le' in French,
唯一不符合这一点的是法语中的'Le'这个词，

192
00:09:10,510 --> 00:09:12,490
which we call a spurious word because it doesn't
我们称之为虚假词，因为它没有

193
00:09:12,490 --> 00:09:15,205
have a direct counterpart in the English sentence,
在英文句子中有一个直接的对应物，

194
00:09:15,205 --> 00:09:16,630
and that's because in English we just say,
那是因为在英语中我们只是说，

195
00:09:16,630 --> 00:09:19,670
'Japan', but in French we say, 'Le Japon'.
'日本'，但用法语我们说'Le Japon'。

196
00:09:19,680 --> 00:09:22,915
So alignment can be a bit more complicated than that.
因此，对齐可能比这更复杂。

197
00:09:22,915 --> 00:09:25,480
For example, alignment can be many-to-one.
例如，对齐可以是多对一的。

198
00:09:25,480 --> 00:09:28,150
In this example, you have, uh,
在这个例子中，你有，呃，

199
00:09:28,150 --> 00:09:32,740
several French words that have multiple English words that correspond to them.
几个法语单词，有多个与之对应的英语单词。

200
00:09:32,740 --> 00:09:35,050
So this is what we call many-to-one alignment.
所以这就是我们所谓的多对一对齐。

201
00:09:35,050 --> 00:09:38,455
Uh, it can go in the other direction too.
呃，它也可以向另一个方向发展。

202
00:09:38,455 --> 00:09:40,075
Alignment can be one-to-many.
对齐可以是一对多。

203
00:09:40,075 --> 00:09:43,015
So here we have a single English word implemented,
所以这里我们实现了一个单词，

204
00:09:43,015 --> 00:09:44,950
which has a one-to-many alignment because there is
它有一对多的对齐，因为有

205
00:09:44,950 --> 00:09:48,355
a three-word French phra-phrase that corresponds to it.
与之相对应的三字法语词组。

206
00:09:48,355 --> 00:09:49,900
So on the left and the right,
所以在左边和右边，

207
00:09:49,900 --> 00:09:52,390
we have two ways of depicting the same alignments.
我们有两种描绘相同路线的方法。

208
00:09:52,390 --> 00:09:57,410
It's either a kind of chart or it can be a, a graph.
它可以是一种图表，也可以是图表。

209
00:09:58,020 --> 00:10:01,345
So here's another example, um,
所以这是另一个例子，嗯，

210
00:10:01,345 --> 00:10:04,960
of a one-to-many, well, sorry, right.
一对多，好吧，抱歉，对。

211
00:10:04,960 --> 00:10:08,200
So we call, uh, this word implemented, that is one-to-many.
所以我们称之为，呃，实施了这个词，即一对多。

212
00:10:08,200 --> 00:10:12,565
We call it a fertile word because the idea is that it has many children in the,
我们称之为一个肥沃的词，因为这个想法是它有许多孩子，

213
00:10:12,565 --> 00:10:14,095
in the target sentence.
在目标句子中。

214
00:10:14,095 --> 00:10:17,065
So in fact, there are some words which are very fertile.
事实上，有些词语非常肥沃。

215
00:10:17,065 --> 00:10:19,690
Here's an example where the source sentence,
这是一个源句子的例子，

216
00:10:19,690 --> 00:10:22,060
'il m'a entarte', means,
'il m'a entarte'，意思是，

217
00:10:22,060 --> 00:10:23,455
'he hit me with a pie',
“他用馅饼打我”，

218
00:10:23,455 --> 00:10:25,090
and here in French,
这里用法语，

219
00:10:25,090 --> 00:10:28,630
this verb, 'entarte' means, uh, to hit someone with a pie,
这个动词，'entarte'意思是，呃，用饼来打人，

220
00:10:28,630 --> 00:10:29,020
[LAUGHTER]
[笑声]

221
00:10:29,020 --> 00:10:35,845
and this word has no single word equivalent in English.
这个词在英语中没有相同的单词。

222
00:10:35,845 --> 00:10:38,560
We don't have a single verb that means to hit someone with a pie.
我们没有单一的动词，意味着用馅饼打人。

223
00:10:38,560 --> 00:10:39,910
[LAUGHTER]
[笑声]

224
00:10:39,910 --> 00:10:41,695
Which I think that is really fun, that French has a word.
我认为这真的很有趣，法语有一个词。

225
00:10:41,695 --> 00:10:43,270
You wonder, maybe they do it so
你想知道，也许他们这样做了

226
00:10:43,270 --> 00:10:45,100
often that they need a single word for that. I don't know.
通常，他们需要一个单词。我不知道。

227
00:10:45,100 --> 00:10:46,750
[LAUGHTER]
[笑声]

228
00:10:46,750 --> 00:10:48,790
So this is an example of a fertile word, right?
所以这是一个肥沃的词的例子，对吧？

229
00:10:48,790 --> 00:10:53,360
Because it needs to have several corresponding English words to translate it.
因为它需要有几个相应的英文单词来翻译它。

230
00:10:54,000 --> 00:10:57,400
So we can have one-to-many, and many-to-one.
所以我们可以有一对多，多对一。

231
00:10:57,400 --> 00:10:59,650
You can also have many-to-many alignments.
您还可以进行多对多对齐。

232
00:10:59,650 --> 00:11:03,085
You could call that kind of phrase level translation, or phrase-to-phrase.
您可以将这种短语级别翻译或短语转换为短语。

233
00:11:03,085 --> 00:11:06,100
So here, uh, the English sentence says,
所以在这里，呃，英文句子说，

234
00:11:06,100 --> 00:11:08,035
'The poor doesn't- don't have any money',
“穷人没有 - 没有钱”，

235
00:11:08,035 --> 00:11:11,350
and here don't have any money corresponds to the French phrase,
这里没有任何钱对应法语短语，

236
00:11:11,350 --> 00:11:14,860
'sont demunis', and this is a many-to-many alignment because there is
'sont demunis'，这是一个多对多的对齐，因为有

237
00:11:14,860 --> 00:11:18,685
no obvious way to break down this phrase-to-phrase alignment into,
没有明显的方法来打破这种短语到短语的对齐，

238
00:11:18,685 --> 00:11:21,770
um, smaller word-to-word alignments.
嗯，较小的单词对齐。

239
00:11:22,860 --> 00:11:25,495
Okay. So that's what alignment is.
好的。这就是对齐方式。

240
00:11:25,495 --> 00:11:27,880
And if you remember, we were thinking about how would you
如果你还记得，我们在想你会怎样

241
00:11:27,880 --> 00:11:30,730
learn this probability distribution of what the alignment is,
学习这种对齐方式的概率分布，

242
00:11:30,730 --> 00:11:33,940
uh, in order to do statistical machine translation.
呃，为了做统计机器翻译。

243
00:11:33,940 --> 00:11:37,180
So the idea is that you learn probability of x and a,
所以我的想法是你学习x和a的概率，

244
00:11:37,180 --> 00:11:41,215
given y as a combination of many factors or many features.
将y视为许多因素或许多特征的组合。

245
00:11:41,215 --> 00:11:43,359
So you consider for example,
所以你考虑一下，例如，

246
00:11:43,359 --> 00:11:47,140
what's the probability of a particular word aligning to another particular word?
特定单词与另一个特定单词对齐的概率是多少？

247
00:11:47,140 --> 00:11:48,940
Like you know, this English word and this French word,
就像你知道的，这个英文单词和这个法语单词，

248
00:11:48,940 --> 00:11:50,065
how often do they align?
他们经常对齐？

249
00:11:50,065 --> 00:11:52,090
But then, this also depends on for example,
但是，这也取决于例如，

250
00:11:52,090 --> 00:11:53,740
what's their position in the sentence?
他们在句子中的立场是什么？

251
00:11:53,740 --> 00:11:56,739
Like if they both appear near the end of the sentences,
就好如果它们都出现在句子的末尾附近，

252
00:11:56,739 --> 00:11:58,810
then it's more likely that they align, whereas,
那么它们更可能是对齐的，而

253
00:11:58,810 --> 00:12:01,855
if one's at the beginning and one's at the end, that's less likely.
如果一个人在开始时和一个人在最后，那就不太可能了。

254
00:12:01,855 --> 00:12:04,240
You would also consider things like, uh,
你也会考虑这样的事情，呃，

255
00:12:04,240 --> 00:12:08,080
what's the probability of this particular French word having this particular fertility?
这个特殊的法语单词具有这种特殊生育能力的概率是多少？

256
00:12:08,080 --> 00:12:10,630
Like, what's the probability of this word having
比如，这个词的概率是多少

257
00:12:10,630 --> 00:12:13,475
three corresponding English words and so on?
三个相应的英文单词等？

258
00:12:13,475 --> 00:12:17,070
So all of these statistics are learned from your parallel data,
所以这些统计数据都是从你的并行数据中学到的，

259
00:12:17,070 --> 00:12:20,240
and there's many other things that you would take into consideration.
还有很多其他事情需要考虑。

260
00:12:20,240 --> 00:12:23,890
So we're looking at a kind of overview of statistical machine translation today.
所以我们今天看一下统计机器翻译的概述。

261
00:12:23,890 --> 00:12:26,020
You're not going to understand it in full detail,
你不会详细了解它，

262
00:12:26,020 --> 00:12:28,150
but we're understanding an overview of how it works,
但我们理解它是如何工作的概述，

263
00:12:28,150 --> 00:12:29,725
because we're going to be, uh,
因为我们会成为，呃，

264
00:12:29,725 --> 00:12:32,780
comparing it to neural machine translation.
将其与神经机器翻译进行比较。

265
00:12:33,510 --> 00:12:37,660
Okay. So we're learning this SMT system,
好的。所以我们正在学习这个SMT系统，

266
00:12:37,660 --> 00:12:40,825
and so far, we've broken it down into these two main components.
到目前为止，我们已将其分解为这两个主要组成部分。

267
00:12:40,825 --> 00:12:42,295
We've got the translation model,
我们有翻译模型，

268
00:12:42,295 --> 00:12:43,900
and we've got the language model,
我们有语言模型，

269
00:12:43,900 --> 00:12:46,690
and we understand a little bit about how you might
我们对你的理解有所了解

270
00:12:46,690 --> 00:12:50,365
learn this translation model by breaking it down into alignments.
通过将其分解为对齐来学习此翻译模型。

271
00:12:50,365 --> 00:12:54,040
So the question remains, how do you do the argmax over y?
所以问题仍然存在，你怎么做argmax超过y？

272
00:12:54,040 --> 00:12:59,560
How do you find your French sentence y that maximizes this probability?
你如何找到最大化这个概率的法语句子？

273
00:12:59,560 --> 00:13:03,340
So one kind of brute force solution is you could say,
所以你可以说，有一种蛮力解决方案，

274
00:13:03,340 --> 00:13:05,830
"'let's enumerate every possible y."
“'让我们列举一切可能的y。”

275
00:13:05,830 --> 00:13:08,305
That's kind of every possible sequence of French words,
这就是法语单词的每一个可能序列，

276
00:13:08,305 --> 00:13:10,270
maybe up to some length, and, uh,
可能达到一定的长度，呃，

277
00:13:10,270 --> 00:13:12,460
we'll calculate this probability for all of them,
我们将计算所有这些概率，

278
00:13:12,460 --> 00:13:14,980
and it should be pretty clear that that is just a no go.
而且应该很清楚，这只是不行。

279
00:13:14,980 --> 00:13:16,405
That's way too expensive,
那太贵了，

280
00:13:16,405 --> 00:13:19,525
and we're not going to be able to get anywhere with that.
而且我们无法在任何地方找到它。

281
00:13:19,525 --> 00:13:22,900
So the answer for how you actually do this in practice is,
那么你在实践中如何实际做到这一点的答案是，

282
00:13:22,900 --> 00:13:25,690
you are going to use some kind of heuristic search algorithm,
你将使用某种启发式搜索算法，

283
00:13:25,690 --> 00:13:28,150
to search for the best translation, y.
搜索最佳翻译，y。

284
00:13:28,150 --> 00:13:33,115
Uh, but along the way, you're going to discard hypotheses that are too low probability.
呃，但在此过程中，你要放弃概率太低的假设。

285
00:13:33,115 --> 00:13:34,555
So you're gonna search,
所以你要搜索，

286
00:13:34,555 --> 00:13:35,785
you're going to discard,
你要丢弃，

287
00:13:35,785 --> 00:13:37,990
and prune the trees as you go to make sure that you're not
并且在你去的时候修剪树木，以确保你不是

288
00:13:37,990 --> 00:13:41,540
keeping too many hypotheses, uh, on each step.
保持太多假设，呃，每一步。

289
00:13:42,090 --> 00:13:47,695
So this process of finding your best sequence is also called decoding.
因此，找到最佳序列的过程也称为解码。

290
00:13:47,695 --> 00:13:50,980
So here is an overview of how that works for SMT.
因此，这里概述了SMT的工作原理。

291
00:13:50,980 --> 00:13:55,750
This an example where you have this German sentence that translates to,
这个例子中你有这个德语句子翻译成，

292
00:13:55,750 --> 00:13:57,565
'He does not go home',
'他不回家'，

293
00:13:57,565 --> 00:14:02,095
and you can see that there is some kind of phrase-to-phrase alignment here.
你可以看到这里有一些短语到短语的对齐方式。

294
00:14:02,095 --> 00:14:06,550
So, uh, an overview of how this decoding would work in SMT,
所以，呃，概述了这种解码在SMT中的作用，

295
00:14:06,550 --> 00:14:10,270
is that you kind of consider lots of different hypotheses,
是你有点考虑很多不同的假设，

296
00:14:10,270 --> 00:14:13,795
for how you might translate these individual words, uh,
你怎么翻译这些单词，呃，

297
00:14:13,795 --> 00:14:18,430
and then you build it up to consider how you might translate,
然后你构建它来考虑你可以如何翻译，

298
00:14:18,430 --> 00:14:21,235
uh, individual phrases, and the phrases get bigger.
呃，个别短语和短语变得更大。

299
00:14:21,235 --> 00:14:23,740
So for example, you can see that on the top right,
例如，您可以在右上方看到

300
00:14:23,740 --> 00:14:25,855
if it's not too small, you can see that the, uh,
如果它不是太小，你可以看到，呃，

301
00:14:25,855 --> 00:14:27,415
the German word for house, uh,
德语单词for house，呃，

302
00:14:27,415 --> 00:14:29,365
could be translated into the English word,
可以翻译成英文单词，

303
00:14:29,365 --> 00:14:32,110
'house' or 'home', or 'chamber', and so on.
'house'或'home'，或'chamber'，等等。

304
00:14:32,110 --> 00:14:34,690
Uh, so we consider all of these different hypotheses,
呃，所以我们考虑所有这些不同的假设，

305
00:14:34,690 --> 00:14:37,210
and look into how we might put those together to
并研究我们如何把它们放在一起

306
00:14:37,210 --> 00:14:40,390
translate phrases but you don't keep all of them all the time.
翻译短语，但你不会一直保留所有这些短语。

307
00:14:40,390 --> 00:14:43,180
You get rid of the ones that are too low probability.
你摆脱了概率太低的那些。

308
00:14:43,180 --> 00:14:46,225
So this can also be depicted as a kind of a tree,
所以这也可以描述为一种树，

309
00:14:46,225 --> 00:14:49,000
where you are exploring different options.
你在哪里探索不同的选择。

310
00:14:49,000 --> 00:14:51,265
You are searching through the space of options,
您正在搜索选项空间，

311
00:14:51,265 --> 00:14:53,125
but then you prune the tree as you go.
但随后你去修剪树。

312
00:14:53,125 --> 00:14:54,790
So I know this is a very,
所以我知道这是一个非常的，

313
00:14:54,790 --> 00:14:56,005
very high level, uh,
非常高的，呃，

314
00:14:56,005 --> 00:14:57,580
description of how decoding might walk.
解码如何走的描述。

315
00:14:57,580 --> 00:14:59,080
And in fact, later in this lecture,
事实上，在本讲座的后期，

316
00:14:59,080 --> 00:15:02,150
you're going to see a detailed explanation
你会看到一个详细的解释

317
00:15:02,150 --> 00:15:05,970
of how this kind of decoding works for neural machine translation.
这种解码如何用于神经机器翻译。

318
00:15:07,320 --> 00:15:10,300
Okay. So what's our, um,
好的。那么我们是什么，嗯，

319
00:15:10,300 --> 00:15:12,460
overview of statistical machine translation,
统计机器翻译概述，

320
00:15:12,460 --> 00:15:14,080
uh, was it effective?
呃，它有效吗？

321
00:15:14,080 --> 00:15:17,080
Uh, so SMT was a huge research field,
呃，所以SMT是一个巨大的研究领域，

322
00:15:17,080 --> 00:15:20,425
uh, from the 1990s to about, maybe, uh, 2013.
呃，从20世纪90年代到约，也许，呃，2013年。

323
00:15:20,425 --> 00:15:23,800
And the best systems during this time were extremely complex.
在此期间，最好的系统非常复杂。

324
00:15:23,800 --> 00:15:26,725
They were extremely sophisticated and impressive systems and, uh,
他们是非常复杂和令人印象深刻的系统，呃，

325
00:15:26,725 --> 00:15:30,130
SMT made the best machine translation systems in the world.
SMT成为世界上最好的机器翻译系统。

326
00:15:30,130 --> 00:15:31,840
But they were very complex.
但它们非常复杂。

327
00:15:31,840 --> 00:15:32,980
So for example, you know,
例如，你知道，

328
00:15:32,980 --> 00:15:36,610
there were hundreds of important details that we haven't mentioned here at all.
我们在这里根本没有提到数百个重要细节。

329
00:15:36,610 --> 00:15:38,740
There were many, many techniques to make it, uh,
有很多很多技术可以做到，呃，

330
00:15:38,740 --> 00:15:40,435
more complex and more,
更复杂，更多，

331
00:15:40,435 --> 00:15:43,315
um, sophisticated than what I've described today.
嗯，比我今天描述的要复杂。

332
00:15:43,315 --> 00:15:48,565
In particular, the systems had to have many separately designed, uh, sub-components.
特别是，系统必须具有许多单独设计的子组件。

333
00:15:48,565 --> 00:15:50,035
So we already saw how you, uh,
所以我们已经看到了你，呃，

334
00:15:50,035 --> 00:15:53,155
break down the translation model into two separate parts.
将翻译模型分解为两个独立的部分。

335
00:15:53,155 --> 00:15:55,645
Uh, but there was, you know, many more sub-components than that,
呃，但是，你知道，还有更多的子组件，

336
00:15:55,645 --> 00:15:57,970
and often they had to be learned separately.
通常他们必须分开学习。

337
00:15:57,970 --> 00:16:01,795
This meant the engineers had to do a lot of feature engineering.
这意味着工程师必须进行大量的功能工程。

338
00:16:01,795 --> 00:16:03,730
Uh, you have to design features to capture
呃，你必须设计要捕捉的功能

339
00:16:03,730 --> 00:16:07,225
the particular language phenomena that you were interested in.
您感兴趣的特定语言现象。

340
00:16:07,225 --> 00:16:10,330
So this meant that they had to require a lot of
所以这意味着他们不得不需要很多

341
00:16:10,330 --> 00:16:12,865
compiling and maintaining of extra resources,
编制和维护额外资源，

342
00:16:12,865 --> 00:16:14,455
and in fact, you had to have, uh,
事实上，你必须拥有，呃，

343
00:16:14,455 --> 00:16:16,495
different resources for different languages.
不同语言的不同资源。

344
00:16:16,495 --> 00:16:19,900
So the work kind of multiplied the more languages you had.
所以工作种类繁多，你拥有的语言越多。

345
00:16:19,900 --> 00:16:22,120
An example of this, is you had to have,
这方面的一个例子是，你必须拥有，

346
00:16:22,120 --> 00:16:23,920
uh, tables of equivalent phrases.
呃，等同短语的表格。

347
00:16:23,920 --> 00:16:27,175
So for example if you're doing French and English translation, then, uh,
所以，例如，如果你正在做法语和英语翻译，那么，呃，

348
00:16:27,175 --> 00:16:30,055
they would be collecting these phrases of, uh,
他们会收集这些短语，呃，

349
00:16:30,055 --> 00:16:32,590
sorry these tables of phrases that they considered similar,
对不起这些他们认为相似的短语表，

350
00:16:32,590 --> 00:16:33,925
and those were learned from the data.
这些都是从数据中学到的。

351
00:16:33,925 --> 00:16:37,360
But this was a lot of information that had to be stored and maintained.
但这是必须存储和维护的大量信息。

352
00:16:37,360 --> 00:16:40,930
So overall, this was just a lot of human effort to maintain.
总的来说，这只是人类维护的一大部分。

353
00:16:40,930 --> 00:16:42,550
Uh, and again, yes,
呃，又一次，是的，

354
00:16:42,550 --> 00:16:45,130
you had to put more human effort in if you wanted to
如果你愿意的话，你必须付出更多的努力

355
00:16:45,130 --> 00:16:48,490
learn an SMT system for a new language pair.
学习SMT系统以获得新的语言对。

356
00:16:48,490 --> 00:16:51,550
Okay, are there any questions here about, uh, SMT?
好的，这里有什么问题，呃，SMT？

357
00:16:51,550 --> 00:16:59,290
[NOISE] Okay.
[NOISE]好的。

358
00:16:59,290 --> 00:17:01,480
Uh, so moving on, that's SMT.
呃，继续前进，这就是SMT。

359
00:17:01,480 --> 00:17:02,815
[NOISE] Now, we're gonna move on to,
[NOISE]现在，我们要继续前进，

360
00:17:02,815 --> 00:17:04,660
uh, section two of this lecture.
呃，这个讲座的第二部分。

361
00:17:04,660 --> 00:17:10,030
So I want to take you back to the year 2014,
所以我想带你回到2014年，

362
00:17:10,030 --> 00:17:12,580
for a dramatic re-enactment of what happened in
对于发生的事情进行了戏剧性的重演

363
00:17:12,580 --> 00:17:15,415
the world of machine translation research.
机器翻译研究的世界。

364
00:17:15,415 --> 00:17:17,980
So in 2014, something very dramatic happened and
所以在2014年，发生了非常戏剧性的事情

365
00:17:17,980 --> 00:17:20,800
that thing that happened is called neural machine translation,
发生的那件事叫做神经机器翻译，

366
00:17:20,800 --> 00:17:22,810
and [LAUGHTER] I think it looks a little
和[笑声]我觉得它看起来有点儿

367
00:17:22,810 --> 00:17:25,855
bit like this [NOISE] if I'm not being too dramatic.
如果我不太戏剧化，就像这样[NOISE]

368
00:17:25,855 --> 00:17:28,825
So what is neural machine translation?
那么什么是神经机器翻译？

369
00:17:28,825 --> 00:17:31,450
The idea is that NMT is a way to do
这个想法是NMT是一种方法

370
00:17:31,450 --> 00:17:35,260
machine translation but using just a single neural network.
机器翻译，但只使用一个神经网络。

371
00:17:35,260 --> 00:17:38,115
[NOISE] And the neural network architecture that they use
[NOISE]和他们使用的神经网络架构

372
00:17:38,115 --> 00:17:41,220
is called Sequence-to-Sequence or sometimes just called seq2seq,
称为序​​列到序列或有时称为seq2seq，

373
00:17:41,220 --> 00:17:44,210
uh, and involves two RNNs.
呃，涉及两个RNN。

374
00:17:44,210 --> 00:17:46,180
So, uh, it's called sequence-to-sequence,
所以，呃，它被称为序列到序列，

375
00:17:46,180 --> 00:17:48,385
because you're mapping one sequence to the other.
因为你将一个序列映射到另一个序列。

376
00:17:48,385 --> 00:17:52,000
The source sentence [NOISE] to the target sentence and you need two RNNs,
目标句子的源句[NOISE]，你需要两个RNN，

377
00:17:52,000 --> 00:17:54,685
basically to handle those two different sentences.
基本上处理那两个不同的句子。

378
00:17:54,685 --> 00:17:59,110
All right, lets look at the diagram to see what sequences-to-sequence is in detail.
好吧，让我们看一下图表，看看详细的序列到序列。

379
00:17:59,110 --> 00:18:01,690
So we start off with our source sentence,
所以我们从源语句开始，

380
00:18:01,690 --> 00:18:03,400
and we're gonna use our example from before
我们将使用之前的例子

381
00:18:03,400 --> 00:18:07,300
il a m'entarte, which means, he hit me with a pie.
il a m'entarte，这意味着，他用馅饼打我。

382
00:18:07,300 --> 00:18:10,810
So we, uh, feed this into our encoder RNN,
所以我们，呃，把它喂给我们的编码器RNN，

383
00:18:10,810 --> 00:18:12,835
and this is as you've seen before,
这就像你以前见过的那样，

384
00:18:12,835 --> 00:18:15,700
I've drawn a uni-directional RNN,
我画了一个单向的RNN，

385
00:18:15,700 --> 00:18:17,275
but this could be bi-directional.
但这可能是双向的。

386
00:18:17,275 --> 00:18:19,045
It also could be multi-layer.
它也可以是多层的。

387
00:18:19,045 --> 00:18:22,675
It could be vanilla, or it could be LSTM, and so on.
它可能是香草，也可能是LSTM，依此类推。

388
00:18:22,675 --> 00:18:25,570
Uh, another thing to note is that [NOISE] we are
呃，另外需要注意的是我们是[NOISE]

389
00:18:25,570 --> 00:18:28,240
passing word embeddings into this encoder RNN,
将字嵌入传递给此编码器RNN，

390
00:18:28,240 --> 00:18:31,000
but I'm just not explicitly depicting that step.
但我只是没有明确地描述那一步。

391
00:18:31,000 --> 00:18:33,280
[NOISE] Okay.
[NOISE]好的。

392
00:18:33,280 --> 00:18:36,430
So the idea of the encoder RNN is that it's going to
所以编码器RNN的想法就是这样

393
00:18:36,430 --> 00:18:39,820
produce some kind of encoding of this source sentence.
产生这种源句的某种编码。

394
00:18:39,820 --> 00:18:43,690
So for now, let's assume that the encoding of the source sentence is going to be,
那么现在，让我们假设源句的编码将是，

395
00:18:43,690 --> 00:18:47,305
uh, the final hidden state of this encoder RNN.
呃，这个编码器RNN的最终隐藏状态。

396
00:18:47,305 --> 00:18:51,700
So what happens next is we pass this encoding of the source sentence.
那么接下来发生的是我们传递源句的这种编码。

397
00:18:51,700 --> 00:18:54,055
We pass it over to the decoder RNN,
我们把它传递给解码器RNN，

398
00:18:54,055 --> 00:18:56,635
which is going to translate into English.
这将翻译成英文。

399
00:18:56,635 --> 00:18:59,635
So the decoder RNN is a language model.
因此解码器RNN是语言模型。

400
00:18:59,635 --> 00:19:01,795
In particular, it's a conditional language model,
特别是，它是一种条件语言模型，

401
00:19:01,795 --> 00:19:03,340
like we talked about last time.
就像我们上次谈到的那样。

402
00:19:03,340 --> 00:19:06,430
So it's conditional because it's going to produce the target sentence,
所以它是有条件的，因为它会产生目标句子，

403
00:19:06,430 --> 00:19:08,425
but conditioned on this encoding,
但是以这种编码为条件，

404
00:19:08,425 --> 00:19:12,445
and the encoding is that vector that has the orange box around it.
编码是那个周围有橙色框的矢量。

405
00:19:12,445 --> 00:19:15,805
So how does this work? Uh, we start off by feeding, uh,
那么这是如何工作的呢？呃，我们从喂食开始，呃，

406
00:19:15,805 --> 00:19:20,500
the start token into the decoder, and then, uh,
启动令牌进入解码器，然后，呃，

407
00:19:20,500 --> 00:19:22,765
we can get the first state of the decoder,
我们可以得到解码器的第一个状态，

408
00:19:22,765 --> 00:19:24,025
because we're using, uh,
因为我们正在使用，呃，

409
00:19:24,025 --> 00:19:28,745
the encoding of the source sentence as the initial hidden state for the decoder.
源句的编码作为解码器的初始隐藏状态。

410
00:19:28,745 --> 00:19:31,800
So then we get our first output from the decoder,
那么我们从解码器得到第一个输出，

411
00:19:31,800 --> 00:19:34,725
which is a probability distribution of what word might come next,
这是接下来可能出现的单词的概率分布，

412
00:19:34,725 --> 00:19:37,140
and let's suppose that we take the argmax over that,
让我们假设我们把argmax放在那个，

413
00:19:37,140 --> 00:19:39,420
and then that gets us the word, uh, he.
然后那就让我们知道了，呃，他。

414
00:19:39,420 --> 00:19:41,070
Which is in this case is correct,
在这种情况下是正确的，

415
00:19:41,070 --> 00:19:43,355
because that's probably the word you should start with.
因为这可能就是你应该开始的那个词。

416
00:19:43,355 --> 00:19:45,280
Okay, so then we just take the word,
好的，那么我们接受这个词，

417
00:19:45,280 --> 00:19:48,730
he and then we feed it back into the decoder on the next step,
他然后我们在下一步将它反馈到解码器中，

418
00:19:48,730 --> 00:19:50,740
and then we do the same thing again.
然后我们再做同样的事情。

419
00:19:50,740 --> 00:19:53,965
We take argmax and we get a new word and we get he hit.
我们采取argmax，我们得到一个新词，我们得到他的命中。

420
00:19:53,965 --> 00:19:58,750
So the idea here is you can co- uh, continue doing this operation and in that way,
所以这里的想法是你可以继续这样做，并以这种方式，

421
00:19:58,750 --> 00:20:00,220
you're going to generate, uh,
你要生成，呃，

422
00:20:00,220 --> 00:20:02,335
your target sentence, uh,
你的目标句子，呃，

423
00:20:02,335 --> 00:20:05,215
which will be something like he hit me with a pie,
这就像他用馅饼打我一样，

424
00:20:05,215 --> 00:20:09,620
and you stop once your decoder produces the end token.
并且一旦解码器产生结束令牌就停止。

425
00:20:09,780 --> 00:20:12,715
So an important thing to note here,
所以在这里要注意一件重要的事情，

426
00:20:12,715 --> 00:20:16,510
is that this picture is showing you what happens at test time.
是这张照片告诉你在测试时发生了什么。

427
00:20:16,510 --> 00:20:18,610
This shows you how to generate text.
这将向您展示如何生成文本。

428
00:20:18,610 --> 00:20:20,365
Uh, this isn't what happens during training.
呃，这不是训练期间发生的事情。

429
00:20:20,365 --> 00:20:21,955
I'll show you what happens [NOISE] during training later.
我会告诉你在训练过程中会发生什么[NOISE]。

430
00:20:21,955 --> 00:20:23,635
Uh, but this thing with the,
呃，但这件事，

431
00:20:23,635 --> 00:20:26,005
the pink dotted arrows where you feed the word back in.
粉红色虚线箭头，你把这个词喂回来。

432
00:20:26,005 --> 00:20:29,275
This is what you do to generate text at test time.
这是您在测试时生成文本的方法。

433
00:20:29,275 --> 00:20:35,365
Any questions on this? Uh, oh,
对此有任何疑问？呃，哦，

434
00:20:35,365 --> 00:20:39,940
another thing I should note is that you need two separate sets of word embeddings, right?
我应该注意的另一件事是你需要两套独立的字嵌入，对吧？

435
00:20:39,940 --> 00:20:43,360
You need word embeddings for French words, and you need English word embeddings,
你需要为法语单词嵌入单词，你需要英文单词嵌入，

436
00:20:43,360 --> 00:20:44,680
so that's kind of two separate sets,
所以这是两个独立的集合，

437
00:20:44,680 --> 00:20:48,925
two separate vocabularies. Um, yeah.
两个单独的词汇表。嗯，是的

438
00:20:48,925 --> 00:20:51,790
Okay. So as a side note, uh,
好的。所以作为旁注，呃，

439
00:20:51,790 --> 00:20:54,595
this architecture called sequence-to-sequence is actually pretty versatile.
这种称为序列到序列的架构实际上非常通用。

440
00:20:54,595 --> 00:20:56,800
It's not just a machine translation architecture.
它不仅仅是一个机器翻译架构。

441
00:20:56,800 --> 00:20:59,410
Uh, you can, uh, uh,
呃，你可以，呃，呃，

442
00:20:59,410 --> 00:21:03,535
phrase quite a few NLP tasks as sequence-to-sequence tasks.
将相当多的NLP任务称为序列到序列任务。

443
00:21:03,535 --> 00:21:05,590
Uh, so for example a summarization is
呃，举个例子就是总结

444
00:21:05,590 --> 00:21:10,120
a sequence-to-sequence task because in goes your long text and out comes your short text.
一个序列到序列的任务，因为在你的长文本和你的短文本。

445
00:21:10,120 --> 00:21:12,640
Uh, dialogue can [NOISE] be seq2seq because in
呃，对话可以[NOISE]是seq2seq，因为在

446
00:21:12,640 --> 00:21:15,730
goes the previous utterance and out comes your next utterance, uh,
是先前的话语，然后是你的下一个话语，呃，

447
00:21:15,730 --> 00:21:19,345
parsing can even be thought of as a sequence-to-sequence task,
解析甚至可以被认为是一个序列到序列的任务，

448
00:21:19,345 --> 00:21:21,910
because you could say in goes the input text and
因为你可以说输入文本和

449
00:21:21,910 --> 00:21:24,730
the output parse is going to be expressed as a sequence.
输出解析将表示为序列。

450
00:21:24,730 --> 00:21:28,030
This might not be the best way to do parsing but it is a way you can try.
这可能不是解析的最佳方式，但这是一种可以尝试的方法。

451
00:21:28,030 --> 00:21:31,480
Lastly, you could even do something like code generation.
最后，您甚至可以执行代码生成等操作。

452
00:21:31,480 --> 00:21:34,150
So suppose you want to build a system that takes some kind of,
所以假设你想构建一个需要某种系统的系统，

453
00:21:34,150 --> 00:21:35,440
uh, natural language input,
呃，自然语言输入，

454
00:21:35,440 --> 00:21:39,520
such as sum up the numbers from 1-10 and then it outputs,
比如将1-10中的数字相加然后输出，

455
00:21:39,520 --> 00:21:41,665
let's say some Python code that says,
让我们说一些Python代码说，

456
00:21:41,665 --> 00:21:44,920
sum open brackets range 10 or something like that.
和开放括号范围10或类似的东西。

457
00:21:44,920 --> 00:21:47,140
Uh, so if you wanted to train,
呃，如果你想训练，

458
00:21:47,140 --> 00:21:48,880
um, an assistant to do this.
嗯，这是一个助手。

459
00:21:48,880 --> 00:21:51,655
You could in a way view that as a translation task,
您可以在某种程度上将其视为翻译任务，

460
00:21:51,655 --> 00:21:53,740
where you're translating from English to Python.
你在哪里从英语翻译成Python。

461
00:21:53,740 --> 00:21:55,960
It's a pretty challenging translation task.
这是一项非常具有挑战性的翻译任务。

462
00:21:55,960 --> 00:21:58,510
It probably requires a lot more logic than just uh, you know,
它可能需要更多的逻辑，而不仅仅是呃，你知道，

463
00:21:58,510 --> 00:22:01,390
French to English [NOISE] but you can try and people have tried.
法语到英语[NOISE]，但你可以尝试，人们已经尝试过。

464
00:22:01,390 --> 00:22:05,930
There are research papers where people have used seq2seq to do this kind of task.
有研究论文，人们使用seq2seq来完成这类任务。

465
00:22:09,510 --> 00:22:12,370
Okay. So to recap, uh,
好的。所以回顾一下，呃，

466
00:22:12,370 --> 00:22:14,950
seq2seq is an example of a conditional language model.
seq2seq是条件语言模型的示例。

467
00:22:14,950 --> 00:22:17,695
Uh, it's a language model because the decoder
呃，这是一种语言模型，因为解码器

468
00:22:17,695 --> 00:22:20,905
is a language model that's predicting the next target words.
是一种预测下一个目标词的语言模型。

469
00:22:20,905 --> 00:22:24,280
But it's a conditional language model because it's also conditioning on
但它是一种条件语言模型，因为它也适应

470
00:22:24,280 --> 00:22:29,390
your source sentence which is represented by the encoding of the source sentence.
你的源句，由源句的编码表示。

471
00:22:30,150 --> 00:22:32,305
So you could look,
所以你可以看，

472
00:22:32,305 --> 00:22:33,700
you could view it like this.
你可以这样看。

473
00:22:33,700 --> 00:22:36,700
NMT is directly calculating the probability
NMT直接计算概率

474
00:22:36,700 --> 00:22:39,835
of the target sentence y given the source sentence x.
给定源句x的目标句子y。

475
00:22:39,835 --> 00:22:42,445
So if you look at this, you see that this is just, uh,
所以如果你看看这个，你会发现这只是，呃，

476
00:22:42,445 --> 00:22:45,085
breaking down the probability of the sequence y,
打破序列y的概率，

477
00:22:45,085 --> 00:22:46,450
which we suppose is of length, uh,
我们认为是长度的，呃，

478
00:22:46,450 --> 00:22:50,110
capital T. You can break it down into the being the probability of
资本T.你可以把它分解为概率

479
00:22:50,110 --> 00:22:54,280
the first word of y given x and then the probability of the second word of y given,
y给出x的第一个单词，然后给出y的第二个单词的概率，

480
00:22:54,280 --> 00:22:56,920
uh, the words that came before, and x, and so on.
呃，之前的话，和x，等等。

481
00:22:56,920 --> 00:23:00,370
So in fact, you can see that each of the terms in this product on the right,
所以实际上，你可以看到右边的这个产品中的每个术语，

482
00:23:00,370 --> 00:23:02,710
those are probabilities of the next target word
那些是下一个目标词的概率

483
00:23:02,710 --> 00:23:05,365
given all the ones so far, and also the source sentence,
考虑到目前为止所有的，以及源句，

484
00:23:05,365 --> 00:23:09,670
and that's exactly the conditional probability that your language model produces.
这正是语言模型产生的条件概率。

485
00:23:09,670 --> 00:23:14,020
So the reason I'm highlighting this is because if you remember in SMT, uh,
所以我突出这个的原因是因为如果你记得SMT，呃，

486
00:23:14,020 --> 00:23:18,190
we didn't directly learn the translation model p of y given x,
我们没有直接学习给定x的y的翻译模型p，

487
00:23:18,190 --> 00:23:19,765
we broke it down into,
我们把它分解成了，

488
00:23:19,765 --> 00:23:22,210
uh, uh, smaller components.
呃，呃，较小的组件。

489
00:23:22,210 --> 00:23:23,995
Whereas here in NMT,
而在NMT，

490
00:23:23,995 --> 00:23:26,440
we are directly learning this model.
我们正在直接学习这个模型。

491
00:23:26,440 --> 00:23:29,335
And this is in some ways an advantage because it's simpler to do.
这在某些方面是一个优势因为它更简单。

492
00:23:29,335 --> 00:23:32,545
You don't have to learn all of these different systems and optimize them separately.
您无需学习所有这些不同的系统并单独进行优化。

493
00:23:32,545 --> 00:23:35,720
It's, uh, kind of, simpler and easier.
它，呃，更简单，更容易。

494
00:23:36,660 --> 00:23:38,710
So, uh, this is,
所以，呃，这是，

495
00:23:38,710 --> 00:23:40,150
this is the model that we're learning.
这是我们正在学习的模型。

496
00:23:40,150 --> 00:23:43,165
Uh, the question is, how do we train this NMT system?
呃，问题是，我们如何训练这个NMT系统？

497
00:23:43,165 --> 00:23:46,195
So hopefully, you should already have a good idea of how this would work,
所以希望你应该已经知道这将如何工作，

498
00:23:46,195 --> 00:23:49,075
given that we've already seen how you would train a language model.
鉴于我们已经看到你将如何培养语言模型。

499
00:23:49,075 --> 00:23:50,830
But here are the details just in case.
但这里有详细信息以防万一。

500
00:23:50,830 --> 00:23:53,080
So you get your big, uh, parallel corpus.
所以你得到了你的大而平行的语料库。

501
00:23:53,080 --> 00:23:55,075
Uh, and then, uh,
呃，然后，呃，

502
00:23:55,075 --> 00:23:58,930
let's say you have your sentence pair from your parallel corpus.
假设您从平行语料库中得到了句子对。

503
00:23:58,930 --> 00:24:01,915
Uh, so this is what happens during training.
呃，这就是训练期间发生的事情。

504
00:24:01,915 --> 00:24:05,005
Uh, you feed your source sentence into the encoder RNN, uh,
呃，你把你的源句写入编码器RNN，呃，

505
00:24:05,005 --> 00:24:09,475
and then you feed your target sentence into the decoder RNN,
然后你将你的目标句子送入解码器RNN，

506
00:24:09,475 --> 00:24:10,900
and you're going to pass over that
你会过去的

507
00:24:10,900 --> 00:24:14,005
final hidden state to be the initial hidden state of the decoder.
最终隐藏状态是解码器的初始隐藏状态。

508
00:24:14,005 --> 00:24:18,445
And then, uh, for every step of the decoder RNN,
然后，呃，对于解码器RNN的每一步，

509
00:24:18,445 --> 00:24:20,065
you're going to produce the, uh,
你要生产，呃，

510
00:24:20,065 --> 00:24:22,060
probability distribution of what comes next,
接下来的概率分布，

511
00:24:22,060 --> 00:24:23,905
which is the, the y hats.
是帽子，是帽子。

512
00:24:23,905 --> 00:24:25,375
And then from those,
然后从那些，

513
00:24:25,375 --> 00:24:26,875
you can compute your loss.
你可以计算你的损失。

514
00:24:26,875 --> 00:24:29,410
And the loss is just the same as we saw for,
损失与我们看到的一样，

515
00:24:29,410 --> 00:24:31,165
u h, unconditional language models.
呃，无条件的语言模型。

516
00:24:31,165 --> 00:24:33,580
It's, uh, the cross entropy or you could also
这是，呃，交叉熵，或者你也可以

517
00:24:33,580 --> 00:24:37,330
say negative log-likelihood of the true next word.
说真实的下一个单词的负对数似然。

518
00:24:37,330 --> 00:24:39,730
So for example, on those selected ones, uh,
那么，例如，那些选定的，呃，

519
00:24:39,730 --> 00:24:44,335
the loss is the negative log probability of the correct next word.
损失是正确的下一个单词的负对数概率。

520
00:24:44,335 --> 00:24:47,680
And then as before, we're going to average all of
然后和以前一样，我们将平均所有

521
00:24:47,680 --> 00:24:50,995
these losses to get the total loss for the example.
这些损失可以得到总损失的例子。

522
00:24:50,995 --> 00:24:55,440
Uh, so a thing you might notice people saying in,
呃，你可能会注意到有人在说，

523
00:24:55,440 --> 00:24:58,770
for example, research papers is this phrase end-to-end.
例如，研究论文是端到端的这个短语。

524
00:24:58,770 --> 00:25:02,415
And this is an example of learning a system end-to-end.
这是学习端到端系统的一个例子。

525
00:25:02,415 --> 00:25:07,275
And what we mean by this is that the backpropagation is happening end-to-end, one end is,
我们的意思是反向传播是端到端发生的，一端是，

526
00:25:07,275 --> 00:25:09,000
is losses, the loss functions,
是损失，损失功能，

527
00:25:09,000 --> 00:25:11,160
and the other end I guess is kind of like the,
而另一端我觉得有点像，

528
00:25:11,160 --> 00:25:12,930
the beginning of the encoder RNN.
编码器RNN的开头。

529
00:25:12,930 --> 00:25:14,310
The point is that you, uh,
关键是你，呃，

530
00:25:14,310 --> 00:25:18,045
backpropagation, uh, flows throughout the entire system,
反向传播，呃，流过整个系统，

531
00:25:18,045 --> 00:25:23,280
and you learn the entire system with respect to this single, uh, loss. Yep?
并且你学习了整个系统，关于这个单一的，呃，损失。是的？

532
00:25:23,280 --> 00:25:24,030
[inaudible]
[听不见]

533
00:25:24,030 --> 00:25:36,280
The question is,
问题是，

534
00:25:36,280 --> 00:25:41,035
if the decoder RNN outputs the end token too early,
如果解码器RNN过早输出结束令牌，

535
00:25:41,035 --> 00:25:43,840
then how can you measure the loss on,
那你怎么测量损失，

536
00:25:43,840 --> 00:25:45,700
uh, the words that came after that?
呃，之后的话呢？

537
00:25:45,700 --> 00:25:48,850
So this is the difference between training time and test time,
所以这是培训时间和测试时间之间的差异，

538
00:25:48,850 --> 00:25:50,020
which is pretty confusing.
这很令人困惑。

539
00:25:50,020 --> 00:25:51,880
So, uh, during training,
那么，呃，在训练中，

540
00:25:51,880 --> 00:25:55,990
we have this picture where you feed the token back in.
我们有这张图片，您可以将令牌送回来。

541
00:25:55,990 --> 00:25:57,160
So in this scenario,
所以在这种情况下，

542
00:25:57,160 --> 00:25:58,195
once you produce end,
一旦你产生了结束，

543
00:25:58,195 --> 00:26:02,080
then you have to stop because you can't feed end in as the initial next step.
然后你必须停止，因为你不能作为最初的下一步结束。

544
00:26:02,080 --> 00:26:07,060
But in training, you don't feed the thing that you produced into the next step.
但是在训练中，你不会将你制作的东西喂给下一步。

545
00:26:07,060 --> 00:26:11,080
During training, you feed the target sentence from the corpus.
在训练期间，您从语料库中提取目标句子。

546
00:26:11,080 --> 00:26:14,470
So like the gold target sentence into the model.
所以就像进入模型的黄金目标句子一样。

547
00:26:14,470 --> 00:26:16,750
So no matter what the, uh,
无论如何，呃，

548
00:26:16,750 --> 00:26:19,225
the decoder predicts on a step,
解码器预测一步，

549
00:26:19,225 --> 00:26:23,780
you kind of, you don't use that for anything other than computing loss.
你有点，除了计算损失之外，你不要用它。

550
00:26:24,150 --> 00:26:26,995
Any other questions? Yeah.
还有其他问题吗？是啊。

551
00:26:26,995 --> 00:26:31,405
Is there a reason why you would, uh, backpropagation end-to-end instead of maybe
你有没有理由，呃，反向传播端到端而不是可能

552
00:26:31,405 --> 00:26:37,720
training an encoder like [inaudible] model and then [inaudible] together?
像[听不清]模型一样训练编码器，然后一起[听不清]？

553
00:26:37,720 --> 00:26:41,530
The question is, is there a reason why you would want to train end-to-end when,
问题是，你有什么理由想要在端到端训练的时候，

554
00:26:41,530 --> 00:26:45,205
for example, you might want to train the encoder and the decoder separately?
例如，您可能想要单独训练编码器和解码器？

555
00:26:45,205 --> 00:26:48,580
Uh, so I think, uh, people view training end-to-end as favorable
呃，我想，呃，人们认为端到端培训是有利的

556
00:26:48,580 --> 00:26:52,000
because the idea is that you can optimize the system as a whole.
因为这个想法是你可以整体优化系统。

557
00:26:52,000 --> 00:26:54,625
You might think that if you optimize the part separately,
您可能会认为如果您单独优化零件，

558
00:26:54,625 --> 00:26:56,065
then when you put them together,
然后当你把它们放在一起时，

559
00:26:56,065 --> 00:26:58,090
they will not be optimal together necessarily.
它们不一定是最佳的。

560
00:26:58,090 --> 00:27:01,405
So if possible, directly optimizing the thing that you care abou-
所以如果可能的话，直接优化你关心的东西 -

561
00:27:01,405 --> 00:27:05,110
about with respect to all of the parameters is more likely to succeed.
关于所有参数的概率更有可能成功。

562
00:27:05,110 --> 00:27:07,405
However, there is a notion of pre-training.
但是，有一种预训练的概念。

563
00:27:07,405 --> 00:27:09,865
And as you said, maybe you'd want to learn your, um,
正如你所说，也许你想学习你的，嗯，

564
00:27:09,865 --> 00:27:13,570
decoder RNN as a kind of a language model,
解码器RNN作为一种语言模型，

565
00:27:13,570 --> 00:27:15,325
an unconditional language model by itself.
一个无条件的语言模型本身。

566
00:27:15,325 --> 00:27:16,825
And that's something that people do.
这就是人们所做的事情。

567
00:27:16,825 --> 00:27:19,645
You might, uh, learn a very strong language model,
你可能，呃，学习一个非常强大的语言模型，

568
00:27:19,645 --> 00:27:22,870
and then use that to initialize your decoder RNN,
然后用它来初始化你的解码器RNN，

569
00:27:22,870 --> 00:27:24,625
and then fine-tune it on your task.
然后在你的任务上微调它。

570
00:27:24,625 --> 00:27:27,430
That's a, a valid thing you might try to do.
这是一个，你可能会尝试做的有效事情。

571
00:27:27,430 --> 00:27:31,230
Yep.
是的。

572
00:27:31,230 --> 00:27:37,990
Are you always [inaudible]
你总是[音频不清晰]

573
00:27:37,990 --> 00:27:39,760
The question is, is the length of
问题是，是长度

574
00:27:39,760 --> 00:27:42,310
the source sentence and the length of the target sentence fixed?
源句和目标句的长度是固定的吗？

575
00:27:42,310 --> 00:27:44,530
So for example, is the source sentence always length 4?
那么例如，源句总是长度为4？

576
00:27:44,530 --> 00:27:45,745
Uh, no.
呃，不。

577
00:27:45,745 --> 00:27:48,310
That's definitely not true because in your parallel corpus,
这绝对不是真的，因为在你的平行语料库中，

578
00:27:48,310 --> 00:27:50,125
you're going to have sentences of all lengths.
你会得到各种长度的句子。

579
00:27:50,125 --> 00:27:53,725
Uh, so this is more kind of an implementation or a practicality question.
呃，所以这更像是一种实现或实用性问题。

580
00:27:53,725 --> 00:27:56,620
Uh, the idea is that this is what you mathematically want
呃，这个想法是这就是你在数学上想要的

581
00:27:56,620 --> 00:27:59,290
to be computing during training for each example,
在每个例子的训练期间计算，

582
00:27:59,290 --> 00:28:01,045
and you're going to have batches of examples.
而你将会有一批例子。

583
00:28:01,045 --> 00:28:04,900
But the question is, how do you actually implement them in, uh, in practice?
但问题是，你如何在实践中实际实现它们？

584
00:28:04,900 --> 00:28:08,380
So what you usually do just because it's easier to assume that your
所以你通常做的只是因为你更容易假设你的

585
00:28:08,380 --> 00:28:12,535
batch is this kind of even-sized tensor where everything is the same length,
批量是这种均匀尺寸的张量，其中一切都是相同的长度，

586
00:28:12,535 --> 00:28:17,440
is you pad any short sentences up to some predefined maximum length,
你是否填写任何预定义的最大长度的短句，

587
00:28:17,440 --> 00:28:21,220
or maybe the length of the maximum example in your batch, uh,
或者你的批次中最大例子的长度，呃，

588
00:28:21,220 --> 00:28:23,755
and then you make sure that you don't
然后你确保你没有

589
00:28:23,755 --> 00:28:27,420
use any hidden states that came from the padding. Yep.
使用来自填充的任何隐藏状态。是的。

590
00:28:27,420 --> 00:28:37,210
I believe two languages together [inaudible]
我相信两种语言[音频不清晰]

591
00:28:37,210 --> 00:28:38,410
possible to have a system
可能有一个系统

592
00:28:38,410 --> 00:28:48,730
[inaudible] that will be kind of universal with similar languages or something like that?
[音频不清晰]，类似的语言或其类似的东西会是什么样的？

593
00:28:48,730 --> 00:28:51,340
Okay. So the question I think is,
好的。所以我认为的问题是，

594
00:28:51,340 --> 00:28:54,820
uh, it seems like sometimes you wouldn't want to train things end-to-end,
呃，有时候你似乎不想端到端地训练东西，

595
00:28:54,820 --> 00:28:58,120
and there are circumstances in which you might want to train things separately,
在某些情况下你可能想要分开训练，

596
00:28:58,120 --> 00:28:59,575
and you mentioned, for example,
你提到过，例如，

597
00:28:59,575 --> 00:29:01,825
having, uh, different languages mapped to each other.
有，呃，不同的语言相互映射。

598
00:29:01,825 --> 00:29:03,355
So this is a totally valid point,
所以这是一个完全有效的观点，

599
00:29:03,355 --> 00:29:05,170
and in fact, uh, so far we've, kind of,
事实上，呃，到目前为止，我们有点，

600
00:29:05,170 --> 00:29:09,430
assumed that you want to learn language A to language B as a pair, right?
假设你想学习语言A作为一对语言B，对吗？

601
00:29:09,430 --> 00:29:13,075
And that's different to language A to language C or even language B to language A.
这与语言A到语言C甚至语言B到语言A的不同。

602
00:29:13,075 --> 00:29:17,350
And, um, that does mean you have kind of n-squared many systems in the number of,
并且，嗯，这确实意味着你有许多n平方的系统，

603
00:29:17,350 --> 00:29:18,910
uh, languages you're considering.
呃，你正在考虑的语言。

604
00:29:18,910 --> 00:29:20,830
So, yeah, that's actually a valid idea,
所以，是的，这实际上是一个有效的想法，

605
00:29:20,830 --> 00:29:22,540
and this is something that people have researched.
这是人们研究过的东西。

606
00:29:22,540 --> 00:29:24,370
The idea that maybe you could have a, kind of,
也许你可以有一种，一种，

607
00:29:24,370 --> 00:29:26,815
mix and match with your encoders and decoders.
与您的编码器和解码器混合搭配。

608
00:29:26,815 --> 00:29:29,920
And you could try to, uh, train a kind of general purpose,
你可以尝试，呃，培养一种通用目的，

609
00:29:29,920 --> 00:29:34,345
let's say English decoder and then match it up with your different encoders.
让我们说英语解码器，然后将其与您的不同编码器匹配。

610
00:29:34,345 --> 00:29:37,315
Uh, but this is, I think fairly complex to train to,
呃，但是，我认为训练相当复杂，

611
00:29:37,315 --> 00:29:38,965
to make sure that they all work together.
确保他们一起工作。

612
00:29:38,965 --> 00:29:41,290
But that, that is certainly something that people have done.
但那，这肯定是人们所做的事情。

613
00:29:41,290 --> 00:29:43,850
Let me just check on the time.
我来看看时间。

614
00:29:43,890 --> 00:29:47,335
Okay. Let's take one more question. Yep.
好的。我们再问一个问题。是的。

615
00:29:47,335 --> 00:29:53,300
So does the word embedding also come from the same corpus that we are training on?
嵌入这个词也来自我们正在训练的同一语料库吗？

616
00:29:53,340 --> 00:29:56,020
The question is, does the word embedding
问题是，嵌入这个词了吗？

617
00:29:56,020 --> 00:29:57,970
also come from the corpus that you're training on?
也来自你正在训练的语料库？

618
00:29:57,970 --> 00:30:02,109
So I think there's a few options just as we saw with language models; you could download,
所以我认为有一些选项正如我们在语言模型中看到的那样;你可以下载，

619
00:30:02,109 --> 00:30:04,900
uh, pretrained word vectors like Word2Vec or GloVe,
呃，预训练的单词向量，如Word2Vec或GloVe，

620
00:30:04,900 --> 00:30:06,265
and you could use those.
你可以用那些。

621
00:30:06,265 --> 00:30:08,410
And then you can either, kind of, freeze them or you could
然后你可以，或者某种程度，冻结他们或你可以

622
00:30:08,410 --> 00:30:10,825
fine-tune them as part of the end-to-end training,
将它们作为端到端培训的一部分进行微调，

623
00:30:10,825 --> 00:30:13,540
or you could just initialize your word vectors as,
或者您可以将单词向量初始化为，

624
00:30:13,540 --> 00:30:17,140
uh, you know, close to zero random and then learn them from scratch.
呃，你知道，接近零随机，然后从头开始学习它们。

625
00:30:17,140 --> 00:30:19,585
All right. Okay, moving on.
行。好的，继续前进。

626
00:30:19,585 --> 00:30:24,670
Uh, so now we understand how you would train a neural machine translation system.
呃，现在我们了解你将如何训练神经机器翻译系统。

627
00:30:24,670 --> 00:30:26,800
And we talked briefly about how you might,
我们简要谈到了你的意愿，

628
00:30:26,800 --> 00:30:28,795
uh, do decoding or generation.
呃，做解码或生成。

629
00:30:28,795 --> 00:30:32,020
So what I showed you before is something called, uh, greedy decoding,
所以我之前向你展示的东西叫做呃贪婪的解码，

630
00:30:32,020 --> 00:30:33,925
which is this idea that on each step,
这个想法就是每一步，

631
00:30:33,925 --> 00:30:35,110
you just choose the argmax,
你只需选择argmax，

632
00:30:35,110 --> 00:30:36,415
the top one best word,
最好的一个词，

633
00:30:36,415 --> 00:30:38,755
and then you feed that in on the next step.
然后在下一步中将其输入。

634
00:30:38,755 --> 00:30:42,400
So this is called greedy decoding because you're just taking the best, uh,
所以这被称为贪婪解码，因为你只是采取最好的，呃，

635
00:30:42,400 --> 00:30:44,815
the best option that you can see right now,
你现在能看到的最佳选择，

636
00:30:44,815 --> 00:30:47,200
and then you really don't have a way to go back.
然后你真的没有办法回去。

637
00:30:47,200 --> 00:30:52,330
So can anyone see a problem with this method? Maybe I've kind of given it away but, uh, yeah.
那么有人能看到这个方法的问题吗？也许我有点放弃了，但是，呃，是的。

638
00:30:52,330 --> 00:30:59,665
[inaudible].
[听不见的。

639
00:30:59,665 --> 00:31:01,525
You said too expensive.
你说太贵了。

640
00:31:01,525 --> 00:31:04,390
Um, I guess I mean it is expensive in that you have to do
嗯，我想我的意思是你要做的事情很贵

641
00:31:04,390 --> 00:31:07,450
a sequence and the sequence is usually worse than something you can do in parallel.
一个序列和序列通常比你可以并行做的更糟糕。

642
00:31:07,450 --> 00:31:10,030
But I suppose, um, maybe what's wrong with the greediness?
但我想，嗯，也许贪婪可能有什么问题？

643
00:31:10,030 --> 00:31:11,650
Can anyone suggest what's wrong with the greediness, yeah?
任何人都可以说贪婪有什么不对，是吗？

644
00:31:11,650 --> 00:31:19,210
[inaudible] [NOISE] That's not
[听不清] [NOISE]那不是

645
00:31:19,210 --> 00:31:21,820
necessarily gonna give you the argmax over the entire sentence.
必须要给你整个句子的argmax。

646
00:31:21,820 --> 00:31:23,335
That's exactly right. That's, uh,
这是完全正确的。那是，呃，

647
00:31:23,335 --> 00:31:26,080
kind of what, uh, what greediness means.
什么，呃，贪婪意味着什么。

648
00:31:26,080 --> 00:31:29,035
So in practice, this might give you something like this.
所以在实践中，这可能会给你这样的东西。

649
00:31:29,035 --> 00:31:30,715
Uh, we're trying to translate
呃，我们正在尝试翻译

650
00:31:30,715 --> 00:31:34,000
our running example sentence and let's suppose on the first step we say,
我们运行的例句，让我们假设在第一步，我们说，

651
00:31:34,000 --> 00:31:35,320
"He," and then we say,
“他，”然后我们说，

652
00:31:35,320 --> 00:31:37,000
"He hit," and then we say,
“他打了，”然后我们说，

653
00:31:37,000 --> 00:31:39,190
"He hit a," oh no, that wasn't right.
“他打了一个，”哦不，那不对。

654
00:31:39,190 --> 00:31:42,880
That wasn't the best thing to choose but we kinda have no way to go back now, right.
这不是最好的选择，但我们现在无法回头，对吧。

655
00:31:42,880 --> 00:31:45,430
We just have to continue and try to make the best of it after saying,
我们必须继续并尝试充分利用它，然后说，

656
00:31:45,430 --> 00:31:48,760
"He hit a," which isn't gonna work out well.
“他打了一个，”这不会很好。

657
00:31:48,760 --> 00:31:50,920
So that's the main problem with greedy decoding.
所以这是贪婪解码的主要问题。

658
00:31:50,920 --> 00:31:53,965
There's kind of no way to backtrack, no way to go back.
有一种无法回溯的方式，无法回头。

659
00:31:53,965 --> 00:31:56,290
So how can we fix this?
那我们怎么解决这个问题呢？

660
00:31:56,290 --> 00:31:57,940
And this relates back to, uh,
这与呃相关

661
00:31:57,940 --> 00:32:00,100
what I told you earlier about how we might use, uh,
我之前告诉你的关于我们如何使用的信息，呃，

662
00:32:00,100 --> 00:32:04,220
a kind of searching algorithm to do decoding and SMT.
一种搜索算法，用于解码和SMT。

663
00:32:04,310 --> 00:32:07,785
Uh, but first, you might,
呃，但首先，你可能，

664
00:32:07,785 --> 00:32:09,990
uh, think exhaustive search is a good idea.
呃，想想详尽的搜索是个好主意。

665
00:32:09,990 --> 00:32:13,125
Well, probably not because it's still a bad idea for the same reasons as before.
嗯，可能不是因为它仍然是一个坏主意，出于与以前相同的原因。

666
00:32:13,125 --> 00:32:14,955
So if you did want to do exhaustive search,
所以，如果你确实想做详尽的搜索，

667
00:32:14,955 --> 00:32:18,615
and search through the space of all possible French translations, uh,
并搜索所有可能的法语翻译空间，呃，

668
00:32:18,615 --> 00:32:19,890
then you would be again,
然后你会再次，

669
00:32:19,890 --> 00:32:23,010
trying to consider which Y maximizes,
试图考虑哪个Y最大化，

670
00:32:23,010 --> 00:32:26,885
uh, this product of all of these individual probability distributions.
呃，所有这些个体概率分布的这个乘积。

671
00:32:26,885 --> 00:32:29,740
So as before, if you try to do this,
和以前一样，如果你试着这样做，

672
00:32:29,740 --> 00:32:32,229
uh, then on each step T of the decoder,
呃，然后在解码器的每个步骤T，

673
00:32:32,229 --> 00:32:37,750
you're gonna be having to track V to the power of T possible partial translations,
你将不得不追踪V到T的可能部分翻译的力量，

674
00:32:37,750 --> 00:32:40,330
uh, where V is your vocabulary size.
呃，V是你的词汇量。

675
00:32:40,330 --> 00:32:42,130
So here when I say partial translation,
所以当我说部分翻译时，

676
00:32:42,130 --> 00:32:43,435
I just mean, uh, a kinda,
我只是说，呃，有点儿，

677
00:32:43,435 --> 00:32:46,945
you know, like, half of a sentence so far, or something like that.
你知道，到目前为止一半的句子，或类似的东西。

678
00:32:46,945 --> 00:32:48,850
So, of course, this, uh,
那么，当然，这个，呃，

679
00:32:48,850 --> 00:32:52,000
exponential in V complexity is just far too expensive.
V复杂度的指数过于昂贵。

680
00:32:52,000 --> 00:32:54,790
So yes, we're gonna use some kind of search algorithm,
所以，是的，我们将使用某种搜索算法，

681
00:32:54,790 --> 00:32:57,970
and in particular, we're gonna use a beam search decoding.
特别是，我们将使用波束搜索解码。

682
00:32:57,970 --> 00:33:03,550
So the core idea of beam search decoding is that on each step of the decoder,
因此，光束搜索解码的核心思想是在解码器的每一步，

683
00:33:03,550 --> 00:33:08,515
you're gonna be keeping track of the K most probable partial translations,
你要跟踪K最可能的部分翻译，

684
00:33:08,515 --> 00:33:10,975
and we call partial translations hypotheses,
我们称之为部分翻译假设，

685
00:33:10,975 --> 00:33:14,410
because we're kind of tracking multiple of them and we're not sure which one is best.
因为我们有点跟踪他们的多个，我们不确定哪一个是最好的。

686
00:33:14,410 --> 00:33:16,550
So we're thinking about several.
所以我们考虑几个。

687
00:33:16,550 --> 00:33:21,240
Here K is an integer and we call this the beam size,
这里K是一个整数，我们称之为光束大小，

688
00:33:21,240 --> 00:33:25,095
and in practice for NMT this is usually maybe 5-10.
在NMT的实践中，这通常可能是5-10。

689
00:33:25,095 --> 00:33:26,745
So you can think of, uh,
所以你可以想到，呃，

690
00:33:26,745 --> 00:33:30,330
K kind of as how big is your search space at any one time.
K类似于你的搜索空间在任何时候都有多大。

691
00:33:30,330 --> 00:33:32,190
So if you increase K, then you're going to be
所以，如果你增加K，那么你将会是

692
00:33:32,190 --> 00:33:34,860
considering more different options on each step
考虑每个步骤的更多不同选项

693
00:33:34,860 --> 00:33:36,810
and you might hope that this will mean that you get
你可能希望这意味着你得到了

694
00:33:36,810 --> 00:33:40,930
the best quality solution in the end though of course it'll be more expensive.
最终质量最好的解决方案，当然它会更贵。

695
00:33:41,030 --> 00:33:44,010
So I said that we want to keep track of
所以我说我们要跟踪

696
00:33:44,010 --> 00:33:47,865
the K most probable partial translations, that is, hypotheses.
K最可能的部分翻译，即假设。

697
00:33:47,865 --> 00:33:50,205
So this means that we need some kind of notion of, you know,
所以这意味着我们需要某种概念，你知道，

698
00:33:50,205 --> 00:33:53,195
how probable is this hypothesis or what's its score.
这个假设有多大可能或者得分是多少。

699
00:33:53,195 --> 00:33:56,065
So the score of a hypothesis and, uh,
那么假设的得分，呃，

700
00:33:56,065 --> 00:33:59,110
we're representing that as Y_1 up to Y_T,
我们将Y_1表示为Y_T，

701
00:33:59,110 --> 00:34:02,995
um, is just its log probability.
嗯，只是它的对数概率。

702
00:34:02,995 --> 00:34:07,180
So, uh, the log probability of this partial translation, uh,
那么，呃，这部分翻译的对数概率，呃，

703
00:34:07,180 --> 00:34:11,200
according to the language model can be broken down as we saw before into the sum of
根据语言模型可以分解为我们之前看到的总和

704
00:34:11,200 --> 00:34:16,220
the individual log probabilities of the words given everything that came before.
给出之前所有内容的单词的单独日志概率。

705
00:34:16,920 --> 00:34:19,390
So it's, if it's not obvious, uh,
所以，如果它不明显，呃，

706
00:34:19,390 --> 00:34:22,090
these scores are all negative because we're taking log of,
这些分数都是负数，因为我们正在记录，

707
00:34:22,090 --> 00:34:24,100
uh, of a number between 0 and 1.
呃，0到1之间的数字。

708
00:34:24,100 --> 00:34:29,230
Uh, and a higher score is better.
呃，分数越高越好。

709
00:34:29,230 --> 00:34:32,830
Yes, because you want a higher probability of,
是的，因为你想要更高的概率，

710
00:34:32,830 --> 00:34:37,465
uh, of the hypothesis according to the language model.
呃，根据语言模型的假设。

711
00:34:37,465 --> 00:34:40,660
So the idea is that we're gonna use this score, uh,
所以这个想法是我们要用这个分数，呃，

712
00:34:40,660 --> 00:34:42,220
and the search algorithm to search for
以及搜索的搜索算法

713
00:34:42,220 --> 00:34:46,255
high-scoring hypotheses and we're gonna track the top K on each step.
高分的假设，我们将在每一步追踪前K。

714
00:34:46,255 --> 00:34:49,285
So I'm gonna show you a detailed example in a moment,
所以我马上就会给你一个详细的例子，

715
00:34:49,285 --> 00:34:51,250
but the important thing is to know
但重要的是要知道

716
00:34:51,250 --> 00:34:54,910
that beam search is not guaranteed to find an optimal solution.
光束搜索无法保证找到最佳解决方案。

717
00:34:54,910 --> 00:34:57,295
Uh, exhaustive search, the one where you enumerate,
呃，详尽的搜索，你列举的那个，

718
00:34:57,295 --> 00:35:00,820
enumerate all V to the T possible translations, that is guaranteed to find
枚举所有V到T的可能翻译，保证找到

719
00:35:00,820 --> 00:35:04,960
the optimal solution but it is just completely infeasible because it's so expensive.
最佳解决方案，但它是完全不可行的，因为它是如此昂贵。

720
00:35:04,960 --> 00:35:08,080
So beam search is not guaranteed to find the optimal solution,
所以光束搜索不能保证找到最佳解决方案，

721
00:35:08,080 --> 00:35:11,860
but it is much more efficient than exhaustive search of course.
但它当然比穷举搜索更有效率。

722
00:35:11,860 --> 00:35:16,945
Okay. So, um, here's an example of beam search decoding in action.
好的。那么，嗯，这是一个实际的光束搜索解码的例子。

723
00:35:16,945 --> 00:35:19,420
Uh, so let's suppose the beam size equals K, uh,
呃，我们假设光束大小等于K，呃，

724
00:35:19,420 --> 00:35:22,795
is 2 and then as a reminder, we have, uh,
是2然后作为提醒，我们有，呃，

725
00:35:22,795 --> 00:35:25,630
this is the score that you apply to a partial, uh,
这是你应用于部分的分数，呃，

726
00:35:25,630 --> 00:35:28,540
hypothesis, uh, a partial translation,
假设，呃，部分翻译，

727
00:35:28,540 --> 00:35:29,935
which is a hypothesis.
这是一个假设。

728
00:35:29,935 --> 00:35:32,590
So we start off with our starting token,
所以我们从我们的起始令牌开始，

729
00:35:32,590 --> 00:35:34,570
and the idea is that we're going to compute
我们的想法是我们要计算

730
00:35:34,570 --> 00:35:37,765
the probability distribution of what word might come next.
接下来可能会出现什么词的概率分布。

731
00:35:37,765 --> 00:35:41,880
So having computed that probability distribution using our seq2seq model,
因此，使用我们的seq2seq模型计算了该概率分布，

732
00:35:41,880 --> 00:35:45,435
then we just take the top K, that is top two possible options.
然后我们只选择前K，这是两个可能的选择。

733
00:35:45,435 --> 00:35:48,695
So let's suppose that the top two are the words "He" and "I".
所以我们假设前两位是“他”和“我”。

734
00:35:48,695 --> 00:35:52,930
So the idea is that we can compute the score of these two hypotheses,
所以我们的想法是我们可以计算出这两个假设的得分，

735
00:35:52,930 --> 00:35:55,540
uh, by using the formula above.
呃，使用上面的公式。

736
00:35:55,540 --> 00:35:59,845
It's just the log probability of this word given the context so far.
到目前为止，这只是给出上下文的这个词的对数概率。

737
00:35:59,845 --> 00:36:05,125
So here, let's say that "He" has a score of -0,7 and "I" has a score of -0,9.
所以在这里，让我们说“他”得分为-0,7，“我”得分为-0,9。

738
00:36:05,125 --> 00:36:07,750
So this means that he is currently the best one.
所以这意味着他目前是最好的一个。

739
00:36:07,750 --> 00:36:09,640
Okay. So what we do is, uh,
好的。所以我们做的是，呃，

740
00:36:09,640 --> 00:36:12,655
we have our two, uh, K hypotheses,
我们有两个，呃，K假设，

741
00:36:12,655 --> 00:36:14,710
and then for each of those,
然后对于每一个，

742
00:36:14,710 --> 00:36:18,160
we find the top K words that could come next.
我们找到了接下来可能出现的前K个词。

743
00:36:18,160 --> 00:36:20,005
And we calculate their scores.
我们计算他们的分数。

744
00:36:20,005 --> 00:36:24,055
So this means that for both "He" and "I" we find the top two words that could come next.
所以这意味着对于“他”和“我”，我们找到了接下来可能出现的前两个词。

745
00:36:24,055 --> 00:36:26,440
And for each of these four possibilities, uh,
对于这四种可能性中的每一种，呃，

746
00:36:26,440 --> 00:36:29,395
the score of the hypothesis is equal to, uh,
假设的得分等于，呃，

747
00:36:29,395 --> 00:36:32,980
the log probability of this new word given the context so far plus
给出上下文的这个新单词的对数概率加上

748
00:36:32,980 --> 00:36:36,820
the score so far because you can accumulate the sum of low probabilities.
到目前为止的分数，因为你可以积累低概率的总和。

749
00:36:36,820 --> 00:36:39,620
You don't have to compute it from scratch each time.
您不必每次都从头开始计算它。

750
00:36:40,170 --> 00:36:44,350
So here you can see that we have these four possibilities and that
所以在这里你可以看到我们有这四种可能性

751
00:36:44,350 --> 00:36:48,610
the top two scores are -1,6 and -1,7.
前两个得分分别为-1,6和-1,7。

752
00:36:48,610 --> 00:36:52,180
So this means that hit and was are the two best ones.
所以这意味着击中并且是最好的两个。

753
00:36:52,180 --> 00:36:55,570
So the idea is that of these K squared equals 4 hypotheses,
所以这个想法是这些K平方等于4个假设，

754
00:36:55,570 --> 00:36:58,795
we're just gonna keep the K equals 2 top ones.
我们要保持K等于2个顶级。

755
00:36:58,795 --> 00:37:00,835
And then we just keep doing the same thing.
然后我们继续做同样的事情。

756
00:37:00,835 --> 00:37:03,955
For these two, we expand to get the two next ones.
对于这两个，我们扩展以获得下两个。

757
00:37:03,955 --> 00:37:06,295
And then of those we compute the scores,
然后我们计算得分，

758
00:37:06,295 --> 00:37:11,905
and then we keep the two best ones and discard the others and then of those, we expand.
然后我们保留两个最好的并丢弃其他的，然后我们扩展。

759
00:37:11,905 --> 00:37:13,675
So we keep doing this again and again,
所以我们一次又一次地这样做，

760
00:37:13,675 --> 00:37:18,700
expanding and then just keeping the top K and expanding like this until,
扩展，然后保持顶部K并像这样扩展，直到，

761
00:37:18,700 --> 00:37:22,045
uh, you get some kinda, uh, finished translation.
呃，你得到一些，呃，完成翻译。

762
00:37:22,045 --> 00:37:25,570
I'm going to tell you more in a moment about what exactly the stopping criterion is.
我稍后会告诉你更多关于停止标准的内容。

763
00:37:25,570 --> 00:37:27,550
But let's suppose that we stop here.
但我们假设我们就此止步。

764
00:37:27,550 --> 00:37:31,150
Uh, looking at the four hypotheses that we have on the far right,
呃，看看我们在最右边的四个假设，

765
00:37:31,150 --> 00:37:33,220
the one with the top score is, uh,
得分最高的是，呃，

766
00:37:33,220 --> 00:37:36,265
the top pie one with -4,3.
顶部馅饼一个-4,3。

767
00:37:36,265 --> 00:37:38,170
So let's suppose that we are gonna stop now when we
所以我们假设我们现在要停止了

768
00:37:38,170 --> 00:37:39,955
decide that this is the top hypothesis,
认为这是最重要的假设，

769
00:37:39,955 --> 00:37:42,340
then all we need to do is just backtrack
那么我们所需要做的只是回溯

770
00:37:42,340 --> 00:37:45,070
through this tree in order to find the full translation,
通过这棵树，以找到完整的翻译，

771
00:37:45,070 --> 00:37:48,790
which is "He hit me with the pie."
这是“他用馅饼打我。”

772
00:37:48,790 --> 00:37:53,185
All right. So, um, let me tell you more detail about how exactly we decide when to stop.
行。所以，嗯，让我告诉你更多关于我们如何决定何时停止的细节。

773
00:37:53,185 --> 00:37:55,524
So if you remember in greedy decoding,
所以，如果你记得在贪婪的解码，

774
00:37:55,524 --> 00:37:59,350
usually we just keep decoding until the model produces the END token.
通常我们只是继续解码，直到模型产生END令牌。

775
00:37:59,350 --> 00:38:03,460
So for example, this means that your model is actually producing the sequence, uh,
因此，例如，这意味着您的模型实际上正在生成序列，呃，

776
00:38:03,460 --> 00:38:04,525
I guess it doesn't produce START,
我猜它不会产生START，

777
00:38:04,525 --> 00:38:09,265
you give it START but then it produces the sequence "He hit me with a pie" END.
你给它START，然后它产生序列“他用馅饼打我”END。

778
00:38:09,265 --> 00:38:12,310
So the problem in beam search decoding is
所以光束搜索解码的问题是

779
00:38:12,310 --> 00:38:14,920
that you're considering all these different hypotheses,
你正在考虑所有这些不同的假设，

780
00:38:14,920 --> 00:38:17,785
K different hypotheses at once and the thing is
K不同的假设一下子就是了

781
00:38:17,785 --> 00:38:21,250
those hypotheses might produce END tokens at different times.
这些假设可能会在不同时间产生END令牌。

782
00:38:21,250 --> 00:38:24,100
So there's no one obvious place to stop.
所以没有一个明显的地方可以阻止。

783
00:38:24,100 --> 00:38:26,005
So what we do in practice,
那么我们在实践中做了什么，

784
00:38:26,005 --> 00:38:28,510
is when a hypothesis produces the END token,
当假设产生END令牌时，

785
00:38:28,510 --> 00:38:32,935
then we regard this hypothesis as complete and we kind of place it aside.
那么我们认为这个假设是完整的，我们把它放在一边。

786
00:38:32,935 --> 00:38:35,425
We have a collection of completed hypothesis.
我们收集了完整的假设。

787
00:38:35,425 --> 00:38:37,000
So we kind of take it out of beam search,
所以我们把它从光束搜索中拿出来，

788
00:38:37,000 --> 00:38:39,565
we no longer keep exploring it because it's finished,
我们不再继续探索它，因为它已经完成，

789
00:38:39,565 --> 00:38:41,800
uh, and we, yeah, place it aside.
呃，我们，是的，把它放在一边。

790
00:38:41,800 --> 00:38:45,685
And you continue exploring other hypotheses with beam search.
然后你继续探索光束搜索的其他假设。

791
00:38:45,685 --> 00:38:49,120
So the remaining question is when do you stop doing beam search?
所以剩下的问题是你什么时候停止进行光束搜索？

792
00:38:49,120 --> 00:38:51,535
When do you stop iterating through this algorithm?
你什么时候停止迭代这个算法？

793
00:38:51,535 --> 00:38:55,105
So there's, uh, uh, multiple possible stopping criterion
所以，呃，呃，多个可能的停止标准

794
00:38:55,105 --> 00:38:57,910
but two common ones are you might say, uh,
但你可能会说两个常见的，呃，

795
00:38:57,910 --> 00:39:01,150
we're gonna stop doing beam search once we reach time step T,
一旦我们到达时间步T，我们将停止进行光束搜索，

796
00:39:01,150 --> 00:39:02,545
where T is some, uh,
T是某些，呃，

797
00:39:02,545 --> 00:39:04,120
predefined threshold that you choose.
您选择的预定义阈值。

798
00:39:04,120 --> 00:39:05,230
So you might say, uh,
所以你可能会说，呃，

799
00:39:05,230 --> 00:39:08,380
we're gonna stop beam search after 30 steps because we don't want
因为我们不想要，我们将在30步后停止光束搜索

800
00:39:08,380 --> 00:39:11,755
any output sentences that are longer than 30 words for example,
例如，超过30个单词的任何输出句子，

801
00:39:11,755 --> 00:39:13,120
or you might say, "Uh,
或者你可能会说，“呃，

802
00:39:13,120 --> 00:39:17,410
we're gonna stop doing beam search once we've collected at least N completed hypotheses."
一旦我们收集了至少N个已完成的假设，我们就会停止进行光束搜索。“

803
00:39:17,410 --> 00:39:18,820
So you might say, "Uh, I want
所以你可能会说，“呃，我想要

804
00:39:18,820 --> 00:39:23,510
at least 10 complete translations before I stop doing beam search."
在我停止进行光束搜索之前至少有10个完整的翻译。“

805
00:39:24,510 --> 00:39:27,550
Okay. So what's the final thing you have to do?
好的。那你最后要做的是什么？

806
00:39:27,550 --> 00:39:29,350
Uh, we finished doing beam search, um,
呃，我们完成了光束搜索，嗯，

807
00:39:29,350 --> 00:39:32,350
we have this collection of completed hypotheses.
我们有这个完整假设的集合。

808
00:39:32,350 --> 00:39:34,525
Uh, we want to choose the top one.
呃，我们想选择最好的一个。

809
00:39:34,525 --> 00:39:36,835
Uh, the one that we're going to use is our translation.
呃，我们将要使用的是我们的翻译。

810
00:39:36,835 --> 00:39:41,380
So, uh, how do we select the top one that has the highest score?
那么，呃，我们如何选择得分最高的顶级？

811
00:39:41,380 --> 00:39:43,360
Uh, you might think this is simple given that all of
呃，你可能会觉得这很简单

812
00:39:43,360 --> 00:39:45,850
these hypotheses already have scores attached.
这些假设已经附加了分数。

813
00:39:45,850 --> 00:39:47,440
But if we just look at this, uh,
但是，如果我们只看这个，呃，

814
00:39:47,440 --> 00:39:51,745
formula again, uh, for what the score is of each hypothesis.
公式，呃，每个假设得分。

815
00:39:51,745 --> 00:39:55,120
Uh, can anyone see a problem with this?
呃，有人能看到这个问题吗？

816
00:39:55,120 --> 00:39:57,115
If we have our sets of hypotheses,
如果我们有一套假设，

817
00:39:57,115 --> 00:39:59,290
and then we're choosing the top one
然后我们选择了最顶一个

818
00:39:59,290 --> 00:40:02,750
based on the one that has the best score, can anyone see a problem?
基于得分最高的人，任何人都可以看到问题吗？

819
00:40:05,130 --> 00:40:09,460
Yeah. [NOISE] So the answer was you're gonna end up choosing the shortest one.
是啊。 [NOISE]所以答案是你最终会选择最短的一个。

820
00:40:09,460 --> 00:40:13,630
The problem here is that longer hypotheses have lower scores in
这里的问题是较长的假设得分较低

821
00:40:13,630 --> 00:40:18,190
general because you're multiplying more probabilities so you're getting a smaller,
一般是因为你增加了更多的概率，所以你变小了，

822
00:40:18,190 --> 00:40:20,440
a smaller overall value or I guess if we're adding
总体价值较小，或者我猜是否正在增加

823
00:40:20,440 --> 00:40:22,855
low probabilities we're gonna get more negative values.
低概率我们会得到更多负面价值。

824
00:40:22,855 --> 00:40:25,255
So it's not quite that you will definitely choose
所以你肯定不会选择它

825
00:40:25,255 --> 00:40:28,690
the shortest hypothesis because if you could overall have,
最短的假设，因为如果你可以整体拥有，

826
00:40:28,690 --> 00:40:33,550
uh, a lower score but there's definitely going to be a bias towards shorter translations,
呃，分数较低，但肯定会偏向较短的翻译，

827
00:40:33,550 --> 00:40:35,995
uh, because they'll in general have lower scores.
呃，因为他们一般都会得分较低。

828
00:40:35,995 --> 00:40:38,620
So the way you can fix this is pretty simple,
所以你可以解决这个问题很简单，

829
00:40:38,620 --> 00:40:40,210
you just normalize by length.
你只是按长度标准化。

830
00:40:40,210 --> 00:40:43,720
So instead of using the tools we have above, you're going to use, uh,
因此，不是使用我们上面使用的工具，而是使用，呃，

831
00:40:43,720 --> 00:40:48,490
the score divided by [inaudible].
得分除以[听不清]。

832
00:40:48,490 --> 00:40:51,010
And then you use this to select the top one.
然后你用它来选择最上面的一个。

833
00:40:51,010 --> 00:41:00,235
Any questions on this? [NOISE].
对此有任何疑问？ [噪声]。

834
00:41:00,235 --> 00:41:02,680
Yeah.
是啊。

835
00:41:02,680 --> 00:41:06,970
Can we train with the END token so that it is possible to [inaudible]
我们可以使用END令牌进行训练，以便[听不清]

836
00:41:06,970 --> 00:41:09,475
I didn't quite hear that, can you train with the END token?
我听不到，你能用END令牌训练吗？

837
00:41:09,475 --> 00:41:12,230
Yeah, like we had an END token.
是的，就像我们有一个END令牌。

838
00:41:12,690 --> 00:41:16,435
Yes. So you train with the END token, if that's your question.
是。所以你用END令牌训练，如果这是你的问题。

839
00:41:16,435 --> 00:41:19,930
Um, because the whole point is you're relying on your language model,
嗯，因为重点在于你是依靠你的语言模型，

840
00:41:19,930 --> 00:41:24,145
your decoder to produce the END token in order to know when to stop.
你的解码器产生END令牌，以便知道何时停止。

841
00:41:24,145 --> 00:41:27,370
So you need to train it to produce the END token by giving it examples of
因此，您需要通过提供示例来训练它以生成END令牌

842
00:41:27,370 --> 00:41:33,490
training sentences with END tokens. Yeah.
用END标记训练句子。是啊。

843
00:41:33,490 --> 00:41:37,270
Why don't we use this score being changed [inaudible]
为什么我们不改变这个分数[音频不清晰]

844
00:41:37,270 --> 00:41:38,650
Great question. The question is,
好问题。问题是，

845
00:41:38,650 --> 00:41:40,360
why don't we use this normalized score,
为什么我们不使用这个标准化分数，

846
00:41:40,360 --> 00:41:43,315
the one at the bottom of the screen during beam search in the first place?
首先在光束搜索期间屏幕底部的那个？

847
00:41:43,315 --> 00:41:45,205
So the reason why that's not necessary,
所以没有必要，

848
00:41:45,205 --> 00:41:46,825
you could, but it's not necessary,
你可以，但没有必要，

849
00:41:46,825 --> 00:41:48,790
is because during beam search,
是因为在光束搜索期间，

850
00:41:48,790 --> 00:41:54,100
we only ever compare the scores of hypotheses that have the same length, right?
我们只比较具有相同长度的假设得分，对吧？

851
00:41:54,100 --> 00:41:55,990
So in each of these steps, the way we look at,
所以在每个步骤中，我们看待的方式，

852
00:41:55,990 --> 00:42:00,040
let's say the top k squared and we want to choose which ones are the top k,
让我们说前k平方，我们想选择哪些是前k，

853
00:42:00,040 --> 00:42:04,030
we're comparing the scores of four different hypotheses that are of length,
我们比较四种不同长度的假设的得分，

854
00:42:04,030 --> 00:42:05,755
one, two, three, four, five.
一二三四五。

855
00:42:05,755 --> 00:42:10,135
So, um, it's true that these scores are getting lower and lower,
所以，嗯，这些得分越来越低，

856
00:42:10,135 --> 00:42:12,760
but in the same way because they're all length five right now.
但是以同样的方式，因为他们现在都是五岁。

857
00:42:12,760 --> 00:42:19,095
[NOISE] Okay.
[NOISE]好的。

858
00:42:19,095 --> 00:42:23,580
So we now understand how you would train an NMT system and how would you- you
所以我们现在了解你将如何训练NMT系统，你会怎样

859
00:42:23,580 --> 00:42:27,900
would use your trained NMT system to generate your translations using,
将使用您训练有素的NMT系统生成您的翻译，使用，

860
00:42:27,900 --> 00:42:29,585
let's say, beam search.
让我们说，光束搜索。

861
00:42:29,585 --> 00:42:32,290
So let's all take a step back and think about,
让我们都退后一步思考，

862
00:42:32,290 --> 00:42:36,730
what are the overall advantages of NMT in comparison to SMT?
与SMT相比，NMT的整体优势是什么？

863
00:42:36,730 --> 00:42:41,020
Uh, so the first advantage is just better performance.
呃，所以第一个优势就是更好的表现。

864
00:42:41,020 --> 00:42:45,715
Uh, NMT systems tend to give better output than SMT systems in several ways.
呃，NMT系统往往在几个方面提供比SMT系统更好的输出。

865
00:42:45,715 --> 00:42:48,760
One is that the output often tends to be more fluent.
一个是输出往往更流畅。

866
00:42:48,760 --> 00:42:51,325
Uh, this is probably because NMT, uh,
呃，这可能是因为NMT，呃，

867
00:42:51,325 --> 00:42:53,425
this is probably because RNNs are particularly good at
这可能是因为RNN特别擅长

868
00:42:53,425 --> 00:42:56,005
learning language models as you learned last week.
学习上周学到的语言模型。

869
00:42:56,005 --> 00:42:58,510
Uh, another way that they're better is they often use,
呃，他们经常使用的另一种方式是更好的，

870
00:42:58,510 --> 00:43:00,760
uh, the context better, that is,
呃，环境更好，也就是说，

871
00:43:00,760 --> 00:43:02,500
uh, they're better at conditioning on
呃，他们更善于调理

872
00:43:02,500 --> 00:43:05,935
the source sentence and using that to change the output.
源句和使用它来改变输出。

873
00:43:05,935 --> 00:43:09,040
Another way they're better is they often, uh,
他们更好的另一种方式是他们经常，呃，

874
00:43:09,040 --> 00:43:13,795
are more able to generalize what they learn about phrases and how to translate them.
更能够概括他们对短语的学习以及如何翻译它们。

875
00:43:13,795 --> 00:43:17,410
So for example, if it sees an example of how to translate a certain
例如，如果它看到了如何翻译某个特定的例子

876
00:43:17,410 --> 00:43:21,850
source phrase and then later it sees a slightly different version of that source phrase,
源短语，然后它看到该源短语的略有不同的版本，

877
00:43:21,850 --> 00:43:24,490
it's, uh, more able to generalize what
它，呃，更能够概括什么

878
00:43:24,490 --> 00:43:28,040
it learned about the first phrase than SMT systems will.
它了解了第一个短语而不是SMT系统。

879
00:43:28,950 --> 00:43:33,580
Another big advantage of NMT systems compared to SMT that we talked about
与我们谈到的SMT相比，NMT系统的另一大优势

880
00:43:33,580 --> 00:43:37,505
before is that it's a single neural network that can be optimized end-to-end.
之前是它是一个可以端到端优化的单一神经网络。

881
00:43:37,505 --> 00:43:42,490
And the- the advantage here I suppose is primarily simplicity and convenience.
而且 - 我认为这里的优势主要是简单和方便。

882
00:43:42,620 --> 00:43:47,140
So there's no subcomponents that need to be individually optimized.
因此，没有需要单独优化的子组件。

883
00:43:47,250 --> 00:43:52,030
Another big advantage is that it requires much less human engineering efforts.
另一个很大的优势是它需要更少的人力工程努力。

884
00:43:52,030 --> 00:43:54,130
When I told you earlier about all the different things that
当我早些时候告诉你所有不同的事情时

885
00:43:54,130 --> 00:43:56,485
people had to do to build, uh, big,
人们不得不做，呃，大，

886
00:43:56,485 --> 00:43:58,765
uh, powerful SMT systems, uh,
呃，强大的SMT系统，呃，

887
00:43:58,765 --> 00:44:01,555
there's relatively less engineering effort for NMT.
NMT的工程工作量相对较少。

888
00:44:01,555 --> 00:44:03,010
And NMT is certainly not easy,
NMT肯定不容易，

889
00:44:03,010 --> 00:44:05,485
but it's- is less complicated than SMT.
但它 - 比SMT复杂。

890
00:44:05,485 --> 00:44:08,245
In particular, there's no feature engineering.
特别是，没有特征工程。

891
00:44:08,245 --> 00:44:10,480
You don't have to define what features of,
你不必定义什么功能，

892
00:44:10,480 --> 00:44:12,520
uh, linguistic phenomena that you want to capture.
呃，你想要捕捉的语言现象。

893
00:44:12,520 --> 00:44:15,550
You can mostly just view it as a sequence of words although,
您可以将其视为一系列单词，但是，

894
00:44:15,550 --> 00:44:18,530
uh, there are different views on that.
呃，对此有不同的看法。

895
00:44:19,500 --> 00:44:22,930
Uh, lastly, a great thing about NMT is that you can
呃，最后，关于NMT的一件好事就是你可以

896
00:44:22,930 --> 00:44:26,005
use pretty much the same method for all language pairs.
对所有语言对使用几乎相同的方法。

897
00:44:26,005 --> 00:44:27,370
So if you've, uh, you know,
所以，如果你，呃，你知道，

898
00:44:27,370 --> 00:44:29,200
built your French-to-English translation system
建立了您的法语 - 英语翻译系统

899
00:44:29,200 --> 00:44:31,075
and now you want to build a Spanish-to-English one, [NOISE] uh,
现在你想建立一个西班牙语到英语的，[NOISE]呃，

900
00:44:31,075 --> 00:44:34,375
you can probably use basically the same architecture and the same method
你可以使用基本相同的架构和相同的方法

901
00:44:34,375 --> 00:44:38,720
as long as you can go find a big enough parallel corpus of Spanish-to-English.
只要你能找到一个足够大的西班牙语到英语的平行语料库。

902
00:44:38,880 --> 00:44:43,060
All right. So what are the disadvantages of NMT, uh, remaining?
行。那么NMT的缺点是什么，呃，还剩下什么？

903
00:44:43,060 --> 00:44:44,560
So compared to SMT,
所以与SMT相比，

904
00:44:44,560 --> 00:44:46,090
there are some disadvantages.
有一些缺点。

905
00:44:46,090 --> 00:44:48,970
One is that NMT is less interpretable.
一个是NMT不太可解释。

906
00:44:48,970 --> 00:44:51,670
Uh, what I mean by this is you
呃，我的意思是你

907
00:44:51,670 --> 00:44:54,385
feed in your source sentence into the neural network and then it feeds
将源语句中的内容输入神经网络，然后输入

908
00:44:54,385 --> 00:44:57,310
out some target sentence and you didn't
出一些目标句而你却没有

909
00:44:57,310 --> 00:45:00,430
really have any way to figure out why that happened, right?
真的有办法弄清楚为什么会这样，对吧？

910
00:45:00,430 --> 00:45:03,430
So in particular, if the target sentence con- contains some kind of error,
所以特别是，如果目标句子包含某种错误，

911
00:45:03,430 --> 00:45:06,985
um, you can't really look at the neurons and understand what happened.
嗯，你无法真正看到神经元并了解发生了什么。

912
00:45:06,985 --> 00:45:08,800
It's pretty hard to attribute errors.
归因于错误很难。

913
00:45:08,800 --> 00:45:10,240
So this means that, uh,
所以这意味着，呃，

914
00:45:10,240 --> 00:45:12,235
NMT systems are pretty hard to debug.
NMT系统很难调试。

915
00:45:12,235 --> 00:45:15,850
So by comparison, SMT systems were more
相比之下，SMT系统更多

916
00:45:15,850 --> 00:45:17,860
interpretable in that you had all of
可解释的是你拥有所有的

917
00:45:17,860 --> 00:45:20,500
these different sub-components that were doing different jobs.
这些不同的子组件正在做不同的工作。

918
00:45:20,500 --> 00:45:22,600
And, uh, you were more able to look at those.
而且，呃，你更能看到那些。

919
00:45:22,600 --> 00:45:24,910
They weren't, you know, neurons often would be, uh,
他们不是，你知道，神经元经常是，呃，

920
00:45:24,910 --> 00:45:27,880
you know, probabilities of certain words given other words and so on.
你知道，给定其他单词的某些单词的概率等等。

921
00:45:27,880 --> 00:45:30,070
And, you know, that's by no means easy to
而且，你知道，这绝不容易

922
00:45:30,070 --> 00:45:33,295
interpret but it was at least more interpretable than NMT.
解释但它至少比NMT更容易理解。

923
00:45:33,295 --> 00:45:38,350
Uh, another disadvantage is NMT is pretty difficult to control.
呃，另一个缺点是NMT很难控制。

924
00:45:38,350 --> 00:45:40,210
So, uh, for example,
所以，呃，例如，

925
00:45:40,210 --> 00:45:42,490
if your NMT system is,
如果您的NMT系统是，

926
00:45:42,490 --> 00:45:43,990
uh, doing a particular error,
呃，做一个特别的错误，

927
00:45:43,990 --> 00:45:46,405
it's not very easy for you, the, uh,
这对你来说不是很容易，呃，

928
00:45:46,405 --> 00:45:48,490
programmer to specify some kind of
程序员指定某种

929
00:45:48,490 --> 00:45:51,280
rule or guideline that you want the NMT system to follow.
您希望NMT系统遵循的规则或准则。

930
00:45:51,280 --> 00:45:52,915
So for example, if you want to say,
所以，例如，如果你想说，

931
00:45:52,915 --> 00:45:56,380
I want to always translate this word in this way.
我想一直用这种方式翻译这个词。

932
00:45:56,380 --> 00:45:59,290
Um, when- when this other thing is present,
嗯，当这个其他的东西存在的时候，

933
00:45:59,290 --> 00:46:02,725
like that's not particularly easy to, uh,
那样的不是特别容易，呃，

934
00:46:02,725 --> 00:46:05,635
to impose as a rule on the NMT system, uh,
通常对NMT系统强加，呃，

935
00:46:05,635 --> 00:46:07,225
because you can't, uh,
因为你不能，呃，

936
00:46:07,225 --> 00:46:10,540
easily control what it's doing on a step-by-step basis.
轻松控制它一步一步做的事情。

937
00:46:10,540 --> 00:46:12,385
So sometimes you have some kind of, uh,
所以有时你会有某种，呃，

938
00:46:12,385 --> 00:46:14,320
post-processing rules you might try to do,
你可能会尝试做的后处理规则，

939
00:46:14,320 --> 00:46:16,255
but overall you can't.
但总的来说你做不到。

940
00:46:16,255 --> 00:46:18,985
It- it's- it's harder than you'd expect to try to, um,
它 - 它比你期望的更难，嗯，

941
00:46:18,985 --> 00:46:22,870
impose a fairly simple form.
强加一个相当简单的形式。

942
00:46:22,870 --> 00:46:25,360
[NOISE] So this means it has some kind of safety concerns in fact.
[NOISE]因此，这意味着它实际上存在某种安全问题。

943
00:46:25,360 --> 00:46:26,590
Because, uh, let's say, you know,
因为，呃，让我们说，你知道，

944
00:46:26,590 --> 00:46:29,650
you don't want your NMT system to say bad things, right?
你不希望你的NMT系统说坏话，对吧？

945
00:46:29,650 --> 00:46:31,750
It's- it's pretty hard to actually put, um,
这真的很难，嗯，

946
00:46:31,750 --> 00:46:34,180
these, uh, controls in
这些，呃，控制在

947
00:46:34,180 --> 00:46:36,745
place to stop it from saying these things that you don't want it to say.
阻止它说出你不想说的这些东西的地方。

948
00:46:36,745 --> 00:46:39,640
I mean, on the level of maybe just never saying particular bad words,
我的意思是，在可能只是从不说特别坏话的水平上，

949
00:46:39,640 --> 00:46:41,815
then sure you can remove them from the vocabulary.
然后确定你可以从词汇表中删除它们。

950
00:46:41,815 --> 00:46:44,005
But overall they're pretty hard to control,
但总的来说，它们很难控制，

951
00:46:44,005 --> 00:46:47,635
and we're actually gonna see some examples of NMT systems being,
我们实际上会看到一些NMT系统的例子，

952
00:46:47,635 --> 00:46:48,760
you know, doing things that their
你知道，做他们的事

953
00:46:48,760 --> 00:46:51,620
uh, designers certainly didn't intend.
呃，设计师肯定不打算。

954
00:46:52,350 --> 00:46:57,250
Okay. So, uh, how do we evaluate MT?
好的。所以，呃，我们如何评估MT？

955
00:46:57,250 --> 00:46:59,500
Uh, every good NLP task needs to have
呃，每个好的NLP任务都需要

956
00:46:59,500 --> 00:47:02,395
an automatic metric so that we can, uh, measure our progress.
一个自动指标，以便我们可以，呃，衡量我们的进步。

957
00:47:02,395 --> 00:47:05,980
So the, uh, most commonly used evaluation metric for MT is
因此，对于MT来说，最常用的评估指标是

958
00:47:05,980 --> 00:47:10,240
called BLEU and that stands for Bilingual Evaluation Understudy.
称为BLEU，代表双语评估Understudy。

959
00:47:10,240 --> 00:47:13,090
So the main idea is that BLEU is gonna
所以主要的想法是BLEU会

960
00:47:13,090 --> 00:47:17,575
compare the translation that's produced by your machine translation system.
比较机器翻译系统产生的翻译。

961
00:47:17,575 --> 00:47:19,150
It's gonna compare that to
它会与之相比较

962
00:47:19,150 --> 00:47:22,885
one or maybe several human written translations of the same sentence.
同一句话的一个或几个人类书面翻译。

963
00:47:22,885 --> 00:47:27,685
And then it's gonna compute a similarity score that's based on n-gram precision.
然后它将计算基于n-gram精度的相似性得分。

964
00:47:27,685 --> 00:47:29,545
So when I say n-gram precision,
所以当我说n-gram精度时，

965
00:47:29,545 --> 00:47:31,750
I mean you're gonna look at all the one, two, three,
我的意思是你会看到所有的一，二，三，

966
00:47:31,750 --> 00:47:34,150
and four grams that appear in your, uh,
你身上出现的四克，呃，

967
00:47:34,150 --> 00:47:37,345
machine written translation and your human written translation.
机器书面翻译和你的人工书面翻译。

968
00:47:37,345 --> 00:47:39,865
And then n-gram precision is basically saying,
然后n-gram精度基本上说，

969
00:47:39,865 --> 00:47:42,895
for all of the n-grams that appeared in the machine-written translation,
对于机器翻译中出现的所有n-gram，

970
00:47:42,895 --> 00:47:44,920
how many of those appeared in, you know,
你知道，有多少人出现在

971
00:47:44,920 --> 00:47:48,460
at least one of the human-written translations?
至少有一个人性化的翻译？

972
00:47:48,460 --> 00:47:53,170
Another thing that you need to add to BLEU is a brevity penalty.
你需要添加到BLEU的另一件事是简洁惩罚。

973
00:47:53,170 --> 00:47:56,050
Uh, so you're saying that you get a lower BLEU score if
呃，所以你说如果你获得较低的BLEU分数

974
00:47:56,050 --> 00:47:57,970
your system translation is significantly
你的系统翻译很重要

975
00:47:57,970 --> 00:48:00,925
shorter than all of the human-written translations.
比所有人工书面翻译都要短。

976
00:48:00,925 --> 00:48:03,610
And the reason why you need to add this is because
你需要添加它的原因是因为

977
00:48:03,610 --> 00:48:07,750
n-gram precision alone doesn't [NOISE] really punish using, uh, fewer words.
单独的n-gram精度不会[NOISE]真正惩罚使用，呃，更少的单词。

978
00:48:07,750 --> 00:48:12,640
So you might try to maximize n-gram precision by being very conservative and writing,
所以你可以尝试通过非常保守和写作来最大化n-gram精度，

979
00:48:12,640 --> 00:48:15,790
uh, short sentences that only contain words that you're really sure about,
呃，只包含你真正相信的单词的短句，

980
00:48:15,790 --> 00:48:17,425
and then you get a good precision score.
然后你获得了很好的精确分数。

981
00:48:17,425 --> 00:48:20,260
But this doesn't make a good translation because you're probably missing a bunch of
但这并不是一个很好的翻译，因为你可能错过了很多

982
00:48:20,260 --> 00:48:23,125
information that you needed to translate from the source sentence.
您需要从源句翻译的信息。

983
00:48:23,125 --> 00:48:26,260
So that's why you need to add the brevity, uh, penalty.
这就是为什么你需要添加简洁，呃，惩罚。

984
00:48:26,260 --> 00:48:31,045
So overall, um, BLEU is very useful because, uh,
总的来说，嗯，BLEU非常有用，因为，呃，

985
00:48:31,045 --> 00:48:34,180
we need an automatic metric in order to, uh, measure progress,
我们需要一个自动指标来衡量进度，

986
00:48:34,180 --> 00:48:36,160
you can't measure progress on human evaluation
你无法衡量人类评价的进展

987
00:48:36,160 --> 00:48:38,560
alone because it takes too long [NOISE] to compute.
单独因为计算需要太长时间[NOISE]。

988
00:48:38,560 --> 00:48:41,470
Um, but of course it's pretty and perfect.
嗯，但当然它非常完美。

989
00:48:41,470 --> 00:48:44,350
So for example, you can think about how there
例如，你可以考虑一下

990
00:48:44,350 --> 00:48:47,170
are many ways- many valid ways to translate a sentence.
有许多方法 - 翻译句子的许多有效方法。

991
00:48:47,170 --> 00:48:48,700
At the very beginning of this lecture,
在本讲座的开头，

992
00:48:48,700 --> 00:48:50,995
I asked how do we translate that sentence, uh,
我问我们如何翻译那句话，呃，

993
00:48:50,995 --> 00:48:54,025
by Rousseau and there were at least a few different options that came up.
通过Rousseau，至少有几个不同的选择出现了。

994
00:48:54,025 --> 00:48:57,744
Uh, so if there's many valid ways to translate a sentence,
呃，如果有很多有效的翻译方法，

995
00:48:57,744 --> 00:48:59,575
how does BLEU recognize that?
BLEU如何认识到这一点？

996
00:48:59,575 --> 00:49:03,820
BLEU is [NOISE] rewarding sentences that have a high n-gram overlap with,
BLEU是[NOISE]奖励与n-gram重叠的句子，

997
00:49:03,820 --> 00:49:07,315
uh, one or some of the human-written translations.
呃，一个或一些人性化的翻译。

998
00:49:07,315 --> 00:49:08,815
But, if, uh, you write one,
但是，如果，呃，你写一个，

999
00:49:08,815 --> 00:49:11,650
if your model writes one valid translation and the humans wrote
如果你的模型写了一个有效的翻译并且人类写了

1000
00:49:11,650 --> 00:49:15,040
a different valid translation and they don't have high n-gram overlap,
一个不同的有效翻译，他们没有高n-gram重叠，

1001
00:49:15,040 --> 00:49:18,520
then BLEU is going to, uh, give you a low score.
那么BLEU会去，呃，给你一个低分。

1002
00:49:18,520 --> 00:49:23,425
So, um, you're going to learn about BLEU in detail in Assignment 4,
所以，嗯，你将在作业4中详细了解BLEU，

1003
00:49:23,425 --> 00:49:24,850
and in fact Assignment 4 has
事实上，作业4有

1004
00:49:24,850 --> 00:49:28,225
a full description- mathematical description of what the BLEU score is.
完整描述 -  BLEU分数的数学描述。

1005
00:49:28,225 --> 00:49:31,390
So I'm not gonna tell you about that now, uh, yes,
所以我现在不打算告诉你，呃，是的，

1006
00:49:31,390 --> 00:49:34,270
so you're gonna think about BLEU and the- the ways in which it's
所以你要考虑BLEU以及它的方式

1007
00:49:34,270 --> 00:49:38,810
imperfect but useful. Yeah.
不完美但有用。是啊。

1008
00:49:38,940 --> 00:49:43,660
So would one- one n-gram, be a one to one equivalency?
那么一个n-gram会是一对一的等价吗？

1009
00:49:43,660 --> 00:49:44,680
What?
什么？

1010
00:49:44,680 --> 00:49:47,420
Would a one n-gram be a one to one equivalency?
一个n-gram是一对一的等价吗？

1011
00:49:47,420 --> 00:49:51,250
The question is, would a one n-gram be a one-to-one equivalency?
问题是，一个n-gram是一对一的等价吗？

1012
00:49:51,250 --> 00:49:54,355
I'm not sure I understand the question. You're asking about alignment or something else?
我不确定我理解这个问题。你问的是对齐还是其他什么？

1013
00:49:54,355 --> 00:49:58,300
Uh, just trying to get an idea about how they're doing n-gram checks,
呃，只是想知道他们如何进行n-gram检查，

1014
00:49:58,300 --> 00:50:03,520
is it doing all n-gram permutations or is it doing like window size of one?
它是在做所有n-gram排列还是像窗口大小一样？

1015
00:50:03,520 --> 00:50:05,980
Well, I guess one- one n-gram it doesn't
嗯，我猜一个n-gram它没有

1016
00:50:05,980 --> 00:50:07,780
make a difference because you can't permute a one-gram.
有所作为，因为你无法置换一克。

1017
00:50:07,780 --> 00:50:08,830
Okay. So you're asking for examples,
好的。所以你要问的例子，

1018
00:50:08,830 --> 00:50:10,990
for four grams are they checking, uh,
四克是他们检查，呃，

1019
00:50:10,990 --> 00:50:13,090
whether this exact sequence of four paired or
是否这四个配对或精确序列

1020
00:50:13,090 --> 00:50:15,280
any permutation of it, its exact sequences?
它的任何排列，它的确切序列？

1021
00:50:15,280 --> 00:50:19,660
So by definition, n-grams are sequences where the order matters.
因此，根据定义，n-gram是顺序重要的序列。

1022
00:50:19,660 --> 00:50:23,875
Okay. All right.
好的。行。

1023
00:50:23,875 --> 00:50:26,395
So, uh, that's how you evaluate machine translation.
所以，呃，这就是你评估机器翻译的方式。

1024
00:50:26,395 --> 00:50:28,450
So now you can understand this metric of how we
所以现在你可以理解我们的这个指标

1025
00:50:28,450 --> 00:50:30,550
evaluate our progress on machine translation,
评估我们在机器翻译方面的进展

1026
00:50:30,550 --> 00:50:34,030
um, I can show you this graph and you might understand what it means.
嗯，我可以告诉你这个图表，你可能会理解这意味着什么。

1027
00:50:34,030 --> 00:50:35,800
So this is a, uh,
所以这是一个，呃，

1028
00:50:35,800 --> 00:50:41,860
bar plot which shows in a nutshell how NMT changed the machine translation,
条形图简洁地显示了NMT如何改变机器翻译，

1029
00:50:41,860 --> 00:50:44,110
uh, landscape in just a few years.
呃，几年后的景观。

1030
00:50:44,110 --> 00:50:48,070
So in this plot, we've got BLEU score is the Y-axis.
所以在这个情节中，我们得到的BLEU得分是Y轴。

1031
00:50:48,070 --> 00:50:50,860
Uh, and you have two different types of SMT
呃，你有两种不同类型的SMT

1032
00:50:50,860 --> 00:50:53,560
which is the red and the dark blue, uh, bar plots.
这是红色和深蓝色，呃，酒吧情节。

1033
00:50:53,560 --> 00:50:55,165
And what's happening is,
而且发生了什么，

1034
00:50:55,165 --> 00:50:56,725
uh, in 2015, uh,
呃，在2015年，呃，

1035
00:50:56,725 --> 00:51:01,630
Neural MT enters the scene for the first time and it isn't doi- doing as well as SMT,
神经MT第一次进入场景，它不像SMT那样做，

1036
00:51:01,630 --> 00:51:04,735
and then the next year it's suddenly outperforming SMT.
然后第二年它突然超越了SMT。

1037
00:51:04,735 --> 00:51:08,380
And here these are BLEU scores on some particular fixed dataset like,
这里是一些特定固定数据集的BLEU分数，如：

1038
00:51:08,380 --> 00:51:10,450
uh, a shared task that many people were,
呃，很多人都有共同的任务，

1039
00:51:10,450 --> 00:51:12,100
um, submitting systems for.
嗯，提交系统。

1040
00:51:12,100 --> 00:51:15,130
[NOISE] So the main thing to notice here is
[NOISE]所以这里要注意的主要是

1041
00:51:15,130 --> 00:51:18,550
that the progress that was being made by SMT systems was,
SMT系统正在取得的进展是，

1042
00:51:18,550 --> 00:51:21,490
you know, a fairly gentle increase in BLEU year-by-year.
你知道，BLEU逐年增加相当温和。

1043
00:51:21,490 --> 00:51:23,305
And then in just one year,
然后在短短一年内，

1044
00:51:23,305 --> 00:51:25,435
NMT arrives and is suddenly doing,
NMT到达并突然发生，

1045
00:51:25,435 --> 00:51:27,040
uh, much more rapid progress.
呃，进展更快。

1046
00:51:27,040 --> 00:51:31,790
So I think this justifies why the picture of the meteor maybe isn't too Jurassic here.
所以我认为这证明了为什么流星的图片在这里可能不是太侏罗纪。

1047
00:51:33,000 --> 00:51:38,545
So you could in fact call NMT the biggest success story of NLP in deep learning.
因此，你可以将NMT称为NLP深度学习的最大成功故事。

1048
00:51:38,545 --> 00:51:40,930
Uh, because if you think about the history of this,
呃，因为如果你想一想这个的历史，

1049
00:51:40,930 --> 00:51:45,850
NMT went from being a fringe research activity in 2014 to being actually
NMT从2014年的边缘研究活动变为实际

1050
00:51:45,850 --> 00:51:51,250
the leading standard methodfor machine translation in the world in 2016.
2016年全球机器翻译的领先标准方法。

1051
00:51:51,250 --> 00:51:53,109
In particular, in 2014,
特别是在2014年，

1052
00:51:53,109 --> 00:51:54,880
the first seq2seq paper was published.
第一篇seq2seq论文发表。

1053
00:51:54,880 --> 00:51:59,185
And in 2016, Google Translate switches from SMT to NMT.
2016年，谷歌翻译从SMT切换到NMT。

1054
00:51:59,185 --> 00:52:03,175
This is a pretty remarkable turnaround for just two years.
这是一个非常显着的转变仅仅两年。

1055
00:52:03,175 --> 00:52:05,245
So this is amazing,
所以这太棒了，

1056
00:52:05,245 --> 00:52:07,060
not just because it was a quick turnaround,
不只是因为它是一个快速的转变，

1057
00:52:07,060 --> 00:52:10,180
but also if you think about the level of human effort involved.
但是如果你考虑到人类努力程度的话。

1058
00:52:10,180 --> 00:52:12,310
Uh, these SMT systems, for example
呃，这些SMT系统，例如

1059
00:52:12,310 --> 00:52:15,010
the Google Translate SMT system was built by
谷歌翻译SMT系统是由

1060
00:52:15,010 --> 00:52:17,965
doubtless hundreds of engineers over many years.
多年来无疑是数百名工程师。

1061
00:52:17,965 --> 00:52:23,875
And this, uh, this SMT system was outperformed by an NMT system that was trained by,
而且，呃，这个SMT系统的性能优于受过以下培训的NMT系统，

1062
00:52:23,875 --> 00:52:27,265
uh, you know, relatively few like a handful of engineers in a few months.
呃，你知道，几个月内几乎没有几个工程师。

1063
00:52:27,265 --> 00:52:29,680
So I'm not- I'm not diminishing how difficult it is to,
所以我不是 - 我没有减少它的难度，

1064
00:52:29,680 --> 00:52:31,165
um, build NMT systems,
嗯，建立NMT系统，

1065
00:52:31,165 --> 00:52:33,670
and certainly I'm sure Google's NMT system
当然我肯定谷歌的NMT系统

1066
00:52:33,670 --> 00:52:36,745
today is built by more than a handful of engineers in a few months.
今天是由几个月内的少数工程师建造的。

1067
00:52:36,745 --> 00:52:38,650
I'm sure it's a very big operation now.
我相信现在这是一个非常大的操作。

1068
00:52:38,650 --> 00:52:40,509
Uh, but when NMT,
呃，但是当NMT，

1069
00:52:40,509 --> 00:52:42,355
uh, began to outperform SMT,
呃，开始超越SMT，

1070
00:52:42,355 --> 00:52:45,220
it was pretty remarkable how it was able to do that,
如何能够做到这一点非常了不起，

1071
00:52:45,220 --> 00:52:48,550
uh, based on the amount of effort involved. Yeah.
呃，基于所涉及的努力量。是啊。

1072
00:52:48,550 --> 00:52:55,785
Given the [inaudible] cons of NMT has there been research on  combining the two and if there is, what does that look like?
鉴于NMT的[听不清]缺点，有关于两者结合的研究，如果有的话，那看起来是什么样的？

1073
00:52:55,785 --> 00:53:00,010
Yeah, great. The question is given that we know that there are
很好。问题是我们知道有

1074
00:53:00,010 --> 00:53:03,865
some disadvantages of NMT even in comparison to SMT,
与SMT相比，NMT的一些缺点，

1075
00:53:03,865 --> 00:53:05,770
is there any work on combining the two?
是否有任何合并两者的工作？

1076
00:53:05,770 --> 00:53:07,300
So, yes. I think there is.
所以，是的。我觉得有。

1077
00:53:07,300 --> 00:53:10,390
Uh, there's a lot of NMT research ongoing and in particular,
呃，正在进行大量的NMT研究，特别是

1078
00:53:10,390 --> 00:53:13,195
people sometimes focus on these particular shortcomings.
人们有时会关注这些特殊的缺点。

1079
00:53:13,195 --> 00:53:16,660
And, uh, there's a lot of work in kind of taking techniques
而且，呃，有很多工作采取技术

1080
00:53:16,660 --> 00:53:20,200
and ideas and wisdom from the many decades of SMT research,
几十年的SMT研究中的思想和智慧，

1081
00:53:20,200 --> 00:53:23,005
and then integrating them into the new NMT paradigm.
然后将它们整合到新的NMT范例中。

1082
00:53:23,005 --> 00:53:30,590
So yes. [NOISE]. Okay.
是的[噪声]。好的。

1083
00:53:31,050 --> 00:53:35,290
So is machine translation solved?
机器翻译解决了吗？

1084
00:53:35,290 --> 00:53:38,470
Can we all go home? I think the answer is clearly no.
我们都可以回家吗？我认为答案显然不是。

1085
00:53:38,470 --> 00:53:42,085
Uh, NMT definitely is not doing machine translation perfectly.
呃，NMT肯定不是完美的机器翻译。

1086
00:53:42,085 --> 00:53:46,480
So, um, just to highlight some of the difficulties that remained with NMT.
所以，嗯，只是为了强调NMT仍然存在的一些困难。

1087
00:53:46,480 --> 00:53:48,610
Uh, one is out-of-vocabulary words.
呃，一个是词汇外的单词。

1088
00:53:48,610 --> 00:53:52,090
Uh, this is a kind of basic problem but it is pretty tricky.
呃，这是一个基本的问题，但它非常棘手。

1089
00:53:52,090 --> 00:53:54,445
You know, what do you do if you're trying to translate
你知道，如果你想翻译，你会怎么做？

1090
00:53:54,445 --> 00:53:57,610
a sentence that contains a word that is not in your source vocabulary,
包含不在源词汇表中的单词的句子，

1091
00:53:57,610 --> 00:54:00,925
or what if you're trying to produce a word that's not in your target vocabulary?
或者如果你试图产生一个不在你的目标词汇中的单词怎么办？

1092
00:54:00,925 --> 00:54:03,625
Um, there's certainly been lots of work on doing this,
嗯，这方面肯定有很多工作要做，

1093
00:54:03,625 --> 00:54:05,920
and you're going to hear later in the class how you
你会在课堂上听到你的意见

1094
00:54:05,920 --> 00:54:08,305
might try to attack this with for example,
可能试图用例如攻击这个，

1095
00:54:08,305 --> 00:54:10,435
uh, sub-word modeling can make it easier.
呃，子词建模可以使它更容易。

1096
00:54:10,435 --> 00:54:13,435
Uh, but this is a significant problem.
呃，但这是一个重大问题。

1097
00:54:13,435 --> 00:54:15,775
Another one is domain mismatch.
另一个是域名不匹配。

1098
00:54:15,775 --> 00:54:20,170
So let's suppose that you train your machine translation system on a bunch of fairly,
所以我们假设您在一堆公平的情况下培训您的机器翻译系统，

1099
00:54:20,170 --> 00:54:21,760
uh, formal text, like let's say,
呃，正式文本，就像我们说的那样，

1100
00:54:21,760 --> 00:54:23,785
uh, Wikipedia or something like that.
呃，维基百科或类似的东西。

1101
00:54:23,785 --> 00:54:26,560
Uh, but then you try to deploy it to translate
呃，但是你试着将它部署到翻译

1102
00:54:26,560 --> 00:54:29,590
informal text, like people chatting on Twitter or something.
非正式文本，就像在Twitter上聊天的人一样。

1103
00:54:29,590 --> 00:54:32,080
Then often, you'll find that it doesn't perform very well
然后经常，你会发现它表现不佳

1104
00:54:32,080 --> 00:54:34,480
on this different domain because you've got a domain mismatch.
在这个不同的域上，因为你有一个域不匹配。

1105
00:54:34,480 --> 00:54:37,060
Uh, so that's quite a big problem.
呃，这是一个很大的问题。

1106
00:54:37,060 --> 00:54:40,465
Another one is maintaining context over longer text.
另一个是保持较长文本的上下文。

1107
00:54:40,465 --> 00:54:43,120
So everything we've talked about so far has assumed that you were
所以我们到目前为止所谈论的一切都假设你是

1108
00:54:43,120 --> 00:54:45,805
just translating a single sentence to a single sentence,
只是将一个句子翻译成一个句子，

1109
00:54:45,805 --> 00:54:48,655
and there's no other wider context.
而且没有其他更广泛的背景。

1110
00:54:48,655 --> 00:54:50,560
Uh, but, you know if you want to use
呃，但是，你知道你是否想要使用

1111
00:54:50,560 --> 00:54:54,640
a machine translation system to translate a whole news article and maybe even a book,
翻译整个新闻文章甚至书籍的机器翻译系统，

1112
00:54:54,640 --> 00:54:58,270
then you're probably going to want to use the context that came in
然后你可能会想要使用进来的上下文

1113
00:54:58,270 --> 00:55:03,475
previous sentences in order to translate things correctly in the current sentence.
以前的句子，以便在当前句子中正确翻译。

1114
00:55:03,475 --> 00:55:06,070
So, uh, this is an active area of research,
所以，呃，这是一个活跃的研究领域，

1115
00:55:06,070 --> 00:55:08,410
how can you get an NMT system to condition on
你如何让NMT系统成为现实

1116
00:55:08,410 --> 00:55:12,620
larger pieces of context without it becoming too expensive and so on?
更大的背景而不会变得太昂贵等等？

1117
00:55:13,200 --> 00:55:16,600
Another difficulty is low-resource language pairs.
另一个困难是低资源语言对。

1118
00:55:16,600 --> 00:55:19,690
Um, everything we've talked about so far has assumed that you have
嗯，到目前为止我们所谈论的一切都假设你有

1119
00:55:19,690 --> 00:55:22,915
access to a very large parallel corpus, but what if you don't?
访问一个非常大的平行语料库，但如果你不这样做呢？

1120
00:55:22,915 --> 00:55:25,180
What if you are trying to translate to or from a language that
如果您尝试翻译或使用某种语言，该怎么办？

1121
00:55:25,180 --> 00:55:27,430
has relatively little text available,
可用的文字相对较少，

1122
00:55:27,430 --> 00:55:29,380
um, online for example?
嗯，在网上举个例子？

1123
00:55:29,380 --> 00:55:31,630
So that can be pretty difficult.
所以这可能非常困难。

1124
00:55:31,630 --> 00:55:35,170
Here a few examples of machine translation screwing up,
这里有一些机器翻译搞砸的例子，

1125
00:55:35,170 --> 00:55:37,525
uh, with specific errors.
呃，有特定的错误。

1126
00:55:37,525 --> 00:55:43,195
So, here's an example of how common sense is really difficult for NMT systems.
所以，这是NMT系统常识难以实现的一个例子。

1127
00:55:43,195 --> 00:55:45,640
On the left, we have the English phrase paper jam,
在左边，我们有英文短语卡纸，

1128
00:55:45,640 --> 00:55:47,380
which means when your printer
这意味着你的打印机

1129
00:55:47,380 --> 00:55:50,830
gets jammed up with paper and it's all, uh, tangled inside.
被纸张堵住，这就是全部，呃，纠缠在里面。

1130
00:55:50,830 --> 00:55:52,255
And then on the right,
然后在右边，

1131
00:55:52,255 --> 00:55:54,940
we have a very literal translation of that into Spanish,
我们有一个非常直译的西班牙语，

1132
00:55:54,940 --> 00:55:56,395
and it's essentially saying jam,
它本质上是说堵塞，

1133
00:55:56,395 --> 00:55:58,420
edible jam made of paper,
由纸制成的可食用果酱，

1134
00:55:58,420 --> 00:56:01,240
which clearly isn't the right interpretation.
这显然不是正确的解释。

1135
00:56:01,240 --> 00:56:04,060
So here, we have an NMT system that's just doing
所以在这里，我们有一个正在做的NMT系统

1136
00:56:04,060 --> 00:56:07,120
very literal translation and clearly doesn't have any notion of common sense.
非常字面的翻译，显然没有任何常识的概念。

1137
00:56:07,120 --> 00:56:11,365
You can't make jam from paper. Uh, here's another example.
你不能用纸制作果酱。呃，这是另一个例子。

1138
00:56:11,365 --> 00:56:14,350
NMT can pick up biases in the training data.
NMT可以获取训练数据中的偏差。

1139
00:56:14,350 --> 00:56:16,585
We already talked about this at the,
我们已经在讨论了这个问题，

1140
00:56:16,585 --> 00:56:18,310
uh, the word embedding level,
呃，嵌入级别，

1141
00:56:18,310 --> 00:56:19,885
the representation of words.
单词的表示。

1142
00:56:19,885 --> 00:56:22,255
Uh, but it can also be a problem at the you know,
呃，但是你知道它也可能是一个问题，

1143
00:56:22,255 --> 00:56:24,175
the sentence level when you're translating things.
翻译事物时的句子级别。

1144
00:56:24,175 --> 00:56:26,170
So here in this example,
所以在这个例子中，

1145
00:56:26,170 --> 00:56:27,340
uh, on the left,
呃，在左边，

1146
00:56:27,340 --> 00:56:30,460
we have two sentences in Malay that roughly mean,
我们在马来语中有两个句子大致意思是，

1147
00:56:30,460 --> 00:56:34,090
uh, they work as a nurse, and they work as a programmer.
呃，他们是一名护士，他们是一名程序员。

1148
00:56:34,090 --> 00:56:35,350
The point is on the left,
这一点在左边，

1149
00:56:35,350 --> 00:56:38,260
there is no information about gender in the pronouns.
代词中没有关于性别的信息。

1150
00:56:38,260 --> 00:56:40,645
But when it gets translated to English,
但当它被翻译成英文时，

1151
00:56:40,645 --> 00:56:43,150
then we've suddenly got gender coming out of nowhere,
然后我们突然间突然出现性别，

1152
00:56:43,150 --> 00:56:46,105
she works as a nurse, and he works as a programmer.
她是一名护士，他是一名程序员。

1153
00:56:46,105 --> 00:56:48,430
This is likely happening because in our training data,
这可能是因为在我们的培训数据中，

1154
00:56:48,430 --> 00:56:52,090
we had more examples of female nurses and male programmers.
我们有更多的女护士和男性程序员的例子。

1155
00:56:52,090 --> 00:56:55,600
So you can understand why from a machine learning, uh,
所以你可以从机器学习中理解为什么，呃，

1156
00:56:55,600 --> 00:56:57,880
maximizing the objective point of view the,
最大化客观的观点，

1157
00:56:57,880 --> 00:56:59,995
uh, English language model has learned to do that.
呃，英语语言模型已经学会了这样做。

1158
00:56:59,995 --> 00:57:02,530
But the problem here is this isn't good machine translation.
但这里的问题是这不是好机器翻译。

1159
00:57:02,530 --> 00:57:08,590
Uh, here the system is making up information that was not present in the source sentence.
呃，这里系统正在编写源句中没有的信息。

1160
00:57:08,590 --> 00:57:10,720
So this is certainly an error that
所以这肯定是一个错误

1161
00:57:10,720 --> 00:57:13,780
the machine translation shouldn't be doing because it's just simply inaccurate.
机器翻译不应该这样做，因为它只是不准确。

1162
00:57:13,780 --> 00:57:17,575
And even worse, it's propagating, uh, gender roles.
更糟糕的是，它正在传播，呃，性别角色。

1163
00:57:17,575 --> 00:57:20,860
Here's another pretty weird example.
这是另一个非常奇怪的例子。

1164
00:57:20,860 --> 00:57:29,680
[LAUGHTER] What is happening here?
[大笑]这里发生了什么？

1165
00:57:29,680 --> 00:57:30,880
Uh, on the left,
呃，在左边，

1166
00:57:30,880 --> 00:57:33,100
we have a nonsense sentence,
我们有一个无意义的句子，

1167
00:57:33,100 --> 00:57:35,970
this is just kind of a syllable repeated.
这只是一种重复的音节。

1168
00:57:35,970 --> 00:57:38,610
And we're supposedly translating from Somali.
我们据说是从索马里翻译的。

1169
00:57:38,610 --> 00:57:40,590
Uh, and then we're asking to translate this into
呃，然后我们要求把它翻译成

1170
00:57:40,590 --> 00:57:43,350
English and then we're getting this out of nowhere.
英语，然后我们无处不在。

1171
00:57:43,350 --> 00:57:46,665
Um, as the name of the Lord was written in the Hebrew language,
嗯，正如耶和华的名字是用希伯来语写的，

1172
00:57:46,665 --> 00:57:48,675
it was written in the language of the Hebrew nation,
它是用希伯来民族的语言写成的，

1173
00:57:48,675 --> 00:57:51,720
and you might be thinking "Where on earth did that come from?"
而你可能会想到“这究竟来自哪里？”

1174
00:57:51,720 --> 00:57:54,675
And in fact, this got reported in the media as you know,
事实上，据媒体报道，你知道，

1175
00:57:54,675 --> 00:57:57,390
Google Translate wants to convert you to its religion or whatever.
谷歌翻译希望将你转变为宗教或其他任何东西。

1176
00:57:57,390 --> 00:58:00,960
[LAUGHTER] Um, so for sure,
[笑声]嗯，所以可以肯定的是，

1177
00:58:00,960 --> 00:58:02,175
it is very startling.
这是非常令人吃惊的。

1178
00:58:02,175 --> 00:58:05,860
But the thing is there's actually quite a reasonable explanation.
但问题实际上是非常合理的解释。

1179
00:58:05,860 --> 00:58:08,830
So what's going on here is that,
那么这里发生的是，

1180
00:58:08,830 --> 00:58:12,175
um, often for low resource languages,
嗯，通常用于低资源语言，

1181
00:58:12,175 --> 00:58:14,095
such as for example Somali, um,
例如索马里，嗯，

1182
00:58:14,095 --> 00:58:19,420
one of the best resources of parallel text is the Bible.
平行文本的最佳资源之一是圣经。

1183
00:58:19,420 --> 00:58:24,040
So you train for example Somali to English using the Bible as a training text,
因此，您使用圣经作为训练文本训练索马里语为英语，

1184
00:58:24,040 --> 00:58:25,615
maybe among other texts.
也许在其他文本中。

1185
00:58:25,615 --> 00:58:27,520
Okay, that's the first puzzle piece.
好的，那是第一个拼图。

1186
00:58:27,520 --> 00:58:30,460
But the other puzzle piece is the nonsensical input.
但另一个难题是无意义的输入。

1187
00:58:30,460 --> 00:58:35,020
So when the input isn't really Somali or any kind of text, right?
所以当输入不是真正的索马里或任何一种文本时，对吧？

1188
00:58:35,020 --> 00:58:36,895
It's just the same syllable over and over.
它只是一遍又一遍的音节。

1189
00:58:36,895 --> 00:58:40,615
Then the NMT system doesn't really have anything sensible to condition on.
然后NMT系统真的没有任何明智的条件。

1190
00:58:40,615 --> 00:58:42,595
Its basically nonsense, it's just noise.
它基本上是胡说八道，只是噪音。

1191
00:58:42,595 --> 00:58:44,545
So what does the NMT system do?
那NMT系统做了什么？

1192
00:58:44,545 --> 00:58:48,100
Right? It can't really use, it can't really  condition on the source sentence.
对？它无法真正使用，它无法真正对源语句进行条件限制。

1193
00:58:48,100 --> 00:58:51,475
So what it does, is it just uses the English language model, right?
它的作用是什么，它只是使用英语模型，对吗？

1194
00:58:51,475 --> 00:58:54,460
You can think of it as like the English language model of the decoder RNN
您可以将其视为解码器RNN的英语语言模型

1195
00:58:54,460 --> 00:58:57,835
just kind of goes into autopilot and starts generating random text,
只是进入自动驾驶仪并开始生成随机文本，

1196
00:58:57,835 --> 00:59:00,760
kind of like we saw last week when we saw, uh,
有点像我们上周看到的那样，呃，

1197
00:59:00,760 --> 00:59:02,380
a language model trained on Obama's speeches or
奥巴马演讲或演讲的语言模型

1198
00:59:02,380 --> 00:59:04,720
Harry Potter would just generate texts in that style.
哈利波特只会以这种风格生成文本。

1199
00:59:04,720 --> 00:59:06,760
That's kind of what's happening here with the Bible,
这就是圣经中发生的事情，

1200
00:59:06,760 --> 00:59:08,740
because we don't have any useful information,
因为我们没有任何有用的信息，

1201
00:59:08,740 --> 00:59:11,320
um, from the sentence on the left.
嗯，从左边的句子。

1202
00:59:11,320 --> 00:59:15,700
Um, so, this is an example why, uh,
嗯，这是一个例子，为什么，呃，

1203
00:59:15,700 --> 00:59:18,730
neural machine translation in particular makes these kinds of errors,
特别是神经机器翻译会产生这些错误，

1204
00:59:18,730 --> 00:59:20,890
uh, because the system is uninterpretable.
呃，因为系统无法解释。

1205
00:59:20,890 --> 00:59:23,665
So you don't know that this is going to happen until it happens,
所以你不知道这种情况会发生，直到它发生，

1206
00:59:23,665 --> 00:59:25,450
and perhaps Google didn't know this was going to
也许谷歌不知道这是怎么回事

1207
00:59:25,450 --> 00:59:27,640
happen until it happened and it got reported.
发生直到它发生并报告。

1208
00:59:27,640 --> 00:59:32,410
Um, so this is one downside of uninterpretability is that really weird effects can
嗯，所以这是一个无法解释的缺点，就是真正奇怪的效果可以

1209
00:59:32,410 --> 00:59:34,690
happen and you don't see them coming and it's not
发生了，你没有看到他们来了，事实并非如此

1210
00:59:34,690 --> 00:59:37,210
always even easy to explain why they happened. Yeah?
总是很容易解释他们为什么会这样。是吗？

1211
00:59:37,210 --> 00:59:38,830
[inaudible].
[听不见的。

1212
00:59:38,830 --> 00:59:47,530
Ah, the question is what happens if you did translate from Irish?
啊，问题是如果你从爱尔兰语翻译会发生什么？

1213
00:59:47,530 --> 00:59:49,810
I suppose that's the part where Google tries to autodetect
我想这是Google尝试自动检测的部分

1214
00:59:49,810 --> 00:59:53,050
the language, maybe it thinks that ag ag ag is more like Irish than Somali,
这种语言，也许它认为ag ag ag更像爱尔兰人而不是索马里人，

1215
00:59:53,050 --> 00:59:56,530
[LAUGHTER] I imagine if you did put Irish to English,
[大笑]我想如果你把爱尔兰语用于英语，

1216
00:59:56,530 --> 00:59:58,615
there's probably more, uh,
可能还有更多，呃，

1217
00:59:58,615 --> 01:00:00,130
training data for Irish to English.
爱尔兰语到英语的培训数据。

1218
01:00:00,130 --> 01:00:02,725
So maybe it wouldn't be so Bible-focused.
所以也许它不会那么以圣经为中心。

1219
01:00:02,725 --> 01:00:05,680
Um, yeah, and there's a lot of examples of these online where
嗯，是的，这里有很多这些在线的例子

1220
01:00:05,680 --> 01:00:09,260
you do different kinds of nonsense syllables in different languages.
你用不同的语言做不同种类的无意义音节。

1221
01:00:09,870 --> 01:00:14,035
So there's a lot of, uh, challenges remaining in NMT.
所以NMT还有很多呃挑战。

1222
01:00:14,035 --> 01:00:16,360
And, uh, the research continues.
而且，呃，研究还在继续。

1223
01:00:16,360 --> 01:00:21,370
So NMT, I think remains one of the flagship tasks for NLP Deep Learning.
所以NMT，我认为仍然是NLP深度学习的旗舰任务之一。

1224
01:00:21,370 --> 01:00:24,400
In fact, NMT research has pioneered many of
事实上，NMT研究已经开创了许多领域

1225
01:00:24,400 --> 01:00:27,205
the successful innovations of NLP Deep Learning in general.
NLP深度学习的成功创新。

1226
01:00:27,205 --> 01:00:30,400
Uh, so today in 2019, uh,
呃，今天是2019年，呃，

1227
01:00:30,400 --> 01:00:32,710
NMT research continues to thrive, there's still many,
NMT研究继续蓬勃发展，还有很多，

1228
01:00:32,710 --> 01:00:35,530
many papers, uh, published all the time on NMT.
许多论文，呃，一直在NMT上发表。

1229
01:00:35,530 --> 01:00:38,260
And in fact, uh, researchers have found lots of
事实上，呃，研究人员发现了很多

1230
01:00:38,260 --> 01:00:41,770
improvements to the fairly vanilla seq2seq models that I've shown you today.
我今天向您展示的相当香草的seq2seq模型的改进。

1231
01:00:41,770 --> 01:00:43,285
Uh, but in fact,
呃，但事实上，

1232
01:00:43,285 --> 01:00:45,849
there is one improvement that is so integral
有一个改进是如此不可或缺的

1233
01:00:45,849 --> 01:00:48,655
to seq2seq that you could regard it as the new vanilla.
到seq2seq，你可以把它当作新的香草。

1234
01:00:48,655 --> 01:00:50,785
And that's the improvement we're going to learn about today,
这就是我们今天要学到的改进，

1235
01:00:50,785 --> 01:00:53,050
and it's called attention.
它被称为关注。

1236
01:00:53,050 --> 01:00:58,225
Okay. So section three is on attention. What is attention?
好的。所以第三节正在引起关注。什么是关注？

1237
01:00:58,225 --> 01:01:01,855
First, I'm going to motivate why we need this thing called attention.
首先，我要激励为什么我们需要这个叫注意的东西。

1238
01:01:01,855 --> 01:01:05,440
So let's look at this diagram that we saw before of sequence-to-sequence.
那么让我们看一下我们在序列到序列之前看到的这个图。

1239
01:01:05,440 --> 01:01:07,705
And remember when we assumed that this,
记得当我们假设这个时，

1240
01:01:07,705 --> 01:01:09,370
uh, encoding of the source sentence,
呃，源句的编码，

1241
01:01:09,370 --> 01:01:13,315
the, t he one in the orange box is going to represent the whole sentence.
橙色框中的一个将代表整个句子。

1242
01:01:13,315 --> 01:01:18,320
Uh, can anyone volunteer a problem you can see with this architecture?
呃，任何人都可以自愿提出这个架构可以看到的问题吗？

1243
01:01:19,230 --> 01:01:23,530
In particular perhaps, a problem with this idea that that single vector is
特别是，这个想法可能存在单个向量的问题

1244
01:01:23,530 --> 01:01:27,100
the encoding of the source sentence. Yeah?
源句的编码。是吗？

1245
01:01:27,100 --> 01:01:39,385
[inaudible]
[听不见]

1246
01:01:39,385 --> 01:01:41,650
Okay, so the answer is something like, um, you're only looking at
好吧，所以答案是这样的，嗯，你只是在看

1247
01:01:41,650 --> 01:01:44,095
one word, you mean like the last word in the source sentence?
一句话，你的意思就像源句中的最后一个字？

1248
01:01:44,095 --> 01:01:45,955
And you're not seeing more information.
你没有看到更多的信息。

1249
01:01:45,955 --> 01:01:47,710
Yeah some- it's, it's something like that.
是的，它就是这样的。

1250
01:01:47,710 --> 01:01:49,045
Any other ideas? Yep.
还有其他想法吗？是的。

1251
01:01:49,045 --> 01:01:50,905
We might have lost information in
我们可能已经丢失了信息

1252
01:01:50,905 --> 01:01:53,575
the beginning of the sentence by the time you get to the end.
到达结束时句子的开头。

1253
01:01:53,575 --> 01:01:56,530
Yeah. You might have lost information from [NOISE] the beginning of the sentence by,
是啊。您可能在句子开头[NOISE]丢失了信息，

1254
01:01:56,530 --> 01:01:59,740
by the time you get to the end, especially if it was longer than four words.
到你结束的时候，特别是如果它超过四个字。

1255
01:01:59,740 --> 01:02:01,561
Right. I think these are different ways of saying a
对。我认为这些是说不同的方式

1256
01:02:01,561 --> 01:02:06,730
similar idea [NOISE] which is that we have a kind of informational bottleneck.
类似的想法[NOISE]，这是我们有一种信息瓶颈。

1257
01:02:06,730 --> 01:02:10,600
Uh, we're forcing all of the information about the source sentence to be captured
呃，我们强制要捕获关于源句的所有信息

1258
01:02:10,600 --> 01:02:14,675
in this single vector because that's the only thing that gets given to the decoder.
在这个单一的向量中，因为这是唯一被赋予解码器的东西。

1259
01:02:14,675 --> 01:02:17,265
If some information about source sentence isn't in our vector,
如果有关源句的某些信息不在我们的向量中，

1260
01:02:17,265 --> 01:02:20,610
then there's no way the decoder is gonna be able to translate it correctly.
那么解码器就无法正确翻译它。

1261
01:02:20,610 --> 01:02:21,930
So this is the, yeah,
所以这就是，是的，

1262
01:02:21,930 --> 01:02:23,310
this is an informational bottleneck.
这是一个信息瓶颈。

1263
01:02:23,310 --> 01:02:25,650
[NOISE] It's putting kind of too much pressure on
[NOISE]这会给你带来太大的压力

1264
01:02:25,650 --> 01:02:29,180
this single vector to be a good representation [NOISE] of the encoder.
这个单一矢量是编码器的良好表示[NOISE]。

1265
01:02:29,180 --> 01:02:32,290
So this is the motivation for attention.
所以这是引起注意的动力。

1266
01:02:32,290 --> 01:02:36,445
Attention is a neural technique and it provides a solution to the bottleneck problem.
注意力是一种神经技术，它为瓶颈问题提供了解决方案。

1267
01:02:36,445 --> 01:02:39,520
The core idea is that on each step [NOISE] of the decoder,
核心思想是解码器的每一步[NOISE]，

1268
01:02:39,520 --> 01:02:42,459
you're gonna use a direct connection to the encoder
你将使用与编码器的直接连接

1269
01:02:42,459 --> 01:02:46,520
to focus on a particular part of the source sequence.
专注于源序列的特定部分。

1270
01:02:47,100 --> 01:02:50,560
So first I'm gonna show you what attention is
首先，我要告诉你注意力是什么

1271
01:02:50,560 --> 01:02:53,140
via a diagram so that's kind of an intuitive explanation.
通过图表，这是一种直观的解释。

1272
01:02:53,140 --> 01:02:55,405
And then I'm gonna show you the equations later.
然后我会告诉你方程式。

1273
01:02:55,405 --> 01:02:59,455
So here's how seq- sequence-to-sequence with attention works.
所以这里是seq-序列到序列的注意力如何工作。

1274
01:02:59,455 --> 01:03:01,960
So on the first step of our decoder,
那么在解码器的第一步，

1275
01:03:01,960 --> 01:03:05,095
uh, we have our first decoder hidden state.
呃，我们有第一个解码器隐藏状态。

1276
01:03:05,095 --> 01:03:08,320
So what we do, is we take the dot-product between
所以我们做的是，我们采取点之间的产品

1277
01:03:08,320 --> 01:03:11,590
that decoder hidden state and the first [NOISE] encoder hidden state.
解码器隐藏状态和第一个[NOISE]编码器隐藏状态。

1278
01:03:11,590 --> 01:03:13,090
And then we get something called
然后我们得到了一些东西

1279
01:03:13,090 --> 01:03:16,570
an attention score which I'm representing by a dot. So that's a scalar.
一个注意分数，我用点代表。这是一个标量。

1280
01:03:16,570 --> 01:03:18,070
[NOISE] And in fact,
[NOISE]事实上，

1281
01:03:18,070 --> 01:03:20,305
we take the dot-product between the decoder hidden state
我们采用解码器隐藏状态之间的点积

1282
01:03:20,305 --> 01:03:23,050
and all of the encoder hidden states.
和所有编码器隐藏状态。

1283
01:03:23,050 --> 01:03:27,550
So this means that we get one attention score or one scalar for each of these,
所以这意味着我们每个都得到一个注意力得分或一个标量，

1284
01:03:27,550 --> 01:03:29,980
uh, source words effectively.
呃，有效地说出来的话。

1285
01:03:29,980 --> 01:03:35,980
So next what we do, is we take those four number scores and we apply the softmax,
接下来我们做的是，我们采用这四个数字得分，我们应用softmax，

1286
01:03:35,980 --> 01:03:38,020
uh, distribution, uh, the softmax function to
呃，分配，呃，softmax功能

1287
01:03:38,020 --> 01:03:41,155
them and then we get a probability distribution.
然后我们得到一个概率分布。

1288
01:03:41,155 --> 01:03:45,685
So here, I'm going to represent that probability distribution as a bar chart.
所以在这里，我将把概率分布表示为条形图。

1289
01:03:45,685 --> 01:03:50,200
Um, and we call this the attention distribution and this one sums up to 1.
嗯，我们称之为注意力分布，这个总和为1。

1290
01:03:50,200 --> 01:03:54,805
So here, you can see that most of the probability mass is on the first word.
所以在这里，您可以看到大多数概率质量都在第一个单词上。

1291
01:03:54,805 --> 01:03:58,300
And that kinda makes sense because our first word essentially means "he" and,
这有点意义，因为我们的第一个字基本上意味着“他”，

1292
01:03:58,300 --> 01:04:02,375
uh, were gonna be producing the word "he" first in our target sentence.
呃，在我们的目标句子中首先要生成“他”这个词。

1293
01:04:02,375 --> 01:04:04,965
So once we've got this attention distribution,
所以一旦我们得到了这种关注分布，

1294
01:04:04,965 --> 01:04:11,295
uh, we're going to use it to produce something called the attention output.
呃，我们将用它来产生一种称为注意力输出的东西。

1295
01:04:11,295 --> 01:04:15,400
So the idea is that the attention output is a weighted sum of
所以这个想法是注意力输出是加权和

1296
01:04:15,400 --> 01:04:20,515
the encoder hidden states and the weighting is the attention distribution.
编码器隐藏状态和加权是注意分布。

1297
01:04:20,515 --> 01:04:22,450
So I've got these dotted arrows that go from
所以我有这些虚线箭头

1298
01:04:22,450 --> 01:04:24,400
the attention distribution to the attention output,
关注分布到注意力输出，

1299
01:04:24,400 --> 01:04:26,380
probably there should be dotted arrows also from
可能还应该有点箭头

1300
01:04:26,380 --> 01:04:28,270
the encoder RNN but that's hard to depict.
编码器RNN，但很难描述。

1301
01:04:28,270 --> 01:04:32,215
[NOISE] But the idea is that you're summing up these encoder RNN, uh,
[NOISE]但是这个想法是你在总结这些编码器RNN，呃，

1302
01:04:32,215 --> 01:04:34,480
hidden states, [NOISE] but you're gonna weight each
隐藏的状态，[NOISE]，但你会减肥

1303
01:04:34,480 --> 01:04:37,555
one according to how much attention distribution it has on them.
一个根据它对它们的注意力分布。

1304
01:04:37,555 --> 01:04:41,620
So this means that your attention output which is a single vector is going to be
所以这意味着您的注意力输出将是一个单一的向量

1305
01:04:41,620 --> 01:04:45,370
mostly containing information from the hidden states that had high attention.
主要包含隐藏状态的信息，这些信息受到高度关注。

1306
01:04:45,370 --> 01:04:49,730
In this case, it's gonna be mostly information from the first hidden state.
在这种情况下，它将主要来自第一个隐藏状态的信息。

1307
01:04:52,230 --> 01:04:54,370
So after you do this,
所以你这样做之后

1308
01:04:54,370 --> 01:04:58,390
you're going to use the attention output to influence your prediction of the next word.
你将使用注意力输出来影响你对下一个词的预测。

1309
01:04:58,390 --> 01:05:00,520
So what you usually do is you concatenate
所以你通常做的就是连接

1310
01:05:00,520 --> 01:05:03,610
the attention output with your decoder hidden state and then,
关注输出与您的解码器隐藏状态然后，

1311
01:05:03,610 --> 01:05:06,070
uh, use that kind of concatenated pair in the way
呃，在路上使用那种级联对

1312
01:05:06,070 --> 01:05:08,905
you would have used the decoder [NOISE] hidden state alone before.
你之前会单独使用解码器[NOISE]隐藏状态。

1313
01:05:08,905 --> 01:05:11,590
So that way you can get your probability distribution,
这样你就可以获得概率分布，

1314
01:05:11,590 --> 01:05:14,215
uh, y hat 1 of what's coming next.
呃，接下来会发生什么。

1315
01:05:14,215 --> 01:05:17,170
So as before, we can use that to sample your next word.
和以前一样，我们可以用它来抽样你的下一个单词。

1316
01:05:17,170 --> 01:05:19,360
[NOISE] So on the next step,
[NOISE]所以在下一步，

1317
01:05:19,360 --> 01:05:20,740
you just do the same thing again.
你只是再做同样的事情。

1318
01:05:20,740 --> 01:05:22,690
You've got your second decoder hidden state.
你有第二个解码器隐藏状态。

1319
01:05:22,690 --> 01:05:25,480
Again, you take dot-product with all of the encoder hidden states.
再次，您将带有所有编码器隐藏状态的点积。

1320
01:05:25,480 --> 01:05:28,225
You take softmax over that to the get attention distribution.
你可以通过softmax获得注意力分配。

1321
01:05:28,225 --> 01:05:30,685
And here, you can see the attention distribution is different.
在这里，你可以看到注意力分布是不同的。

1322
01:05:30,685 --> 01:05:33,595
We're putting more attention on, uh, the,
我们正在更加关注，呃，

1323
01:05:33,595 --> 01:05:36,520
the word entarté because we're about to produce the word hit.
entarté这个词，因为我们即将产生命中这个词。

1324
01:05:36,520 --> 01:05:38,590
Uh, but we're also attending a little bit to
呃，但我们也参加了一些

1325
01:05:38,590 --> 01:05:42,295
the second word a because that's telling us that hit is a past tense.
第二个词是因为那告诉我们命中是过去式。

1326
01:05:42,295 --> 01:05:46,495
So a cool thing that's happening here is we're getting [NOISE] a soft alignment.
所以这里发生的一件很酷的事情就是让我们的[NOISE]柔和对齐。

1327
01:05:46,495 --> 01:05:49,555
If you remember when we looked at alignment in SMT systems,
如果你还记得我们在SMT系统中查看对齐的时候，

1328
01:05:49,555 --> 01:05:50,665
it was mostly this, uh,
主要是这个，呃，

1329
01:05:50,665 --> 01:05:54,595
hard binary thing with on or off, either these words are aligned or they're not.
打开或关闭硬二进制的东西，这些单词是对齐的还是它们不对齐。

1330
01:05:54,595 --> 01:05:59,140
Here, you have a much more flexible soft notion of alignments where,
在这里，你有一个更加灵活的对齐软概念，其中，

1331
01:05:59,140 --> 01:06:01,300
uh, each word kind of has a distribution over
呃，每个词都有分布

1332
01:06:01,300 --> 01:06:04,135
the corresponding words in the source sentence.
源句中的相应单词。

1333
01:06:04,135 --> 01:06:06,805
So another thing to note kind of a side note,
另外要注意的是旁注，

1334
01:06:06,805 --> 01:06:08,440
is that sometimes, uh,
有时候，呃，

1335
01:06:08,440 --> 01:06:11,920
we take the attention output from the previous hidden state, uh,
我们把前一个隐藏状态的注意力输出，呃，

1336
01:06:11,920 --> 01:06:16,495
and we kind of feed it into the decoder again along with the usual word.
我们将它与通常的词一起再次送入解码器。

1337
01:06:16,495 --> 01:06:19,510
So that would mean you take the attention output from the first step and kind of
所以这意味着你从第一步开始注意力输出

1338
01:06:19,510 --> 01:06:23,080
concatenate it to the word vector for he and then use it in the decoder.
将它连接到他的单词vector，然后在解码器中使用它。

1339
01:06:23,080 --> 01:06:26,515
Uh, the reason for this is sometimes it's useful to have this, uh,
呃，这个的原因有时候这个有用，呃，

1340
01:06:26,515 --> 01:06:30,715
information from the, the attention on the previous step on the next step.
来自的信息，关注下一步的上一步。

1341
01:06:30,715 --> 01:06:33,475
So I'm telling you this because this is something we do in Assignment 4
所以我告诉你这是因为这是我们在作业4中所做的

1342
01:06:33,475 --> 01:06:37,130
and it's a fairly common technique but also sometimes people don't do it.
这是一种相当常见的技术，但有时人们却不这样做。

1343
01:06:37,170 --> 01:06:40,390
Okay. So, um, the theory is,
好的。所以，嗯，理论是，

1344
01:06:40,390 --> 01:06:42,010
that you just do this attention,
那你只是注意了，

1345
01:06:42,010 --> 01:06:44,245
uh, computation on every step.
呃，每一步的计算。

1346
01:06:44,245 --> 01:06:46,690
And on each step, you're going to be attending to different things.
在每一步，你将会参加不同的事情。

1347
01:06:46,690 --> 01:06:48,850
So in our example on this third step,
所以在我们这第三步的例子中，

1348
01:06:48,850 --> 01:06:51,730
we look at m' which means me when we
我们看看m'，这意味着我

1349
01:06:51,730 --> 01:06:53,350
produce me and then on the last
生产我，然后在最后

1350
01:06:53,350 --> 01:06:55,105
three [NOISE] we're probably mostly just gonna be looking at this,
三个[NOISE]我们大概只是要看这个，

1351
01:06:55,105 --> 01:06:58,600
uh, fertile word entarté to produce hit me with a pie.
呃，生产的肥沃的词entarté用馅饼打我。

1352
01:06:58,600 --> 01:07:02,005
[NOISE] I'm gonna keep going because we don't have a lot of time.
[NOISE]我会继续前进，因为我们没有很多时间。

1353
01:07:02,005 --> 01:07:04,690
Uh, so here are the equations to describe attention.
呃，所以这里有描述注意力的方程式。

1354
01:07:04,690 --> 01:07:06,520
Uh, I think it's probably easier to look at these in
呃，我觉得看看这些可能更容易

1355
01:07:06,520 --> 01:07:09,025
your own time later rather than look at them in the lecture now.
你自己的时间，而不是现在在讲座中看他们。

1356
01:07:09,025 --> 01:07:10,870
But these are the equations that essentially
但这些是基本上的方程式

1357
01:07:10,870 --> 01:07:13,330
say the same thing as what the diagram just said.
说出与图中所说的相同的东西。

1358
01:07:13,330 --> 01:07:17,110
So you have your encoder hidden states h_1 up to h_N.
所以你的编码器隐藏状态h_1高达h_N。

1359
01:07:17,110 --> 01:07:19,795
And then on timestep t of the decoder,
然后在解码器的时间步长，

1360
01:07:19,795 --> 01:07:22,855
we also have a decoder hidden state, uh, S_t.
我们还有一个解码器隐藏状态，呃，S_t。

1361
01:07:22,855 --> 01:07:27,100
So we're gonna get the attention score which we're gonna call et by taking
因此，我们将得到我们将要采取的注意力得分

1362
01:07:27,100 --> 01:07:30,730
the dot-product of your decoder hidden state with each of the encoder hidden states.
解码器隐藏状态的点积与每个编码器隐藏状态。

1363
01:07:30,730 --> 01:07:32,425
[NOISE] And that gives you, uh,
[NOISE]这会给你，呃，

1364
01:07:32,425 --> 01:07:34,900
a vector of same length as the, uh,
一个与呃相同长度的向量，

1365
01:07:34,900 --> 01:07:39,355
encoder [NOISE] sentence because you've got one score per source word.
编码器[NOISE]句子，因为每个源词有一个分数。

1366
01:07:39,355 --> 01:07:42,580
Next you take softmax over these scores
接下来，你将softmax超过这些分数

1367
01:07:42,580 --> 01:07:45,280
to get attention distribution that sums up to 1,
获得总计达1的注意力分配，

1368
01:07:45,280 --> 01:07:47,635
and we call that alpha.
我们称之为alpha。

1369
01:07:47,635 --> 01:07:50,860
And then you use alpha to take a weighted sum of
然后你使用alpha来获取加权和

1370
01:07:50,860 --> 01:07:54,535
the encoder hidden states and that gives you your attention output.
编码器隐藏状态，并提供您的注意力输出。

1371
01:07:54,535 --> 01:07:57,490
So [NOISE] the attention output which we call a is a vector that's
所以[NOISE]我们称之为a的注意力输出是一个向量

1372
01:07:57,490 --> 01:08:01,645
the same size as your encoder hidden states.
与编码器隐藏状态大小相同。

1373
01:08:01,645 --> 01:08:06,610
Lastly, you take your attention output a and then you, [NOISE] uh,
最后，你把你的注意力输出a然后你，[NOISE]呃，

1374
01:08:06,610 --> 01:08:09,640
concatenate it with your decoder hidden states and then
然后将它与解码器隐藏状态连接起来

1375
01:08:09,640 --> 01:08:13,940
proceed with that as you were taught before in the no attention model.
正如你之前在无注意模型中所教导的那样继续。

1376
01:08:14,520 --> 01:08:17,770
So attention, if it's not clear, it's pretty cool.
所以注意，如果不清楚的话，这很酷。

1377
01:08:17,770 --> 01:08:19,510
It has a number of advantages.
它有许多优点。

1378
01:08:19,510 --> 01:08:23,905
So one advantage is that attention just significantly improves NMT performance.
因此，一个优点是注意力可以显着提高NMT的性能。

1379
01:08:23,905 --> 01:08:25,675
And the main reason why it improves it,
它改善它的主要原因，

1380
01:08:25,675 --> 01:08:28,690
is because it turns out it's super useful to allow the decoder [NOISE] to
是因为事实证明，允许解码器[NOISE]是非常有用的

1381
01:08:28,690 --> 01:08:32,215
focus on certain parts of the source sentence when it's translating.
在翻译时，关注源句的某些部分。

1382
01:08:32,215 --> 01:08:33,940
And you can see why this makes sense, right?
你可以看出为什么这是有道理的，对吧？

1383
01:08:33,940 --> 01:08:35,740
Because there's a very natural notion of alignment,
因为有一种非常自然的对齐概念，

1384
01:08:35,740 --> 01:08:38,619
and if you can focus on the specific word or words you're translating,
如果您可以专注于您正在翻译的特定单词或单词，

1385
01:08:38,619 --> 01:08:40,585
you can probably do a better job.
你可以做得更好。

1386
01:08:40,585 --> 01:08:44,410
Another reason why attention is cool is that [NOISE] it solves the bottleneck problem.
注意力很酷的另一个原因是[NOISE]它解决了瓶颈问题。

1387
01:08:44,410 --> 01:08:46,540
Uh, we were noting that the problem with having
呃，我们注意到了问题

1388
01:08:46,540 --> 01:08:49,870
a single vector that has to represent the entire source sentence [NOISE] and that's
一个向量，必须代表整个源句[NOISE]，那就是

1389
01:08:49,870 --> 01:08:52,780
the only way information can pass from encoder to decoder
信息可以从编码器传递到解码器的唯一方式

1390
01:08:52,780 --> 01:08:56,590
means that if that encoding isn't very good then, uh, you're not gonna do well.
意味着，如果那个编码不是很好，那么，呃，你不会做得好。

1391
01:08:56,590 --> 01:08:59,140
So by contrast in, uh, with attention,
相比之下，呃，注意力，

1392
01:08:59,140 --> 01:09:01,690
the decoder can look directly at the encoder and
解码器可以直接查看编码器和

1393
01:09:01,690 --> 01:09:04,240
the source sentence and translate without the bottleneck.
源句和翻译没有瓶颈。

1394
01:09:04,240 --> 01:09:07,060
[NOISE]
[噪声]

1395
01:09:07,060 --> 01:09:10,915
Another great thing about attention is that it helps with the vanishing gradient problem,
注意力的另一个好处是它有助于消除梯度问题，

1396
01:09:10,915 --> 01:09:13,240
especially if your sentences are quite long.
特别是如果你的句子很长。

1397
01:09:13,240 --> 01:09:16,060
Uh, the reason why attention helps is because you have
呃，注意力有帮助的原因是因为你有

1398
01:09:16,060 --> 01:09:19,390
these direct connections between the decoder and the encoder,
解码器和编码器之间的这些直接连接，

1399
01:09:19,390 --> 01:09:21,145
kind of over many time steps.
多种时间步骤。

1400
01:09:21,145 --> 01:09:22,825
So it's like a shortcut connection.
所以它就像一个快捷方式连接。

1401
01:09:22,825 --> 01:09:24,730
And just as we learned last time about, uh,
正如我们上次了解到的那样，呃，

1402
01:09:24,730 --> 01:09:28,255
skip connections being [NOISE] useful for reducing vanishing gradient.
跳过连接[NOISE]有助于减少消失的梯度。

1403
01:09:28,255 --> 01:09:29,590
Here it's the same notion.
这是相同的概念。

1404
01:09:29,590 --> 01:09:31,270
We have these, uh, long distance
我们有这些，呃，长距离

1405
01:09:31,270 --> 01:09:34,580
[NOISE] direct connections that help the gradients flow better.
[NOISE]直接连接，有助于梯度更好地流动。

1406
01:09:34,620 --> 01:09:38,230
Another great thing about attention is it provides some interpretability.
关注的另一个好处是它提供了一些可解释性。

1407
01:09:38,230 --> 01:09:41,155
Uh, if you look at the attention distribution,
呃，如果看一下注意力分布，

1408
01:09:41,155 --> 01:09:42,880
often you've produced your translation.
通常你已经制作了翻译。

1409
01:09:42,880 --> 01:09:46,360
Uh, you can see what the decoder was focusing on on each step.
呃，你可以看到解码器在每一步都关注的是什么。

1410
01:09:46,360 --> 01:09:48,820
So for example if we run our system and we translate our,
所以，例如，如果我们运行我们的系统，我们翻译我们的，

1411
01:09:48,820 --> 01:09:50,125
our running example here,
我们这里运行的例子，

1412
01:09:50,125 --> 01:09:51,670
then we can produce a plot,
然后我们可以制作一个情节，

1413
01:09:51,670 --> 01:09:54,490
kind of like this that shows the attention distribution.
有点像这样显示了注意力的分布。

1414
01:09:54,490 --> 01:09:58,255
So here, dark means high attention and white means low attention.
所以在这里，黑暗意味着高度关注，白色意味着低关注。

1415
01:09:58,255 --> 01:10:00,295
So you might see something like this where,
所以你可能会看到这样的东西，

1416
01:10:00,295 --> 01:10:03,550
um, it was, it was focusing on the different words and different steps.
嗯，它是，它专注于不同的单词和不同的步骤。

1417
01:10:03,550 --> 01:10:06,160
And this is basically the same kind of
这基本上是相同的

1418
01:10:06,160 --> 01:10:08,710
plot that we had earlier with a hard notion of alignment,
我们之前有一个难以对齐的概念的情节，

1419
01:10:08,710 --> 01:10:11,245
uh, in SNT except that we, uh,
呃，在SNT，除了我们，呃，

1420
01:10:11,245 --> 01:10:14,380
we have more flexibility to have a more soft version of alignment
我们可以更灵活地使用更柔和的对齐版本

1421
01:10:14,380 --> 01:10:17,995
like for example when we produce the English word hit,
例如，当我们生成英文单词hit时，

1422
01:10:17,995 --> 01:10:22,400
perhaps we were mostly looking at entarte, but we're also looking at a little bit of A.
也许我们主要看的是entarte，但我们也在考虑一点A.

1423
01:10:22,880 --> 01:10:25,980
So this, uh, means that we're getting,
所以，这，呃，意味着我们得到了，

1424
01:10:25,980 --> 01:10:27,315
uh, alignment for free.
呃，免费对齐。

1425
01:10:27,315 --> 01:10:31,424
And the reason I say for free is because when you remember the SNT systems,
我免费说的原因是因为当你记得SNT系统时，

1426
01:10:31,424 --> 01:10:33,210
the whole point there is that you had to learn
总的来说，你必须要学习

1427
01:10:33,210 --> 01:10:36,580
an alignment system deliberately and separately.
故意和分开地对准系统。

1428
01:10:36,580 --> 01:10:38,410
You had to define the notion of alignment,
你必须定义对齐的概念，

1429
01:10:38,410 --> 01:10:40,060
you had to define the model of calculating,
你必须定义计算模型，

1430
01:10:40,060 --> 01:10:42,865
what the probability of different alignments were and train it.
不同路线的概率是多少，并训练它。

1431
01:10:42,865 --> 01:10:47,035
Whereas here, we never told the NMT system about alignments.
在这里，我们从未告诉NMT系统有关对齐。

1432
01:10:47,035 --> 01:10:49,105
We never explicitly trained an alignment system.
我们从未明确培训过校准系统。

1433
01:10:49,105 --> 01:10:52,225
We never had a loss function that tells you how good your alignment was.
我们从未有过一个能够告诉你对齐有多好的损失函数。

1434
01:10:52,225 --> 01:10:55,600
We just gave the NMT system the apparatus to
我们刚给了NMT系统设备

1435
01:10:55,600 --> 01:10:59,245
do something like alignments and told it to maximize the,
做一些像对齐，告诉它最大化，

1436
01:10:59,245 --> 01:11:02,380
uh, the cross-entropy loss for doing machine translation.
呃，机器翻译的交叉熵损失。

1437
01:11:02,380 --> 01:11:05,560
And then the network just learned alignment by itself.
然后网络本身就学会了对齐。

1438
01:11:05,560 --> 01:11:07,900
I think this is the coolest thing about attention,
我认为这是关注最酷的事情，

1439
01:11:07,900 --> 01:11:12,410
is that it's learned some structure in a somewhat unsupervised way.
是因为它以某种无人监督的方式学习了一些结构。

1440
01:11:13,290 --> 01:11:15,520
Okay, so in the last few minutes,
好的，所以在最后几分钟，

1441
01:11:15,520 --> 01:11:18,340
I'm just going to, uh, generalize the notion of attention.
我只是想，呃，概括一下注意力的概念。

1442
01:11:18,340 --> 01:11:21,280
Because it turns out that attention is actually a very general, uh,
因为事实证明注意力实际上是非常普遍的，呃，

1443
01:11:21,280 --> 01:11:25,315
deep learning technique that you can apply in lots of different circumstances.
深度学习技巧，你可以在很多不同的情况下应用。

1444
01:11:25,315 --> 01:11:27,610
So you've seen that attention is a great way to improve
所以你已经看到注意力是改善的好方法

1445
01:11:27,610 --> 01:11:29,875
the sequence-to-sequence model for MT,
MT的序列到序列模型，

1446
01:11:29,875 --> 01:11:32,710
but you can actually use attention for other architectures that aren't
但实际上，您可以将注意力集中用于其他非体系结构

1447
01:11:32,710 --> 01:11:36,220
seq2seq and also tasks that aren't MT.
seq2seq以及非MT的任务。

1448
01:11:36,220 --> 01:11:37,975
So to understand this,
所以要理解这一点，

1449
01:11:37,975 --> 01:11:42,085
I'm going to somewhat redefine attention to a more general definition.
我将在某种程度上重新关注更普遍的定义。

1450
01:11:42,085 --> 01:11:44,185
So here's our more general definition.
所以这是我们更一般的定义。

1451
01:11:44,185 --> 01:11:46,885
Suppose you have a set of values,
假设你有一组值，

1452
01:11:46,885 --> 01:11:48,190
each of which is a vector,
每个都是矢量，

1453
01:11:48,190 --> 01:11:51,370
and you also have a single vector in which you're calling the query.
并且您还有一个向量，您可以在其中调用查询。

1454
01:11:51,370 --> 01:11:53,650
Then attention is a way, uh,
然后注意是一种方式，呃，

1455
01:11:53,650 --> 01:11:56,350
to compute a weighted sum of the values.
计算值的加权和。

1456
01:11:56,350 --> 01:11:59,380
But the way you weight it is dependent on the query.
但你加权的方式取决于查询。

1457
01:11:59,380 --> 01:12:03,925
[NOISE] So we often phrase this,
[NOISE]所以我们经常说这个，

1458
01:12:03,925 --> 01:12:06,655
uh, as saying that the query is attending to the values.
呃，说查询是关注值的。

1459
01:12:06,655 --> 01:12:09,520
The idea being that you have all this information that's in the values and
这个想法是你拥有价值观和价值观中的所有信息

1460
01:12:09,520 --> 01:12:13,810
the query is somehow determining how it's going to pay attention to the values.
查询以某种方式确定它将如何关注值。

1461
01:12:13,810 --> 01:12:16,345
So for example in seq2seq, uh,
所以例如在seq2seq，呃，

1462
01:12:16,345 --> 01:12:19,435
the decoder hidden state is the query.
解码器隐藏状态是查询。

1463
01:12:19,435 --> 01:12:22,450
Uh the decoder hidden state on a particular time step is the query
呃，特定时间步骤的解码器隐藏状态是查询

1464
01:12:22,450 --> 01:12:27,050
and is attending to all the encoder hidden states which are the values.
并且正在关注作为值的所有编码器隐藏状态。

1465
01:12:27,090 --> 01:12:29,515
All right, here's our definition again.
好的，这是我们的定义。

1466
01:12:29,515 --> 01:12:33,445
So here's a way to kind of understand this intuitively, two alternative ways.
所以这是一种直观地理解这种方式的两种方式。

1467
01:12:33,445 --> 01:12:35,530
One is to think of it like this.
一个是这样想的。

1468
01:12:35,530 --> 01:12:38,290
You could think of it as the weighted sum is like
您可以将其视为加权和就像

1469
01:12:38,290 --> 01:12:42,235
a selective summary of the information in the values.
有价值信息的选择性摘要。

1470
01:12:42,235 --> 01:12:45,760
And I say selective because your choice of how much you choose
我说有选择性因为你选择了多少

1471
01:12:45,760 --> 01:12:49,345
to draw from each value depends on the attention distribution.
从每个值中抽取取决于注意力分布。

1472
01:12:49,345 --> 01:12:53,095
Uh, so the distribution, uh, depends on the query.
呃，所以分发，呃，取决于查询。

1473
01:12:53,095 --> 01:12:57,595
So the query is determining how much you're going to select from different, uh, values.
因此查询确定您将从不同的值中选择多少值。

1474
01:12:57,595 --> 01:13:01,555
And this is kind of similar to LSTM that learned about earlier this week.
这有点类似于本周早些时候了解到的LSTM。

1475
01:13:01,555 --> 01:13:04,300
LSTMs rule based on the idea of a gate that, uh,
LSTM基于门的概念来统治，呃，

1476
01:13:04,300 --> 01:13:07,150
[NOISE] that defines how much information shou-
[NOISE]定义了多少信息 -

1477
01:13:07,150 --> 01:13:08,845
should [NOISE] come from different elements.
应该[噪音]来自不同的元素。

1478
01:13:08,845 --> 01:13:11,395
And the gate depends on the context.
大门取决于背景。

1479
01:13:11,395 --> 01:13:14,710
So the strength of LSTMs came from the idea that based on the context,
因此，LSTM的优势来自于基于背景的观点，

1480
01:13:14,710 --> 01:13:17,185
you decide where you're going to draw information from.
你决定从哪里提取信息。

1481
01:13:17,185 --> 01:13:19,435
And this is kind of like the same idea.
这有点像同样的想法。

1482
01:13:19,435 --> 01:13:24,070
The second way to think about attention is you could say that it's a way to obtain
考虑注意力的第二种方式是你可以说它是一种获得方式

1483
01:13:24,070 --> 01:13:28,420
a fixed-size representation from an arbitrary set of representations.
来自任意一组表示的固定大小表示。

1484
01:13:28,420 --> 01:13:29,965
So when I say arbitrary sets,
所以当我说任意组时，

1485
01:13:29,965 --> 01:13:32,875
I'm saying we have this set of vectors called the values, right?
我说我们有这组称为值的向量，对吧？

1486
01:13:32,875 --> 01:13:34,090
And you could have 10 values.
你可以有10个值。

1487
01:13:34,090 --> 01:13:35,380
You could have 100 values.
你可以有100个值。

1488
01:13:35,380 --> 01:13:36,400
You can have, uh,
你可以拥有，呃，

1489
01:13:36,400 --> 01:13:38,305
any [NOISE] arbitrary number of these vectors.
任何[NOISE]任意数量的这些向量。

1490
01:13:38,305 --> 01:13:42,460
But attention gives you a way to get a single vector,
但注意力为你提供了获得单个矢量的方法，

1491
01:13:42,460 --> 01:13:48,350
um, summary of that which is the attention output, uh, using your query.
嗯，使用您的查询，注意输出的总结，呃。

1492
01:13:48,420 --> 01:13:51,055
Okay, uh, so the last thing, uh,
好的，呃，最后一件事，呃，

1493
01:13:51,055 --> 01:13:53,710
is that there's actually several variants of
实际上有几种变体

1494
01:13:53,710 --> 01:13:57,655
attention and this is something were are going to look at a little in Assignment 4.
注意，这是分配4中的一些内容。

1495
01:13:57,655 --> 01:13:59,920
So in our more general setting,
所以在我们更一般的环境中，

1496
01:13:59,920 --> 01:14:02,290
we've seen that we have some values in the query.
我们已经看到我们在查询中有一些值。

1497
01:14:02,290 --> 01:14:06,265
Doing attention always involves computing the attention [NOISE] scores,
注意力总是涉及计算注意力[NOISE]得分，

1498
01:14:06,265 --> 01:14:09,880
and then you apply softmax to get the attention distribution.
然后你应用softmax来获得注意力分配。

1499
01:14:09,880 --> 01:14:13,870
And then you use that attention distribution to take a weighted sum.
然后你使用该注意力分布来加权。

1500
01:14:13,870 --> 01:14:17,240
So this is, uh, always the outline of how attention works.
所以这就是，呃，总是注意力如何运作的轮廓。

1501
01:14:17,240 --> 01:14:19,920
The part that can be different is this, uh, number one.
可能不同的部分是这个，呃，第一。

1502
01:14:19,920 --> 01:14:23,520
There are multiple ways you can compute the scores.
您可以通过多种方式计算分数。

1503
01:14:23,520 --> 01:14:27,135
So, uh, last slide,
那么，呃，最后一张幻灯片，

1504
01:14:27,135 --> 01:14:30,035
here all the different ways you can repeat the scores.
这里有所有不同的方法可以重复分数。

1505
01:14:30,035 --> 01:14:34,615
So the first one which you've already seen today is basic dot-product attention.
所以你今天看到的第一个是基本的点产品关注。

1506
01:14:34,615 --> 01:14:38,575
And the idea here is that [NOISE] the score for a particular, a particular value,
这里的想法是[NOISE]特定值，特定值的得分，

1507
01:14:38,575 --> 01:14:42,850
HI, is just the dot-product of the query and that particular value.
HI，只是查询和特定值的点积。

1508
01:14:42,850 --> 01:14:46,720
[NOISE] And, uh, in particular this assumes that the size
[NOISE]而且，呃，特别是这个假设的大小

1509
01:14:46,720 --> 01:14:48,190
of your query vector and the size of
您的查询向量和大小

1510
01:14:48,190 --> 01:14:50,740
your value vector has to be the same because you're taking dot-product.
你的价值向量必须相同，因为你正在使用点积。

1511
01:14:50,740 --> 01:14:54,865
[NOISE] Another, uh, version of,
[NOISE]另一个，呃，版本的，

1512
01:14:54,865 --> 01:14:57,685
uh, attention is called multiplicative attention.
呃，注意力被称为乘法注意力。

1513
01:14:57,685 --> 01:15:00,385
And here, the idea is that the score of your, uh,
在这里，这个想法是你的得分，呃，

1514
01:15:00,385 --> 01:15:03,220
value HI, is going to be this, uh,
价值HI，将是这个，呃，

1515
01:15:03,220 --> 01:15:06,625
bi-linear function of your query and that value.
查询和该值的双线性函数。

1516
01:15:06,625 --> 01:15:08,500
So in particular, we're pushing this weight matrix in
所以特别是，我们正在推动这个权重矩阵

1517
01:15:08,500 --> 01:15:10,690
the middle and that's a learnable parameter.
中间，这是一个可学习的参数。

1518
01:15:10,690 --> 01:15:14,650
You're learning the best way matric- ma- weight matrix in order to get the scores,
你正在学习基质矩阵的最佳方法来获得分数，

1519
01:15:14,650 --> 01:15:16,885
the attention scores that are useful.
有用的注意力得分。

1520
01:15:16,885 --> 01:15:19,855
The last one is called additive attention.
最后一个被称为加性注意。

1521
01:15:19,855 --> 01:15:26,215
So what's happening here is that the score of the value HI is, uh,
那么这里发生的是HI值的得分是，呃，

1522
01:15:26,215 --> 01:15:28,435
you get it by applying
你通过申请得到它

1523
01:15:28,435 --> 01:15:33,400
a linear transformation to both the value and the query and then you add them together.
对值和查询进行线性转换，然后将它们一起添加。

1524
01:15:33,400 --> 01:15:35,875
And then you put them through a non-linearity like tanh.
然后你把它们放在像tanh这样的非线性中。

1525
01:15:35,875 --> 01:15:37,390
And then lastly, uh,
最后，呃，

1526
01:15:37,390 --> 01:15:40,270
you take that vector and you take the dot-product with
你拿那个矢量，你拿点产品

1527
01:15:40,270 --> 01:15:43,720
a weight vector to give you a single number that is the score.
一个权重向量，给你一个分数的数字。

1528
01:15:43,720 --> 01:15:46,450
[NOISE] So here, you've got
[NOISE]所以在这里，你有

1529
01:15:46,450 --> 01:15:47,980
two different weight matrices and [NOISE]
两种不同的权重矩阵和[NOISE]

1530
01:15:47,980 --> 01:15:51,190
also a weight vector which are the learnable parameters.
也是一个可学习参数的权重向量。

1531
01:15:51,190 --> 01:15:55,765
One thing that's different here is that there's kind of an additional hyperparameter,
这里有一点不同的是，还有一种额外的超参数，

1532
01:15:55,765 --> 01:15:58,000
which is the attention dimensionality.
这是注意维度。

1533
01:15:58,000 --> 01:16:00,295
So [NOISE] that's kind of, uh, the,
所以[NOISE]那是什么，呃，

1534
01:16:00,295 --> 01:16:04,810
I think it's the heights of the W1 and W2 and this is the length of V, right?
我认为它是W1和W2的高度，这是V的长度，对吗？

1535
01:16:04,810 --> 01:16:07,420
You can choose what size that dimension is.
您可以选择尺寸的大小。

1536
01:16:07,420 --> 01:16:10,225
It's kind of like a hidden layer in the computation.
它有点像计算中的隐藏层。

1537
01:16:10,225 --> 01:16:14,995
So, um, you can decide how big you want that intermediate representation to be.
所以，嗯，你可以决定你想要多大的中间表示。

1538
01:16:14,995 --> 01:16:17,470
Okay, so I'm not going to tell you any more about that because that's
好的，所以我不会再告诉你了，因为那是

1539
01:16:17,470 --> 01:16:19,510
actually one of the questions in the assignment, uh,
实际上是作业中的一个问题，呃，

1540
01:16:19,510 --> 01:16:20,860
Assignment 4 is to think about
作业4是要考虑的

1541
01:16:20,860 --> 01:16:24,010
the relative advantages and disadvantages of these models.
这些模型的相对优缺点。

1542
01:16:24,010 --> 01:16:25,645
[NOISE] Okay.
[NOISE]好的。

1543
01:16:25,645 --> 01:16:26,890
So here's a summary of today.
所以这是今天的总结。

1544
01:16:26,890 --> 01:16:28,150
[NOISE] It really is the last slide,
[NOISE]这真的是最后一张幻灯片，

1545
01:16:28,150 --> 01:16:29,800
[BACKGROUND] second last, last time,
[背景]倒数第二次，最后一次，

1546
01:16:29,800 --> 01:16:30,820
but this was the last slide.
但这是最后一张幻灯片。

1547
01:16:30,820 --> 01:16:33,010
[BACKGROUND] So we learned about the history of MT.
[背景]因此我们了解了MT的历史。

1548
01:16:33,010 --> 01:16:37,960
[NOISE] We learned about how in 2014 [NOISE] Neural MT revolutionized MT.
[NOISE]我们了解了2014年[NOISE]神经MT如何彻底改变MT。

1549
01:16:37,960 --> 01:16:41,005
[NOISE] We learned about how sequence-to-sequence
[NOISE]我们了解了序列到序列的方式

1550
01:16:41,005 --> 01:16:44,395
is the right architecture for NMT and it uses two RNNs.
是NMT的正确架构，它使用两个RNN。

1551
01:16:44,395 --> 01:16:47,080
And lastly, we learned about how attention [NOISE] is a way to focus
最后，我们了解了注意力[NOISE]是一种集中注意力的方式

1552
01:16:47,080 --> 01:16:50,660
on particular parts of the input. All right, thanks.
在输入的特定部分。好的，谢谢。

1553


