1
00:00:04,160 --> 00:00:07,470
Okay. Hi everyone.

2
00:00:07,470 --> 00:00:10,410
Okay. Let's get started.

3
00:00:10,410 --> 00:00:12,480
Um- great to see you all here.

4
00:00:12,480 --> 00:00:17,385
Welcome back for um- week two of CS224N.

5
00:00:17,385 --> 00:00:21,345
Um- so- so this is a little preview

6
00:00:21,345 --> 00:00:25,800
of what's coming up in the class for this week and next week.

7
00:00:25,800 --> 00:00:29,790
Um- you know, this week is perhaps the worst week of this class.

8
00:00:29,790 --> 00:00:36,435
[LAUGHTER]. Um- so in week two of the class our hope is to actually kind of

9
00:00:36,435 --> 00:00:43,360
go through some of the nitty gritty of neural networks and how they're trained,

10
00:00:43,360 --> 00:00:48,795
and how we can learn good neural networks by backpropagation,

11
00:00:48,795 --> 00:00:52,820
which means in particular we're gonna be sort of talking about the training

12
00:00:52,820 --> 00:00:58,100
algorithms and doing calculus to work out gradients from proving them.

13
00:00:58,100 --> 00:01:01,580
Um, so we are looking a bi- a little bit,

14
00:01:01,580 --> 00:01:07,130
at- um- um, word window classification named entity recognition.

15
00:01:07,130 --> 00:01:12,290
So there's a teeny bit of natural language processing in there, but basically,

16
00:01:12,290 --> 00:01:15,290
sort of week two is sort of,

17
00:01:15,290 --> 00:01:18,440
um- math of deep learning and

18
00:01:18,440 --> 00:01:23,045
neural network models and sort of really neural network fundamentals.

19
00:01:23,045 --> 00:01:26,130
Um, but the hope is that that will give you kind

20
00:01:26,130 --> 00:01:29,740
of a good understanding of how these things really work,

21
00:01:29,740 --> 00:01:32,570
and we'll give you all the information you need to do,

22
00:01:32,570 --> 00:01:35,240
um- the coming up homework and so then,

23
00:01:35,240 --> 00:01:38,280
in week three we kind of flips.

24
00:01:38,280 --> 00:01:42,110
So, then week three is going to be mainly about

25
00:01:42,110 --> 00:01:45,020
natural language processing so we then gonna talk about how

26
00:01:45,020 --> 00:01:48,230
to put syntactic structures over sentences,

27
00:01:48,230 --> 00:01:51,215
um- for building dependency parses of sentences

28
00:01:51,215 --> 00:01:54,260
which is then actually what's used in homework three.

29
00:01:54,260 --> 00:01:56,060
So we're chugging along rapidly.

30
00:01:56,060 --> 00:01:59,210
And then we'll talk about this idea of the probability of

31
00:01:59,210 --> 00:02:02,870
a sentence which leads into neural language models.

32
00:02:02,870 --> 00:02:05,640
Um- so on the homeworks.

33
00:02:05,640 --> 00:02:09,665
Homework one was due approximately two minutes ago,

34
00:02:09,665 --> 00:02:15,190
um- so I hope everyone has submitted their homework one, I mean as,

35
00:02:15,190 --> 00:02:18,570
um- one just sort of admonition,

36
00:02:18,570 --> 00:02:21,950
um- in general so you know homework one we

37
00:02:21,950 --> 00:02:25,550
hope you found was a good warm up and not too too hard

38
00:02:25,550 --> 00:02:29,059
and so really be best to get homework one in quickly

39
00:02:29,059 --> 00:02:32,690
rather than to burn lots of your late days doing homework one.

40
00:02:32,690 --> 00:02:35,419
Um, and now right now out on the website,

41
00:02:35,419 --> 00:02:37,160
um there's homework two.

42
00:02:37,160 --> 00:02:39,895
Um so, we are chugging along.

43
00:02:39,895 --> 00:02:44,025
So homework two kind of corresponds to this week's lectures.

44
00:02:44,025 --> 00:02:47,430
So on the first part of that we are expecting you to

45
00:02:47,430 --> 00:02:52,335
grind through some math problems of working out gradient derivations.

46
00:02:52,335 --> 00:02:55,970
Um- and then the second part of that is then implementing

47
00:02:55,970 --> 00:02:59,480
your own version of word2vec making use of NumPy.

48
00:02:59,480 --> 00:03:02,480
And so this time sort of writing a Python program.

49
00:03:02,480 --> 00:03:04,555
It's no longer an IPython notebook.

50
00:03:04,555 --> 00:03:07,575
Um, I encourage you to get early,

51
00:03:07,575 --> 00:03:10,545
um- look at the materials,

52
00:03:10,545 --> 00:03:11,880
um- on the web.

53
00:03:11,880 --> 00:03:15,635
I mean, in particular corresponding to today's lecture there's,

54
00:03:15,635 --> 00:03:18,980
um- some quite good tutorial materials that are available

55
00:03:18,980 --> 00:03:22,250
on the website and so also encourage you to look at those.

56
00:03:22,250 --> 00:03:23,520
[NOISE].

57
00:03:23,520 --> 00:03:24,650
Um- more generally,

58
00:03:24,650 --> 00:03:27,950
just to make a couple more comments on things.

59
00:03:27,950 --> 00:03:32,990
I mean, I guess this is true of a lot of classes at Stanford but,

60
00:03:32,990 --> 00:03:37,460
you know when we get the course reviews for this class we always get

61
00:03:37,460 --> 00:03:42,710
the full spectrum from people who say the class is terrible and it's way too much work,

62
00:03:42,710 --> 00:03:45,050
um- to the people who say it's a really great class,

63
00:03:45,050 --> 00:03:47,180
one of their favorite classes at Stanford,

64
00:03:47,180 --> 00:03:49,520
obvious the instructors care, et cetera.

65
00:03:49,520 --> 00:03:52,910
And I mean, partly this reflects that we get this very,

66
00:03:52,910 --> 00:03:57,750
um- wide range of people coming to take this class on the one hand,

67
00:03:57,750 --> 00:04:01,550
on the right hand margin perhaps we have the physics PhDs,

68
00:04:01,550 --> 00:04:06,140
and on the left hand margin we have some fresh who think this will be fun to do anyway.

69
00:04:06,140 --> 00:04:09,125
Um, we welcome e- we welcome everybody,

70
00:04:09,125 --> 00:04:13,205
um- but in principle this is uh, graduate level class.

71
00:04:13,205 --> 00:04:15,470
You know, that doesn't mean we want to fail people out,

72
00:04:15,470 --> 00:04:20,295
we'd like everyone to succeed but also like graduate level class.

73
00:04:20,295 --> 00:04:22,000
Um- we'd like you to- you know,

74
00:04:22,000 --> 00:04:24,575
take some initiative in your success.

75
00:04:24,575 --> 00:04:26,900
Meaning, if there are things that you need to know to

76
00:04:26,900 --> 00:04:29,375
do the assignments and you don't know them,

77
00:04:29,375 --> 00:04:33,755
um- then you should be taking some initiative to find some tutorials,

78
00:04:33,755 --> 00:04:37,100
come to office hours and talk to people and get

79
00:04:37,100 --> 00:04:41,615
any help you need and learn to sort of for any holes in your knowledge.

80
00:04:41,615 --> 00:04:44,420
Okay. So here's the plan for today.

81
00:04:44,420 --> 00:04:47,280
Um- so that was the course information update.

82
00:04:47,280 --> 00:04:49,675
So you know, is- this is sort of,

83
00:04:49,675 --> 00:04:53,030
in some sense you know machine learning neural nets intro- Just to

84
00:04:53,030 --> 00:04:56,400
try and make sure everyone else is up to speed on all of this stuff.

85
00:04:56,400 --> 00:04:59,510
So I'll talk a little bit about classification, um,

86
00:04:59,510 --> 00:05:02,000
introduce neural networks, um,

87
00:05:02,000 --> 00:05:04,505
little detour into named Entity Recognition,

88
00:05:04,505 --> 00:05:08,580
then sort of show a model of doing um

89
00:05:08,580 --> 00:05:13,330
Window- Word Window classification and then the end part,

90
00:05:13,330 --> 00:05:17,030
we sort of then dive deeper into what kind of tools we

91
00:05:17,030 --> 00:05:21,920
need to learn neural networks and so today um we're gonna go

92
00:05:21,920 --> 00:05:29,060
through um somewhere between review and primer of matrix calculus and then that will

93
00:05:29,060 --> 00:05:32,720
lead into next time's lecture where it's talking

94
00:05:32,720 --> 00:05:37,040
more about backpropagation and computation graphs.

95
00:05:37,040 --> 00:05:42,755
So, yeah. So this material was especially the part at the end.

96
00:05:42,755 --> 00:05:45,740
You know for some people it'll seem really

97
00:05:45,740 --> 00:05:49,430
babyish if- it's the kind of stuff you do every week, um,

98
00:05:49,430 --> 00:05:52,490
for other people it um- might seem impossibly

99
00:05:52,490 --> 00:05:56,300
difficult but hopefully for a large percentage of you in the middle

100
00:05:56,300 --> 00:06:00,290
this will be kind of a useful review of doing this kind of

101
00:06:00,290 --> 00:06:06,125
matrix calculus and the kind of things that we hope that you can do on homework two.

102
00:06:06,125 --> 00:06:09,840
Um, okay. So um, yeah.

103
00:06:09,840 --> 00:06:12,000
So sorry if I'm boring some people.

104
00:06:12,000 --> 00:06:15,300
If you sat through 229 last quarter you

105
00:06:15,300 --> 00:06:19,510
saw um what a classifier was like and hopefully this

106
00:06:19,510 --> 00:06:23,200
will seem familiar but I'm just sort of hoping to try and have

107
00:06:23,200 --> 00:06:27,160
everyone in week two sort of up to speed and on roughly the same page.

108
00:06:27,160 --> 00:06:29,380
So here's our classification setup.

109
00:06:29,380 --> 00:06:34,800
So we have assumed we have a- training data set where we have these um vector

110
00:06:34,800 --> 00:06:41,245
x um of our x points and then for each one of them we have a class.

111
00:06:41,245 --> 00:06:46,344
So the input might be words or sentences documents or something,

112
00:06:46,344 --> 00:06:48,415
there are d to mention vector, um,

113
00:06:48,415 --> 00:06:52,490
the Yi, the labels or classes that we want to

114
00:06:52,490 --> 00:06:57,155
classify to and we've got a set of C classes that we're trying to predict.

115
00:06:57,155 --> 00:07:00,770
And so those might be something like the topic of the document,

116
00:07:00,770 --> 00:07:03,860
the sentiment positive or negative um of

117
00:07:03,860 --> 00:07:08,125
a document or later we'll look a bit more at named entities.

118
00:07:08,125 --> 00:07:12,645
Okay. So if we have that um-

119
00:07:12,645 --> 00:07:18,570
for this sort of intuition is we got this vector space which we again have

120
00:07:18,570 --> 00:07:22,730
a 2D picture and we have points in that vector space which

121
00:07:22,730 --> 00:07:27,620
correspond to Rx items and what we'd want to do is we'll

122
00:07:27,620 --> 00:07:31,370
look at the ones in our training sample and see which ones are

123
00:07:31,370 --> 00:07:35,810
green and red for our two classes here and then we want to sort of learn

124
00:07:35,810 --> 00:07:39,680
a line that could divide between the green and

125
00:07:39,680 --> 00:07:45,670
the red ones as best as possible and that learned line is our classifier.

126
00:07:45,670 --> 00:07:50,285
So on traditional machine learning or statistics we have the sort of

127
00:07:50,285 --> 00:07:54,860
XI vectors that are data items that are purely fixed but

128
00:07:54,860 --> 00:08:00,245
we're going to then multiply those XI by

129
00:08:00,245 --> 00:08:03,440
some estimated weight vector and

130
00:08:03,440 --> 00:08:08,215
that estimated weight vector will then go into a classification decision.

131
00:08:08,215 --> 00:08:10,880
And the classifier that I'm showing here is

132
00:08:10,880 --> 00:08:15,050
a softmax classifier which is almost identical but not quite to

133
00:08:15,050 --> 00:08:20,090
logistic regression classifier which you should've seen in CS 109 or a stats

134
00:08:20,090 --> 00:08:26,110
class or something like that which is giving a probability of different classes.

135
00:08:26,110 --> 00:08:29,780
Okay. And in particular if you've got

136
00:08:29,780 --> 00:08:35,195
a softmax classifier or a logistic- logistic regression classifier,

137
00:08:35,195 --> 00:08:37,805
these are what are called linear classifiers.

138
00:08:37,805 --> 00:08:41,630
So the decision boundary between two classes here

139
00:08:41,630 --> 00:08:45,860
is a line in some suitably high-dimensional space.

140
00:08:45,860 --> 00:08:50,710
So it's a plane or a hyperplane once you've got a bigger expecter.

141
00:08:50,710 --> 00:08:55,140
Okay. So here's our softmax classifier.

142
00:08:55,140 --> 00:08:57,815
Um, and there are sort of two parts to that.

143
00:08:57,815 --> 00:09:02,660
So in the- in the weight matrix double U

144
00:09:02,660 --> 00:09:07,600
we have a row corresponding to each class and then for

145
00:09:07,600 --> 00:09:11,350
that row we're sort of dot-producting it with

146
00:09:11,350 --> 00:09:17,170
our data point vector XI and that's giving us a kind of a score for

147
00:09:17,170 --> 00:09:22,030
how likely it is that the example belongs to that class and then we're

148
00:09:22,030 --> 00:09:27,190
running that through a softmax function and just as we saw on week one,

149
00:09:27,190 --> 00:09:32,825
the softmax takes a bunch of numbers and turn them into a probability distribution.

150
00:09:32,825 --> 00:09:34,605
Does that makes sense to people?

151
00:09:34,605 --> 00:09:38,265
People remember that from last week? Good so far?

152
00:09:38,265 --> 00:09:43,885
Okay. Um, I'm not gonna go to this in detail but I mean,

153
00:09:43,885 --> 00:09:50,285
ah- essentially this is what the logistic regression does as well.

154
00:09:50,285 --> 00:09:58,760
Um, the difference is that here in this setup we have a weight vector um

155
00:09:58,760 --> 00:10:02,590
for each class whereas what

156
00:10:02,590 --> 00:10:07,460
the statisticians doing logistic regression is they say weight,

157
00:10:07,460 --> 00:10:12,425
that gives us one more number of weight vectors than we really need.

158
00:10:12,425 --> 00:10:15,650
We can get away for- for C classes,

159
00:10:15,650 --> 00:10:18,560
we can get away with C minus one weight vectors.

160
00:10:18,560 --> 00:10:20,270
So in particular if you're doing binary

161
00:10:20,270 --> 00:10:23,780
logistic regression you only need one weight vector whereas

162
00:10:23,780 --> 00:10:26,330
this softmax regression formulation you've

163
00:10:26,330 --> 00:10:29,115
actually got two weight vectors one for each class.

164
00:10:29,115 --> 00:10:31,610
Um, so there's that sort of a little difference there which we

165
00:10:31,610 --> 00:10:34,020
could get into but basically the same.

166
00:10:34,020 --> 00:10:38,520
It's just say it's we're either doing softmax or logistic regression, doesn't matter.

167
00:10:38,520 --> 00:10:43,130
Um, so when we're training what we want to

168
00:10:43,130 --> 00:10:48,865
do is we want to be able to predict um the correct class.

169
00:10:48,865 --> 00:10:52,180
And so the way we're gonna do that is we're gonna wanna

170
00:10:52,180 --> 00:10:55,730
train our model so it gives us highest probability as

171
00:10:55,730 --> 00:10:59,750
possible to the correct class and therefore they'll give us

172
00:10:59,750 --> 00:11:06,170
low probability po- as possible um to um the wrong classes.

173
00:11:06,170 --> 00:11:10,850
And so our criterion for doing that is we're going to create

174
00:11:10,850 --> 00:11:16,160
this negative log probability um of our assignments and then

175
00:11:16,160 --> 00:11:21,770
we're gonna want to minimize the negative log probability which corresponds to maximizing

176
00:11:21,770 --> 00:11:28,155
the log probability which corresponds to maximizing um the probability. Um.

177
00:11:28,155 --> 00:11:32,665
And, but, um, sort of,

178
00:11:32,665 --> 00:11:37,465
pretty soon now, we're gonna start doing more stuff with deep learning frameworks,

179
00:11:37,465 --> 00:11:41,275
in particular PyTorch and you can discover in that,

180
00:11:41,275 --> 00:11:43,540
that there's actually a thing called NLL

181
00:11:43,540 --> 00:11:47,515
loss which stands for negative log-likelihood loss.

182
00:11:47,515 --> 00:11:52,150
Basically, no one uses it because the more convenient thing to use is what's called

183
00:11:52,150 --> 00:11:54,820
the cross entropy loss and so you'll

184
00:11:54,820 --> 00:11:58,000
hear everywhere that we're training with cross entropy loss.

185
00:11:58,000 --> 00:12:02,995
So, I just wanted to briefly mention that and explain what's going on there.

186
00:12:02,995 --> 00:12:06,430
Um, so the concept of cross entropy comes from

187
00:12:06,430 --> 00:12:10,855
baby Information Theory which is about the amount of information theory I know.

188
00:12:10,855 --> 00:12:17,800
Um, so, we're assuming that there's some true probability distribution P and our model,

189
00:12:17,800 --> 00:12:20,890
we've built some probability distribution, Q.

190
00:12:20,890 --> 00:12:25,270
That's what we've built with our soft-max regression and we want to have

191
00:12:25,270 --> 00:12:30,685
a measure of whether our estimated probability distribution is a good one.

192
00:12:30,685 --> 00:12:33,595
And the way we do it in cross entropy is,

193
00:12:33,595 --> 00:12:36,370
we go through the classes and we say,

194
00:12:36,370 --> 00:12:40,045
"what's the probability of the class according to the true model?"

195
00:12:40,045 --> 00:12:44,320
Using that waiting, we then work out the log of, um,

196
00:12:44,320 --> 00:12:50,215
the probability according to our estimated model and we sum those up and negate it,

197
00:12:50,215 --> 00:12:53,305
and that is our cross entropy measure.

198
00:12:53,305 --> 00:12:59,635
Okay. Um, but- so this in general gives you

199
00:12:59,635 --> 00:13:06,520
a measure of sort of information, um, between distributions.

200
00:13:06,520 --> 00:13:09,355
But in our particular case,

201
00:13:09,355 --> 00:13:11,725
remember that for each example,

202
00:13:11,725 --> 00:13:14,260
we've sort of assuming that this is a piece of

203
00:13:14,260 --> 00:13:18,025
labeled training data so we are saying for that example,

204
00:13:18,025 --> 00:13:20,470
the right answer is class seven.

205
00:13:20,470 --> 00:13:23,665
So therefore, our true distribution,

206
00:13:23,665 --> 00:13:26,740
our p is- for this example,

207
00:13:26,740 --> 00:13:31,030
it's class seven with probability one and it's class,

208
00:13:31,030 --> 00:13:34,000
um, anything else with probability zero.

209
00:13:34,000 --> 00:13:37,929
So if you think about then what happens with this formula,

210
00:13:37,929 --> 00:13:40,630
you've got this summation of all the classes.

211
00:13:40,630 --> 00:13:45,055
The PFC is gonna be either one or zero and it's gonna be one

212
00:13:45,055 --> 00:13:49,750
only for the true class here and so what you're left with is,

213
00:13:49,750 --> 00:13:54,055
this is going to equal minus the log of qc, um,

214
00:13:54,055 --> 00:14:01,465
for the true class which is sort of what we were then computing in the previous slide.

215
00:14:01,465 --> 00:14:04,930
Okay. So that's- um, yeah.

216
00:14:04,930 --> 00:14:08,425
So that's basically where you'd get with cross entropy loss.

217
00:14:08,425 --> 00:14:12,085
Um, but one other concept to mention.

218
00:14:12,085 --> 00:14:16,300
So when you have a full data-set of a whole bunch of examples,

219
00:14:16,300 --> 00:14:21,010
the cross entropy loss is then taking the per example average.

220
00:14:21,010 --> 00:14:25,270
So, I guess it's what information theory people sometimes call the cross entropy rate.

221
00:14:25,270 --> 00:14:27,280
So additionally, factored in there.

222
00:14:27,280 --> 00:14:33,025
If you are training it on any examples is that one on in vector that's coming in there.

223
00:14:33,025 --> 00:14:36,805
Okay. Um, okay.

224
00:14:36,805 --> 00:14:39,865
Um, so that's cross entropy loss.

225
00:14:39,865 --> 00:14:40,990
Is that okay? Yeah.

226
00:14:40,990 --> 00:14:45,775
[NOISE] There's some- there's some mixture of the actual labels in the ground?

227
00:14:45,775 --> 00:14:47,980
Sure. Good question.

228
00:14:47,980 --> 00:14:52,615
Right. So, the simplest case is that your gold data,

229
00:14:52,615 --> 00:14:55,225
someone has hand labeled it and,

230
00:14:55,225 --> 00:14:58,060
um, they've labeled one and the rest is zero.

231
00:14:58,060 --> 00:15:02,380
Um, they are- you can think of cases where that isn't the case.

232
00:15:02,380 --> 00:15:05,800
I mean, one case is you could believe that human beings

233
00:15:05,800 --> 00:15:09,520
sometimes don't know the right answer so if human beings said,

234
00:15:09,520 --> 00:15:13,870
"I'm not sure whether this should be class three or four," you could imagine that we

235
00:15:13,870 --> 00:15:18,685
can make training data where we put probability half on both of them,

236
00:15:18,685 --> 00:15:21,550
um, and that wouldn't be a crazy thing to do,

237
00:15:21,550 --> 00:15:26,680
and so then you'd have a true cross entropy loss using more of a distribution.

238
00:15:26,680 --> 00:15:34,270
Um, the case where it's much more commonly used in actual practice is,

239
00:15:34,270 --> 00:15:39,490
there are many circumstances in which people wanna do semi-supervised learning.

240
00:15:39,490 --> 00:15:41,230
So, I guess this is a topic that

241
00:15:41,230 --> 00:15:44,740
both my group and Chris Re's group have worked on quite a lot,

242
00:15:44,740 --> 00:15:47,935
where we don't actually have fully labeled data,

243
00:15:47,935 --> 00:15:51,340
but we've got some means of guessing what the labels

244
00:15:51,340 --> 00:15:54,790
of the data are and if we try and guess labels of data,

245
00:15:54,790 --> 00:15:56,845
well then quite often we'll say,

246
00:15:56,845 --> 00:15:58,465
"Here's this data right in.

247
00:15:58,465 --> 00:16:00,760
It's two-thirds chances this label,

248
00:16:00,760 --> 00:16:05,125
but it could be these other four labels," and we'd use a probability distribution,

249
00:16:05,125 --> 00:16:07,825
and yeah, then it's more general cross entropy loss.

250
00:16:07,825 --> 00:16:12,560
Okay? Um, right. So, um,

251
00:16:12,560 --> 00:16:16,335
that's cross entropy loss, pretty good with.

252
00:16:16,335 --> 00:16:19,425
Um, this bottom bit is a little bit different, um,

253
00:16:19,425 --> 00:16:20,895
which is to say, "Well now we,

254
00:16:20,895 --> 00:16:23,130
this is the sort of the full data-set."

255
00:16:23,130 --> 00:16:25,290
The other thing to notice, um,

256
00:16:25,290 --> 00:16:33,285
when we have a full data- we can have a full data-set of x's,

257
00:16:33,285 --> 00:16:37,665
um, and then we have a full set of weights.

258
00:16:37,665 --> 00:16:40,770
Um, where here we're working a row,

259
00:16:40,770 --> 00:16:43,020
a row vector for the weights for one class,

260
00:16:43,020 --> 00:16:45,420
but we're gonna work it out for all classes.

261
00:16:45,420 --> 00:16:48,180
So, we can sort of simplify what we're writing

262
00:16:48,180 --> 00:16:51,015
here and we can start using matrix notation

263
00:16:51,015 --> 00:16:56,980
and just work directly in terms of the matrix w. Okay.

264
00:16:56,980 --> 00:17:01,075
So for traditional ML optimization,

265
00:17:01,075 --> 00:17:05,200
our parameters are these sets of weights,

266
00:17:05,200 --> 00:17:07,240
um, for the different classes.

267
00:17:07,240 --> 00:17:09,280
So for each of the classes,

268
00:17:09,280 --> 00:17:12,085
we have a d-dimensional, um,

269
00:17:12,085 --> 00:17:14,620
row vector of weights because we're gonna

270
00:17:14,620 --> 00:17:19,225
sort of dot-product wi- with rd, dimensional, input vector.

271
00:17:19,225 --> 00:17:28,975
So we have c times d items and our W matrix and those are the parameters of our model.

272
00:17:28,975 --> 00:17:35,305
So if we want to learn that model using the ideas of gradient descent,

273
00:17:35,305 --> 00:17:37,960
stochastic gradient descent, we're gonna do

274
00:17:37,960 --> 00:17:40,585
sort of what we started to talk about last time.

275
00:17:40,585 --> 00:17:43,255
We have these set of parameters.

276
00:17:43,255 --> 00:17:45,295
We work out, um,

277
00:17:45,295 --> 00:17:50,845
the gradient, the partial derivatives of all of these, um,

278
00:17:50,845 --> 00:17:54,790
of the loss with respect to all of these parameters and we

279
00:17:54,790 --> 00:17:58,840
use that to get a gradient update on our loss function,

280
00:17:58,840 --> 00:18:01,240
and we move around the w's,

281
00:18:01,240 --> 00:18:04,930
and moving around the w's corresponds to sort of

282
00:18:04,930 --> 00:18:09,700
moving this line that separates between the classes and we fiddle

283
00:18:09,700 --> 00:18:15,490
that around so as to minimize our loss which corresponds to choosing a line that

284
00:18:15,490 --> 00:18:22,045
best separates between the items of the classes in some sense.

285
00:18:22,045 --> 00:18:25,135
Okay. So, that's a basic classifier.

286
00:18:25,135 --> 00:18:28,945
So the first question is, well,

287
00:18:28,945 --> 00:18:35,840
how are things gonna be different with a neural network classifier?

288
00:18:36,120 --> 00:18:40,765
Um, so the central observation is that sort of

289
00:18:40,765 --> 00:18:46,405
most of the classic classifiers that people used a lot of the time,

290
00:18:46,405 --> 00:18:50,470
so that includes things like Naive Bayes models, um,

291
00:18:50,470 --> 00:18:52,810
basic support vector machines,

292
00:18:52,810 --> 00:18:55,495
Softmax or logistic regressions.

293
00:18:55,495 --> 00:19:00,955
They're sort of fairly simple classifiers.

294
00:19:00,955 --> 00:19:06,100
In particular those are all linear classifiers which are going to classified by drawing

295
00:19:06,100 --> 00:19:08,680
a line or in the higher dimensional space by

296
00:19:08,680 --> 00:19:12,175
drawing some kind of plane that separates examples.

297
00:19:12,175 --> 00:19:18,235
Having a simple classifier like that can be useful in certain circumstances.

298
00:19:18,235 --> 00:19:22,090
I mean, that gives you what a machine learning as a high bias classifiers,

299
00:19:22,090 --> 00:19:25,465
there's lots of, talk of in CS229,

300
00:19:25,465 --> 00:19:27,430
but if you have a data-set, um,

301
00:19:27,430 --> 00:19:32,050
that's like this, you can't do a very good job at classifying

302
00:19:32,050 --> 00:19:34,390
all the points correctly if you have

303
00:19:34,390 --> 00:19:38,080
a high bias classifier because you're gonna only draw a line.

304
00:19:38,080 --> 00:19:41,995
So you'd like to have a more powerful classifier.

305
00:19:41,995 --> 00:19:46,390
Essentially, what's been powering a lot of the use of

306
00:19:46,390 --> 00:19:51,580
deep learning is that in a lot of cases when you have natural signals,

307
00:19:51,580 --> 00:19:53,950
so those are things like, um, speech,

308
00:19:53,950 --> 00:19:56,620
language, images, and things like that,

309
00:19:56,620 --> 00:20:02,440
you have a ton of data so you could learn a quite sophisticated classifier.

310
00:20:02,440 --> 00:20:10,180
Um, but representing the classes in terms of the input data is sort of very complex.

311
00:20:10,180 --> 00:20:14,365
You could never do it by just drawing a line between the two classes.

312
00:20:14,365 --> 00:20:19,150
So, you'd like to use some more complicated kind of classifier.

313
00:20:19,150 --> 00:20:22,060
So neural networks, the multi-layer

314
00:20:22,060 --> 00:20:25,015
neural networks that we're gonna be starting to get into now,

315
00:20:25,015 --> 00:20:31,480
precisely what they do is provide you a way to learn very complex,

316
00:20:31,480 --> 00:20:35,560
you know, almost limitlessly complex classifiers.

317
00:20:35,560 --> 00:20:41,364
So that if you look at the decisions that they're making in terms of the original space,

318
00:20:41,364 --> 00:20:44,110
they can be learning cases like this.

319
00:20:44,110 --> 00:20:47,050
Um, I put this- I put the,

320
00:20:47,050 --> 00:20:50,920
um, pointer on a couple of the slides here.

321
00:20:50,920 --> 00:20:55,330
Um, this- this is a visualization that was done by Andrei Karpathy.

322
00:20:55,330 --> 00:20:58,525
He was a PhD student here until a couple of years ago.

323
00:20:58,525 --> 00:21:00,535
So this is a little JavaScript, um,

324
00:21:00,535 --> 00:21:03,430
app that you can find off his website and it's

325
00:21:03,430 --> 00:21:06,385
actually a lot of fun to play with to see what kind of,

326
00:21:06,385 --> 00:21:11,035
um, decision boundaries you can get a neural net to come up with.

327
00:21:11,035 --> 00:21:21,415
Okay. Um, so for getting- for getting more advanced classification out of,

328
00:21:21,415 --> 00:21:24,760
um, a neural net used for natural language,

329
00:21:24,760 --> 00:21:29,260
there are sort of two things going- that you can do,

330
00:21:29,260 --> 00:21:31,810
that I want to talk about which are in

331
00:21:31,810 --> 00:21:35,110
some sense the same thing when it comes down to it.

332
00:21:35,110 --> 00:21:41,890
But I'll sort of mention separately at the beginning that one of them is that we

333
00:21:41,890 --> 00:21:45,370
have these word vectors and then

334
00:21:45,370 --> 00:21:49,915
the second one is that we're gonna build deeper multi-layer networks.

335
00:21:49,915 --> 00:21:53,065
Okay. So, at first crucial difference said,

336
00:21:53,065 --> 00:21:56,290
um, we already started to see, um,

337
00:21:56,290 --> 00:21:59,770
with what we were doing last week is rather than

338
00:21:59,770 --> 00:22:03,595
sort of having a word being this is the word house,

339
00:22:03,595 --> 00:22:10,720
we instead say house is a vector of real numbers and what we can do is

340
00:22:10,720 --> 00:22:14,620
change the vector that corresponds to house in

341
00:22:14,620 --> 00:22:19,030
such a way as we can build better classifiers,

342
00:22:19,030 --> 00:22:23,080
which means that we are gonna be sort of moving houses representation around

343
00:22:23,080 --> 00:22:27,220
the space to capture things that we're interested in like word similarity,

344
00:22:27,220 --> 00:22:29,605
analogies, and things like that.

345
00:22:29,605 --> 00:22:31,150
So this is actually, you know,

346
00:22:31,150 --> 00:22:35,830
kind of a weird idea compared to conventional steps or ML.

347
00:22:35,830 --> 00:22:40,075
So rather than saying we just have the parameters w,

348
00:22:40,075 --> 00:22:47,290
we also say that all of these word representations are also parameters of our model.

349
00:22:47,290 --> 00:22:49,840
So, we're actually going to change

350
00:22:49,840 --> 00:22:54,685
the representations of words to allow our classifiers to do better.

351
00:22:54,685 --> 00:22:57,320
So, we're simultaneously changing the weights

352
00:22:57,320 --> 00:23:00,290
and we're changing the representation of words,

353
00:23:00,290 --> 00:23:04,685
and we're optimizing both of them at once to try and make our model as,

354
00:23:04,685 --> 00:23:06,410
um, good as possible.

355
00:23:06,410 --> 00:23:10,250
So, this is the sense in which people often talk about

356
00:23:10,250 --> 00:23:15,210
the deep learning models that we're doing representation learning.

357
00:23:15,230 --> 00:23:18,375
I sort of said there are two ways,

358
00:23:18,375 --> 00:23:19,830
I was going to mention two things.

359
00:23:19,830 --> 00:23:21,540
One is this sort of, um,

360
00:23:21,540 --> 00:23:25,050
word vector representation learning and then the second one

361
00:23:25,050 --> 00:23:29,160
is that we're going to start looking at deeper multi layer neural networks.

362
00:23:29,160 --> 00:23:32,835
Um, sort of hidden over here on the slide

363
00:23:32,835 --> 00:23:37,035
is the observation that really you can think of word,

364
00:23:37,035 --> 00:23:40,770
word vector embedding as just putting your,

365
00:23:40,770 --> 00:23:44,280
having a model with one more neural network layer.

366
00:23:44,280 --> 00:23:50,280
So, if you imagine that each word was a one hot vector,

367
00:23:50,280 --> 00:23:53,730
um, with, for the different word types in your model.

368
00:23:53,730 --> 00:23:55,095
So, you had a, uh, you know,

369
00:23:55,095 --> 00:24:00,735
150,000 dimensional vector with the one-hot encoding of different words.

370
00:24:00,735 --> 00:24:03,855
Um, then you could say you have a ma-, um,

371
00:24:03,855 --> 00:24:11,790
matrix L which is sort of your lexicon matrix and you will pass your one-hot vector for

372
00:24:11,790 --> 00:24:15,870
a word through a layer of neural net which

373
00:24:15,870 --> 00:24:20,925
multiplies the one-hot vector or L1, the one-hot vector.

374
00:24:20,925 --> 00:24:22,905
And since this was a one-hot vector,

375
00:24:22,905 --> 00:24:30,525
what that will have the effect of doing is taking out a column of L. So,

376
00:24:30,525 --> 00:24:34,590
really, we've got an extra layer of matrix, um,

377
00:24:34,590 --> 00:24:37,215
in our neural net and we're learning

378
00:24:37,215 --> 00:24:41,595
the parameters of that matrix in the same way as we're learning,

379
00:24:41,595 --> 00:24:44,910
um, a deep neural network for other purposes.

380
00:24:44,910 --> 00:24:48,030
So, mathematically that completely makes sense and

381
00:24:48,030 --> 00:24:51,375
that's sort of a sensible way to think about,

382
00:24:51,375 --> 00:24:53,385
um, what you're doing, um,

383
00:24:53,385 --> 00:24:56,250
with word embeddings in neural networks.

384
00:24:56,250 --> 00:24:58,890
Um, implementation wise, this makes

385
00:24:58,890 --> 00:25:02,100
no sense at all and no one does this because it just doesn't

386
00:25:02,100 --> 00:25:08,100
make sense to do a matrix multiply when the result of the matrix multiply will be, okay.

387
00:25:08,100 --> 00:25:12,405
This is word ID 17, um, sort of,

388
00:25:12,405 --> 00:25:16,380
then constructing a one-hot vector of length a 150,000 with a

389
00:25:16,380 --> 00:25:20,325
one in position 17 and then doing a matrix multiplied, makes no sense.

390
00:25:20,325 --> 00:25:22,065
You just take up, um,

391
00:25:22,065 --> 00:25:24,090
column or, or, the row,

392
00:25:24,090 --> 00:25:29,230
as we've discussed, 17 of your matrix and that's what everyone actually does.

393
00:25:29,230 --> 00:25:35,940
Okay. Here's my one obligatory picture of neurons, um, for the class.

394
00:25:35,940 --> 00:25:38,475
So, don't miss it, I'm not going to show it again, all class.

395
00:25:38,475 --> 00:25:42,585
Okay. So, the origins [LAUGHTER] of Neural Networks, um,

396
00:25:42,585 --> 00:25:47,130
was in some sense to try and construct

397
00:25:47,130 --> 00:25:50,520
an artificial neuron that seemed to in

398
00:25:50,520 --> 00:25:54,540
some sense kind of capture the kind of computations,

399
00:25:54,540 --> 00:25:57,765
um, that go on in human brains.

400
00:25:57,765 --> 00:26:02,205
It's a very loose analogy for what was produced but, you know,

401
00:26:02,205 --> 00:26:04,200
our model here is these are our,

402
00:26:04,200 --> 00:26:07,500
this is our a TB part of our human brains.

403
00:26:07,500 --> 00:26:08,940
So, here are neurons,

404
00:26:08,940 --> 00:26:12,075
this is a neuron cell here and so,

405
00:26:12,075 --> 00:26:14,565
what does a neuron consist of.

406
00:26:14,565 --> 00:26:16,470
Um, so, up the back,

407
00:26:16,470 --> 00:26:19,455
it's got these dendrites, lots of dendrites.

408
00:26:19,455 --> 00:26:25,200
Then it's got a cell body and if there's stuff coming in on the dendrites, um,

409
00:26:25,200 --> 00:26:28,800
the cell body will become active and then it all starts

410
00:26:28,800 --> 00:26:32,880
spiking down this long thing which is called the Axon.

411
00:26:32,880 --> 00:26:35,580
So, then these axons lead to

412
00:26:35,580 --> 00:26:39,300
the dendrites of a different cell or lots of different cells, right.

413
00:26:39,300 --> 00:26:41,640
This one, um, I'm not sure it's shown but

414
00:26:41,640 --> 00:26:44,625
some of these are kind of going to different cells.

415
00:26:44,625 --> 00:26:48,105
Um, and so, you then have these sort of, um,

416
00:26:48,105 --> 00:26:51,120
terminal buttons on the Axon which are kind of close

417
00:26:51,120 --> 00:26:54,270
to the dendrites but have a little gap in them and some min-,

418
00:26:54,270 --> 00:26:56,955
miracles of biochemistry happen there.

419
00:26:56,955 --> 00:26:58,920
So, that's the synapse, of course,

420
00:26:58,920 --> 00:27:03,870
which you'll then have sort of activation flowing which goes into the next neuron.

421
00:27:03,870 --> 00:27:06,870
So, that was the starting off, um,

422
00:27:06,870 --> 00:27:10,710
model that people wanted to try and simulate in computation.

423
00:27:10,710 --> 00:27:15,060
So, people came up with this model of an artificial neuron.

424
00:27:15,060 --> 00:27:22,050
So, that we have things coming in from other neurons at some level of activations.

425
00:27:22,050 --> 00:27:25,710
So, that's a number X0, X1, X2.

426
00:27:25,710 --> 00:27:30,690
Um, then synapses vary depending on how excitable

427
00:27:30,690 --> 00:27:35,535
they are as to how easily they'll let signal cross across the synapse.

428
00:27:35,535 --> 00:27:42,675
So, that's being modeled by multiplying them by a weight W0, W1, W2.

429
00:27:42,675 --> 00:27:46,365
Then the cell body, sort of correctly,

430
00:27:46,365 --> 00:27:50,190
is sort of summing this amount of excitation it's

431
00:27:50,190 --> 00:27:54,510
getting from the different dendrites, um,

432
00:27:54,510 --> 00:27:58,320
and then it can have its own biases to how likely it is to fire,

433
00:27:58,320 --> 00:28:00,210
that's the B. Um, so,

434
00:28:00,210 --> 00:28:06,240
we get that and then it has some overall kind of threshold or propensity for firing.

435
00:28:06,240 --> 00:28:09,405
So, we sort of stick it through an activation function,

436
00:28:09,405 --> 00:28:11,280
um, which will sort of,

437
00:28:11,280 --> 00:28:13,920
will determine a firing rate and that will be,

438
00:28:13,920 --> 00:28:17,205
um, the signal that's going out on the output axon.

439
00:28:17,205 --> 00:28:21,150
So, that was sort of the starting point of that but,

440
00:28:21,150 --> 00:28:22,575
you know, really, um,

441
00:28:22,575 --> 00:28:24,600
for what we've ended up computing.

442
00:28:24,600 --> 00:28:28,890
We just have a little bit of baby math here which actually, um,

443
00:28:28,890 --> 00:28:33,090
looks very familiar to the kind of baby math you see

444
00:28:33,090 --> 00:28:37,920
in linear algebra and statistics and so it's really no different.

445
00:28:37,920 --> 00:28:40,140
So, in particular, um,

446
00:28:40,140 --> 00:28:46,380
a neuron can very easily be a Binary Logistic Regression Unit.

447
00:28:46,380 --> 00:28:49,305
Um, so that, this is sort of,

448
00:28:49,305 --> 00:28:52,800
for logistic regression you're taking for your input X,

449
00:28:52,800 --> 00:28:54,855
you multiply it by a weight vector.

450
00:28:54,855 --> 00:28:58,065
You're adding, um, your, um,

451
00:28:58,065 --> 00:29:01,920
bias term and then you're putting it through,

452
00:29:01,920 --> 00:29:03,990
um, a non linearity,

453
00:29:03,990 --> 00:29:06,315
like the logistic function.

454
00:29:06,315 --> 00:29:10,665
Um, and then, so you're calculating a logistic regression,

455
00:29:10,665 --> 00:29:14,625
um, inside this sort of neuron model.

456
00:29:14,625 --> 00:29:17,535
Um, and so this is the,

457
00:29:17,535 --> 00:29:20,970
this is the difference between the soft maximum logistic regression,

458
00:29:20,970 --> 00:29:26,655
that I was saying that there is the soft-max for two classes has two sets of parameters.

459
00:29:26,655 --> 00:29:30,420
This sort of just has one set of parameters Z and your modeling

460
00:29:30,420 --> 00:29:35,790
the two classes by giving the probability of one class from 0 to one,

461
00:29:35,790 --> 00:29:37,500
depending on whether the input to

462
00:29:37,500 --> 00:29:41,490
logistic regression is highly negative or highly positive.

463
00:29:41,490 --> 00:29:47,625
Okay. So, really, we can just say these artificial neurons are sort of like

464
00:29:47,625 --> 00:29:51,780
binary logistic regression units or we can make variants of

465
00:29:51,780 --> 00:29:56,730
binary logistic regression units by using some different F function.

466
00:29:56,730 --> 00:29:59,925
And we'll come back to that again and pretty soon.

467
00:29:59,925 --> 00:30:04,560
Okay. Um, well, so that gives us one neuron.

468
00:30:04,560 --> 00:30:09,495
So, one neuron is a logistic regression unit for current purposes.

469
00:30:09,495 --> 00:30:13,410
So, crucially what we're wanting to do with neural networks is say, well,

470
00:30:13,410 --> 00:30:16,500
why only run one logistic regression,

471
00:30:16,500 --> 00:30:17,865
why don't we, um,

472
00:30:17,865 --> 00:30:21,960
run a whole bunch of logistic regressions at the same time?

473
00:30:21,960 --> 00:30:27,465
So, you know, here are our inputs and here's our little logistic regression unit, um,

474
00:30:27,465 --> 00:30:30,420
but we could run three logistic regressions at

475
00:30:30,420 --> 00:30:33,855
the same time or we can run any number of them.

476
00:30:33,855 --> 00:30:39,690
Um, well, that's good but sort of for conventional training of

477
00:30:39,690 --> 00:30:42,810
a statistical model which sort of have to

478
00:30:42,810 --> 00:30:47,865
determine for those orange outputs of the logistic regression.

479
00:30:47,865 --> 00:30:51,540
You know, what we're training each of them to try and capture.

480
00:30:51,540 --> 00:30:56,235
We have to have data to predict what they're going to try and capture.

481
00:30:56,235 --> 00:31:02,865
And so, the secret of sort of then building bigger neural networks is to say,

482
00:31:02,865 --> 00:31:06,720
we don't actually want to decide ahead of time what

483
00:31:06,720 --> 00:31:11,595
those little orange logistic regressions are trying to capture.

484
00:31:11,595 --> 00:31:15,450
We want the neural network to self-organize,

485
00:31:15,450 --> 00:31:18,450
so that those orange logistic regression,

486
00:31:18,450 --> 00:31:22,500
um, units learn something useful.

487
00:31:22,500 --> 00:31:25,020
And well, what is something useful?

488
00:31:25,020 --> 00:31:27,270
Well, our idea is to say,

489
00:31:27,270 --> 00:31:31,095
we do actually have some tasks that we want to do.

490
00:31:31,095 --> 00:31:34,650
So, we- we have some tasks that we want to do.

491
00:31:34,650 --> 00:31:39,450
So maybe, we want to sort of decide whether a movie review is positive or negative,

492
00:31:39,450 --> 00:31:41,925
something like sentiment analysis or something like that.

493
00:31:41,925 --> 00:31:44,670
There is something we want to do at the end of the day.

494
00:31:44,670 --> 00:31:47,625
Um, and we're gonna have, uh,

495
00:31:47,625 --> 00:31:52,275
logistic regression classifier there telling us positive or negative.

496
00:31:52,275 --> 00:31:55,440
Um, but the inputs to that aren't going to

497
00:31:55,440 --> 00:31:58,785
directly be something like words in the document.

498
00:31:58,785 --> 00:32:03,480
They're going to be this intermediate layer of logistic regression units

499
00:32:03,480 --> 00:32:09,180
and we're gonna train this whole thing to minimize our cross entropy loss.

500
00:32:09,180 --> 00:32:12,120
Essentially, what we're going to want to have

501
00:32:12,120 --> 00:32:15,015
happen in the back propagation algorithm will do for us,

502
00:32:15,015 --> 00:32:18,015
is to say, you things in the middle,

503
00:32:18,015 --> 00:32:23,280
it's your job to find some useful way to calculate values from

504
00:32:23,280 --> 00:32:29,715
the underlying data such that it'll help our final classifier make a good decision.

505
00:32:29,715 --> 00:32:31,695
I mean in particular, you know,

506
00:32:31,695 --> 00:32:34,605
back to this picture, you know.

507
00:32:34,605 --> 00:32:38,520
The final classifier, its just a linear classifier,

508
00:32:38,520 --> 00:32:40,770
a soft-max or logistic regression.

509
00:32:40,770 --> 00:32:42,915
It's gonna have a line like this.

510
00:32:42,915 --> 00:32:45,900
But if the intermediate classifiers,

511
00:32:45,900 --> 00:32:48,000
they are like a word embedding,

512
00:32:48,000 --> 00:32:52,260
they can kind of sort of re-represent the space and shift things around.

513
00:32:52,260 --> 00:32:57,180
So, they can learn to shift things around in such a way as

514
00:32:57,180 --> 00:33:03,190
you're learning a highly non-linear function of the original input space.

515
00:33:06,810 --> 00:33:10,930
Okay. Um, and so at that point,

516
00:33:10,930 --> 00:33:12,430
it's simply a matter of saying,

517
00:33:12,430 --> 00:33:13,960
well, why stop there?

518
00:33:13,960 --> 00:33:18,085
Maybe it gets even better if we put in more layers.

519
00:33:18,085 --> 00:33:24,400
And this sort of gets us into the area of deep learning and sort of precisely,

520
00:33:24,400 --> 00:33:26,590
um, this is, um,

521
00:33:26,590 --> 00:33:31,180
that sort of there was- sort of being three comings of neural networks.

522
00:33:31,180 --> 00:33:33,940
So the first work in the 50s which is

523
00:33:33,940 --> 00:33:38,320
essentially when people had a model of a single neuron

524
00:33:38,320 --> 00:33:41,710
like this and then only gradually worked out how it

525
00:33:41,710 --> 00:33:45,965
related to more conventional statistics than there was.

526
00:33:45,965 --> 00:33:53,220
Um, the second version of neural networks which we saw the 80s and early 90s, um,

527
00:33:53,220 --> 00:33:56,640
where people, um, built neural networks like this that had

528
00:33:56,640 --> 00:34:01,865
this one hidden layer where a representation could be learned in the middle.

529
00:34:01,865 --> 00:34:06,220
But at that time it really wasn't effective.

530
00:34:06,220 --> 00:34:12,850
Of all people weren't able to build deeper networks and get them to do anything useful.

531
00:34:12,850 --> 00:34:18,040
So you sort of had these neural networks with one hidden layer and so precisely with

532
00:34:18,040 --> 00:34:24,715
research that started in- into deep learning that precisely the motivating question is,

533
00:34:24,715 --> 00:34:29,710
um, we believe we'll be able to do even more sophisticated,

534
00:34:29,710 --> 00:34:32,950
um, classification for more complex tasks.

535
00:34:32,950 --> 00:34:36,760
Things like speech recognition and image recognition if we could

536
00:34:36,760 --> 00:34:40,960
have a deeper network which will be able to more

537
00:34:40,960 --> 00:34:45,100
effectively learn more sophisticated functions of the input which

538
00:34:45,100 --> 00:34:49,405
will allow us to do things like recognize sounds of a language.

539
00:34:49,405 --> 00:34:52,060
How could we possibly train such a,

540
00:34:52,060 --> 00:34:55,210
um, network so they'll work effectively?

541
00:34:55,210 --> 00:34:56,875
And that's the kind of thing,

542
00:34:56,875 --> 00:34:58,465
um, will go on to,

543
00:34:58,465 --> 00:35:03,055
um, more so starting this lecture more so in the next lecture.

544
00:35:03,055 --> 00:35:05,859
But before we get to there,

545
00:35:05,859 --> 00:35:08,050
um, just to underline it again.

546
00:35:08,050 --> 00:35:11,440
So once we have something like this is our,

547
00:35:11,440 --> 00:35:13,765
um, layer of a neural network.

548
00:35:13,765 --> 00:35:16,330
We have a vector of inputs,

549
00:35:16,330 --> 00:35:21,040
we have a vector of outputs and everything is

550
00:35:21,040 --> 00:35:28,135
connected so that we've got this sort of weights along every one of these black lines.

551
00:35:28,135 --> 00:35:32,145
And so we can say A1 is you're taking

552
00:35:32,145 --> 00:35:36,715
weights times each component of X1 and adding a bias term,

553
00:35:36,715 --> 00:35:41,305
um, and then you're going to be running which is sort of

554
00:35:41,305 --> 00:35:47,575
this part and then running it through our non-linearity and that will give us an output.

555
00:35:47,575 --> 00:35:51,265
And we're gonna do that for each of A1, A2, and A3.

556
00:35:51,265 --> 00:35:56,710
Um, so again, we can kind of regard A is a vector and we can

557
00:35:56,710 --> 00:36:02,260
kind of collapse it into this matrix notation for working out the effects of layers.

558
00:36:02,260 --> 00:36:07,780
The fully connected layers are effectively matrices of weights, um,

559
00:36:07,780 --> 00:36:13,720
and commonly rewrite them like this where we have a bias term as a vector of bias terms.

560
00:36:13,720 --> 00:36:15,445
There's sort of a choice there.

561
00:36:15,445 --> 00:36:18,670
You can either have an always on import and then

562
00:36:18,670 --> 00:36:24,190
the bias terms become part of the weights of a slightly bigger matrix with one extra,

563
00:36:24,190 --> 00:36:28,670
uh, one extra either column or row.

564
00:36:30,810 --> 00:36:36,835
One extra, a- row, right?

565
00:36:36,835 --> 00:36:41,530
Or you can just sort of have them separately within those Bs.

566
00:36:41,530 --> 00:36:45,580
Okay. Um, and then the final note here- right?

567
00:36:45,580 --> 00:36:49,600
So once we've calculated this part,

568
00:36:49,600 --> 00:36:54,760
we always put things through non-linearity which is referred to as

569
00:36:54,760 --> 00:36:58,000
the activation function and so something like

570
00:36:58,000 --> 00:37:02,395
the logistic transform I showed earlier is an activation function.

571
00:37:02,395 --> 00:37:07,660
And this is written as sort of vector in port, um,

572
00:37:07,660 --> 00:37:11,319
activation function giving a vector output,

573
00:37:11,319 --> 00:37:16,060
and what this always means is that we apply this function element-wise.

574
00:37:16,060 --> 00:37:19,720
So we're applying the logistic function which is sort of a

575
00:37:19,720 --> 00:37:25,705
naturally a one input one output function like the little graph I showed before.

576
00:37:25,705 --> 00:37:28,180
So when we apply that to a vector,

577
00:37:28,180 --> 00:37:32,845
we apply it to each element of the vector element-wise.

578
00:37:32,845 --> 00:37:39,820
Okay. We will come back very soon to sort of saying

579
00:37:39,820 --> 00:37:46,945
more about non-linearities and what non-linearities people actually use.

580
00:37:46,945 --> 00:37:48,705
Um, but, you know,

581
00:37:48,705 --> 00:37:51,585
something you might be wondering is well,

582
00:37:51,585 --> 00:37:53,850
why does he always have these non-linearities

583
00:37:53,850 --> 00:37:56,265
and say there has to be an f function there?

584
00:37:56,265 --> 00:37:58,140
Why don't we just, um,

585
00:37:58,140 --> 00:38:02,190
calculate Z equals WX plus B in one layer and then go

586
00:38:02,190 --> 00:38:07,250
on to another layer that also does Z2 equals W2,

587
00:38:07,250 --> 00:38:12,040
Z1 plus B and keep on going with layers like that?

588
00:38:12,040 --> 00:38:16,360
And there's a very precise reason for that which is if you want

589
00:38:16,360 --> 00:38:21,540
to have a neural network learn anything interesting,

590
00:38:21,540 --> 00:38:24,960
you have to stick in some function F which is

591
00:38:24,960 --> 00:38:30,365
a non-linear function such as the logistic curve I showed before.

592
00:38:30,365 --> 00:38:36,955
And the reason for that is that if you're sort of doing

593
00:38:36,955 --> 00:38:43,915
linear transforms like WX plus B and then W2 Z1 plus B,

594
00:38:43,915 --> 00:38:49,360
W3Z2 plus B and you're doing a sequence of linear transforms.

595
00:38:49,360 --> 00:38:54,790
Well, multiple linear transforms just composed to become a linear transform, right?

596
00:38:54,790 --> 00:39:01,135
So one linear transform is rotating and stretching the space somehow and you can

597
00:39:01,135 --> 00:39:04,510
rotate and stretch the space again but the result of

598
00:39:04,510 --> 00:39:07,915
that is just one bigger rotate and stretch of the space.

599
00:39:07,915 --> 00:39:11,230
So you don't get any extra power for a classifier

600
00:39:11,230 --> 00:39:14,620
by simply having multiple linear transforms.

601
00:39:14,620 --> 00:39:20,980
But as soon as you stick in almost any kind of non-linearity,

602
00:39:20,980 --> 00:39:23,830
then you get additional power.

603
00:39:23,830 --> 00:39:26,965
And so you know in general,

604
00:39:26,965 --> 00:39:30,760
what we're doing when we're doing deep networks, um,

605
00:39:30,760 --> 00:39:34,210
in the middle of them we're not thinking, "Ah,

606
00:39:34,210 --> 00:39:37,225
it's really important to have

607
00:39:37,225 --> 00:39:41,845
non-linearity thinking about probabilities or something like that."

608
00:39:41,845 --> 00:39:44,500
Our general picture is well,

609
00:39:44,500 --> 00:39:50,095
we want to be able to do effective function approximation or curve fitting.

610
00:39:50,095 --> 00:39:55,750
We'd like to learn a space like this and we can only do that if we're sort of putting in

611
00:39:55,750 --> 00:40:03,085
some non-linearities which allow us to learn these kind of curvy decision, um, patterns.

612
00:40:03,085 --> 00:40:07,390
And so- so F is used effectively for doing accurate

613
00:40:07,390 --> 00:40:14,630
[NOISE] fu- function approximation or sort of pattern matching as you go along.

614
00:40:14,880 --> 00:40:20,365
Okay. You are behind already. Um, okay.

615
00:40:20,365 --> 00:40:25,120
So that was the intro to baby neural networks.

616
00:40:25,120 --> 00:40:28,100
All good? Any questions?

617
00:40:28,200 --> 00:40:31,150
Yes?

618
00:40:31,150 --> 00:40:34,780
Yeah, like er, feature one and feature

619
00:40:34,780 --> 00:40:39,475
four if- if you multiply it together it's highly indicative of like the label Y,

620
00:40:39,475 --> 00:40:41,845
can you get to that product relationship

621
00:40:41,845 --> 00:40:43,960
to just say [NOISE] couple of layers that are linear?

622
00:40:43,960 --> 00:40:46,765
Um, yes. Good question.

623
00:40:46,765 --> 00:40:49,780
So, in conventional stats,

624
00:40:49,780 --> 00:40:53,470
you have your basic input features and when

625
00:40:53,470 --> 00:40:58,210
people are building something like a logistic regression model by hand,

626
00:40:58,210 --> 00:41:00,070
people often say well,

627
00:41:00,070 --> 00:41:03,910
something that's really important for classification is

628
00:41:03,910 --> 00:41:08,260
looking at the pair of feature four and feature seven.

629
00:41:08,260 --> 00:41:09,625
Um, that you know,

630
00:41:09,625 --> 00:41:13,210
if both of those are true at the same time something i-important

631
00:41:13,210 --> 00:41:17,545
happens and so that's referred to normally in stats as an interaction term,

632
00:41:17,545 --> 00:41:22,470
and you can by hand a-add interaction terms to your model.

633
00:41:22,470 --> 00:41:29,490
So, essentially a large part of the secret here is having these intermediate layers.

634
00:41:29,490 --> 00:41:33,890
They can learn, build interaction terms by themselves.

635
00:41:33,890 --> 00:41:36,700
Yeah, so it's sort of, um,

636
00:41:36,700 --> 00:41:42,560
automating the search for higher-order terms that you wanna put into your model.

637
00:41:46,800 --> 00:41:50,635
Okay. I'll go on, other questions?

638
00:41:50,635 --> 00:41:55,360
Okay. Um, so um, yeah.

639
00:41:55,360 --> 00:41:59,230
So here's a brief little interlude on a teeny bit more of

640
00:41:59,230 --> 00:42:03,430
NLP which is sort of a kind of problem we're gonna to look at for a moment.

641
00:42:03,430 --> 00:42:08,860
So this is the task of named entity recognition that I very briefly mentioned last time.

642
00:42:08,860 --> 00:42:12,820
So, um, if we have some text,

643
00:42:12,820 --> 00:42:15,595
wait, it isn't appearing here.

644
00:42:15,595 --> 00:42:17,710
Okay. Uh, okay.

645
00:42:17,710 --> 00:42:19,000
If we have some text,

646
00:42:19,000 --> 00:42:22,525
something that in all sorts of places people want to do

647
00:42:22,525 --> 00:42:28,800
is I'd like to find the names of things that are mentioned.

648
00:42:28,800 --> 00:42:32,230
Um and then normally, as well as,

649
00:42:32,230 --> 00:42:35,920
finding the names of things you'd actually like to classify them,

650
00:42:35,920 --> 00:42:38,800
say it's like to say some of them are organizations,

651
00:42:38,800 --> 00:42:41,065
some of them are people,

652
00:42:41,065 --> 00:42:43,765
um, some of them are places.

653
00:42:43,765 --> 00:42:46,855
And so you know this has lots of uses, you know,

654
00:42:46,855 --> 00:42:49,120
people like to track mentions of companies and

655
00:42:49,120 --> 00:42:51,850
people and newspapers and things like that.

656
00:42:51,850 --> 00:42:56,320
Um, people when they do question-answering that a lot of the time the answers

657
00:42:56,320 --> 00:43:00,520
to questions are what we call named entities the names of people,

658
00:43:00,520 --> 00:43:03,430
locations, organizations, pop songs,

659
00:43:03,430 --> 00:43:07,645
movie names all of those kind of things are named entities.

660
00:43:07,645 --> 00:43:10,570
Um, and if you want to sort of start

661
00:43:10,570 --> 00:43:13,690
building up a knowledge base automatically from a lot of text,

662
00:43:13,690 --> 00:43:15,730
well, what you normally wanna do is get out

663
00:43:15,730 --> 00:43:19,600
the named entities and get out relations between them.

664
00:43:19,600 --> 00:43:21,250
So this is a common task.

665
00:43:21,250 --> 00:43:24,430
So, how can we go about doing that?

666
00:43:24,430 --> 00:43:28,420
And a common way of doing that is to say well,

667
00:43:28,420 --> 00:43:32,905
we're going to go through the words one at a time

668
00:43:32,905 --> 00:43:38,035
and they're gonna be words that are in a context just like they were for word to deck,

669
00:43:38,035 --> 00:43:43,630
and what we're gonna do is run a classifier and we're going to assign them a class.

670
00:43:43,630 --> 00:43:46,420
So we're gonna say first word is organization,

671
00:43:46,420 --> 00:43:48,085
second word is organization,

672
00:43:48,085 --> 00:43:50,440
third word isn't a named entity,

673
00:43:50,440 --> 00:43:51,700
fourth word is a person,

674
00:43:51,700 --> 00:43:54,235
fifth word is a person and continue down.

675
00:43:54,235 --> 00:43:58,300
So in running a classification of a word within

676
00:43:58,300 --> 00:44:03,085
a position in the text so it's got surrounding words around it.

677
00:44:03,085 --> 00:44:07,075
Um and so to say what the entities are

678
00:44:07,075 --> 00:44:11,650
many entities are multi-word terms and so the simplest thing you can

679
00:44:11,650 --> 00:44:15,880
imagine doing is just say we'll take the sequence that are all classified the

680
00:44:15,880 --> 00:44:20,935
same and call that the e-entity Shen Guofang or something like that.

681
00:44:20,935 --> 00:44:23,755
There's a reason why that's slightly defective and so

682
00:44:23,755 --> 00:44:26,635
what people often use is that BIO encoding,

683
00:44:26,635 --> 00:44:31,150
um, that I show on the right but I'll just gonna run ahead and not do that now.

684
00:44:31,150 --> 00:44:37,240
Um so, it might seem at first that named entity recognition is trivial because you know,

685
00:44:37,240 --> 00:44:41,230
you have company names Google and Facebook are company names.

686
00:44:41,230 --> 00:44:47,125
And whenever you see Google or Facebook you just say company and how could you be wrong?

687
00:44:47,125 --> 00:44:49,225
But in practice, there's a lot of subtlety and it's

688
00:44:49,225 --> 00:44:51,310
easy to be wrong in named entity recognition.

689
00:44:51,310 --> 00:44:54,370
So this is sort of just some of the hard cases.

690
00:44:54,370 --> 00:44:58,810
So it's often hard to work out the boundaries of an entity.

691
00:44:58,810 --> 00:44:59,994
So in this sentence,

692
00:44:59,994 --> 00:45:04,840
First National Bank don-donates two vans to Future School of Fort Smith.

693
00:45:04,840 --> 00:45:10,960
So, there's presumably the name of a bank there but is it National Bank and the first is

694
00:45:10,960 --> 00:45:14,320
just the first word of a sentence which is cap-capitalized

695
00:45:14,320 --> 00:45:18,160
like first she ordered some food or something.

696
00:45:18,160 --> 00:45:20,560
So kind of unclear what it is.

697
00:45:20,560 --> 00:45:24,310
Sometimes it's hard to know whether something's an entity at all.

698
00:45:24,310 --> 00:45:29,455
So at the end of this sentence is Future School the name of

699
00:45:29,455 --> 00:45:33,040
some exciting kind of 21st-century school or is it

700
00:45:33,040 --> 00:45:36,970
just meaning it's a future school that's gonna be built in this town, right?

701
00:45:36,970 --> 00:45:39,535
Is it an entity or not at all?

702
00:45:39,535 --> 00:45:44,530
Working out the class of an entity is often difficult so to find out more

703
00:45:44,530 --> 00:45:49,480
about Zig Ziglar and read features by what class is Zig Ziglar?

704
00:45:49,480 --> 00:45:51,370
Kinda hard to tell if you don't know.

705
00:45:51,370 --> 00:45:54,700
Um, it's actually a person's name, um,

706
00:45:54,700 --> 00:45:58,870
and there are various entities that are ambiguous, right?

707
00:45:58,870 --> 00:46:02,230
So Charles Schwab in text is

708
00:46:02,230 --> 00:46:08,425
90% of the time an organization name because there's Charles Schwab Brokerage.

709
00:46:08,425 --> 00:46:10,945
Um, but in this particular sentence here,

710
00:46:10,945 --> 00:46:13,180
in Woodside where Larry Ellison and

711
00:46:13,180 --> 00:46:16,524
Charles Schwab can live discreetly among wooded estates,

712
00:46:16,524 --> 00:46:19,450
that is then a reference to Charles Schwab the person.

713
00:46:19,450 --> 00:46:25,330
So there's sort of a fair bit of understanding variously that's needed to get it right.

714
00:46:25,330 --> 00:46:30,250
Okay. Um, so what are we gonna do with that?

715
00:46:30,250 --> 00:46:32,875
And so this suggests, um,

716
00:46:32,875 --> 00:46:40,270
what we wanna do is build classifiers for language that work inside a context.

717
00:46:40,270 --> 00:46:42,760
Um, so you know, in general,

718
00:46:42,760 --> 00:46:45,460
it's not very interesting classifying a word

719
00:46:45,460 --> 00:46:49,000
outside a context we don't actually do that much in NLP.

720
00:46:49,000 --> 00:46:51,685
Um, but once you're in a context, um,

721
00:46:51,685 --> 00:46:55,360
then it's interesting to do and named entity recognition

722
00:46:55,360 --> 00:46:58,300
is one case there are lots of other places that comes up.

723
00:46:58,300 --> 00:47:00,085
I mean, here's a slightly cool one,

724
00:47:00,085 --> 00:47:02,500
that there are some words that can mean

725
00:47:02,500 --> 00:47:06,190
themselves and their opposite at the same time, right?

726
00:47:06,190 --> 00:47:09,040
So to sanction something can either mean to allow

727
00:47:09,040 --> 00:47:12,460
something or it can mean to punish people who do

728
00:47:12,460 --> 00:47:17,770
things or to seed something can either mean to plant seeds

729
00:47:17,770 --> 00:47:20,440
and things that you're seeding the soil or it can

730
00:47:20,440 --> 00:47:23,275
take seeds out of something like a watermelon, right?

731
00:47:23,275 --> 00:47:27,060
You just need to know the context as to which it is.

732
00:47:27,060 --> 00:47:32,195
Okay. So, that suggests the tasks that we can classify a word

733
00:47:32,195 --> 00:47:36,995
in its context of neighboring words and any has an example of that.

734
00:47:36,995 --> 00:47:39,530
And the question is how might we do that?

735
00:47:39,530 --> 00:47:42,770
And a very simple way to do it might be to say, "Well,

736
00:47:42,770 --> 00:47:46,145
we have a bunch of words in a row

737
00:47:46,145 --> 00:47:50,105
which each have a word vector from something like word to vec.

738
00:47:50,105 --> 00:47:52,490
Um, maybe we could just average

739
00:47:52,490 --> 00:47:56,540
those word vectors and then classify the resulting vector.

740
00:47:56,540 --> 00:48:02,075
The problem is that doesn't work very well because you lose position information.

741
00:48:02,075 --> 00:48:04,550
You don't actually know anymore which of

742
00:48:04,550 --> 00:48:08,270
those word vectors is the one that you're meant to be classifying.

743
00:48:08,270 --> 00:48:12,065
So, a simple way to do better than that is to say,

744
00:48:12,065 --> 00:48:16,190
"Well, why didn't we make a big vector of a word window?"

745
00:48:16,190 --> 00:48:20,510
So, here are words and they each have a word vector,

746
00:48:20,510 --> 00:48:26,960
and so to classify the middle word in the context of here plus or minus two words,

747
00:48:26,960 --> 00:48:31,970
we're simply going to concatenate these five vectors together and say now we have

748
00:48:31,970 --> 00:48:37,280
a bigger vector and let's build a classifier over that vector.

749
00:48:37,280 --> 00:48:41,480
So, we're classifying this x window which is then a vector in,

750
00:48:41,480 --> 00:48:47,150
ah, 5D if we're using D dimensional word vectors.

751
00:48:47,150 --> 00:48:56,525
We can do that um in the kind of way that we did previously which is, um,

752
00:48:56,525 --> 00:48:59,570
that we could say, "Okay,

753
00:48:59,570 --> 00:49:03,110
for that big vector we're going to learn w weights

754
00:49:03,110 --> 00:49:06,965
and we're put- gonna put it through a softmax classifier,

755
00:49:06,965 --> 00:49:09,485
and then we're going to do the decisions."

756
00:49:09,485 --> 00:49:13,445
Um, that's a perfectly good way to do things and,

757
00:49:13,445 --> 00:49:16,640
um, for the purpose of it.

758
00:49:16,640 --> 00:49:19,580
What I want to get to in the last part of this is to

759
00:49:19,580 --> 00:49:23,240
start looking at my, um, matrix calculus.

760
00:49:23,240 --> 00:49:26,885
And you know we could use this model and do

761
00:49:26,885 --> 00:49:30,740
a classifier and learn the weights of it and indeed, um,

762
00:49:30,740 --> 00:49:35,750
for the handout on the website that we suggest you look at it does

763
00:49:35,750 --> 00:49:41,345
do it with a softmax classifier of precisely this kind.

764
00:49:41,345 --> 00:49:46,940
Um, but for the example I do in class I try to make it a bit simpler.

765
00:49:46,940 --> 00:49:51,410
Um, and I've wanted to do this I think very quickly because I'm fast running out of time.

766
00:49:51,410 --> 00:49:56,150
So, one of the famous early papers of neural NLP, um,

767
00:49:56,150 --> 00:49:59,090
was this paper by Collobert and Weston which was first

768
00:49:59,090 --> 00:50:03,755
an ICML paper in 2008 which actually just a couple of weeks ago,

769
00:50:03,755 --> 00:50:08,270
um, won the ICML 2018 test of time award.

770
00:50:08,270 --> 00:50:13,405
Um, and then there's a more recent journal version of it 2011.

771
00:50:13,405 --> 00:50:16,780
And um, they use this idea of

772
00:50:16,780 --> 00:50:21,880
window classification to assign classes like named entities,

773
00:50:21,880 --> 00:50:25,015
ti- to words in context, um,

774
00:50:25,015 --> 00:50:28,255
but they did it in a slightly different way.

775
00:50:28,255 --> 00:50:30,835
So, what they said is, "Well,

776
00:50:30,835 --> 00:50:35,750
we've got these windows and this is one with the, um,

777
00:50:35,750 --> 00:50:38,480
location named entity in the middle and

778
00:50:38,480 --> 00:50:41,660
this is one without a location entity in the middle.

779
00:50:41,660 --> 00:50:47,825
So, what we want to do is have a system that returns a score,

780
00:50:47,825 --> 00:50:52,280
and it should return a high score just as a real number in this case and

781
00:50:52,280 --> 00:50:56,960
it can should return a low score if it- if there isn't,

782
00:50:56,960 --> 00:51:00,605
ah, location name in the middle of the window in this case.

783
00:51:00,605 --> 00:51:05,105
So, explicitly the model just return the score.

784
00:51:05,105 --> 00:51:09,875
So, if you had the top level of your neural network a,

785
00:51:09,875 --> 00:51:13,430
and you just then dot product did with a vector u,

786
00:51:13,430 --> 00:51:16,250
you then kind of with that final dot product,

787
00:51:16,250 --> 00:51:19,295
you just return a real number.

788
00:51:19,295 --> 00:51:22,595
They use that as the basis of their classifier.

789
00:51:22,595 --> 00:51:24,290
So in full glory,

790
00:51:24,290 --> 00:51:27,950
what you had is you had this window of words,

791
00:51:27,950 --> 00:51:32,810
you looked up a word vector for each word, you then, um,

792
00:51:32,810 --> 00:51:35,690
multiplied that the, the- well you

793
00:51:35,690 --> 00:51:38,810
concatenated the word vectors for the window.

794
00:51:38,810 --> 00:51:42,680
You multiplied them by a matrix and edited a bias to get

795
00:51:42,680 --> 00:51:47,300
a second hidden layer which is a and then you multiply that by

796
00:51:47,300 --> 00:51:51,710
a final vector and that gave you a score for the window and you

797
00:51:51,710 --> 00:51:56,525
wanted the score to be large if it was the location and small,

798
00:51:56,525 --> 00:51:58,860
if it wasn't a location.

799
00:51:58,860 --> 00:52:04,900
So, in this sort of pretend example where we have four dimensional word vectors,

800
00:52:04,900 --> 00:52:07,720
um, that's meaning you know for the window,

801
00:52:07,720 --> 00:52:11,245
this is a 20 x 1 vector.

802
00:52:11,245 --> 00:52:14,440
Um, for calculating the next hidden layer we've

803
00:52:14,440 --> 00:52:17,515
got an 8 by 20 matrix plus the bias vector.

804
00:52:17,515 --> 00:52:20,770
Then, we've got this sort of 8-dimensional second hidden layer

805
00:52:20,770 --> 00:52:24,205
and then we are computing a final real number.

806
00:52:24,205 --> 00:52:31,250
Okay. Um, and so crucially this is an example of what the question was about.

807
00:52:31,250 --> 00:52:33,800
Um, we've put in this extra layer here, right?

808
00:52:33,800 --> 00:52:36,620
We could have just said here's a word vector,

809
00:52:36,620 --> 00:52:39,035
a big word vector of, of context.

810
00:52:39,035 --> 00:52:41,240
Let's just stick a softmax or

811
00:52:41,240 --> 00:52:45,455
logistic classification on top to say yes or no for location.

812
00:52:45,455 --> 00:52:47,900
But by putting in that extra hidden layer

813
00:52:47,900 --> 00:52:51,860
precisely this extra hidden layer can calculate

814
00:52:51,860 --> 00:52:56,090
non-linear interactions between the input word vectors.

815
00:52:56,090 --> 00:52:58,745
So, it can calculate things like if

816
00:52:58,745 --> 00:53:03,410
the first word is a word like museum and the second and the second

817
00:53:03,410 --> 00:53:06,710
was a word like the preposition in or

818
00:53:06,710 --> 00:53:11,300
around then that's a very good signal that this should be,

819
00:53:11,300 --> 00:53:14,900
ah, location in the middle position of the window.

820
00:53:14,900 --> 00:53:18,560
So, extra layers of a neural network let us calculate

821
00:53:18,560 --> 00:53:22,850
these kind of interaction terms between our basic features.

822
00:53:22,850 --> 00:53:25,640
Okay. Um, so there's

823
00:53:25,640 --> 00:53:29,735
a few more slides here that sort of go through the details of their model,

824
00:53:29,735 --> 00:53:34,100
but I'm gonna just skip those for now because I'm a little bit behind.

825
00:53:34,100 --> 00:53:38,120
And at the end of it we've just got this score.

826
00:53:38,120 --> 00:53:42,200
So this is our model which is the one that I just outlined where we're

827
00:53:42,200 --> 00:53:47,810
calculating the score and we're wanting a big score, um, for location.

828
00:53:47,810 --> 00:53:52,625
And so, what we're gonna want to do is consider, um,

829
00:53:52,625 --> 00:53:56,405
how we can use this model,

830
00:53:56,405 --> 00:53:59,000
um, to learn, um,

831
00:53:59,000 --> 00:54:01,580
our parameters in a neural network.

832
00:54:01,580 --> 00:54:03,095
Um, so in particular,

833
00:54:03,095 --> 00:54:06,155
remember it's the same story we've had before.

834
00:54:06,155 --> 00:54:08,975
We had a loss function J,

835
00:54:08,975 --> 00:54:11,480
and we're wanting to work out, um,

836
00:54:11,480 --> 00:54:17,180
the gradient with respect to our current theta parameters of the loss function.

837
00:54:17,180 --> 00:54:22,490
Then, we want to sort of subtract a little multiple of that, um,

838
00:54:22,490 --> 00:54:27,320
given by the learning rate from our current parameters to get updated parameters,

839
00:54:27,320 --> 00:54:29,570
and if we repeatedly do then stochastic

840
00:54:29,570 --> 00:54:32,870
gradient descent we'll have better and better parameters

841
00:54:32,870 --> 00:54:35,900
which give higher probability to the things

842
00:54:35,900 --> 00:54:39,305
that we're actually observing in our training data.

843
00:54:39,305 --> 00:54:42,215
So, the thing we want to know is, well,

844
00:54:42,215 --> 00:54:45,350
in general how can we do this um,

845
00:54:45,350 --> 00:54:50,585
differentiation and work out the gradient of our loss function?

846
00:54:50,585 --> 00:54:55,370
And so, I sort of wanted to sort of this the remaining time in this lecture,

847
00:54:55,370 --> 00:54:59,270
um, go through how we can do that by hand, um,

848
00:54:59,270 --> 00:55:02,090
using math and then that'll lead into sort of

849
00:55:02,090 --> 00:55:06,275
discussing and more generally the backpropagation algorithm,

850
00:55:06,275 --> 00:55:08,060
um, for the next one.

851
00:55:08,060 --> 00:55:10,490
Okay. So, if we're doing um,

852
00:55:10,490 --> 00:55:16,700
gradients by hand well we're doing multi-variable calculus, multi-variable derivatives.

853
00:55:16,700 --> 00:55:23,090
But in particular normally the most useful way to think about this is as doing

854
00:55:23,090 --> 00:55:26,780
matrix calculus which means we're directly working with

855
00:55:26,780 --> 00:55:30,935
vectors and matrices to work out our gradients,

856
00:55:30,935 --> 00:55:36,230
and that that's normally sort of much faster and more convenient for

857
00:55:36,230 --> 00:55:41,660
summarizing our neural network layers than trying to do it in a non vectorized way.

858
00:55:41,660 --> 00:55:44,555
But that doesn't mean that's the only way to do it.

859
00:55:44,555 --> 00:55:47,285
If you're sort of confused about what's going on,

860
00:55:47,285 --> 00:55:49,340
sometimes thinking it through in

861
00:55:49,340 --> 00:55:53,780
the non vectorized way can be a better way to understand what's going on and,

862
00:55:53,780 --> 00:55:55,040
um, make more progress.

863
00:55:55,040 --> 00:55:56,795
So, like when, um,

864
00:55:56,795 --> 00:55:59,570
last time I did the word2vec um

865
00:55:59,570 --> 00:56:03,260
derivatives when I was writing too small on that board,

866
00:56:03,260 --> 00:56:08,750
sorry, um, that was doing it in a non vectorized way of working out the weights,

867
00:56:08,750 --> 00:56:10,700
talking about them individually.

868
00:56:10,700 --> 00:56:12,875
Um, but here we're going to do it with,

869
00:56:12,875 --> 00:56:14,945
um, vectors and matrices.

870
00:56:14,945 --> 00:56:19,175
And again, look for the lecture notes to cover this material in more detail.

871
00:56:19,175 --> 00:56:22,025
In particular, so that no one misses it.

872
00:56:22,025 --> 00:56:25,130
Um, let me just clarify what I mean by lecture notes.

873
00:56:25,130 --> 00:56:29,645
So, if you look at the course syllabus on the left-hand column, um,

874
00:56:29,645 --> 00:56:32,840
there's the slides that you can download and,

875
00:56:32,840 --> 00:56:34,400
on straight under the slides,

876
00:56:34,400 --> 00:56:35,915
it says lecture notes.

877
00:56:35,915 --> 00:56:38,105
That's what I'm meaning by the lecture notes.

878
00:56:38,105 --> 00:56:42,530
In the- in the middle column it then has some readings and

879
00:56:42,530 --> 00:56:47,150
actually there are some diffe- additional things there that cover similar material.

880
00:56:47,150 --> 00:56:48,725
Um, so there's, um,

881
00:56:48,725 --> 00:56:51,110
so there's they might be helpful as well.

882
00:56:51,110 --> 00:56:54,830
But first the thing that's closest to what I'm about to present,

883
00:56:54,830 --> 00:56:58,730
it's the lecture notes that appear immediately under the slides link.

884
00:56:58,730 --> 00:57:03,815
Okay. Um, so my hope here, um,

885
00:57:03,815 --> 00:57:06,520
my hope here is the following: Um,

886
00:57:06,520 --> 00:57:10,730
if you can't remembered how to do single variable calculus,

887
00:57:10,730 --> 00:57:13,565
sorry you're basically sunken and might as well leave now.

888
00:57:13,565 --> 00:57:15,560
Um, [LAUGHTER] I'm assuming you know how to do

889
00:57:15,560 --> 00:57:21,245
single-variable calculus and I'm assuming you know what a um a vector and a matrix is.

890
00:57:21,245 --> 00:57:23,960
Um, but you know, um,

891
00:57:23,960 --> 00:57:27,680
I sort of hope that even if you never

892
00:57:27,680 --> 00:57:31,820
did multi-variable calculus or you can't remember any of it,

893
00:57:31,820 --> 00:57:34,220
it's sort of for what we have to do here,

894
00:57:34,220 --> 00:57:37,445
not that hard and you can do it.

895
00:57:37,445 --> 00:57:40,430
So, here's what, um, what you do.

896
00:57:40,430 --> 00:57:42,230
Um, all right.

897
00:57:42,230 --> 00:57:46,820
So, if we have a simple function f of x equals x cubed, right.

898
00:57:46,820 --> 00:57:50,765
Its gradient, um, and so the gradient is the slope, right?

899
00:57:50,765 --> 00:57:53,915
Saying how steep or shallow is the slope of something,

900
00:57:53,915 --> 00:57:59,075
and then when we and also saw the direction of slope when we go into multiple dimensions.

901
00:57:59,075 --> 00:58:01,610
Um, its gradient is just as derivatives.

902
00:58:01,610 --> 00:58:04,430
So, its derivative is 3x squared.

903
00:58:04,430 --> 00:58:07,865
Um, so if you're at the point x equals 3, that you know,

904
00:58:07,865 --> 00:58:10,415
the sort of this 27 of sloppiness,

905
00:58:10,415 --> 00:58:12,305
um, is very steep.

906
00:58:12,305 --> 00:58:20,060
Okay. So well, what if we have a function with one output but now it has many inputs?

907
00:58:20,060 --> 00:58:24,170
Um, so that we're sort of doing that sort of, um,

908
00:58:24,170 --> 00:58:31,295
function that was like the dot products where we're doing the sort of the UTV or WTX,

909
00:58:31,295 --> 00:58:33,170
um, to calculate a value.

910
00:58:33,170 --> 00:58:36,410
Well, then what we're gonna calculate is

911
00:58:36,410 --> 00:58:42,815
a gradient which is a vector of partial derivatives with respect to each input.

912
00:58:42,815 --> 00:58:45,065
So, you take, um,

913
00:58:45,065 --> 00:58:49,505
the slope of the function as you change x1,

914
00:58:49,505 --> 00:58:55,040
the slope of the function as you change x2 through the slope of the, ah,

915
00:58:55,040 --> 00:59:00,710
function as you change xn and each of these you can just calculate as if you were doing

916
00:59:00,710 --> 00:59:04,910
single variable calculus and you just put them all in a vector and

917
00:59:04,910 --> 00:59:09,920
that's then giving you the gradient and then the gradient and multi-dimensional,

918
00:59:09,920 --> 00:59:14,570
um, spaces then giving you the direction and slope of a sort of

919
00:59:14,570 --> 00:59:19,730
a surface that touches your multi-dimensional, um, f function.

920
00:59:19,730 --> 00:59:22,670
Okay. So that's getting a bit scarier,

921
00:59:22,670 --> 00:59:24,500
but it gets a little bit scarier than that

922
00:59:24,500 --> 00:59:27,635
because if we have a neutral network layer, um,

923
00:59:27,635 --> 00:59:32,000
we then have a function which will have n inputs,

924
00:59:32,000 --> 00:59:33,770
which are the input neurons,

925
00:59:33,770 --> 00:59:36,305
and it will have m outputs.

926
00:59:36,305 --> 00:59:39,260
So if that's the case, um,

927
00:59:39,260 --> 00:59:44,765
you then have a matrix of partial derivatives which is referred to as the Jacobian.

928
00:59:44,765 --> 00:59:47,435
So in the Jacobian, um,

929
00:59:47,435 --> 00:59:51,695
you're sort of taking these partial derivatives, um,

930
00:59:51,695 --> 00:59:54,125
with respect to each, um,

931
00:59:54,125 --> 01:00:00,455
output along the rows and with respect to each input down the columns.

932
01:00:00,455 --> 01:00:04,235
And so you're getting these m by n partial derivatives,

933
01:00:04,235 --> 01:00:09,620
considering every combination of an output and an input.

934
01:00:09,620 --> 01:00:13,700
Um, but again, you can fill in every cell of this matrix

935
01:00:13,700 --> 01:00:18,545
just by doing single-variable calculus provided you don't get yourself confused.

936
01:00:18,545 --> 01:00:24,665
Okay. Um, then we already saw when we were doing word2vec,

937
01:00:24,665 --> 01:00:29,435
that sort of a central tool that we have to use to work out,

938
01:00:29,435 --> 01:00:32,480
um, to work out, um,

939
01:00:32,480 --> 01:00:35,150
our derivatives of something like

940
01:00:35,150 --> 01:00:37,490
a neural network model is we have

941
01:00:37,490 --> 01:00:40,820
a sequence of functions that we run up one after another.

942
01:00:40,820 --> 01:00:43,280
So, um, in a neural network you're sort of

943
01:00:43,280 --> 01:00:45,875
running a sequence of functions one after another.

944
01:00:45,875 --> 01:00:47,660
So we have to use, um,

945
01:00:47,660 --> 01:00:52,880
the chain rule to work out derivatives when we compose functions.

946
01:00:52,880 --> 01:00:56,240
So if we have one variable function, so we have,

947
01:00:56,240 --> 01:01:00,665
um, C equals 3y and y equals x squared.

948
01:01:00,665 --> 01:01:03,725
If we want to work out, um,

949
01:01:03,725 --> 01:01:07,430
the derivative of z with respect to x,

950
01:01:07,430 --> 01:01:11,180
we say, aha, that's a composition of two functions.

951
01:01:11,180 --> 01:01:13,190
So I use the chain rule.

952
01:01:13,190 --> 01:01:18,350
And so that means what I do is I multiply, um, the derivative.

953
01:01:18,350 --> 01:01:21,620
So I take, um, dz/dy.

954
01:01:21,620 --> 01:01:25,220
So that's 2x, um,

955
01:01:25,220 --> 01:01:29,030
wait, [NOISE] Sorry, I said that wrong, right?

956
01:01:29,030 --> 01:01:30,530
Is my example wrong?

957
01:01:30,530 --> 01:01:32,290
Oh yeah, its right, dz/dy.

958
01:01:32,290 --> 01:01:34,180
So yeah, dz/dy is just three.

959
01:01:34,180 --> 01:01:36,790
That's, right, that's the derivative of the top line,

960
01:01:36,790 --> 01:01:40,120
and then dy/dx is 2x.

961
01:01:40,120 --> 01:01:44,005
And I multiply those together and I get the answer, um,

962
01:01:44,005 --> 01:01:48,490
that the derivative of z with respect to x is 6x.

963
01:01:48,490 --> 01:01:53,990
Okay. Um, this bit then gets a little bit freakier, but it's true.

964
01:01:53,990 --> 01:01:57,560
If you have lots of variables at once,

965
01:01:57,560 --> 01:02:02,465
you simply multiply the Jacobians and you get the right answer.

966
01:02:02,465 --> 01:02:05,390
So if we're now imagining our neural net,

967
01:02:05,390 --> 01:02:07,910
well sort of, this is our typical neural net right?

968
01:02:07,910 --> 01:02:11,480
So we're doing the neural net layer where we have

969
01:02:11,480 --> 01:02:15,290
our weight matrix multiplied their input vector plus,

970
01:02:15,290 --> 01:02:19,520
um, the bias, and then we're putting it through a non-linearity.

971
01:02:19,520 --> 01:02:24,875
And then if we want to know what's the partials of h with respect to x,

972
01:02:24,875 --> 01:02:27,320
we just say, huh, it's a function composition.

973
01:02:27,320 --> 01:02:28,820
So this is easy to do.

974
01:02:28,820 --> 01:02:31,340
We work out our first Jacobian,

975
01:02:31,340 --> 01:02:34,025
which is the partials of h with respect to z,

976
01:02:34,025 --> 01:02:38,225
and then we just multiply it by the partials of z with respect to x,

977
01:02:38,225 --> 01:02:39,965
and we get the right answer.

978
01:02:39,965 --> 01:02:44,135
Um, easy.

979
01:02:44,135 --> 01:02:47,840
Um, so here's sort of um

980
01:02:47,840 --> 01:02:53,660
an example Jacobian which is a special case that comes up a lot.

981
01:02:53,660 --> 01:02:58,850
Um, so it's just good to realize this one which we'll see with our neural net.

982
01:02:58,850 --> 01:03:03,560
So well one of the things that we have are these element-wise activation function.

983
01:03:03,560 --> 01:03:06,035
So we have h equals f of z.

984
01:03:06,035 --> 01:03:09,200
So, um, what is the, um,

985
01:03:09,200 --> 01:03:14,330
partial derivative of h with respect to z. Um,

986
01:03:14,330 --> 01:03:18,710
well the thing- remember that we sort of apply this element-wise.

987
01:03:18,710 --> 01:03:22,475
So we're actually saying hi equals f of zi.

988
01:03:22,475 --> 01:03:28,460
So, you know, formally this function has n inputs and n outputs,

989
01:03:28,460 --> 01:03:33,035
so it's partial derivatives are going to be an n by n Jacobian.

990
01:03:33,035 --> 01:03:36,529
But if we think about what's happening there,

991
01:03:36,529 --> 01:03:40,615
um, what we're actually going to find is, sort of,

992
01:03:40,615 --> 01:03:45,010
when we're working out the terms of this so we're working out,

993
01:03:45,010 --> 01:03:50,985
how does f of zi change as you change zj?

994
01:03:50,985 --> 01:03:55,280
Well, if j is not equal to i,

995
01:03:55,280 --> 01:03:57,410
it's gonna make no difference at all, right?

996
01:03:57,410 --> 01:04:00,020
So if my f function is something like putting it through

997
01:04:00,020 --> 01:04:04,160
the logistic function or anything else absolute valuing a number,

998
01:04:04,160 --> 01:04:07,880
it's gonna make no difference for the calculation of f of zi

999
01:04:07,880 --> 01:04:12,005
if I chains zj because it's just not in the equation.

1000
01:04:12,005 --> 01:04:16,925
And so, therefore, the only terms that are actually going to occur

1001
01:04:16,925 --> 01:04:22,235
and be non-zero are the terms where i equals j.

1002
01:04:22,235 --> 01:04:28,300
So for working out these partial derivatives if i does not equal j, um, it's zero.

1003
01:04:28,300 --> 01:04:30,564
If i does equal j,

1004
01:04:30,564 --> 01:04:34,390
then we have to work out a single-variable calculus.

1005
01:04:34,390 --> 01:04:37,090
What's the derivative, um,

1006
01:04:37,090 --> 01:04:41,290
of the, um, activation function, um,

1007
01:04:41,290 --> 01:04:44,670
for- and so this is what,

1008
01:04:44,670 --> 01:04:49,445
a um, Jacobian looks like for an activation function.

1009
01:04:49,445 --> 01:04:51,605
It's a diagonal matrix.

1010
01:04:51,605 --> 01:04:53,510
Everything else is zero,

1011
01:04:53,510 --> 01:04:55,640
and we thought this activation function,

1012
01:04:55,640 --> 01:04:57,455
we work out its derivative,

1013
01:04:57,455 --> 01:05:00,095
and then we calculate that for the difference, um,

1014
01:05:00,095 --> 01:05:05,630
we have it for the different kind of um, zi values.

1015
01:05:05,630 --> 01:05:10,970
Okay. Um, so that's a,

1016
01:05:10,970 --> 01:05:14,165
um, Jacobians for an activation function.

1017
01:05:14,165 --> 01:05:15,950
What are the other main cases,

1018
01:05:15,950 --> 01:05:18,410
uh, that we need for a neural network?

1019
01:05:18,410 --> 01:05:23,690
And these I'll go in through a little bit more slowly in the same lecture notes.

1020
01:05:23,690 --> 01:05:27,965
But they're kind of similar to what we saw in the very first class.

1021
01:05:27,965 --> 01:05:34,490
So if we are wanting to work out the partial derivatives of wx plus b with respect to x,

1022
01:05:34,490 --> 01:05:39,000
um, what we get is w. Um,

1023
01:05:39,580 --> 01:05:47,225
and if we want to work out the partial derivative of wx plus b with respect to b,

1024
01:05:47,225 --> 01:05:54,080
um, that means that we get an identity matrix because b is sort of like a 1b, right?

1025
01:05:54,080 --> 01:05:56,270
It's this almost always on vector,

1026
01:05:56,270 --> 01:06:00,455
so you're just getting the ones coming out to preserve the b. Um,

1027
01:06:00,455 --> 01:06:02,585
this was the case, um,

1028
01:06:02,585 --> 01:06:05,030
that we saw, um,

1029
01:06:05,030 --> 01:06:07,640
when we were doing the word vectors.

1030
01:06:07,640 --> 01:06:13,415
That if you have a vector dot product of u and h and you say,

1031
01:06:13,415 --> 01:06:17,870
what's the partial derivatives of that with respect to u,

1032
01:06:17,870 --> 01:06:21,530
then you get out h transpose.

1033
01:06:21,530 --> 01:06:25,340
Um, if you haven't seen those before,

1034
01:06:25,340 --> 01:06:29,420
um, look at the lecture notes handouts, um,

1035
01:06:29,420 --> 01:06:34,010
and see if you can compute them and they make sense at home, um,

1036
01:06:34,010 --> 01:06:38,540
but for the moment we're gonna believe those and use those to

1037
01:06:38,540 --> 01:06:44,030
see how we can then work out derivatives inside the neural network.

1038
01:06:44,030 --> 01:06:48,695
Okay. So here's the same neural network we saw before.

1039
01:06:48,695 --> 01:06:51,365
So we have a window of words,

1040
01:06:51,365 --> 01:06:53,150
we're looking at word vectors,

1041
01:06:53,150 --> 01:06:55,040
we're putting it through a hidden layer,

1042
01:06:55,040 --> 01:06:57,590
and then we're just doing a vector modal, um,

1043
01:06:57,590 --> 01:07:00,935
vector dot product, you get this final score.

1044
01:07:00,935 --> 01:07:05,840
And so, what we [NOISE] want to do to be able to train our neural network,

1045
01:07:05,840 --> 01:07:15,425
is we want to find out how- how s changes depending on all the parameters of the model.

1046
01:07:15,425 --> 01:07:18,950
The x, the w, the b, the u. Um,

1047
01:07:18,950 --> 01:07:24,320
and so we want to work out partial derivatives of S with respect

1048
01:07:24,320 --> 01:07:30,110
to each of those because we can then work out okay if you move b up,

1049
01:07:30,110 --> 01:07:32,465
um, the score gets better,

1050
01:07:32,465 --> 01:07:36,350
which is good if it's actually a plus in the middle,

1051
01:07:36,350 --> 01:07:38,435
and therefore we'll want to nudge up,

1052
01:07:38,435 --> 01:07:41,270
um, elements of b appropriately.

1053
01:07:41,270 --> 01:07:45,050
Okay, um, and so I'm just doing the gradient with

1054
01:07:45,050 --> 01:07:48,725
respect to the score here and I skipped over those couple of slides.

1055
01:07:48,725 --> 01:07:50,930
Um, so if you're just, sort of,

1056
01:07:50,930 --> 01:07:52,970
staring at this picture and say, well,

1057
01:07:52,970 --> 01:07:58,400
how do I work out the partial derivative of s with respect to b?

1058
01:07:58,400 --> 01:08:00,950
Um, probably it doesn't look obvious.

1059
01:08:00,950 --> 01:08:04,520
So the first thing here that you want to do is sort of break up

1060
01:08:04,520 --> 01:08:09,080
the eq- equations into simple pieces that compose together, right?

1061
01:08:09,080 --> 01:08:12,170
So you have the input x,

1062
01:08:12,170 --> 01:08:16,475
and then that goes into z equals wx plus b,

1063
01:08:16,475 --> 01:08:19,580
and then you compose that with the next thing.

1064
01:08:19,580 --> 01:08:23,240
So h equals f of z, our activation function,

1065
01:08:23,240 --> 01:08:27,500
and then this h goes into the next thing of s equals uTh.

1066
01:08:27,500 --> 01:08:29,960
So we've got these sequence of functions.

1067
01:08:29,960 --> 01:08:35,480
And pretty much you want to break things up as much as you can.

1068
01:08:35,480 --> 01:08:37,880
I mean, I could have broken this up even further.

1069
01:08:37,880 --> 01:08:41,000
I could have said z1 equals wx,

1070
01:08:41,000 --> 01:08:44,165
z equals z1 plus b. Um,

1071
01:08:44,165 --> 01:08:45,620
it turns out um,

1072
01:08:45,620 --> 01:08:48,200
but if you've just got things added and subtracted,

1073
01:08:48,200 --> 01:08:52,790
you can sort of do that in one step because that sort of pathway separating the,

1074
01:08:52,790 --> 01:08:54,170
when doing the derivatives,

1075
01:08:54,170 --> 01:08:59,090
but sort of anything else that composes together you want to pull it out for the pieces.

1076
01:08:59,090 --> 01:09:05,605
Okay. So now our neural net is doing a sequence of function compositions.

1077
01:09:05,605 --> 01:09:07,000
And when we say, okay,

1078
01:09:07,000 --> 01:09:09,295
we know how to do that, the chain rule.

1079
01:09:09,295 --> 01:09:13,975
So if you wanna work out the partials of s with respect to b,

1080
01:09:13,975 --> 01:09:19,960
it's just going to be the product of the derivatives of each step along the way.

1081
01:09:19,960 --> 01:09:25,130
So it's gonna be um the partial of s with respect to h times h with

1082
01:09:25,130 --> 01:09:30,530
respect to z times z with respect to b and that will give us the right answer.

1083
01:09:30,530 --> 01:09:35,225
So then all we have to do is actually compute that.

1084
01:09:35,225 --> 01:09:38,580
Um, so, I think this just sort of

1085
01:09:38,580 --> 01:09:42,040
shows okay we're taking the partials of each step of that composition.

1086
01:09:42,040 --> 01:09:44,975
Okay. So now we want to compute that.

1087
01:09:44,975 --> 01:09:49,800
And so this is where I'm going to sort of use the Jacobians that I

1088
01:09:49,800 --> 01:09:54,415
sort of asserted without much proof on the preceding slide.

1089
01:09:54,415 --> 01:10:00,280
Okay. So first of all um we have ds/dh.

1090
01:10:00,280 --> 01:10:03,900
Well, that's just the dot product of two vectors.

1091
01:10:03,900 --> 01:10:11,070
So the um, the Jacobian for that is just h transpose.

1092
01:10:11,070 --> 01:10:12,640
Okay, that's a start.

1093
01:10:12,640 --> 01:10:15,935
Then we have um h equals f of z.

1094
01:10:15,935 --> 01:10:18,505
Well, that's the activation function.

1095
01:10:18,505 --> 01:10:22,270
So the um Jacobian of that is

1096
01:10:22,270 --> 01:10:27,615
this diagonal matrix made of the element wise um derivative of the function

1097
01:10:27,615 --> 01:10:32,190
f. And then we have the partial of z

1098
01:10:32,190 --> 01:10:37,080
with respect to b and that's the bit that comes out as the identity matrix.

1099
01:10:37,080 --> 01:10:45,950
And so that's then giving us our calculation of the partial of s with respect to b.

1100
01:10:46,460 --> 01:10:51,890
And so we can see that the- the identity matrix sort of goes

1101
01:10:51,890 --> 01:10:57,905
away so we end up with this composition of ht times f prime of z.

1102
01:10:57,905 --> 01:11:05,960
Okay, suppose we then want to go on and compute now the partial of s with respect to w?

1103
01:11:05,960 --> 01:11:08,410
Well, as starting off point is

1104
01:11:08,410 --> 01:11:13,640
exactly the same chain rule that we work out each of the stages.

1105
01:11:13,640 --> 01:11:19,120
So, that first of all you're working

1106
01:11:19,120 --> 01:11:24,500
out the z from the wx part then putting it through the non linearity,

1107
01:11:24,500 --> 01:11:27,860
then doing the dot product of the vectors.

1108
01:11:27,860 --> 01:11:29,700
So that part is the same.

1109
01:11:29,700 --> 01:11:33,500
And what you should notice is that if you

1110
01:11:33,500 --> 01:11:39,620
compare the partial of s with respect to w versus s with respect to b,

1111
01:11:39,620 --> 01:11:45,510
most of them are the same and it's only the part at the end that's different.

1112
01:11:45,510 --> 01:11:49,260
And that sort of makes sense in terms of our neural net right?

1113
01:11:49,260 --> 01:11:57,370
That when we had our neural net that the w and the b were coming in here.

1114
01:11:57,370 --> 01:12:01,740
And once you've sort of done some stuff with them you're putting things through

1115
01:12:01,740 --> 01:12:07,230
the same activation function and doing the same dot product to create a score.

1116
01:12:07,230 --> 01:12:11,085
So, you're sort of doing the same calculations that you're then composing with.

1117
01:12:11,085 --> 01:12:13,675
So it sort of makes sense that you should be getting

1118
01:12:13,675 --> 01:12:16,100
the same derivatives that are

1119
01:12:16,100 --> 01:12:20,270
occur- same partial derivatives that occurring at that point.

1120
01:12:20,270 --> 01:12:26,410
Oops. And so effectively you know

1121
01:12:26,410 --> 01:12:29,930
these partial dev- derivatives correspond to

1122
01:12:29,930 --> 01:12:35,650
the computations in the neural network that are above where w and b are.

1123
01:12:35,650 --> 01:12:40,585
And so those are commonly referred to as delta,

1124
01:12:40,585 --> 01:12:44,880
note delta which is different from partial derivative d. And so

1125
01:12:44,880 --> 01:12:49,295
delta is referred to as the error signal and neural network talk.

1126
01:12:49,295 --> 01:12:52,520
So, it's the what you're calculating as

1127
01:12:52,520 --> 01:12:55,070
the partial derivatives above

1128
01:12:55,070 --> 01:12:59,885
the parameters that you are working out the partial derivatives with respect to.

1129
01:12:59,885 --> 01:13:04,270
So, a lot of the secret as we'll see next time,

1130
01:13:04,270 --> 01:13:11,240
a lot of the secret of what happens with backpropagation is

1131
01:13:11,240 --> 01:13:15,160
just we want to do efficient computation in

1132
01:13:15,160 --> 01:13:19,525
the sort of way that's computer science people like to do efficient computation.

1133
01:13:19,525 --> 01:13:23,170
And so precisely what we want to notice is that there is

1134
01:13:23,170 --> 01:13:28,510
one error signal that comes from above and we want to compute it once.

1135
01:13:28,510 --> 01:13:31,145
And then reuse that when calculating

1136
01:13:31,145 --> 01:13:36,075
both partial derivatives with respect to w and with b.

1137
01:13:36,075 --> 01:13:43,100
Okay. So there's sort of two things to still do.

1138
01:13:43,100 --> 01:13:46,920
So one is well,

1139
01:13:46,920 --> 01:13:49,990
it'd be kind of useful to know what the partial derivative

1140
01:13:49,990 --> 01:13:53,150
of s with respect to w actually looks like.

1141
01:13:53,150 --> 01:13:55,065
I mean, is that a number, a vector,

1142
01:13:55,065 --> 01:13:58,185
a matrix, a three-dimensional tensor?

1143
01:13:58,185 --> 01:14:02,000
And then we actually want to work out its values

1144
01:14:02,000 --> 01:14:05,970
and to work out its values we're going to still have to work

1145
01:14:05,970 --> 01:14:08,640
out the partial derivative of z with respect to

1146
01:14:08,640 --> 01:14:13,755
w. But if first of all we just try and work out its shape,

1147
01:14:13,755 --> 01:14:17,160
what kind of shape does it have?

1148
01:14:17,160 --> 01:14:20,900
And this is actually sort of a bit tricky and is

1149
01:14:20,900 --> 01:14:25,240
sort of a dirty underbelly of doing this kind of matrix calculus.

1150
01:14:25,240 --> 01:14:31,060
So, since our weight vector is an n by m matrix,

1151
01:14:31,060 --> 01:14:37,870
the end result of the partial of s with respect to w is we have a function with

1152
01:14:37,870 --> 01:14:45,995
n times m inputs all of the elements of w and simply one output which is our score.

1153
01:14:45,995 --> 01:14:49,920
So, that makes it sound like according to what I said before we

1154
01:14:49,920 --> 01:14:54,380
should have a one by n times m Jacobian.

1155
01:14:54,380 --> 01:14:57,010
But it turns out that's not really what we want, right?

1156
01:14:57,010 --> 01:15:01,450
Because what we wanted to do is use what we calculate

1157
01:15:01,450 --> 01:15:07,380
inside this stochastic gradient descent update algorithm.

1158
01:15:07,380 --> 01:15:12,470
And if we're doing this with sort of like to have

1159
01:15:12,470 --> 01:15:19,130
the old weight matrix and we'd like to subtract a bit format to get a new weight matrix.

1160
01:15:19,130 --> 01:15:29,465
So, be kind of nice if the shape of our Jacobian was the same shape as w. And so we-

1161
01:15:29,465 --> 01:15:32,520
we and in general what you always want to do with

1162
01:15:32,520 --> 01:15:37,500
neural nets is follow what we call the shape convention which

1163
01:15:37,500 --> 01:15:45,875
is we're going to sort of represent the Jacobian so it's in the same shape as the inputs.

1164
01:15:45,875 --> 01:15:50,775
And this whole thing is kind of the- the bad part of

1165
01:15:50,775 --> 01:15:56,105
the bad part of doing matrix calculus.

1166
01:15:56,105 --> 01:16:00,615
Like there's a lot of inconsistency as to how people represent matrix calculus.

1167
01:16:00,615 --> 01:16:03,030
That in general if you just go to different fields like

1168
01:16:03,030 --> 01:16:05,890
economics and physics some people use a numerator convention.

1169
01:16:05,890 --> 01:16:08,085
Some people use a denominator convention.

1170
01:16:08,085 --> 01:16:09,350
We're using neither of those.

1171
01:16:09,350 --> 01:16:12,580
We're going to use this shape convention so we match the shape of

1172
01:16:12,580 --> 01:16:16,655
the input so it makes it easy to do our weight updates.

1173
01:16:16,655 --> 01:16:23,125
Okay. So. Right. So that's what we want the answer to look like.

1174
01:16:23,125 --> 01:16:26,690
So, then the final thing we need to do to work out on the

1175
01:16:26,690 --> 01:16:30,320
partial of s with respect to w is we have the error signal delta

1176
01:16:30,320 --> 01:16:34,105
that's gonna be part of the answer and then we want to work out the partial

1177
01:16:34,105 --> 01:16:39,415
of z with respect to w. Well,

1178
01:16:39,415 --> 01:16:44,360
um what's that going to be.

1179
01:16:44,360 --> 01:16:47,690
Well, it turns out and I'm about to be

1180
01:16:47,690 --> 01:16:51,195
saved by the bell here since I'm down to two minutes left.

1181
01:16:51,195 --> 01:16:57,070
Um, it turns out that what we end up with for that is we take

1182
01:16:57,070 --> 01:17:03,250
the product of the partial- the product of delta times x.

1183
01:17:03,250 --> 01:17:08,020
So effectively we've got the local error signal above w. And then we

1184
01:17:08,020 --> 01:17:13,715
have the inputs x and we are working out an outer product of them.

1185
01:17:13,715 --> 01:17:19,565
And the sort of way to think about this is sort of for the w's.

1186
01:17:19,565 --> 01:17:23,190
You know, we've got the elements of the w matrix,

1187
01:17:23,190 --> 01:17:26,780
these different connections between our neurons.

1188
01:17:26,780 --> 01:17:32,170
And so each one of these is connecting one output to one input.

1189
01:17:32,170 --> 01:17:35,990
And so we're going to be sort of making this n by

1190
01:17:35,990 --> 01:17:40,700
m matrix of our partial derivatives that are going to be the product of

1191
01:17:40,700 --> 01:17:44,880
the error signal for the appropriate output

1192
01:17:44,880 --> 01:17:50,505
multiplied by input and those goes give us the partial derivatives.

1193
01:17:50,505 --> 01:17:55,980
I'm skipping ahead quickly in my last one minute.

1194
01:17:55,980 --> 01:17:59,850
Okay. So uh, right.

1195
01:17:59,850 --> 01:18:01,430
So this is sort of what I said have used

1196
01:18:01,430 --> 01:18:04,675
the shape con- convention. I'm going to skip that.

1197
01:18:04,675 --> 01:18:10,770
Okay. So, um, I- I ran out of time a teeny bit at the end but I mean,

1198
01:18:10,770 --> 01:18:13,860
I think hopefully that's conveyed most of

1199
01:18:13,860 --> 01:18:18,630
the idea of how you can sort of use the chain rule and

1200
01:18:18,630 --> 01:18:22,080
work out the derivatives and work them out in

1201
01:18:22,080 --> 01:18:26,400
terms of these vector and matrix derivatives.

1202
01:18:26,400 --> 01:18:31,170
[NOISE] And essentially what we wanna do for backpropagation is to say how can we

1203
01:18:31,170 --> 01:18:37,205
do ah get a computer to do this automatically for us and to do it efficiently.

1204
01:18:37,205 --> 01:18:39,820
And that's what's sort of the deep learning frameworks like

1205
01:18:39,820 --> 01:18:43,110
TensorFlow and PyTorch do and how you can do that.

1206
01:18:43,110 --> 01:18:45,600
We'll look at more next time.

