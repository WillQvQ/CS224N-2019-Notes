1
00:00:05,480 --> 00:00:07,890
Okay. Hi, everyone.

2
00:00:07,890 --> 00:00:10,365
Let's get started again.

3
00:00:10,365 --> 00:00:17,100
Okay, so first of all let me just say a bit about Assignment 5.

4
00:00:17,100 --> 00:00:20,715
So Assignment 5 is coming out today.

5
00:00:20,715 --> 00:00:22,580
Um, it's a brand new assignment,

6
00:00:22,580 --> 00:00:25,080
so you guys are the guinea pigs for that.

7
00:00:25,080 --> 00:00:31,365
Um, and so what it's going to be, it essentially builds on Assignment 4.

8
00:00:31,365 --> 00:00:34,560
Um, so it's okay if you didn't do perfectly on Assignment 4,

9
00:00:34,560 --> 00:00:36,405
but I think actually most people did.

10
00:00:36,405 --> 00:00:39,990
Um, and what we're gonna be doing is adding, um,

11
00:00:39,990 --> 00:00:42,290
convolutional neural networks and subword

12
00:00:42,290 --> 00:00:45,590
modeling to the neural machine translation system,

13
00:00:45,590 --> 00:00:47,305
seeking it to make it better.

14
00:00:47,305 --> 00:00:52,850
Um, so this assignment is coding heavy, written questions light.

15
00:00:52,850 --> 00:00:56,360
Um, so I mean the coding that you have

16
00:00:56,360 --> 00:01:00,410
to do sort of isn't actually really more difficult than Assignment 4,

17
00:01:00,410 --> 00:01:02,585
it's kind of like Assignment 4.

18
00:01:02,585 --> 00:01:08,710
But what we're hoping is that this time you will be able to do it on your own.

19
00:01:08,710 --> 00:01:10,725
Now what I mean by that,

20
00:01:10,725 --> 00:01:12,480
um, is for Assignment 4.

21
00:01:12,480 --> 00:01:16,640
Well, there was tons of scaffolding telling you what everything should be,

22
00:01:16,640 --> 00:01:20,270
and there were all of these auto-grader checks and you could keep on working on

23
00:01:20,270 --> 00:01:24,980
your code until they passed all the autograder checks, and everybody did.

24
00:01:24,980 --> 00:01:29,270
Um, and so it was very kind of coddled, shall we say.

25
00:01:29,270 --> 00:01:31,625
Um, but I mean,

26
00:01:31,625 --> 00:01:37,160
I guess what we're really wanting to achieve is to have a more- sorry, question.

27
00:01:37,160 --> 00:01:38,390
[inaudible]?

28
00:01:38,390 --> 00:01:43,510
Yes. So what we're hoping is that this can be, uh, useful.

29
00:01:43,510 --> 00:01:48,380
Um, it'll be short-term pain but useful as being

30
00:01:48,380 --> 00:01:51,350
a more effective ramp to doing

31
00:01:51,350 --> 00:01:55,400
the final project and indeed for the rest of your life, right.

32
00:01:55,400 --> 00:01:58,220
And the- the reality is that in the rest of your life,

33
00:01:58,220 --> 00:02:01,820
you sort of if you're going to be doing things with deep learning,

34
00:02:01,820 --> 00:02:03,920
you kind of have to work out what kind of

35
00:02:03,920 --> 00:02:06,740
model to build and which pieces to stitch together,

36
00:02:06,740 --> 00:02:11,090
and how to write some tests to see if it's doing something sensible.

37
00:02:11,090 --> 00:02:13,775
And if it's not doing something sensible, um,

38
00:02:13,775 --> 00:02:17,509
to figure out how you could change things and try different things,

39
00:02:17,509 --> 00:02:19,355
and get it to work sensibly.

40
00:02:19,355 --> 00:02:21,155
And so that's what we're hoping,

41
00:02:21,155 --> 00:02:22,730
um, that people, um,

42
00:02:22,730 --> 00:02:24,650
can do in Assignment 5,

43
00:02:24,650 --> 00:02:25,850
so you've got to,

44
00:02:25,850 --> 00:02:28,090
um, figure things out.

45
00:02:28,090 --> 00:02:31,790
Um, should write your own testing code.

46
00:02:31,790 --> 00:02:34,490
Um, we don't have a public autograder,

47
00:02:34,490 --> 00:02:38,510
so you should- that's part of working out your own sanity checks,

48
00:02:38,510 --> 00:02:44,150
trying to do things like what I talked about last week of sort of getting

49
00:02:44,150 --> 00:02:47,420
simple bits working, confirming that they work on

50
00:02:47,420 --> 00:02:51,620
minute amounts of test data and so on, and doing things more sensibly.

51
00:02:51,620 --> 00:02:53,999
I mean in particular,

52
00:02:55,490 --> 00:02:59,100
um, the one particular part of that,

53
00:02:59,100 --> 00:03:02,640
that we were planning to do, um, for,

54
00:03:02,640 --> 00:03:06,015
um, this assignment, I was looking for it,

55
00:03:06,015 --> 00:03:07,920
um, but it's on the next slide.

56
00:03:07,920 --> 00:03:11,620
Um, so, um, for this assignment and beyond, um,

57
00:03:11,620 --> 00:03:16,260
we're going to enforce rules like more like they are in CS107,

58
00:03:16,260 --> 00:03:18,130
for those of you who are undergrads,

59
00:03:18,130 --> 00:03:22,835
meaning that the TAs don't look at and debug your code for you.

60
00:03:22,835 --> 00:03:24,860
Um, and so, you know,

61
00:03:24,860 --> 00:03:29,930
of course we still want TAs to be helpful, come to them with your problems, um,

62
00:03:29,930 --> 00:03:33,514
talk about how you're meant to use different things,

63
00:03:33,514 --> 00:03:35,420
um, in the PyTorch library,

64
00:03:35,420 --> 00:03:41,300
um, but you shouldn't be regarding it as the TA's job of, here's a big Python file.

65
00:03:41,300 --> 00:03:43,580
Um, can you tell me what's wrong with it,

66
00:03:43,580 --> 00:03:45,830
and fix it up for you.

67
00:03:45,830 --> 00:03:49,485
Okay. Um, the precise policy for that's,

68
00:03:49,485 --> 00:03:51,870
um, written up on Piazza.

69
00:03:51,870 --> 00:03:58,130
Okay. So after- any questions about that or do I go straight on in?

70
00:03:59,580 --> 00:04:05,145
Okay. Um, yes so today's lecture,

71
00:04:05,145 --> 00:04:10,650
um, in some sense today's lecture is an easy lecture.

72
00:04:10,650 --> 00:04:13,350
Um, so last time's lecture,

73
00:04:13,350 --> 00:04:16,420
there was really sort of a ton of new stuff

74
00:04:16,420 --> 00:04:19,975
of other stuff on neural networks that you haven't seen before,

75
00:04:19,975 --> 00:04:23,410
and we did Convnets and we did pooling layers,

76
00:04:23,410 --> 00:04:26,260
and we did highway and residual connections,

77
00:04:26,260 --> 00:04:29,970
and batch norms, and I don't know, whatever else we did.

78
00:04:29,970 --> 00:04:32,525
Um, size one convolutions I guess.

79
00:04:32,525 --> 00:04:36,110
So there are tons of new stuff really

80
00:04:36,110 --> 00:04:39,580
in this lecture in terms of sort of neural network machinery,

81
00:04:39,580 --> 00:04:41,300
there isn't any new stuff at all.

82
00:04:41,300 --> 00:04:42,500
So this is really easy.

83
00:04:42,500 --> 00:04:48,875
Um, and this is also really a new lecture but it was sort of put in for a reason.

84
00:04:48,875 --> 00:04:53,570
And the reason for this relates to a kinda remark I made

85
00:04:53,570 --> 00:04:58,700
last time about how lots of stuff keeps changing in neural network land.

86
00:04:58,700 --> 00:05:01,850
So at the time we first designed this class and

87
00:05:01,850 --> 00:05:05,090
the way as- that a lot of the structure of it still is.

88
00:05:05,090 --> 00:05:10,830
Um, that sort of around 2014-2015 when we designed this class,

89
00:05:10,830 --> 00:05:13,175
it was basically axiomatic that

90
00:05:13,175 --> 00:05:18,185
all deep learning models for natural language processing worked off words.

91
00:05:18,185 --> 00:05:22,400
And therefore it completely made sense that we start with word vectors,

92
00:05:22,400 --> 00:05:26,170
and then we start looking at things like recurrent models over words.

93
00:05:26,170 --> 00:05:31,580
Whereas the fact of the matter is in the last approximately three years,

94
00:05:31,580 --> 00:05:36,455
there's been a ton of new work including some of the most influential new work.

95
00:05:36,455 --> 00:05:40,960
There's building language models that aren't- isn't -isn't aren't, um,

96
00:05:40,960 --> 00:05:46,895
being built over words that they're building, built over pieces of words or characters.

97
00:05:46,895 --> 00:05:49,640
And so this lecture is sort of meant to give you

98
00:05:49,640 --> 00:05:52,985
some sense of these other ways of doing things and,

99
00:05:52,985 --> 00:05:56,900
um, some orientation to some of the things that's going on.

100
00:05:56,900 --> 00:05:59,930
But the actual kind of models that we're looking at, uh,

101
00:05:59,930 --> 00:06:03,920
sort of using all of the building blocks that we've already looked at, things like,

102
00:06:03,920 --> 00:06:08,030
um, RNNs and ConvNets and things like that.

103
00:06:08,030 --> 00:06:09,950
So let's get into this.

104
00:06:09,950 --> 00:06:13,130
Um, so I'm going to start off with a teeny bit

105
00:06:13,130 --> 00:06:17,105
of linguistics of learning about the structure of language, um,

106
00:06:17,105 --> 00:06:22,520
at first sort of lower level units of language and then we'll see how that pans out,

107
00:06:22,520 --> 00:06:25,345
um, for things like character level models.

108
00:06:25,345 --> 00:06:27,440
So in linguistics, um,

109
00:06:27,440 --> 00:06:30,980
if you start at the bottom of the totem pole,

110
00:06:30,980 --> 00:06:34,099
the first-level of linguistics is phonetics,

111
00:06:34,099 --> 00:06:38,420
which is sort of understanding the sounds and the physiology of human speech.

112
00:06:38,420 --> 00:06:40,860
So that's sort of like physics, or physiology,

113
00:06:40,860 --> 00:06:44,355
or something, right, there are mouth parts that move,

114
00:06:44,355 --> 00:06:46,529
there are ear parts that act as filters,

115
00:06:46,529 --> 00:06:50,030
and there's, um, audio waves in between the two of them.

116
00:06:50,030 --> 00:06:53,270
So that's kind of uncontroversial in some sense.

117
00:06:53,270 --> 00:06:55,420
Um, but above that level,

118
00:06:55,420 --> 00:07:00,460
the standard thing that people do for the analysis of human languages is to say,

119
00:07:00,460 --> 00:07:06,230
well human languages may seem to make use of a relatively small set of

120
00:07:06,230 --> 00:07:12,320
distinctive units which are then commonly called phonemes which are actually categorical.

121
00:07:12,320 --> 00:07:16,480
And the idea here is that well,

122
00:07:16,480 --> 00:07:19,890
uh, our mouths are continuous spaces, right.

123
00:07:19,890 --> 00:07:23,030
That they've got these various bits of their mouths like, you know,

124
00:07:23,030 --> 00:07:24,890
tongues and pharynges and so on,

125
00:07:24,890 --> 00:07:27,020
but it's a continuous space.

126
00:07:27,020 --> 00:07:31,610
So actually, um, we can make an infinite variety of sounds, right.

127
00:07:31,610 --> 00:07:37,830
So if I open my mouth and apply voicing and just wiggle my tongue around, I can go [NOISE].

128
00:07:37,830 --> 00:07:41,570
And I can make an infinite variety of different sounds.

129
00:07:41,570 --> 00:07:46,990
But the reality is that human languages aren't like that,

130
00:07:46,990 --> 00:07:49,915
that out of that infinite variety of sounds,

131
00:07:49,915 --> 00:07:53,230
we distinguish a small space of sounds.

132
00:07:53,230 --> 00:07:58,570
Um, and something that happens when languages change is, um,

133
00:07:58,570 --> 00:08:01,535
that the space of sounds that are seen as

134
00:08:01,535 --> 00:08:05,365
important and distinguished in a language change.

135
00:08:05,365 --> 00:08:10,570
And that happens even with inside one language as- as English.

136
00:08:10,570 --> 00:08:13,255
And I'm about to give an example of that.

137
00:08:13,255 --> 00:08:20,630
Um, so people in cog psych talk about the phenomenon of categorical perception.

138
00:08:20,630 --> 00:08:24,535
And what that means is that really there's something continuous,

139
00:08:24,535 --> 00:08:30,410
but that humans perceive it as belonging to fairly sharp categories.

140
00:08:30,410 --> 00:08:31,720
Um, and, you know,

141
00:08:31,720 --> 00:08:34,345
you can use that for sort of, you know,

142
00:08:34,345 --> 00:08:38,350
styles of clothing or whether someone counts as fat or not.

143
00:08:38,350 --> 00:08:43,745
Um, but the most famous examples of categorical perception are in language,

144
00:08:43,745 --> 00:08:46,930
where we can make an infinite variety of sounds but

145
00:08:46,930 --> 00:08:50,435
people per- perceive them as categories.

146
00:08:50,435 --> 00:08:55,775
And so effectively what that means is when you have categorical perception,

147
00:08:55,775 --> 00:09:00,650
the differences within a category are sort of perceived to have shrunk.

148
00:09:00,650 --> 00:09:03,700
You barely notice them at all where differences

149
00:09:03,700 --> 00:09:07,615
across categories are expanded and very clear.

150
00:09:07,615 --> 00:09:09,850
And so one of the cases that sort of studied

151
00:09:09,850 --> 00:09:12,770
a lot is what's called- referred to as sort of,

152
00:09:12,770 --> 00:09:14,770
um, voice onset time.

153
00:09:14,770 --> 00:09:19,930
So lots of languages including English have pairs of sounds like p and b, uh,

154
00:09:19,930 --> 00:09:25,150
pah and bah, and they differ based on when voicing starts.

155
00:09:25,150 --> 00:09:29,165
So buh, it has a voice sound like a vowel with an r in it.

156
00:09:29,165 --> 00:09:31,510
And well that's a continuous parameter,

157
00:09:31,510 --> 00:09:36,835
you can sort of make any point along a spectrum between a p and a b but, um,

158
00:09:36,835 --> 00:09:39,710
human beings, um, who speak English,

159
00:09:39,710 --> 00:09:43,010
um, perceive just two points on that spectrum.

160
00:09:43,010 --> 00:09:47,320
And you don't sort of really notice the fine differences between them.

161
00:09:47,320 --> 00:09:50,380
Um, some languages distinguish more points on the spectrum.

162
00:09:50,380 --> 00:09:53,980
So Thai distinguishes three different consonant sounds,

163
00:09:53,980 --> 00:09:56,810
um, in depending on the voice onset time.

164
00:09:56,810 --> 00:09:58,840
Um, something that might be, um,

165
00:09:58,840 --> 00:10:01,400
more accessible to you is,

166
00:10:01,400 --> 00:10:03,920
um, this is an example of language change.

167
00:10:03,920 --> 00:10:06,375
So for a speaker like me, um,

168
00:10:06,375 --> 00:10:10,480
there was caught and cot and those are different vowels,

169
00:10:10,480 --> 00:10:12,815
and I hear them as different vowels.

170
00:10:12,815 --> 00:10:17,455
But if you are someone who grew up in the southwest of the United States,

171
00:10:17,455 --> 00:10:21,820
um, then these are exactly the same vowel and you don't distinguish them.

172
00:10:21,820 --> 00:10:26,390
Then- you thought I said the same thing twice even though I'm saying two different vowels.

173
00:10:26,390 --> 00:10:34,120
And so that's where in even at a dialectal issue- level that people develop

174
00:10:34,120 --> 00:10:36,420
categorical perception as to

175
00:10:36,420 --> 00:10:43,400
which- which distinctions and sounds they're sensitive to or not sensitive to.

176
00:10:43,950 --> 00:10:46,535
Okay. And summing- and,

177
00:10:46,535 --> 00:10:51,370
I mean why I'm mentioning this is in some senses these sound distinctions of

178
00:10:51,370 --> 00:10:54,460
categorical sound distinctions that are what a lot of

179
00:10:54,460 --> 00:10:58,975
our language writing systems that we'll come to in a minute record.

180
00:10:58,975 --> 00:11:03,500
Okay. Um, so in traditional linguistics, um,

181
00:11:03,500 --> 00:11:08,195
you have sounds, but sounds don't have any meanings in language.

182
00:11:08,195 --> 00:11:10,805
So pah and bah don't have meanings,

183
00:11:10,805 --> 00:11:12,935
and a and e don't have meanings.

184
00:11:12,935 --> 00:11:15,875
And so people then normally distinguish as

185
00:11:15,875 --> 00:11:20,150
the next level up morphology is parts of words.

186
00:11:20,150 --> 00:11:23,540
And this is seen as the minimal level that has meaning.

187
00:11:23,540 --> 00:11:27,620
And so the idea is lots of words are complex and can be made

188
00:11:27,620 --> 00:11:31,660
u- made up of pieces but these pieces do have meanings.

189
00:11:31,660 --> 00:11:36,050
So fortune has a meaning, um, fortunate,

190
00:11:36,050 --> 00:11:38,805
you end in this ate ending,

191
00:11:38,805 --> 00:11:42,510
um, which sort of gives the- gives fortune to somebody.

192
00:11:42,510 --> 00:11:45,405
So that means you know, having fortune, um,

193
00:11:45,405 --> 00:11:46,610
that has a meaning,

194
00:11:46,610 --> 00:11:49,360
un has a meaning which means to reverse that.

195
00:11:49,360 --> 00:11:54,195
So unfortunate means that you don't have fortune, and ly, um,

196
00:11:54,195 --> 00:11:57,265
then has a meaning of turning this all into an adverb,

197
00:11:57,265 --> 00:11:59,825
and you can say unfortunately not having,

198
00:11:59,825 --> 00:12:02,290
um, gotten fortune, something happened.

199
00:12:02,290 --> 00:12:04,400
And so these sort of pieces of, um,

200
00:12:04,400 --> 00:12:07,910
words are then the minimal things that have meaning.

201
00:12:07,910 --> 00:12:10,775
Um, almost no work in deep learning has tried to

202
00:12:10,775 --> 00:12:13,480
make use of this sort of morpheme level of structure.

203
00:12:13,480 --> 00:12:17,785
Actually me and a couple of students six years ago did actually

204
00:12:17,785 --> 00:12:22,515
try and build a system where it built these tree structured neural networks,

205
00:12:22,515 --> 00:12:25,710
that put together meanings of words out of their pieces.

206
00:12:25,710 --> 00:12:29,875
Um, but that really isn't an idea that's taken on widely.

207
00:12:29,875 --> 00:12:33,660
There's sort of a reason why it hasn't taken on widely,

208
00:12:33,660 --> 00:12:38,490
which is doing this and working out the semantically meaningful pieces of words,

209
00:12:38,490 --> 00:12:44,925
is kind of hard and a lot of the time in NLP what people have found

210
00:12:44,925 --> 00:12:48,215
is you can just about get the same kind of

211
00:12:48,215 --> 00:12:52,090
results if you just work with character n-grams.

212
00:12:52,090 --> 00:12:55,560
The kind of units that you put into the convolutional neural net.

213
00:12:55,560 --> 00:12:59,105
Because if you just have a model that uses, um,

214
00:12:59,105 --> 00:13:05,045
character trigrams and you have sort of start of word, un, and nfo, and so on.

215
00:13:05,045 --> 00:13:08,770
For going through the ly end of word,

216
00:13:08,770 --> 00:13:10,840
that those different units.

217
00:13:10,840 --> 00:13:13,085
There's different character trigrams,

218
00:13:13,085 --> 00:13:15,280
in a distributed way will pick up

219
00:13:15,280 --> 00:13:19,765
all the important meaning components of the word pretty well,

220
00:13:19,765 --> 00:13:21,545
and that that's just good enough.

221
00:13:21,545 --> 00:13:27,035
And that's actually a very classic idea that's sort of been revived.

222
00:13:27,035 --> 00:13:30,220
Um, so back in the second coming of

223
00:13:30,220 --> 00:13:35,045
neural networks in the mid-80s into the early 90s, um,

224
00:13:35,045 --> 00:13:37,255
there was qu- um,

225
00:13:37,255 --> 00:13:41,440
there was quite a bit of sort of controversial work on

226
00:13:41,440 --> 00:13:46,030
the structure of language and in particular Dave Rumelhart and Jay McClelland.

227
00:13:46,030 --> 00:13:48,500
So Jay McClelland's still in the psych department here,

228
00:13:48,500 --> 00:13:50,725
if you want to look him up in your spare time.

229
00:13:50,725 --> 00:13:57,460
Um, they proposed a model of how to model generating past tense forms in English.

230
00:13:57,460 --> 00:14:00,610
So this was sort of a cog-psych experiment of can we build

231
00:14:00,610 --> 00:14:04,045
a system that can learn past tenses of English verbs?

232
00:14:04,045 --> 00:14:07,780
And the difficult part there is some, many verbs are regular,

233
00:14:07,780 --> 00:14:09,740
you add the kinda -ed ending,

234
00:14:09,740 --> 00:14:14,015
but some words are irregular and you had to sort of learn about the irregular patterning.

235
00:14:14,015 --> 00:14:17,015
Um, but the way they did that.

236
00:14:17,015 --> 00:14:23,530
I mean partly because this was sort of early days with respect to, um, sequence models,

237
00:14:23,530 --> 00:14:27,094
is that they used a representation where they represented

238
00:14:27,094 --> 00:14:31,255
words precisely with these sort of character trigrams.

239
00:14:31,255 --> 00:14:35,780
And that was the representation of words that they used and fed forward in their model.

240
00:14:35,780 --> 00:14:40,220
And that, um, idea was met with a lot of controversy by

241
00:14:40,220 --> 00:14:43,535
linguists, philosophers, and other people with their ideas of

242
00:14:43,535 --> 00:14:47,075
language and so there was a lot of debate in those days about that.

243
00:14:47,075 --> 00:14:50,500
But from a- as a purely engineering solution that

244
00:14:50,500 --> 00:14:54,070
sort of proved to be a pretty good way to do things.

245
00:14:54,070 --> 00:14:59,245
And so this decade there's been other work which includes the model, um,

246
00:14:59,245 --> 00:15:02,995
developed at Microsoft of a sort of a deep, um,

247
00:15:02,995 --> 00:15:05,860
semantics model where what they're using as these kind of

248
00:15:05,860 --> 00:15:09,420
character n-grams to put meaning over words.

249
00:15:09,420 --> 00:15:17,195
Okay so, um, so now we might be interested in building models that aren't over words.

250
00:15:17,195 --> 00:15:21,065
So we're going to have a word written as characters

251
00:15:21,065 --> 00:15:25,355
and we're going to do something with it such as build character n-grams.

252
00:15:25,355 --> 00:15:28,775
And so something that is just useful, um,

253
00:15:28,775 --> 00:15:31,010
to know is there's actually

254
00:15:31,010 --> 00:15:33,980
a fair amount of variation between languages when you do this.

255
00:15:33,980 --> 00:15:36,200
So it's not all the same stuff, right?

256
00:15:36,200 --> 00:15:38,465
So the first problem is, um,

257
00:15:38,465 --> 00:15:42,455
there are some languages that don't put spaces between words.

258
00:15:42,455 --> 00:15:45,125
The most famous example is Chinese.

259
00:15:45,125 --> 00:15:51,545
Um, but an interesting fact for those people of European ancestry, um,

260
00:15:51,545 --> 00:15:56,630
is that you know if- for- when the ancient Greeks wrote ancient Greek,

261
00:15:56,630 --> 00:15:59,525
um, they didn't put spaces between words either.

262
00:15:59,525 --> 00:16:02,390
It was actually a later invention of

263
00:16:02,390 --> 00:16:06,170
medieval scholars who are recopying their manuscripts,

264
00:16:06,170 --> 00:16:08,690
who they decided [NOISE] maybe that'll be easier to read if we

265
00:16:08,690 --> 00:16:11,720
put spaces in and then they started doing it.

266
00:16:11,720 --> 00:16:20,000
Um, [NOISE] most languages these days do put spaces in between words but even,

267
00:16:20,000 --> 00:16:22,580
then there's sort of a lot of fine cases.

268
00:16:22,580 --> 00:16:26,930
So in particular, a lot of languages have some sort of little bits

269
00:16:26,930 --> 00:16:31,550
of stuff which might be pronouns, or prepositions,

270
00:16:31,550 --> 00:16:36,380
or various kind of joining words like and, and so,

271
00:16:36,380 --> 00:16:42,050
and which sometimes they write together and sometimes separately.

272
00:16:42,050 --> 00:16:44,750
So in French, um,

273
00:16:44,750 --> 00:16:49,865
you get these kind of prepositional, I'm sorry,

274
00:16:49,865 --> 00:16:55,865
pronominal, um, markers for you, I, you, have brought.

275
00:16:55,865 --> 00:16:58,640
Um, and you know, these kind of little words and

276
00:16:58,640 --> 00:17:01,940
pronunciation just sort of run together as je vous ai,

277
00:17:01,940 --> 00:17:05,060
and arguably it's almost one word,

278
00:17:05,060 --> 00:17:07,325
but it's written as separate words.

279
00:17:07,325 --> 00:17:11,480
Um, where there are other languages which sort of stick things together,

280
00:17:11,480 --> 00:17:13,580
where arguably they're separate words.

281
00:17:13,580 --> 00:17:19,550
So in Arabic, you get pronominal clitics and some of these sort of joining words like so,

282
00:17:19,550 --> 00:17:23,960
and and, and they are sort of written together as one word,

283
00:17:23,960 --> 00:17:27,485
where arguably that they should really be four words.

284
00:17:27,485 --> 00:17:31,790
Another famous case of that is with compound nouns.

285
00:17:31,790 --> 00:17:34,025
Um, so in English,

286
00:17:34,025 --> 00:17:37,160
we write compound nouns with spaces between them,

287
00:17:37,160 --> 00:17:38,690
so you can see each noun.

288
00:17:38,690 --> 00:17:42,290
Um, even though in many respects compound nouns

289
00:17:42,290 --> 00:17:46,460
something like whiteboard behaves like it's one word, or high-school.

290
00:17:46,460 --> 00:17:48,770
Um, whereas other languages,

291
00:17:48,770 --> 00:17:50,480
German is the most famous case,

292
00:17:50,480 --> 00:17:52,580
but also other Germanic languages,

293
00:17:52,580 --> 00:17:57,470
just write them all as one word and you get very long words like that.

294
00:17:57,470 --> 00:18:03,680
So we can get different words if we just use spaces and don't do much else. Um, good.

295
00:18:03,680 --> 00:18:07,310
Okay. Yes. So for dealing with words,

296
00:18:07,310 --> 00:18:10,175
there are these practical problems.

297
00:18:10,175 --> 00:18:13,070
Um, and we sort of already started to

298
00:18:13,070 --> 00:18:16,339
touch on them that if you're trying to build word-based models,

299
00:18:16,339 --> 00:18:18,785
there's this huge space of words,

300
00:18:18,785 --> 00:18:21,740
and well, strictly there's an infinite space of words

301
00:18:21,740 --> 00:18:24,845
because once you allow in things like numbers,

302
00:18:24,845 --> 00:18:28,985
let alone FedEx routing numbers, or, um,

303
00:18:28,985 --> 00:18:31,520
or if you allow just morphology,

304
00:18:31,520 --> 00:18:34,175
when you can make those ones like unfortunately.

305
00:18:34,175 --> 00:18:37,190
Yeah, sort of, you can just expand the space of words,

306
00:18:37,190 --> 00:18:40,265
so you get this large open vocabulary.

307
00:18:40,265 --> 00:18:43,340
Um, English, you know, a bit problematic.

308
00:18:43,340 --> 00:18:46,400
It gets way more problematic than a lot of other languages.

309
00:18:46,400 --> 00:18:49,160
So here's a lovely Czech word, um,

310
00:18:49,160 --> 00:18:51,635
to the worst farmable one, um,

311
00:18:51,635 --> 00:18:56,420
where you can make sort of much more complex words in lots of other languages.

312
00:18:56,420 --> 00:18:59,150
Um, many Native American languages,

313
00:18:59,150 --> 00:19:03,170
other European languages like Finnish have these sort of very complex words,

314
00:19:03,170 --> 00:19:05,570
Turkish has very complex words.

315
00:19:05,570 --> 00:19:07,895
Um, so that's bad news.

316
00:19:07,895 --> 00:19:10,190
Um, there are other more reasons we'd like to be able to

317
00:19:10,190 --> 00:19:12,410
look at words below the word level,

318
00:19:12,410 --> 00:19:13,970
to know things about them.

319
00:19:13,970 --> 00:19:16,115
So when you're translating,

320
00:19:16,115 --> 00:19:18,110
there's a wide space of things,

321
00:19:18,110 --> 00:19:23,405
especially names, where translation is essentially transliteration,

322
00:19:23,405 --> 00:19:29,015
that you're going to rewrite the sound of somebody's name as just roughly, you know,

323
00:19:29,015 --> 00:19:31,790
perhaps not perfectly correctly but roughly correctly

324
00:19:31,790 --> 00:19:34,730
according to the sound systems of the different language.

325
00:19:34,730 --> 00:19:36,020
And well, if we want to do that,

326
00:19:36,020 --> 00:19:38,900
we essentially want to work- operate at the letter level,

327
00:19:38,900 --> 00:19:40,310
not the word level.

328
00:19:40,310 --> 00:19:46,340
But another huge modern reason why we'd like to start modeling below the word level is,

329
00:19:46,340 --> 00:19:51,350
we live in this age of social media and if you're in the social media land,

330
00:19:51,350 --> 00:19:53,810
there's a lot of stuff that's written not

331
00:19:53,810 --> 00:19:56,975
using the canonical words that you find in the dictionary,

332
00:19:56,975 --> 00:19:58,730
and somehow we'd wanna start,

333
00:19:58,730 --> 00:20:00,005
um, to model that.

334
00:20:00,005 --> 00:20:02,180
So in some sense this is the, um,

335
00:20:02,180 --> 00:20:03,545
easy case.

336
00:20:03,545 --> 00:20:06,260
Um, good vibes.

337
00:20:06,260 --> 00:20:08,720
Um, but nevertheless this is spelled with one,

338
00:20:08,720 --> 00:20:10,370
two, three, four, five, six,

339
00:20:10,370 --> 00:20:13,010
seven O's, and one, two,

340
00:20:13,010 --> 00:20:15,875
three, four, five, oh and also seven S's, they match.

341
00:20:15,875 --> 00:20:20,270
I don't know if that's deliberate or not [LAUGHTER]. Um, okay.

342
00:20:20,270 --> 00:20:24,905
So this sty le of writing is very common, um, and well,

343
00:20:24,905 --> 00:20:27,830
you know, we kind of sunk if we're

344
00:20:27,830 --> 00:20:31,100
treating things at the word level and we're trying to model this right.

345
00:20:31,100 --> 00:20:34,730
That's clearly not what human beings are doing, we're sort of looking at

346
00:20:34,730 --> 00:20:38,675
the characters and recognizing what goes on.

347
00:20:38,675 --> 00:20:45,290
Um, in some sense that's kind of the easy case that you could imagine preprocessing out.

348
00:20:45,290 --> 00:20:49,100
Um, there's a lot of harder stuff that then turns up.

349
00:20:49,100 --> 00:20:53,645
Um, I guess there's sort of the abbreviation speak, like I don't care.

350
00:20:53,645 --> 00:20:58,640
Um, but then you sort of get a lot of creative spellings, um,

351
00:20:58,640 --> 00:21:04,565
that come off of kind of reduced pronunciations like I'mma go, sumn.

352
00:21:04,565 --> 00:21:08,450
Um, and it seems like somehow we need something other than

353
00:21:08,450 --> 00:21:13,265
canonical words if we're going to start to deal better with a lot of this text.

354
00:21:13,265 --> 00:21:22,070
Oops. Okay. So that suggests we sort of want to start doing that with our models.

355
00:21:22,070 --> 00:21:28,055
And so, that's led to a lot of interest in using character level models.

356
00:21:28,055 --> 00:21:35,300
Um, and I mean there are sort of two extents to which you can do this,

357
00:21:35,300 --> 00:21:38,045
and we'll look at them both a bit.

358
00:21:38,045 --> 00:21:40,940
Um, one level is to say,

359
00:21:40,940 --> 00:21:43,820
look we're still gonna have words in our system.

360
00:21:43,820 --> 00:21:47,345
Basically, we're going to build a system that works over words,

361
00:21:47,345 --> 00:21:49,670
but we want to be able to create

362
00:21:49,670 --> 00:21:54,950
word representations for any character sequence and we'd like to

363
00:21:54,950 --> 00:21:58,910
do it in a way that takes advantage of being able to

364
00:21:58,910 --> 00:22:03,050
recognize parts of the character sequence that look familiar,

365
00:22:03,050 --> 00:22:07,790
so that we can probably guess what vibes means.

366
00:22:07,790 --> 00:22:10,670
Um, and so that sort of then solves the problems

367
00:22:10,670 --> 00:22:13,489
with unknown words and we get similar words,

368
00:22:13,489 --> 00:22:18,125
similar embeddings for words with similar terms, spellings, et cetera.

369
00:22:18,125 --> 00:22:20,810
But the other alternative is to say,

370
00:22:20,810 --> 00:22:23,820
oh no, just forget about these words altogether, who needs- um,

371
00:22:23,820 --> 00:22:28,880
why don't we just do all of our language processing on sequence of characters,

372
00:22:28,880 --> 00:22:30,410
it'll work out fine.

373
00:22:30,410 --> 00:22:35,090
Um, both of these methods have been proven to work very successfully.

374
00:22:35,090 --> 00:22:38,810
Um, and I just wanted to dwell on that for one moment,

375
00:22:38,810 --> 00:22:41,480
and that sort of goes back to my,

376
00:22:41,480 --> 00:22:44,630
um, morphology slide here.

377
00:22:44,630 --> 00:22:47,840
When people first started proposing that they are going

378
00:22:47,840 --> 00:22:51,245
to build deep learning models over characters.

379
00:22:51,245 --> 00:22:54,650
I mean, my first feeling was oh, that is never going to

380
00:22:54,650 --> 00:22:57,965
work because it sort of seemed like,

381
00:22:57,965 --> 00:23:01,310
okay, words have a meaning it makes sense,

382
00:23:01,310 --> 00:23:03,890
um, that you can do something like build

383
00:23:03,890 --> 00:23:07,160
a word2vec model and that's going to really be able to

384
00:23:07,160 --> 00:23:09,650
sort of see words and their distribution and learn

385
00:23:09,650 --> 00:23:12,905
the meanings of the words because words have a meaning.

386
00:23:12,905 --> 00:23:15,680
The idea that you're going to be able to say, well,

387
00:23:15,680 --> 00:23:19,310
I'm going to come up with a vector representation of h,

388
00:23:19,310 --> 00:23:22,880
and a different vector representation of a,

389
00:23:22,880 --> 00:23:26,404
and a different vec- vector representation of t,

390
00:23:26,404 --> 00:23:29,330
and somehow that'll be useful for representing what a hat

391
00:23:29,330 --> 00:23:32,495
means once I put it through enough neural network layers,

392
00:23:32,495 --> 00:23:36,335
um, frankly it sounded pretty unconvincing to me.

393
00:23:36,335 --> 00:23:40,315
Um, but, um, I guess, you know-

394
00:23:40,315 --> 00:23:44,595
But it, it, totally works so I'm convinced now, empirical proof.

395
00:23:44,595 --> 00:23:48,885
And I think what we so essentially need to realize,

396
00:23:48,885 --> 00:23:52,350
is that with going- that yes,

397
00:23:52,350 --> 00:23:55,755
at some level we just have these characters that don't mean much.

398
00:23:55,755 --> 00:24:02,940
But we then have these very powerful combinatory models with a lot of parameters in them,

399
00:24:02,940 --> 00:24:04,950
things like recurrent neural networks and

400
00:24:04,950 --> 00:24:10,305
convolutional neural networks and that they are respectively able to, sort of, build,

401
00:24:10,305 --> 00:24:15,330
store and build representations of meaning from multi-letter groups,

402
00:24:15,330 --> 00:24:18,720
in such a way that they can model the meanings of

403
00:24:18,720 --> 00:24:22,590
morphemes and larger units and therefore put together word meanings.

404
00:24:22,590 --> 00:24:24,420
Um, yeah. So, um,

405
00:24:24,420 --> 00:24:27,930
one more detail on using characters,

406
00:24:27,930 --> 00:24:30,195
um, from writing systems.

407
00:24:30,195 --> 00:24:33,660
So if you're a linguist you tend to think of sounds as primary.

408
00:24:33,660 --> 00:24:36,900
Those were the phonemes that we- I mentioned beforehand.

409
00:24:36,900 --> 00:24:40,170
You know, um, essentially,

410
00:24:40,170 --> 00:24:43,250
um, deep learning hasn't tried to use phonemes at all.

411
00:24:43,250 --> 00:24:46,425
Traditional speech recognizers often did use phonemes,

412
00:24:46,425 --> 00:24:48,135
but in the deep learning land,

413
00:24:48,135 --> 00:24:53,580
you want to have a lot of data and the way you get a lot of data is you just use, um,

414
00:24:53,580 --> 00:24:55,340
written stuff because, you know,

415
00:24:55,340 --> 00:25:00,920
it's the easily found data where you can get millions and billions of words of stuff.

416
00:25:00,920 --> 00:25:04,220
Um, so that sort of makes sense from a data point of view.

417
00:25:04,220 --> 00:25:07,805
But the thing that ends up as a little weird about that,

418
00:25:07,805 --> 00:25:11,005
is that when you're then building a character level model,

419
00:25:11,005 --> 00:25:13,714
what your character level model is,

420
00:25:13,714 --> 00:25:17,325
actually varies depending on the writing system of the language.

421
00:25:17,325 --> 00:25:18,875
And so you, kind of,

422
00:25:18,875 --> 00:25:21,990
have these quite different writing systems.

423
00:25:21,990 --> 00:25:27,405
So you have some writing systems which are just completely phonemic,

424
00:25:27,405 --> 00:25:32,130
that there are letters that have a particular sound and you say that sound.

425
00:25:32,130 --> 00:25:34,575
Something like Spanish is pretty much phonemic.

426
00:25:34,575 --> 00:25:36,810
Sometimes it's a teeny bit complicated.

427
00:25:36,810 --> 00:25:38,580
So you might have a digraph.

428
00:25:38,580 --> 00:25:41,700
So this digraph, ngabulu, is, kind of, like,

429
00:25:41,700 --> 00:25:46,470
the N-G of English that is used for "ng" sound like at the end of seeing,

430
00:25:46,470 --> 00:25:49,080
but, you know, basically this is just 'jiyawu',

431
00:25:49,080 --> 00:25:50,939
each letter is a sound,

432
00:25:50,939 --> 00:25:52,860
you can read it, um,

433
00:25:52,860 --> 00:25:54,960
and it's just, um, phonemic.

434
00:25:54,960 --> 00:25:58,140
Um, that then contrasts from something like English where

435
00:25:58,140 --> 00:26:01,890
all the non-native speakers know the spelling is terrible.

436
00:26:01,890 --> 00:26:04,515
It's got this, sort of, highly fossilized,

437
00:26:04,515 --> 00:26:05,700
once upon a time,

438
00:26:05,700 --> 00:26:08,885
phonemic system in the tenth century or something.

439
00:26:08,885 --> 00:26:12,030
Um, but now we have this system that words have

440
00:26:12,030 --> 00:26:17,700
fairly arbitrary spelling that doesn't actually represent the sounds, um, very clearly.

441
00:26:17,700 --> 00:26:20,490
But it's sort of a phonemic system.

442
00:26:20,490 --> 00:26:23,430
But then there are languages that use larger units.

443
00:26:23,430 --> 00:26:25,235
Um, this is, um,

444
00:26:25,235 --> 00:26:29,715
Canadian and Inuktitut which I just put in there because it's such a pretty writing system.

445
00:26:29,715 --> 00:26:35,820
Um, but there are a lot of languages that represent syllables by their characters.

446
00:26:35,820 --> 00:26:38,880
Um, so you'd have something like this in Korean for example,

447
00:26:38,880 --> 00:26:41,190
with Korean Hangul, that each, um,

448
00:26:41,190 --> 00:26:46,860
letter is then being a syllable of a sort of consonant vowel combination like bar.

449
00:26:46,860 --> 00:26:52,455
Um, if you can then go up a level from that and if we get back to Chinese again,

450
00:26:52,455 --> 00:26:57,465
well, um, this is sort of also a syllabic system, you could say.

451
00:26:57,465 --> 00:27:02,130
But really, the Chinese characters are much more than just the sound.

452
00:27:02,130 --> 00:27:03,515
They also have a meaning.

453
00:27:03,515 --> 00:27:06,435
That this is really then an ideographic system

454
00:27:06,435 --> 00:27:09,570
where there are characters with particular meanings attached to them.

455
00:27:09,570 --> 00:27:10,860
So they're, sort of, uh,

456
00:27:10,860 --> 00:27:14,025
whole morphemes in- written as one letter.

457
00:27:14,025 --> 00:27:16,964
And, you know, another example of such language,

458
00:27:16,964 --> 00:27:19,500
um, was Egyptian hieroglyphs, if you've seen those.

459
00:27:19,500 --> 00:27:23,700
That they're, sort of, ideographic systems where you have letters with meanings.

460
00:27:23,700 --> 00:27:27,420
Um, and then you have language systems that sort of mix several of those.

461
00:27:27,420 --> 00:27:31,875
So Japanese is sort of a mixture of partly moraic,

462
00:27:31,875 --> 00:27:34,875
partly ideographic systems mixed together.

463
00:27:34,875 --> 00:27:36,100
So if you just,

464
00:27:36,100 --> 00:27:37,695
sort of, start off and say,

465
00:27:37,695 --> 00:27:41,805
"Okay, I'm gonna build a character-based system." That's fine.

466
00:27:41,805 --> 00:27:45,495
But effectively, your character units like

467
00:27:45,495 --> 00:27:50,610
letter trigrams are just very different in a language like Chinese,

468
00:27:50,610 --> 00:27:54,030
where commonly a letter trigram will be, sort of,

469
00:27:54,030 --> 00:27:55,140
a word and a half,

470
00:27:55,140 --> 00:27:57,915
three morphemes with meaning.

471
00:27:57,915 --> 00:27:59,820
Whereas if you're in something like English,

472
00:27:59,820 --> 00:28:03,105
your character trigram will be something like T-H-O

473
00:28:03,105 --> 00:28:06,980
which is still sort of much too small a unit to have any meaning.

474
00:28:06,980 --> 00:28:08,810
So moving right ahead.

475
00:28:08,810 --> 00:28:11,550
So these two kind of approaches, um,

476
00:28:11,550 --> 00:28:15,875
one was just do a completely character level model and then the other one was,

477
00:28:15,875 --> 00:28:18,120
sort of, you make use of characters

478
00:28:18,120 --> 00:28:20,640
to build bigger things that you're then gonna put something,

479
00:28:20,640 --> 00:28:22,860
like, into a more word level model.

480
00:28:22,860 --> 00:28:25,455
So I'll do this one first and the other one.

481
00:28:25,455 --> 00:28:27,810
So for purely character level models,

482
00:28:27,810 --> 00:28:30,785
I actually showed an example of that last time. Do you remember?

483
00:28:30,785 --> 00:28:33,820
So there was that very deep convolutional network from

484
00:28:33,820 --> 00:28:37,710
the Conneau et-al word for text classification at the end, um,

485
00:28:37,710 --> 00:28:39,660
and that just started with a big line of

486
00:28:39,660 --> 00:28:43,620
characters and built these convolutional layers on top of that,

487
00:28:43,620 --> 00:28:47,565
in the vision like network and classified the documents.

488
00:28:47,565 --> 00:28:51,450
Um, so that was, sort of, a completely character-level model.

489
00:28:51,450 --> 00:28:54,405
Um, but here's a bit more work on this.

490
00:28:54,405 --> 00:28:59,325
So people for machine translation have built, um,

491
00:28:59,325 --> 00:29:05,250
machine translation systems that just read characters and write characters.

492
00:29:05,250 --> 00:29:08,615
And when people first tried to do that,

493
00:29:08,615 --> 00:29:11,940
um, it, sort of, didn't work, right?

494
00:29:11,940 --> 00:29:13,740
The people thought it might help to build

495
00:29:13,740 --> 00:29:17,775
character-level models especially for languages like Chinese.

496
00:29:17,775 --> 00:29:21,240
But people just weren't able to build models that worked as

497
00:29:21,240 --> 00:29:25,365
well as word-based models and either the pre-neural,

498
00:29:25,365 --> 00:29:28,095
the non-neural or the neural world.

499
00:29:28,095 --> 00:29:30,930
But gradually that started to change.

500
00:29:30,930 --> 00:29:36,480
So people started to have successful character-level decoders and then,

501
00:29:36,480 --> 00:29:40,065
sort of, around 2015 and '16,

502
00:29:40,065 --> 00:29:41,790
um, people started to show,

503
00:29:41,790 --> 00:29:45,840
look you could- can actually do machine translation very

504
00:29:45,840 --> 00:29:50,055
well at just a character level with a few asterisks.

505
00:29:50,055 --> 00:29:53,910
And so, um, here's a bit of work, um, that we did.

506
00:29:53,910 --> 00:29:56,400
Um, the Luong and Manning one, from, um,

507
00:29:56,400 --> 00:29:59,145
2015 on the last slide.

508
00:29:59,145 --> 00:30:03,570
So this is looking at English to Czech translation and Czech's

509
00:30:03,570 --> 00:30:07,800
a good language to use if you want to motivate doing things at the character level,

510
00:30:07,800 --> 00:30:10,650
because it had those big horrible words with lots of

511
00:30:10,650 --> 00:30:15,555
morphology like the example I showed you before and I'll show you some more later.

512
00:30:15,555 --> 00:30:20,145
So people had built word-level models for Czech.

513
00:30:20,145 --> 00:30:22,650
Um, and, you know, they didn't work great,

514
00:30:22,650 --> 00:30:25,320
partly because of some of these vocab problems.

515
00:30:25,320 --> 00:30:26,880
So, um, the, sort of,

516
00:30:26,880 --> 00:30:31,905
word-level state of the art was at this time was 15,7 BLEU,

517
00:30:31,905 --> 00:30:37,680
which as you know is much less than we will accept for full grades in your homework.

518
00:30:37,680 --> 00:30:39,555
[LAUGHTER] Um, but, you know,

519
00:30:39,555 --> 00:30:43,740
what counts as a good BLEU score depends on how difficult the language pair is.

520
00:30:43,740 --> 00:30:46,320
Uh, um, and so you're not doing Czech.

521
00:30:46,320 --> 00:30:48,875
Um, but, um, so this was, sort of, the,

522
00:30:48,875 --> 00:30:51,690
kind of, neural MT model that we've talked about.

523
00:30:51,690 --> 00:30:53,400
So it was a Seq2Seq model,

524
00:30:53,400 --> 00:31:00,375
with attention and then it had extra stuff for substituting UNKs with either,

525
00:31:00,375 --> 00:31:05,790
uh, single word translation or by copying stuff from the source.

526
00:31:05,790 --> 00:31:07,200
So it was, sort of, basically,

527
00:31:07,200 --> 00:31:12,825
state of the art neural MT of 2015, got 15,7 BLEU.

528
00:31:12,825 --> 00:31:14,970
And the difference isn't big,

529
00:31:14,970 --> 00:31:16,380
um, but we were able to show,

530
00:31:16,380 --> 00:31:18,875
look we can build this completely, um,

531
00:31:18,875 --> 00:31:22,530
character-level model and then actually, it did fractionally better.

532
00:31:22,530 --> 00:31:24,300
Um, so this, sort of,

533
00:31:24,300 --> 00:31:28,260
showed that in terms of translation quality, um,

534
00:31:28,260 --> 00:31:32,760
character, purely character-based models were completely viable at

535
00:31:32,760 --> 00:31:37,380
capturing the meaning of text as well as word-based models.

536
00:31:37,380 --> 00:31:40,395
Um, was this a great result?

537
00:31:40,395 --> 00:31:42,630
Um, in many, in some ways,

538
00:31:42,630 --> 00:31:45,105
yes, in another way, no.

539
00:31:45,105 --> 00:31:48,810
I mean, this model was truly terrible to train, right?

540
00:31:48,810 --> 00:31:52,590
So it took about three weeks for us to train this model and at run-time,

541
00:31:52,590 --> 00:31:54,855
it also worked very slowly.

542
00:31:54,855 --> 00:31:57,240
And so the problem with character-level models,

543
00:31:57,240 --> 00:32:00,135
if you're putting them into something like an LSTM,

544
00:32:00,135 --> 00:32:02,580
is your sequences get way longer, right.

545
00:32:02,580 --> 00:32:07,110
So you've got about seven times as long sequences as you used to have.

546
00:32:07,110 --> 00:32:10,440
And since there's not much information, the characters,

547
00:32:10,440 --> 00:32:15,840
you have to do back propagation through time much further back.

548
00:32:15,840 --> 00:32:19,040
And so we were running back propagation through time for

549
00:32:19,040 --> 00:32:22,590
600 steps before we were trun- truncating it.

550
00:32:22,590 --> 00:32:23,990
And so this, sort of, made,

551
00:32:23,990 --> 00:32:25,210
maybe that was excessive,

552
00:32:25,210 --> 00:32:27,770
but it made the models, um, very slow.

553
00:32:27,770 --> 00:32:32,520
But we were able to show that it was able to get some of these good effects, right.

554
00:32:32,520 --> 00:32:33,705
So here's a Czech,

555
00:32:33,705 --> 00:32:35,230
um, translating to Czech,

556
00:32:35,230 --> 00:32:36,540
her 11 year-old daughter,

557
00:32:36,540 --> 00:32:39,645
Shani Bart, said it felt a little bit weird.

558
00:32:39,645 --> 00:32:42,885
And, um, I don't know, probably.

559
00:32:42,885 --> 00:32:45,480
Does anyone speak Czech, any Czech speakers?

560
00:32:45,480 --> 00:32:48,615
Um, no Czech speakers?

561
00:32:48,615 --> 00:32:51,195
Okay, um, I don't speak Czech either, um,

562
00:32:51,195 --> 00:32:57,390
but we can see that the [LAUGHTER] we can see that this does interesting things, right.

563
00:32:57,390 --> 00:33:00,045
So the second line is the human translation

564
00:33:00,045 --> 00:33:03,010
into Czech which we can use for some guidance.

565
00:33:03,010 --> 00:33:04,590
And so in particular, um,

566
00:33:04,590 --> 00:33:07,815
in Czech there's a word for 11 years old,

567
00:33:07,815 --> 00:33:11,295
um, which you can see is that blue word on the second line.

568
00:33:11,295 --> 00:33:15,645
And you can see that despite 11-year-old was, um,

569
00:33:15,645 --> 00:33:19,425
that for 11-year-old it's just able to perfectly, um,

570
00:33:19,425 --> 00:33:21,720
produce letter by letter, um,

571
00:33:21,720 --> 00:33:25,830
the Czech word for 11 years old and that works beautifully.

572
00:33:25,830 --> 00:33:29,160
In contrast, for the word-level model, um,

573
00:33:29,160 --> 00:33:33,585
11 year-old was an unknown word because that wasn't in the vocabulary.

574
00:33:33,585 --> 00:33:37,920
And so then it had two mechanisms to try and deal with, um, unknown words.

575
00:33:37,920 --> 00:33:39,435
It could either do, uh,

576
00:33:39,435 --> 00:33:43,485
unigram translation of them or it could just copy them.

577
00:33:43,485 --> 00:33:46,230
And for whatever reason, it decided here that the best strategy

578
00:33:46,230 --> 00:33:49,580
was to copy and so that was a complete fail.

579
00:33:49,580 --> 00:33:53,115
Um, and if we go along for the character-level model,

580
00:33:53,115 --> 00:33:55,890
another thing that it gets right that's really cool,

581
00:33:55,890 --> 00:33:57,995
um, is the name Shani Bart.

582
00:33:57,995 --> 00:34:02,370
It's able to do this transliteration tasks that I mentioned just perfectly.

583
00:34:02,370 --> 00:34:04,755
And it turns that to Shani Bartova

584
00:34:04,755 --> 00:34:07,760
which is exactly what the human translator did as well.

585
00:34:07,760 --> 00:34:11,440
And so, you know, it's actually doing some really kind of nice, um,

586
00:34:11,440 --> 00:34:15,635
human translator, um, like things.

587
00:34:15,635 --> 00:34:17,480
I mean, in fact,

588
00:34:17,480 --> 00:34:20,605
as best I can tell from spending a bit of time on Google Translate,

589
00:34:20,605 --> 00:34:23,195
it actually does a pretty good job in this sentence, period.

590
00:34:23,195 --> 00:34:25,880
All right, this part here starts to be different,

591
00:34:25,880 --> 00:34:27,815
um, from the human translator.

592
00:34:27,815 --> 00:34:29,480
But it's not actually bad.

593
00:34:29,480 --> 00:34:31,460
It's sort of a more literal translation.

594
00:34:31,460 --> 00:34:33,210
So this citi um,

595
00:34:33,210 --> 00:34:37,340
actually translates feel like in the English texts.

596
00:34:37,340 --> 00:34:38,875
Whereas the human, sort of,

597
00:34:38,875 --> 00:34:43,230
didn't actually use the word feel in the Czech version that they just went,

598
00:34:43,230 --> 00:34:45,550
um, was a little bit,

599
00:34:45,550 --> 00:34:49,360
um, weird or strange. So that's cool.

600
00:34:49,360 --> 00:34:54,965
Okay. So here are a couple more results from this.

601
00:34:54,965 --> 00:34:58,535
So here's another system that was built the next year.

602
00:34:58,535 --> 00:35:00,830
By these people Jason Lee,

603
00:35:00,830 --> 00:35:03,425
Kyunghyun Cho and Thomas Hoffman.

604
00:35:03,425 --> 00:35:09,515
So they wanted to do something that was, I don't know,

605
00:35:09,515 --> 00:35:12,980
much more complex and neural and

606
00:35:12,980 --> 00:35:16,385
understanding the meaning of the text on the source side.

607
00:35:16,385 --> 00:35:20,555
And so they were more using the kind of technologies we saw last time.

608
00:35:20,555 --> 00:35:29,960
So on the encoder side you started off with a letter sequence of character embeddings.

609
00:35:29,960 --> 00:35:34,850
And then you're sort of using convolutions of four,

610
00:35:34,850 --> 00:35:39,710
three and five of characters to get representations up here.

611
00:35:39,710 --> 00:35:44,270
You're then doing a max pooling with a stride of five.

612
00:35:44,270 --> 00:35:49,370
So you're getting a max pooled representation of pieces of the text for each of the three,

613
00:35:49,370 --> 00:35:51,680
four and five convolutions.

614
00:35:51,680 --> 00:35:54,590
You're then feeding that through multiple layers of

615
00:35:54,590 --> 00:35:57,815
highway network and feeding that through

616
00:35:57,815 --> 00:36:05,570
a bidirectional gated recurrent unit and that's giving you your source representation.

617
00:36:05,570 --> 00:36:07,519
On the decoder side,

618
00:36:07,519 --> 00:36:09,440
it was sort of the same as our decoder,

619
00:36:09,440 --> 00:36:13,680
it was just running a character level sequence model.

620
00:36:14,110 --> 00:36:21,590
So overall, so they were doing the opposite task.

621
00:36:21,590 --> 00:36:24,330
This is Czech to English.

622
00:36:25,120 --> 00:36:28,325
But, so they are starting to get better scores.

623
00:36:28,325 --> 00:36:31,610
But I mean actually if you're sort of looking at these different numbers,

624
00:36:31,610 --> 00:36:34,475
where I'll explain this system more in a minute,

625
00:36:34,475 --> 00:36:41,630
I mean it sort of seems like the place where they get a lot of value is that using

626
00:36:41,630 --> 00:36:46,700
the character level decoder gives them a lot of value by

627
00:36:46,700 --> 00:36:53,550
this very complex model on the source side is giving them almost no value at all.

628
00:36:54,760 --> 00:36:58,085
One even more recent paper,

629
00:36:58,085 --> 00:37:03,425
so this is Colin Cherry and fellow researchers at Google.

630
00:37:03,425 --> 00:37:09,560
So they last year did one more exploration of

631
00:37:09,560 --> 00:37:11,570
doing LSTM sequence to sequence

632
00:37:11,570 --> 00:37:16,505
style models of comparing word and character-based models.

633
00:37:16,505 --> 00:37:19,220
And this is English to French and this

634
00:37:19,220 --> 00:37:23,090
is um, Czech to English which is just what we were doing.

635
00:37:23,090 --> 00:37:27,440
And so in both cases when you have a big model,

636
00:37:27,440 --> 00:37:30,005
the character model wins for them.

637
00:37:30,005 --> 00:37:34,670
The blue model comes out on top but the sort of interesting thing as you

638
00:37:34,670 --> 00:37:36,830
sort of see these different effects depending on

639
00:37:36,830 --> 00:37:39,140
the morphological complexity of the language.

640
00:37:39,140 --> 00:37:41,555
So for a language like Czech,

641
00:37:41,555 --> 00:37:43,850
it's a really good idea,

642
00:37:43,850 --> 00:37:45,995
if you want to build a good model,

643
00:37:45,995 --> 00:37:49,760
to use character level that they're getting about a BLEU point of difference there,

644
00:37:49,760 --> 00:37:54,710
whereas for a model without putting French or English there's

645
00:37:54,710 --> 00:38:01,295
actually a tiny but very little gain from using a character level model.

646
00:38:01,295 --> 00:38:05,300
Okay so let me just explain these models,

647
00:38:05,300 --> 00:38:08,405
so these models are models of different sizes.

648
00:38:08,405 --> 00:38:17,720
So these models are using bidirectional LSTM encoders and one-directional LSTM decoders.

649
00:38:17,720 --> 00:38:20,405
So the simplest model just has

650
00:38:20,405 --> 00:38:28,430
a shallow bidirectional LSTM encoder and a two layer LSTM decoder.

651
00:38:28,430 --> 00:38:33,080
The middle model has a three deep stack of

652
00:38:33,080 --> 00:38:39,935
bidirectional LSTM encoders and a four deep stack of LSTM decoders.

653
00:38:39,935 --> 00:38:43,940
And the most complex model has a six deep stack of

654
00:38:43,940 --> 00:38:51,530
bidirectional LSTM encoders and an eight deep stack of LSTM decoders.

655
00:38:51,530 --> 00:38:53,660
This is where it helps to work at Google.

656
00:38:53,660 --> 00:38:55,310
Probably for your projects,

657
00:38:55,310 --> 00:38:59,750
you don't want to go beyond three or four. Stay over here.

658
00:38:59,750 --> 00:39:04,310
Okay yeah so, so these are the results.

659
00:39:04,310 --> 00:39:07,010
So basically what you're finding is if you're making

660
00:39:07,010 --> 00:39:10,100
sort of smaller models you're better off with words,

661
00:39:10,100 --> 00:39:15,680
but as you go to big models especially if you're in a morphologically rich language,

662
00:39:15,680 --> 00:39:18,425
you clearly start to win from the characters.

663
00:39:18,425 --> 00:39:21,410
But there is still a loss which is essentially

664
00:39:21,410 --> 00:39:27,635
exactly the same loss that we were suffering from in 2015, right?

665
00:39:27,635 --> 00:39:35,510
This is the time graph and so these are the same three models as over here,

666
00:39:35,510 --> 00:39:40,610
it's just the axis is changed to sort of sum the total number of LSTM layers.

667
00:39:40,610 --> 00:39:45,050
And so essentially, if you're at the word level,

668
00:39:45,050 --> 00:39:51,800
you can run any of these three models and they are fast that you can be translating in

669
00:39:51,800 --> 00:39:59,000
sort of not much time but for the character level models your slope is much higher.

670
00:39:59,000 --> 00:40:04,770
So it starts to get quite expensive to run the deep character level models.

671
00:40:05,860 --> 00:40:09,035
Okay, so that's that section.

672
00:40:09,035 --> 00:40:12,810
So then chugging along.

673
00:40:12,910 --> 00:40:18,395
I then wanted to look at other ways of doing things.

674
00:40:18,395 --> 00:40:23,930
And so these are models that in some sense still do have words but where

675
00:40:23,930 --> 00:40:29,675
we're going to want to sort of build word representations out of pieces.

676
00:40:29,675 --> 00:40:35,465
And there are essentially two families of ways that people have explored doing this.

677
00:40:35,465 --> 00:40:39,830
One way of doing it is to say look we just want to use

678
00:40:39,830 --> 00:40:44,495
exactly the same architecture as we use for a word model

679
00:40:44,495 --> 00:40:48,710
except our words aren't really going to

680
00:40:48,710 --> 00:40:53,390
be words at least sometimes they're going to be pieces of words.

681
00:40:53,390 --> 00:40:56,750
And so those are often called word piece models.

682
00:40:56,750 --> 00:40:59,450
And in particular, there's one commonest way of doing it.

683
00:40:59,450 --> 00:41:03,740
It's called BPE, which I'll go through in some detail.

684
00:41:03,740 --> 00:41:06,695
The other alternative is to say,

685
00:41:06,695 --> 00:41:09,710
well, we're gonna kind of make a mixture or a hybrid.

686
00:41:09,710 --> 00:41:14,420
So our main model is going to work in terms of words but we're

687
00:41:14,420 --> 00:41:19,415
going to have some kind of facility where we can construct a representation,

688
00:41:19,415 --> 00:41:21,530
for otherwise unknown words,

689
00:41:21,530 --> 00:41:24,380
by doing things that at a character or a lower level.

690
00:41:24,380 --> 00:41:26,750
And I'll show you a bit of that as well.

691
00:41:26,750 --> 00:41:31,370
Okay, so this is BPE.

692
00:41:31,370 --> 00:41:36,080
BPE is actually a pretty simple idea which has nothing to

693
00:41:36,080 --> 00:41:41,150
do with deep learning but the use of BPE has sort of become

694
00:41:41,150 --> 00:41:49,340
pretty standard and successful for representing pieces of words to allow you to

695
00:41:49,340 --> 00:41:52,685
have an infinite vocabulary

696
00:41:52,685 --> 00:41:58,490
while an infinite effective vocabulary while actually working with a finite vocabulary.

697
00:41:58,490 --> 00:42:01,670
So the origins of Byte Pair Encoding and

698
00:42:01,670 --> 00:42:07,760
the name byte pair has nothing to do with natural language processing or neural nets,

699
00:42:07,760 --> 00:42:10,115
we're just writing a compression algorithm.

700
00:42:10,115 --> 00:42:14,375
So this is something like compressing your documents with gzip.

701
00:42:14,375 --> 00:42:18,830
So what basic Byte Pair Encoding is,

702
00:42:18,830 --> 00:42:23,090
that you've got a collection of stuff with bytes and you are

703
00:42:23,090 --> 00:42:28,820
looking for the most frequent sequence of two bytes and you say,

704
00:42:28,820 --> 00:42:32,675
okay, I'm going to add that sequence of two bytes as

705
00:42:32,675 --> 00:42:37,700
a new element to my dictionary of possible values.

706
00:42:37,700 --> 00:42:42,830
And that means I can have 257 different values for bytes so to

707
00:42:42,830 --> 00:42:45,110
speak that I can shrink the length of

708
00:42:45,110 --> 00:42:49,370
my sequence and I can repeat over and do that again.

709
00:42:49,370 --> 00:42:53,900
And so essentially, this work suggested,

710
00:42:53,900 --> 00:42:58,700
well we can apply this kind of compression algorithm and use it as

711
00:42:58,700 --> 00:43:03,890
a way of coming up with pieces of words that were

712
00:43:03,890 --> 00:43:08,480
useful, doing it not strictly with bytes despite

713
00:43:08,480 --> 00:43:13,400
the name but instead with characters and character n-grams.

714
00:43:13,400 --> 00:43:16,190
And so the most common way to do this with

715
00:43:16,190 --> 00:43:19,850
characters and character n-grams and if you're up with modern times,

716
00:43:19,850 --> 00:43:22,790
you know that means there's unicode and you can represent

717
00:43:22,790 --> 00:43:26,060
all of these lovely letters like Canadian Inuktitut's

718
00:43:26,060 --> 00:43:28,445
syllabics and stuff like that.

719
00:43:28,445 --> 00:43:31,340
But there's actually a problem with Unicode,

720
00:43:31,340 --> 00:43:34,595
which is there actually a lot of Unicode characters.

721
00:43:34,595 --> 00:43:36,815
I forget the number theoretically.

722
00:43:36,815 --> 00:43:40,520
I think there's about 200,000 possible Unicode characters.

723
00:43:40,520 --> 00:43:44,900
But at any rate, if you want to handle a bunch of languages which include East Asian languages,

724
00:43:44,900 --> 00:43:49,205
maybe you need something like 20,000 characters and that's sort of a lot.

725
00:43:49,205 --> 00:43:54,920
So there are actually some people who've literally gone back to bytes and said,

726
00:43:54,920 --> 00:43:57,650
"You know 200,000, that's a really big vocabulary.

727
00:43:57,650 --> 00:43:59,270
I don't want to deal with anything."

728
00:43:59,270 --> 00:44:01,700
Sorry, 200,000 is a really big vocabulary.

729
00:44:01,700 --> 00:44:03,845
I don't even want to deal with anything that large.

730
00:44:03,845 --> 00:44:10,010
So why don't I actually just do these kind of algorithms over bytes?

731
00:44:10,010 --> 00:44:14,495
And so that means that in UTF-8 encoding,

732
00:44:14,495 --> 00:44:17,870
Chinese characters take three bytes each.

733
00:44:17,870 --> 00:44:21,890
And so you're actually have to- you only get whole characters if you

734
00:44:21,890 --> 00:44:27,100
actually merge together several bytes that are common sequencers.

735
00:44:27,100 --> 00:44:30,600
Okay. So more concretely,

736
00:44:30,600 --> 00:44:32,040
um, how does this work?

737
00:44:32,040 --> 00:44:37,110
So we're sort of doing this bottom-up clustering of short sequences.

738
00:44:37,110 --> 00:44:40,245
So we start with a unigram vocabulary,

739
00:44:40,245 --> 00:44:44,120
which is all of the Unicode characters and some data.

740
00:44:44,120 --> 00:44:49,310
We then sort of ask, what's the most frequent ngram here?

741
00:44:49,310 --> 00:44:53,985
Um, initially it will be a bigram pair and we add that to our vocabulary.

742
00:44:53,985 --> 00:44:56,610
So if we start off, you know,

743
00:44:56,610 --> 00:44:59,670
we can take our text that's- um,

744
00:44:59,670 --> 00:45:00,810
I'll come back to this in a minute.

745
00:45:00,810 --> 00:45:05,895
Let's assume we have a text that has been divided into words so we do have word tokens.

746
00:45:05,895 --> 00:45:11,670
And so we can represent as a dictionary and say here are some words with their frequency.

747
00:45:11,670 --> 00:45:17,985
Um, and so now we look for a common letter sequence and we say, "Oh, es."

748
00:45:17,985 --> 00:45:20,610
That occurs nine times, um,

749
00:45:20,610 --> 00:45:25,250
in this data because we have the counts for the words on the left side.

750
00:45:25,250 --> 00:45:30,600
So, um, we start with our vocabulary being all the individual letters.

751
00:45:30,600 --> 00:45:34,515
We find a commonest letter sequence like es,

752
00:45:34,515 --> 00:45:40,245
and so we say, "Let's clump that together and make that a new thing in our vocabulary."

753
00:45:40,245 --> 00:45:43,185
So now we've got an extra thing in our vocabulary.

754
00:45:43,185 --> 00:45:47,340
And now what's the commonest ngram sequence that clumped something?

755
00:45:47,340 --> 00:45:51,030
Well, actually all of these es's are followed by t,

756
00:45:51,030 --> 00:45:52,650
so we also have es,

757
00:45:52,650 --> 00:45:55,065
t with frequency nine,

758
00:45:55,065 --> 00:45:58,050
and so we can add that to our vocabulary.

759
00:45:58,050 --> 00:45:59,670
And then we ask again, well,

760
00:45:59,670 --> 00:46:03,030
what's another common letter sequence?

761
00:46:03,030 --> 00:46:06,825
Let's see, there are seven cases of o double- well,

762
00:46:06,825 --> 00:46:10,170
I guess there are seven cases of either l o or o w,

763
00:46:10,170 --> 00:46:13,380
so we can lump those and then we can lump

764
00:46:13,380 --> 00:46:17,795
again and make an lo w. So if we sort of run this,

765
00:46:17,795 --> 00:46:22,410
we start to build these clumps of common letter sequences,

766
00:46:22,410 --> 00:46:26,205
and so common bits like est,

767
00:46:26,205 --> 00:46:28,450
but also just common words,

768
00:46:28,450 --> 00:46:31,470
something like that in English will very quickly be

769
00:46:31,470 --> 00:46:35,130
clumped together and be a unit of our vocabulary.

770
00:46:35,130 --> 00:46:38,300
Um, and so we do that for a while.

771
00:46:38,300 --> 00:46:40,650
So normally what we do is we decide

772
00:46:40,650 --> 00:46:44,250
a vocabulary size that we want to work with. We say, "Okay.

773
00:46:44,250 --> 00:46:47,400
I want to work with a vocabulary size of 8,000 words."

774
00:46:47,400 --> 00:46:49,905
That'll mean my model will be fast,

775
00:46:49,905 --> 00:46:54,255
and we just sort of keep doing this until we have 8,000 things in our vocabulary.

776
00:46:54,255 --> 00:46:56,900
And that means that our vocabulary will have in it

777
00:46:56,900 --> 00:47:00,650
all single letters because we started with them and it'll

778
00:47:00,650 --> 00:47:07,285
have common subsequences of words like the es and the est that are now in our vocabulary,

779
00:47:07,285 --> 00:47:11,420
but also have whole words whenever there're common words, like, you know,

780
00:47:11,420 --> 00:47:13,280
the, and too, and with,

781
00:47:13,280 --> 00:47:16,835
and so on, will become parts of our vocabulary.

782
00:47:16,835 --> 00:47:21,000
Um, and so then when we have a piece of text we can do

783
00:47:21,000 --> 00:47:25,125
a deterministic longest piece segmentation of words,

784
00:47:25,125 --> 00:47:28,310
and we will say that is our eeset of word pieces.

785
00:47:28,310 --> 00:47:30,900
And so for an input piece of text,

786
00:47:30,900 --> 00:47:32,700
we turn into word pieces,

787
00:47:32,700 --> 00:47:37,695
and then we just run it through our MT system as if we were using words,

788
00:47:37,695 --> 00:47:39,900
but really it's pieces of words,

789
00:47:39,900 --> 00:47:42,000
and then on the output side,

790
00:47:42,000 --> 00:47:45,845
we just concatenate them back together as needed.

791
00:47:45,845 --> 00:47:49,875
Okay. So we get this sort of automatic word-based system.

792
00:47:49,875 --> 00:47:53,525
And that's proved to be a very successful system.

793
00:47:53,525 --> 00:47:57,525
So this idea of using byte pair encoding sort of really

794
00:47:57,525 --> 00:48:02,115
emerged in 2015 and then in 2016, uh,

795
00:48:02,115 --> 00:48:06,480
workshop on machine translation which has been the main sort of annual competition for

796
00:48:06,480 --> 00:48:12,300
MT systems that the several top systems were built using byte pair encoding.

797
00:48:12,300 --> 00:48:14,700
If you look at last year's competition,

798
00:48:14,700 --> 00:48:16,350
there's a bit more variety,

799
00:48:16,350 --> 00:48:21,044
but really a number of the top systems are still using byte pair encoding,

800
00:48:21,044 --> 00:48:24,345
that's just been a good way to do things.

801
00:48:24,345 --> 00:48:29,130
So for Google's Neural Machine Translation,

802
00:48:29,130 --> 00:48:33,450
they effectively use of- a variant of byte pair encoding.

803
00:48:33,450 --> 00:48:37,160
So they don't use exactly the same algorithm, um,

804
00:48:37,160 --> 00:48:40,260
they use a slightly different algorithm where they're

805
00:48:40,260 --> 00:48:44,010
using a language model and they're saying,

806
00:48:44,010 --> 00:48:47,285
what- what- rather than just using pure counts,

807
00:48:47,285 --> 00:48:52,575
they're saying, "What clumping together would maximally reduce

808
00:48:52,575 --> 00:48:58,410
the perplexity of my language model and clump those things and repeat over?"

809
00:48:58,410 --> 00:49:02,280
And so they did- they've done two versions of this model.

810
00:49:02,280 --> 00:49:07,170
So the first version, the wordpiece model kind of like, um,

811
00:49:07,170 --> 00:49:11,585
byte pair encoding assumed that you have an initial tokenization

812
00:49:11,585 --> 00:49:16,035
to words and then you're just sort of having pieces of words,

813
00:49:16,035 --> 00:49:17,880
um, using this algorithm.

814
00:49:17,880 --> 00:49:20,790
And then they did a second version, um,

815
00:49:20,790 --> 00:49:25,050
the sentencepiece model which you can find at this GitHub site which said, "Well,

816
00:49:25,050 --> 00:49:28,560
it's problematic if we need to tokenize into words first

817
00:49:28,560 --> 00:49:30,208
because then we need to have a tokenizer for

818
00:49:30,208 --> 00:49:32,550
every language and that's a lot of work."

819
00:49:32,550 --> 00:49:34,545
Um, so maybe instead of that,

820
00:49:34,545 --> 00:49:36,825
we could just sort of treat,

821
00:49:36,825 --> 00:49:39,080
go from a character sequence,

822
00:49:39,080 --> 00:49:44,855
retain whitespaces and regard that as something that's part of the clumping process,

823
00:49:44,855 --> 00:49:47,045
and so that, um,

824
00:49:47,045 --> 00:49:50,015
you just build your word pieces which

825
00:49:50,015 --> 00:49:54,485
commonly will have spaces on one side or the other of them, um,

826
00:49:54,485 --> 00:49:57,710
because often things inside a word are the

827
00:49:57,710 --> 00:50:01,680
commoner- more common clumps and you build those up,

828
00:50:01,680 --> 00:50:04,755
and that's proven to be quite successful.

829
00:50:04,755 --> 00:50:10,530
Um, in particular, one place where some of you might see this,

830
00:50:10,530 --> 00:50:14,375
um, is, um, we've yet to get to describing it in the class really,

831
00:50:14,375 --> 00:50:19,950
but there's been this recent work which we actually talk about next week in class, are

832
00:50:19,950 --> 00:50:22,980
building these transformer models, in particular,

833
00:50:22,980 --> 00:50:26,160
Google has released this BERT model which gives you

834
00:50:26,160 --> 00:50:29,460
very good, um, word representations.

835
00:50:29,460 --> 00:50:32,835
And if you download BERT and try and use it,

836
00:50:32,835 --> 00:50:36,615
what you will find out is it doesn't operate over words,

837
00:50:36,615 --> 00:50:40,050
it operates over word pieces.

838
00:50:40,050 --> 00:50:43,260
Um, and so it has a large vocabulary.

839
00:50:43,260 --> 00:50:46,470
It's not a vocabulary of like 8,000 words.

840
00:50:46,470 --> 00:50:47,685
I forget the number,

841
00:50:47,685 --> 00:50:50,640
but the models have a large vocabulary,

842
00:50:50,640 --> 00:50:55,380
but they're still not a huge vocabulary and it's using word pieces.

843
00:50:55,380 --> 00:50:57,450
So lots of words are in the vocabulary.

844
00:50:57,450 --> 00:50:59,070
So if you look at the English model,

845
00:50:59,070 --> 00:51:01,140
it not only has words like f in it,

846
00:51:01,140 --> 00:51:04,470
but it even has words like Fairfax and 1910s,

847
00:51:04,470 --> 00:51:06,045
which aren't that common.

848
00:51:06,045 --> 00:51:10,245
Um, but it's nevertheless to cover all words,

849
00:51:10,245 --> 00:51:12,660
it's again using this wordpiece idea.

850
00:51:12,660 --> 00:51:16,075
So if I want a representation for the word hypatia, um,

851
00:51:16,075 --> 00:51:17,895
that's not in the vocabulary,

852
00:51:17,895 --> 00:51:19,800
and so I'm making it up of pieces.

853
00:51:19,800 --> 00:51:21,795
There's an h representation,

854
00:51:21,795 --> 00:51:24,110
and then in the BERT version,

855
00:51:24,110 --> 00:51:26,880
which is different to the Google NMT version,

856
00:51:26,880 --> 00:51:32,625
the non- the non-initial word pieces are represented with two hashes at the start,

857
00:51:32,625 --> 00:51:36,944
so I can put that together with h##yp etc.,

858
00:51:36,944 --> 00:51:40,200
and this would be my representation of hypatia.

859
00:51:40,200 --> 00:51:43,245
So effectively, I have word vectors, um,

860
00:51:43,245 --> 00:51:45,435
for four word pieces,

861
00:51:45,435 --> 00:51:47,880
and then I have to work out what to do with them.

862
00:51:47,880 --> 00:51:51,600
The simplest and quite common way is I just average the four of them.

863
00:51:51,600 --> 00:51:53,340
And there are obviously other things you could do.

864
00:51:53,340 --> 00:51:56,100
You could ConvNet and maxpool or you could run

865
00:51:56,100 --> 00:52:00,640
a little LSTM or something to put together a representation.

866
00:52:00,840 --> 00:52:07,220
Okay. Yeah. So- so those were the models that, um,

867
00:52:07,220 --> 00:52:10,530
sort of, worked with pieces of words to give you

868
00:52:10,530 --> 00:52:14,835
infinite vocabulary and ran them through a normal system.

869
00:52:14,835 --> 00:52:18,335
The other possibility is to say, "Well,

870
00:52:18,335 --> 00:52:22,530
we wanna work with characters so we can deal with an infinite vocabulary,

871
00:52:22,530 --> 00:52:27,945
but we're gonna sort of incorporate those into a bigger system."

872
00:52:27,945 --> 00:52:30,540
And a whole bunch of work has done this

873
00:52:30,540 --> 00:52:33,990
and in some sense it's a fairly obvious thing to do.

874
00:52:33,990 --> 00:52:38,730
Um, so this work in 2014 was one of the early ones.

875
00:52:38,730 --> 00:52:40,005
So they said, "Well,

876
00:52:40,005 --> 00:52:42,165
we could start with characters.

877
00:52:42,165 --> 00:52:47,640
We can do a convolution over the characters to generate word embeddings,

878
00:52:47,640 --> 00:52:53,250
and then we can use those word embeddings for something in a higher level model."

879
00:52:53,250 --> 00:52:58,215
Um, this was actually sort of a fixed window model for doing part of speech tagging.

880
00:52:58,215 --> 00:53:00,240
Um, that makes sense.

881
00:53:00,240 --> 00:53:01,935
Instead of a convolution,

882
00:53:01,935 --> 00:53:03,690
you could use LSTM.

883
00:53:03,690 --> 00:53:06,720
So this was work from a year later,

884
00:53:06,720 --> 00:53:07,835
and they said, "Well,

885
00:53:07,835 --> 00:53:09,795
we're also gonna build up, um,

886
00:53:09,795 --> 00:53:12,245
word representations from characters.

887
00:53:12,245 --> 00:53:17,520
And the way we're gonna do it is we're gonna run character level Bi-LSTMs,

888
00:53:17,520 --> 00:53:20,044
concatenate the two final states,

889
00:53:20,044 --> 00:53:23,430
and we're gonna call that outward representation,

890
00:53:23,430 --> 00:53:27,750
and then we're gonna put that word representation into a

891
00:53:27,750 --> 00:53:34,890
language model which is then a higher level LSTM that works along a sequence of words."

892
00:53:34,890 --> 00:53:37,785
And I thought I'd just- Oh, yeah.

893
00:53:37,785 --> 00:53:41,400
Words, are they training uh, like character-

894
00:53:41,400 --> 00:53:44,130
Yeah. Oh yeah, that's very important to realize.

895
00:53:44,130 --> 00:53:50,550
Yes so yeah so if you're learning- you'll learn- I mean this is the hidden layer.

896
00:53:50,550 --> 00:53:54,150
I guess I'm not actually showing the input layer but for the input layer

897
00:53:54,150 --> 00:53:58,260
you're learning a vector for each character.

898
00:53:58,260 --> 00:54:01,800
So effectively you're doing the same kind of thing we saw

899
00:54:01,800 --> 00:54:07,545
before that you're starting with random representations for each character.

900
00:54:07,545 --> 00:54:13,590
You've got this embedded inside a word sequence LSTM,

901
00:54:13,590 --> 00:54:20,160
your goal is to minimize the perplexity of the higher level LSTM as,

902
00:54:20,160 --> 00:54:26,100
um, as a language model and so it filters back its gradients.

903
00:54:26,100 --> 00:54:30,450
So it's wanting to come up with character vectors such that if it

904
00:54:30,450 --> 00:54:35,860
produces good word vectors which produces low, um, perplexities.

905
00:54:35,900 --> 00:54:41,220
Good question. Um, so here's, um,

906
00:54:41,220 --> 00:54:44,820
a slightly more complex version of trying to

907
00:54:44,820 --> 00:54:48,720
do this that's a bit more recent where again the idea is can we build

908
00:54:48,720 --> 00:54:53,505
a good language model by starting out from characters

909
00:54:53,505 --> 00:54:59,010
and wanting to exploit sort of related sub words and rare words.

910
00:54:59,010 --> 00:55:02,760
And so they built sort of this kind of

911
00:55:02,760 --> 00:55:07,815
this more stacked complex model that we'll go through the stages of wherefore

912
00:55:07,815 --> 00:55:11,805
we start with a word represented as characters.

913
00:55:11,805 --> 00:55:14,700
We have character embeddings which we build

914
00:55:14,700 --> 00:55:17,910
into a convolutional network and then we head upwards.

915
00:55:17,910 --> 00:55:20,354
So if we take that one piece at a time,

916
00:55:20,354 --> 00:55:24,420
um, so you have a character embedding for each character.

917
00:55:24,420 --> 00:55:28,680
Um, you'll then have a convolutional layer which

918
00:55:28,680 --> 00:55:33,540
then sort of rep, has various filters that work over those,

919
00:55:33,540 --> 00:55:37,800
um, character sequence of two, three and four grams of characters.

920
00:55:37,800 --> 00:55:41,340
So you're getting representations of parts of words.

921
00:55:41,340 --> 00:55:48,600
Um, then from those convolutional networks you're then doing max pooling over time which

922
00:55:48,600 --> 00:55:51,510
is effectively sort of like choosing which of

923
00:55:51,510 --> 00:55:56,155
these n-grams best represents the meaning of a word.

924
00:55:56,155 --> 00:56:00,350
Um, then what they do after that is so at that point

925
00:56:00,350 --> 00:56:05,030
they've got an output representation for character n-grams,

926
00:56:05,030 --> 00:56:12,095
and so then they feed that into a highway network like we talked about a bit last time.

927
00:56:12,095 --> 00:56:16,985
And then the output of that then at the word level,

928
00:56:16,985 --> 00:56:20,405
um, goes into an LSTM network,

929
00:56:20,405 --> 00:56:24,365
and this LSTM network is now word-level LSTM network,

930
00:56:24,365 --> 00:56:27,710
and you're trying to sort of maxim- minimize

931
00:56:27,710 --> 00:56:32,365
perplexity like for the neural language models we saw earlier.

932
00:56:32,365 --> 00:56:35,385
Um, so what could they show with this?

933
00:56:35,385 --> 00:56:38,910
Well, the first thing they could show with it is that it

934
00:56:38,910 --> 00:56:43,950
actually again just works well as a language model despite that skepticism

935
00:56:43,950 --> 00:56:46,800
that I hadn't told you of about the fact of

936
00:56:46,800 --> 00:56:50,040
the matter is you could build these kind of character level models and

937
00:56:50,040 --> 00:56:56,535
train them and they work to a first approximation as well as word-level language models.

938
00:56:56,535 --> 00:56:59,790
But one of the observations that they make is that you can be

939
00:56:59,790 --> 00:57:03,840
getting as good results but with much smaller models.

940
00:57:03,840 --> 00:57:05,235
So up at the top here are

941
00:57:05,235 --> 00:57:10,695
the character level LSTM models and word ones that the models they built.

942
00:57:10,695 --> 00:57:14,460
And here are a whole bunch of models over this data-set.

943
00:57:14,460 --> 00:57:19,410
Um, and so as time went by perplexities have been going down,

944
00:57:19,410 --> 00:57:23,790
gone to 78,4 and their point was well we can

945
00:57:23,790 --> 00:57:29,100
build pretty much as good a character model with 78,9 perplexity but

946
00:57:29,100 --> 00:57:32,730
our model is actually much smaller, this model here has

947
00:57:32,730 --> 00:57:35,760
52 million parameters whereas our model that

948
00:57:35,760 --> 00:57:39,240
works on a character level has only 19 million parameters.

949
00:57:39,240 --> 00:57:41,685
So it's about 40% of the size.

950
00:57:41,685 --> 00:57:45,210
And that seems,um, kind of interesting.

951
00:57:45,210 --> 00:57:51,510
But perhaps what's more interesting is to sort of peek inside it and see what

952
00:57:51,510 --> 00:57:55,500
happens with the representation of words when

953
00:57:55,500 --> 00:57:59,745
built out of characters and this part is sort of actually a bit cool.

954
00:57:59,745 --> 00:58:06,064
Um, so what this is showing is for words that are up to the top while,

955
00:58:06,064 --> 00:58:08,235
his, you, Richard, trading.

956
00:58:08,235 --> 00:58:11,520
It's asking what other words are most

957
00:58:11,520 --> 00:58:15,510
similar to it according to the word representations that's computed.

958
00:58:15,510 --> 00:58:21,480
And the top part is the output of a word level LSTM model and that's sort of okay.

959
00:58:21,480 --> 00:58:23,550
Richard comes out as similar to Jonathan,

960
00:58:23,550 --> 00:58:25,515
Robert, Neil and Nancy et cetera.

961
00:58:25,515 --> 00:58:30,810
While although letting though minute mainly okay.

962
00:58:30,810 --> 00:58:35,730
But it's sort of interesting what happens with their character level models,um,

963
00:58:35,730 --> 00:58:37,605
and so in particular, um,

964
00:58:37,605 --> 00:58:41,910
what's kind of interesting is like first of all you remember they sort of had

965
00:58:41,910 --> 00:58:46,845
the character embeddings that went through the convolutional layer and the max pooling.

966
00:58:46,845 --> 00:58:51,150
And if at that point you ask what things are most

967
00:58:51,150 --> 00:58:55,830
similar that basically it's still remembering things about characters.

968
00:58:55,830 --> 00:58:58,350
So the most similar words to while,

969
00:58:58,350 --> 00:59:01,740
chile, whole, meanwhile and white.

970
00:59:01,740 --> 00:59:06,600
So at least for the sort of first ones they all end in LE.

971
00:59:06,600 --> 00:59:10,560
And you see that pattern elsewhere right close to Richard,

972
00:59:10,560 --> 00:59:15,630
hard, rich, richer, richter that hard ends in ARD, rich.

973
00:59:15,630 --> 00:59:18,390
So you're just sort of getting this character sequence similarity,

974
00:59:18,390 --> 00:59:20,595
it's not really doing meaning at all.

975
00:59:20,595 --> 00:59:25,500
But interestingly when they're then putting it through the highway layers,

976
00:59:25,500 --> 00:59:29,775
that the highway layers are suc- successfully learning how to

977
00:59:29,775 --> 00:59:33,285
transform those character sequence representations

978
00:59:33,285 --> 00:59:35,685
into something that does capture meaning.

979
00:59:35,685 --> 00:59:38,850
So if you then say at the output of

980
00:59:38,850 --> 00:59:44,295
the highway layers what words are most similar then it seems to be working pretty well,

981
00:59:44,295 --> 00:59:49,245
While was similar to meanwhile, Richard is similar to Edward, Gerard, Edward with Carl.

982
00:59:49,245 --> 00:59:51,750
They're sort of now working much more like

983
00:59:51,750 --> 00:59:55,755
a word level model in capturing semantic similarity.

984
00:59:55,755 --> 00:59:57,900
So that seems kind of cool.

985
00:59:57,900 --> 01:00:02,070
Um, so then they say well what about if we ask about

986
01:00:02,070 --> 01:00:06,555
words that aren't in the vocabulary of the model.

987
01:00:06,555 --> 01:00:09,150
Well, if they're not in the vocabulary of the model,

988
01:00:09,150 --> 01:00:13,890
the word level model can't do anything and so that's why you get those dashes there.

989
01:00:13,890 --> 01:00:16,650
And what they're wanting to show is that

990
01:00:16,650 --> 01:00:19,470
the character level model still works pretty well.

991
01:00:19,470 --> 01:00:23,310
So if you give it look with seven O's in the middle of

992
01:00:23,310 --> 01:00:27,300
it that it's correctly deciding that look,

993
01:00:27,300 --> 01:00:29,370
look, look, looking are actually

994
01:00:29,370 --> 01:00:33,150
the most similar words to that which is actually working very nicely.

995
01:00:33,150 --> 01:00:36,375
And some of the other examples are similar, computer-aided,

996
01:00:36,375 --> 01:00:38,865
is seen as most similar to computer-guided,

997
01:00:38,865 --> 01:00:44,685
computer-driven, computerized, computer, you're getting pretty similar sensible results.

998
01:00:44,685 --> 01:00:47,010
And then the little picture on the,

999
01:00:47,010 --> 01:00:49,755
um, right is sort of, um,

1000
01:00:49,755 --> 01:00:56,235
showing, um, one of these 2D visualizations of the units that have been learned.

1001
01:00:56,235 --> 01:00:58,770
And so the red,

1002
01:00:58,770 --> 01:01:02,760
the red things are word character prefixes,

1003
01:01:02,760 --> 01:01:05,550
the blue things are character suffixes,

1004
01:01:05,550 --> 01:01:09,180
the orange things are hyphenated things

1005
01:01:09,180 --> 01:01:13,050
like in the middle of computer-guided and gray is everything else.

1006
01:01:13,050 --> 01:01:14,790
And so there's some sort of sense,

1007
01:01:14,790 --> 01:01:19,050
with which it's picking out different important parts of words.

1008
01:01:19,050 --> 01:01:26,880
Okay. Um, and that's why also I guess just another good example of how you can sort of

1009
01:01:26,880 --> 01:01:30,870
compose together different kinds of building blocks to

1010
01:01:30,870 --> 01:01:32,670
make more powerful models that you might

1011
01:01:32,670 --> 01:01:35,415
also want to think about for your final projects.

1012
01:01:35,415 --> 01:01:37,450
Okay.

1013
01:01:45,200 --> 01:01:46,770
Um, so here's

1014
01:01:46,770 --> 01:01:51,135
back to one other example from a neural machine translation system

1015
01:01:51,135 --> 01:01:55,995
of doing this hybrid architecture that has word-level and character level.

1016
01:01:55,995 --> 01:01:59,400
I showed you earlier a purely character level model.

1017
01:01:59,400 --> 01:02:02,580
I mean we built that out of interest to see

1018
01:02:02,580 --> 01:02:05,940
how well it did but we were sort of really wanting to build

1019
01:02:05,940 --> 01:02:09,810
a hybrid model because that seemed like it would be much more practical

1020
01:02:09,810 --> 01:02:14,265
to build something that translated relatively quickly and well.

1021
01:02:14,265 --> 01:02:16,860
Um, so the idea was we'd mainly build

1022
01:02:16,860 --> 01:02:20,550
a word-level neural machine translation system but we'd

1023
01:02:20,550 --> 01:02:26,970
be able to work with character level stuff when we had rare or unseen words.

1024
01:02:26,970 --> 01:02:30,090
Um, and that turned out to work pretty,

1025
01:02:30,090 --> 01:02:33,000
um, successfully at improving performance.

1026
01:02:33,000 --> 01:02:35,355
So the idea of that model is this.

1027
01:02:35,355 --> 01:02:40,575
Um, that we're going to run a pretty standard, um,

1028
01:02:40,575 --> 01:02:47,100
sequence to sequence with attention LSTM neural machine translation system.

1029
01:02:47,100 --> 01:02:51,900
In my pic- I mean, it's actually a four-level deep system but in my picture I

1030
01:02:51,900 --> 01:02:56,490
showed less than four levels stacked to make it easier to see things.

1031
01:02:56,490 --> 01:03:01,920
And we're going to run this with a reasonable vocabulary of 16,000 words.

1032
01:03:01,920 --> 01:03:07,995
So for common words we just have word representations that we're feeding into

1033
01:03:07,995 --> 01:03:13,320
our neural machine translation model but for words that aren't in the vocabulary we're

1034
01:03:13,320 --> 01:03:19,530
going to work out a word representation for them by using a character level LSTM,

1035
01:03:19,530 --> 01:03:23,130
and conversely, when we start to generate words on

1036
01:03:23,130 --> 01:03:28,710
the other side we have a soft max with a vocabulary of 16,000.

1037
01:03:28,710 --> 01:03:34,275
It could just generate words like [NOISE] but one of those words is the UNK symbol.

1038
01:03:34,275 --> 01:03:38,610
And if it generates the UNK symbol we then run a- we take

1039
01:03:38,610 --> 01:03:42,060
this hidden representation and feed it in as

1040
01:03:42,060 --> 01:03:47,370
the initial input into a character level LSTM and then we have the character level

1041
01:03:47,370 --> 01:03:51,390
LSTM generate a character sequence until it generates

1042
01:03:51,390 --> 01:03:56,650
a stop symbol and we use that to generate words. Um-

1043
01:03:57,240 --> 01:04:02,080
Okay. So we end up sort of with this sort of

1044
01:04:02,080 --> 01:04:08,170
hybrid composed stack of eight LSTM layers. Uh, yeah.

1045
01:04:08,170 --> 01:04:12,690
[inaudible] and you always get some probability for the UNK symbol.

1046
01:04:12,690 --> 01:04:15,180
So if you wanted to get the- the proper gradient,

1047
01:04:15,180 --> 01:04:19,230
you- you'll always have to run it for every word but what- what do you-?

1048
01:04:19,230 --> 01:04:22,870
I would often say, you only run during training,

1049
01:04:22,870 --> 01:04:28,990
you only run the character level LSTM when the UNK symbol receives the highest likelihood.

1050
01:04:28,990 --> 01:04:29,905
So we-

1051
01:04:29,905 --> 01:04:30,685
What is that?

1052
01:04:30,685 --> 01:04:34,335
So at training, at training time,

1053
01:04:34,335 --> 01:04:36,510
there's a determinant piece of tech, right?

1054
01:04:36,510 --> 01:04:39,705
You know the source and you know the target,

1055
01:04:39,705 --> 01:04:43,740
and so we're, and at training time,

1056
01:04:43,740 --> 01:04:46,230
we've already decided our vocabulary, right?

1057
01:04:46,230 --> 01:04:51,400
That we've just decided what are the 15,999 most common words,

1058
01:04:51,400 --> 01:04:53,950
those and UNK are our vocabulary.

1059
01:04:53,950 --> 01:04:57,115
So for both the input and the output side,

1060
01:04:57,115 --> 01:05:00,895
we know which words aren't in our vocabulary.

1061
01:05:00,895 --> 01:05:03,400
And so if it's not in our vocabulary,

1062
01:05:03,400 --> 01:05:04,840
we're running this one.

1063
01:05:04,840 --> 01:05:07,450
If if what was the output is not in our vocabulary,

1064
01:05:07,450 --> 01:05:11,950
we're running that one, and otherwise we're just not running it at all, yeah.

1065
01:05:11,950 --> 01:05:18,130
So and and the bit that I didn't explain but is actually important perhaps

1066
01:05:18,130 --> 01:05:23,920
related like when we're calculating a loss that we can back

1067
01:05:23,920 --> 01:05:26,200
propagate, that sort of up here,

1068
01:05:26,200 --> 01:05:28,120
there are sort of two losses.

1069
01:05:28,120 --> 01:05:32,920
There's a loss at the word level that you know you'd like to in this position,

1070
01:05:32,920 --> 01:05:36,085
give probability 1 to generating UNK but really,

1071
01:05:36,085 --> 01:05:41,470
this model we'll softmax, we'll say UNK is you know probability 0,2 or whatever.

1072
01:05:41,470 --> 01:05:44,515
So there's a loss there and then secondarily,

1073
01:05:44,515 --> 01:05:48,400
there's a particular sequence of characters you wanna generate and you've also got

1074
01:05:48,400 --> 01:05:53,410
a loss because you've met the probabilities you put over the characters.

1075
01:05:53,410 --> 01:05:55,825
Um, So then, um,

1076
01:05:55,825 --> 01:05:58,570
I think we- I think Abby sort of briefly mentioned this.

1077
01:05:58,570 --> 01:06:01,810
Commonly, the decoders do some kind of

1078
01:06:01,810 --> 01:06:06,310
beam search to consider different possibilities before deciding,

1079
01:06:06,310 --> 01:06:10,795
um, the highest probability one over a sequence of words.

1080
01:06:10,795 --> 01:06:13,930
And so this was doing a slightly more complex version of that.

1081
01:06:13,930 --> 01:06:17,830
So there's a word-level beam search when running it and then

1082
01:06:17,830 --> 01:06:23,335
also doing a character level beam search to consider different possibilities.

1083
01:06:23,335 --> 01:06:27,235
And so if you wanna integrate the the two of those together.

1084
01:06:27,235 --> 01:06:29,785
Um, but essentially, um,

1085
01:06:29,785 --> 01:06:32,980
this worked pretty well.

1086
01:06:32,980 --> 01:06:40,510
Um, so, um, this was the winning system at WMT 2015 which

1087
01:06:40,510 --> 01:06:43,540
used 30 times as much data and ensembled together

1088
01:06:43,540 --> 01:06:48,115
three other systems compared to the data that was provided for the task.

1089
01:06:48,115 --> 01:06:52,675
This was the system I showed before, they got 18,3.

1090
01:06:52,675 --> 01:06:54,940
Um, and if you remember our character,

1091
01:06:54,940 --> 01:06:58,375
purely character level system got 18,5.

1092
01:06:58,375 --> 01:07:02,500
Um, then by building this hybrid system,

1093
01:07:02,500 --> 01:07:07,600
that we were able to build a much better system that was about 2,5 BLEU points better,

1094
01:07:07,600 --> 01:07:12,805
um, than after- than either this word level or the character level system.

1095
01:07:12,805 --> 01:07:15,070
So that was kind of nice, um,

1096
01:07:15,070 --> 01:07:17,725
and in particular that was the state of the art at the time.

1097
01:07:17,725 --> 01:07:21,355
Now of course, if you were paying very close attention,

1098
01:07:21,355 --> 01:07:24,430
that's now nowhere near the state of the art.

1099
01:07:24,430 --> 01:07:30,370
Because when I showed you that slide way earlier of the Google system,

1100
01:07:30,370 --> 01:07:33,310
you will have noticed that they have

1101
01:07:33,310 --> 01:07:37,600
much higher numbers in the 20s, but that's what happens as the years go by.

1102
01:07:37,600 --> 01:07:39,490
Um, okay.

1103
01:07:39,490 --> 01:07:41,800
But here's an example that shows

1104
01:07:41,800 --> 01:07:46,300
these different systems working and some of the mistakes they make.

1105
01:07:46,300 --> 01:07:48,775
Um, here's a cherry picked example, um,

1106
01:07:48,775 --> 01:07:51,940
where our system, the hybrid system,

1107
01:07:51,940 --> 01:07:54,985
works perfectly because what- that's what you expect to see.

1108
01:07:54,985 --> 01:07:57,685
Um, and so, you know,

1109
01:07:57,685 --> 01:08:02,260
you can see some of the defects of things that can go wrong.

1110
01:08:02,260 --> 01:08:05,155
Um, so in this case,

1111
01:08:05,155 --> 01:08:09,550
you know the character level system didn't work here because it just sort

1112
01:08:09,550 --> 01:08:15,250
of starting with the Steph, it sort of seemed to free associate,

1113
01:08:15,250 --> 01:08:19,690
um, a completely made up name that doesn't really have anything to do with the source.

1114
01:08:19,690 --> 01:08:22,570
So that one isn't very good.

1115
01:08:22,570 --> 01:08:27,595
Um, the word level system went bang here,

1116
01:08:27,595 --> 01:08:30,340
so you remember when it generates an UNK,

1117
01:08:30,340 --> 01:08:36,400
the word level system would have when it generates, it's using attention.

1118
01:08:36,400 --> 01:08:38,890
So when it wants to generate, um,

1119
01:08:38,890 --> 01:08:42,010
it has attention back to words and the source.

1120
01:08:42,010 --> 01:08:45,070
And when it generates UNK has two strategies.

1121
01:08:45,070 --> 01:08:50,410
It can either do unigram translation of the word that it's maximally

1122
01:08:50,410 --> 01:08:56,140
putting attention on or it could copy the word that it's maximally putting attention on.

1123
01:08:56,140 --> 01:08:57,790
Um, so in this case,

1124
01:08:57,790 --> 01:09:02,890
it chose to translate the word that it was maximally putting attention on but the word it

1125
01:09:02,890 --> 01:09:08,080
was maximally putting attention on was after rather than diagnosis.

1126
01:09:08,080 --> 01:09:11,290
And so you just get this po po coming out of after,

1127
01:09:11,290 --> 01:09:14,140
after and we've completely lost the word.

1128
01:09:14,140 --> 01:09:17,965
Um, and in this example, in this example,

1129
01:09:17,965 --> 01:09:20,065
how a hybrid system, um,

1130
01:09:20,065 --> 01:09:24,370
just ends up working beautifully and gives you exactly the right translation.

1131
01:09:24,370 --> 01:09:26,740
Yeah. Um, of course,

1132
01:09:26,740 --> 01:09:29,035
it's not always that good in the real world.

1133
01:09:29,035 --> 01:09:31,000
Um, so here's a different example.

1134
01:09:31,000 --> 01:09:35,455
So this is the example I showed before with the 11-year-old daughter.

1135
01:09:35,455 --> 01:09:40,420
Um, and in this example,

1136
01:09:40,420 --> 01:09:44,350
the hybrid model has the same strength of the character model.

1137
01:09:44,350 --> 01:09:50,065
It correctly generates 11 years old at a character level in its translation,

1138
01:09:50,065 --> 01:09:52,300
but you know this time, for whatever reason,

1139
01:09:52,300 --> 01:09:56,170
it's the hybrid model that goes bang in

1140
01:09:56,170 --> 01:10:01,165
generating the names and it translates Shani Bart as Graham Bart.

1141
01:10:01,165 --> 01:10:04,015
Um, whereas the character level model gets it right.

1142
01:10:04,015 --> 01:10:06,700
Um, actually, I think this is one of the weaknesses of

1143
01:10:06,700 --> 01:10:10,120
this hybrid model compared to the character level model.

1144
01:10:10,120 --> 01:10:13,060
That because of the character level generator is

1145
01:10:13,060 --> 01:10:16,670
kind of this sort of second level.

1146
01:10:17,190 --> 01:10:20,605
For the purely character level model,

1147
01:10:20,605 --> 01:10:27,010
it's able to use the character sequence as conditioning context very effectively.

1148
01:10:27,010 --> 01:10:28,870
Whereas our hybrid model,

1149
01:10:28,870 --> 01:10:32,410
although we feed the hidden representation of

1150
01:10:32,410 --> 01:10:34,450
the word level model in as

1151
01:10:34,450 --> 01:10:37,855
the starting hidden representation of the character level model,

1152
01:10:37,855 --> 01:10:40,570
it doesn't have any representation further

1153
01:10:40,570 --> 01:10:43,615
back than that of what's in the word level model.

1154
01:10:43,615 --> 01:10:47,695
And so it tends to not always do as good a job at representing,

1155
01:10:47,695 --> 01:10:53,030
of capturing the context that allows it to do translation of things like names.

1156
01:10:53,100 --> 01:10:58,510
Okay. Um, very- almost finished but there's

1157
01:10:58,510 --> 01:11:01,120
just sort of one thing I wanted to mention before

1158
01:11:01,120 --> 01:11:03,985
the end which is almost a practical thing.

1159
01:11:03,985 --> 01:11:07,465
Um, So we started off with word embeddings,

1160
01:11:07,465 --> 01:11:10,510
but now we've been talking a lot of character level models.

1161
01:11:10,510 --> 01:11:15,760
So surely, just for word embedding, you should be able to do useful things with them,

1162
01:11:15,760 --> 01:11:18,880
with characters or pieces of words.

1163
01:11:18,880 --> 01:11:21,340
And that's something that people start to play with.

1164
01:11:21,340 --> 01:11:24,790
So in this Cao and Rei paper they said well

1165
01:11:24,790 --> 01:11:30,280
let's train a Word2vec model using exactly the same, um,

1166
01:11:30,280 --> 01:11:34,495
loss as Word2vec uses but let's,

1167
01:11:34,495 --> 01:11:37,795
um, rather than having word representations,

1168
01:11:37,795 --> 01:11:42,160
let's start with character sequences and run

1169
01:11:42,160 --> 01:11:47,425
a bidirectional LSTM to work out word representations,

1170
01:11:47,425 --> 01:11:49,720
and we'll then sort of be effectively

1171
01:11:49,720 --> 01:11:52,780
training this more complex model where we're learning

1172
01:11:52,780 --> 01:12:00,550
character embeddings and LSTM parameters and that will give us our word representations.

1173
01:12:00,550 --> 01:12:04,480
And that's an idea that people have continued to play with,

1174
01:12:04,480 --> 01:12:08,845
and so in particular I just wanted to mention these FastText embeddings.

1175
01:12:08,845 --> 01:12:10,900
Um, so a couple of years ago,

1176
01:12:10,900 --> 01:12:12,820
um, people now at Facebook,

1177
01:12:12,820 --> 01:12:16,015
the same Tomas Mikolov who did the original Word2vec,

1178
01:12:16,015 --> 01:12:18,100
brought out a new set of embeddings,

1179
01:12:18,100 --> 01:12:21,340
the FastText embeddings and their goal was to sort of

1180
01:12:21,340 --> 01:12:24,880
have a next-generation Word2vec, um,

1181
01:12:24,880 --> 01:12:27,940
which is sort of an efficient fast, um,

1182
01:12:27,940 --> 01:12:30,535
word vector learning library, um,

1183
01:12:30,535 --> 01:12:35,215
but it was better for rare words and languages with lots of morphology.

1184
01:12:35,215 --> 01:12:39,010
And the way they did it was that they sort of essentially took the Word2vec skip

1185
01:12:39,010 --> 01:12:43,585
gram model but they augmented it to put in character n-grams.

1186
01:12:43,585 --> 01:12:46,450
So more precisely, this is what they did.

1187
01:12:46,450 --> 01:12:49,720
So, um, when you had a word,

1188
01:12:49,720 --> 01:12:52,000
my example word is where,

1189
01:12:52,000 --> 01:12:57,610
for some n-gram size you represent it as a set of n-gram.

1190
01:12:57,610 --> 01:13:01,180
So this is kind of just about like those, we called phonemes I mentioned

1191
01:13:01,180 --> 01:13:05,080
right at the beginning where you have a kind of a boundary symbol,

1192
01:13:05,080 --> 01:13:06,820
so you know the beginning of the word.

1193
01:13:06,820 --> 01:13:11,965
So if the length is three you have beginning of word WH, WHE, HER,

1194
01:13:11,965 --> 01:13:14,500
ERE, RE end of word,

1195
01:13:14,500 --> 01:13:17,695
as pieces of representation.

1196
01:13:17,695 --> 01:13:20,740
And then you have an additional one for just the whole word.

1197
01:13:20,740 --> 01:13:24,355
So you do still have whole word representations in this model.

1198
01:13:24,355 --> 01:13:28,980
So where is represented by six things and so

1199
01:13:28,980 --> 01:13:34,350
then you're going to use all six of those things in your computation.

1200
01:13:34,350 --> 01:13:37,260
Um, so if you sort of remember the guts of

1201
01:13:37,260 --> 01:13:40,590
Word2vec that what you were doing was you were doing

1202
01:13:40,590 --> 01:13:45,585
these vector dot products between your context representation

1203
01:13:45,585 --> 01:13:48,240
and your center word representation.

1204
01:13:48,240 --> 01:13:52,080
So they're going to do exactly the same thing but for

1205
01:13:52,080 --> 01:13:56,870
the center word they're gonna use all six of these vectors.

1206
01:13:56,870 --> 01:14:00,400
All the vectors corresponding to all six of

1207
01:14:00,400 --> 01:14:03,685
these representations and they're going to sum them.

1208
01:14:03,685 --> 01:14:06,910
And so you're just doing a simple summing operation,

1209
01:14:06,910 --> 01:14:10,765
and that's sort of then giving you your representation of similarity.

1210
01:14:10,765 --> 01:14:13,180
Um, very precisely, they don't quite do that

1211
01:14:13,180 --> 01:14:15,550
because there's a hashing trick but I'll leave that out.

1212
01:14:15,550 --> 01:14:21,370
But what they're able to show is that that model actually works pretty successfully.

1213
01:14:21,370 --> 01:14:24,340
So these are words similarity scores,

1214
01:14:24,340 --> 01:14:28,480
skip gram, they're all CBOW,

1215
01:14:28,480 --> 01:14:31,554
and then this is the sort of new model,

1216
01:14:31,554 --> 01:14:36,925
um, that, um, uses these kind of n-grams.

1217
01:14:36,925 --> 01:14:38,755
And in this, um,

1218
01:14:38,755 --> 01:14:40,960
you know at least for one of the English data sets,

1219
01:14:40,960 --> 01:14:42,235
it doesn't get any better.

1220
01:14:42,235 --> 01:14:47,830
Um, but what they especially notice this is for languages that have more,

1221
01:14:47,830 --> 01:14:52,690
morp- more morphology that you're sort of getting some fairly clear gains.

1222
01:14:52,690 --> 01:14:55,375
70, 69 onto 75,

1223
01:14:55,375 --> 01:14:58,765
59, 60 on to 66 in the right column,

1224
01:14:58,765 --> 01:15:02,440
so then these wordpiece models do give them a better model of

1225
01:15:02,440 --> 01:15:07,330
words and just practically FastText, um,

1226
01:15:07,330 --> 01:15:12,940
library now has sort of word embeddings for about 60 or 70 different languages,

1227
01:15:12,940 --> 01:15:16,930
so it's sort of a good source of word embeddings for multilingual applications.

1228
01:15:16,930 --> 01:15:19,735
Okay, I think I am done.

1229
01:15:19,735 --> 01:15:23,000
So thanks a lot and see you again next week.

