1
00:00:05,000 --> 00:00:08,985
So welcome to the Machine [NOISE] Translation lecture,

2
00:00:08,985 --> 00:00:11,580
which is kind of like a culmination [NOISE] of

3
00:00:11,580 --> 00:00:15,870
this sequence of three lectures on RNNs and related topics.

4
00:00:15,870 --> 00:00:18,420
So let's have a few announcements first.

5
00:00:18,420 --> 00:00:19,590
Uh, the first thing is,

6
00:00:19,590 --> 00:00:20,940
as you probably noticed when you came in,

7
00:00:20,940 --> 00:00:22,770
we're taking attendance today.

8
00:00:22,770 --> 00:00:27,090
Uh, so you need to sign in with the TAs who are outside the auditorium.

9
00:00:27,090 --> 00:00:28,965
Uh, if you missed it,

10
00:00:28,965 --> 00:00:30,540
don't get up now, it's fine.

11
00:00:30,540 --> 00:00:32,985
There will be time to sign in after the lecture.

12
00:00:32,985 --> 00:00:34,880
Uh, and then, if you have any kind of questions about

13
00:00:34,880 --> 00:00:37,130
special cases with the attendance policy, uh,

14
00:00:37,130 --> 00:00:41,090
you should check out a Piazza post that we put up last night with some clarifications.

15
00:00:41,090 --> 00:00:45,230
[NOISE] Uh, you have the reminder that Assignment 4 content is going to be covered today.

16
00:00:45,230 --> 00:00:47,690
So you're gonna have everything you need to do Assignment 4 at the end of today.

17
00:00:47,690 --> 00:00:51,860
[NOISE] And do get started early because the model takes 4 hours to train.

18
00:00:51,860 --> 00:00:54,585
The other announcement is that we're going [NOISE] to be sending out

19
00:00:54,585 --> 00:00:58,580
our mid-quarter feedback survey sometime in the next few days probably,

20
00:00:58,580 --> 00:01:00,110
uh, so please do fill it out.

21
00:01:00,110 --> 00:01:01,775
You'll get 0,5% credit,

22
00:01:01,775 --> 00:01:04,520
and you're also gonna help us to make the class better for the rest of the quarter.

23
00:01:04,520 --> 00:01:08,190
[NOISE] Okay.

24
00:01:08,190 --> 00:01:09,880
So here's the overview of what we're going to do today.

25
00:01:09,880 --> 00:01:11,535
[NOISE] Uh, today, first,

26
00:01:11,535 --> 00:01:13,820
we're going to introduce a new task in NLP,

27
00:01:13,820 --> 00:01:16,190
which is machine translation, [NOISE] and then,

28
00:01:16,190 --> 00:01:20,200
we're going to introduce a new neural architecture called sequence-to-sequence.

29
00:01:20,200 --> 00:01:22,590
And the connection here is that machine translation

30
00:01:22,590 --> 00:01:25,020
is a major use case of sequence-to-sequence.

31
00:01:25,020 --> 00:01:29,525
[NOISE] After that, we're going to introduce a new neural technique called attention,

32
00:01:29,525 --> 00:01:33,415
and this is something that improves sequence-to-sequence a lot.

33
00:01:33,415 --> 00:01:37,230
Okay. So Section 1 of this is gonna be about, uh,

34
00:01:37,230 --> 00:01:40,920
a bit of machine translation history, pre-neural machine translation.

35
00:01:40,920 --> 00:01:45,180
[NOISE] So machine translation or MT,

36
00:01:45,180 --> 00:01:48,140
uh, is the task of translating a sentence x, uh,

37
00:01:48,140 --> 00:01:50,060
which we call the source language,

38
00:01:50,060 --> 00:01:52,250
whatever language you're translating from,

39
00:01:52,250 --> 00:01:54,560
into a sentence y, which is in another language,

40
00:01:54,560 --> 00:01:56,645
which we call the target language.

41
00:01:56,645 --> 00:01:59,875
Uh, so here's an example. Let's suppose x is this French sentence.

42
00:01:59,875 --> 00:02:02,100
Um, [NOISE] could anyone in the audience,

43
00:02:02,100 --> 00:02:04,300
a French speaker, translate this to English for us? [NOISE] [BACKGROUND] Yeah.

44
00:02:04,300 --> 00:02:05,430
Um, the man is born free, and, uh, everywhere, he is in irons.

45
00:02:05,430 --> 00:02:12,500
Great.

46
00:02:12,500 --> 00:02:20,760
So that was something like,

47
00:02:20,760 --> 00:02:22,785
the man is born free, but everywhere, he's in irons.

48
00:02:22,785 --> 00:02:24,395
That was a fairly literal translation.

49
00:02:24,395 --> 00:02:27,770
It's usually translated, this quote by Rousseau is usually translated as,

50
00:02:27,770 --> 00:02:29,880
man is born free, but everywhere, he is in chains.

51
00:02:29,880 --> 00:02:32,475
But there's an ambiguity: [NOISE] should fers be,

52
00:02:32,475 --> 00:02:34,200
um, literally irons or chains?

53
00:02:34,200 --> 00:02:35,880
Also, you could choose to, uh,

54
00:02:35,880 --> 00:02:38,670
translate l'homme as man or maybe humankind.

55
00:02:38,670 --> 00:02:41,570
Uh, so this is an example of machine translation,

56
00:02:41,570 --> 00:02:42,635
and there's already, you know,

57
00:02:42,635 --> 00:02:43,790
quite a few choices you can make.

58
00:02:43,790 --> 00:02:50,505
[NOISE] So the beginning of machine translation as an AI task began in the early 1950s.

59
00:02:50,505 --> 00:02:52,365
So, um, in particular,

60
00:02:52,365 --> 00:02:55,010
there was a lot of work translating Russian to English, uh,

61
00:02:55,010 --> 00:02:57,200
because the West was very interested in listening

62
00:02:57,200 --> 00:02:59,800
to what the Russians were saying during the Cold War.

63
00:02:59,800 --> 00:03:01,920
And we've got a fun video here,

64
00:03:01,920 --> 00:03:05,300
[NOISE] which shows the state of machine translation in 1954.

65
00:03:05,300 --> 00:03:08,910
[MUSIC] They haven't reckoned with

66
00:03:08,910 --> 00:03:13,185
ambiguity when they set out to use computers to translate languages.

67
00:03:13,185 --> 00:03:16,395
A $500,000 simple calculator,

68
00:03:16,395 --> 00:03:18,600
most versatile electronic brain known,

69
00:03:18,600 --> 00:03:20,850
translates Russian into English.

70
00:03:20,850 --> 00:03:22,815
Instead of mathematical wizardry,

71
00:03:22,815 --> 00:03:24,560
a sentence in Russian is to be fed [OVERLAPPING].

72
00:03:24,560 --> 00:03:27,680
One of the first non-numerical applications of computers,

73
00:03:27,680 --> 00:03:29,480
[BACKGROUND] it was hyped as the solution to

74
00:03:29,480 --> 00:03:33,380
the Cold War obsession of keeping tabs on what the Russians were doing.

75
00:03:33,380 --> 00:03:37,280
Claims were made that the computer would replace most human translators.

76
00:03:37,280 --> 00:03:40,380
[inaudible] you're just in the experimental stage.

77
00:03:40,380 --> 00:03:42,120
When you go in for full-scale production,

78
00:03:42,120 --> 00:03:43,410
what will the capacity be?

79
00:03:43,410 --> 00:03:46,456
We should be able to do about,

80
00:03:46,456 --> 00:03:49,860
with the help of a commercial computer, uh, about one to two million words, uh,

81
00:03:49,860 --> 00:03:53,235
an hour, and this will be quite an adequate speed to cope with

82
00:03:53,235 --> 00:03:57,735
the whole alphabet of the Soviet Union in just a few hours' computer time a week.

83
00:03:57,735 --> 00:03:59,970
When do you have to be able to achieve this feat?

84
00:03:59,970 --> 00:04:01,785
If our experiments go well,

85
00:04:01,785 --> 00:04:04,810
then perhaps within, uh, five years or so.

86
00:04:05,210 --> 00:04:08,580
So in this video, I think there's a number of interesting things.

87
00:04:08,580 --> 00:04:11,665
Uh, firstly, we can see an example of about how,

88
00:04:11,665 --> 00:04:13,540
uh, AI hype is nothing new.

89
00:04:13,540 --> 00:04:15,880
Even in 1954, [NOISE] they were talking

90
00:04:15,880 --> 00:04:19,525
this machine translation system as if it was an electronic brain,

91
00:04:19,525 --> 00:04:22,495
which I think, uh, overstates maybe how general it is.

92
00:04:22,495 --> 00:04:24,130
Uh, they were also, at least some of them,

93
00:04:24,130 --> 00:04:26,995
fairly optimistic that this [NOISE] machine translation system

94
00:04:26,995 --> 00:04:30,550
was going to be replacing humans, uh, anytime soon.

95
00:04:30,550 --> 00:04:33,670
Um, so yeah, that's, that's pretty interesting.

96
00:04:33,670 --> 00:04:38,125
And, um, [NOISE] the thing is that these systems actually were mostly rule-based, uh,

97
00:04:38,125 --> 00:04:40,735
by which I mean that they were mostly using

98
00:04:40,735 --> 00:04:43,270
a bilingual dictionary between Russian and English,

99
00:04:43,270 --> 00:04:46,440
and they were essentially mostly just looking up the Russian words, uh,

100
00:04:46,440 --> 00:04:47,939
looking up their English counterparts,

101
00:04:47,939 --> 00:04:51,540
and they were storing these big bilingual dictionaries on these large magnetic tapes.

102
00:04:51,540 --> 00:04:55,390
Um, so certainly, it was a [NOISE] huge technical feat at the time, uh,

103
00:04:55,390 --> 00:04:57,110
but they, uh, some people were probably too

104
00:04:57,110 --> 00:05:00,065
optimistic [NOISE] about how quickly it would replace humans.

105
00:05:00,065 --> 00:05:03,230
So jumping forward several decades in time,

106
00:05:03,230 --> 00:05:06,320
uh, now I want to tell you about statistical machine translation.

107
00:05:06,320 --> 00:05:10,070
So the core idea of statistical machine translation is that you're going to

108
00:05:10,070 --> 00:05:14,335
learn a probabilistic model from the data in order to do the translation.

109
00:05:14,335 --> 00:05:16,800
So as an example, uh, as before,

110
00:05:16,800 --> 00:05:19,295
suppose that we're translating from French to English.

111
00:05:19,295 --> 00:05:23,225
The idea is that you want to find the best English sentence y,

112
00:05:23,225 --> 00:05:24,860
given the French sentence x,

113
00:05:24,860 --> 00:05:26,270
[NOISE] and, uh, mathematically,

114
00:05:26,270 --> 00:05:31,640
you can formulate this as finding argmax y of this conditional probability of y, given x,

115
00:05:31,640 --> 00:05:33,605
[NOISE] and the model that you're learning is

116
00:05:33,605 --> 00:05:38,010
this probability distribution P. [NOISE] So what we usually do is,

117
00:05:38,010 --> 00:05:40,640
we break down this probability into,

118
00:05:40,640 --> 00:05:43,190
uh, its two components using Bayes' Rule.

119
00:05:43,190 --> 00:05:46,995
[NOISE] So this means that finding the y that maximizes,

120
00:05:46,995 --> 00:05:48,585
uh, probability of y, given x,

121
00:05:48,585 --> 00:05:52,340
is equivalent to finding the y that maximizes the probability of x,

122
00:05:52,340 --> 00:05:55,255
given y, times the probability of y.

123
00:05:55,255 --> 00:05:57,900
So the two components here, on the left,

124
00:05:57,900 --> 00:05:59,475
we have a translation model,

125
00:05:59,475 --> 00:06:03,605
and this is keeping track of how words and phrases should be translated.

126
00:06:03,605 --> 00:06:06,560
Uh, so the idea is that it knows, uh, how, uh,

127
00:06:06,560 --> 00:06:10,880
French words and an English word might be translated to each other or maybe small,

128
00:06:10,880 --> 00:06:13,610
small phrases and chunks of words should be translated.

129
00:06:13,610 --> 00:06:15,985
And this is learned from a lot of parallel data,

130
00:06:15,985 --> 00:06:17,780
and I'll be telling you later how we do that.

131
00:06:17,780 --> 00:06:20,270
The second compo- component P(y),

132
00:06:20,270 --> 00:06:22,490
[NOISE] this is just a language model.

133
00:06:22,490 --> 00:06:23,990
[NOISE] We learned about this last week.

134
00:06:23,990 --> 00:06:26,890
A language model is a system that can predict the next word,

135
00:06:26,890 --> 00:06:29,360
but it can also be thought of as a system [NOISE] that tells

136
00:06:29,360 --> 00:06:32,000
you the probability of a sequence of words.

137
00:06:32,000 --> 00:06:34,595
So here, if we're translating from French to English,

138
00:06:34,595 --> 00:06:36,470
P(y) is an English language model.

139
00:06:36,470 --> 00:06:38,560
[NOISE] So the idea is the,

140
00:06:38,560 --> 00:06:40,355
the reason why we want to break down

141
00:06:40,355 --> 00:06:44,870
this single conditiona- conditional probability distribution into the,

142
00:06:44,870 --> 00:06:49,490
the pr- product of two different ones is that this is a kind of division of labor.

143
00:06:49,490 --> 00:06:51,560
The idea is that instead of, uh,

144
00:06:51,560 --> 00:06:55,745
a single conditional probability distribution needing to understand how to translate,

145
00:06:55,745 --> 00:06:57,529
and how to write good English text,

146
00:06:57,529 --> 00:06:58,970
and understand sentence structure,

147
00:06:58,970 --> 00:07:02,060
and everything at once, the idea is that you separate it so that [NOISE]

148
00:07:02,060 --> 00:07:05,735
the translation model on the left in blue mostly just knows about a

149
00:07:05,735 --> 00:07:09,170
local translation of small chunks of words and phrases,

150
00:07:09,170 --> 00:07:12,950
whereas the language model on the right more takes care of writing good English,

151
00:07:12,950 --> 00:07:15,110
good sentence structure, word order, and so on.

152
00:07:15,110 --> 00:07:17,465
[NOISE] So you already know

153
00:07:17,465 --> 00:07:20,240
how to learn a language model [NOISE] because we learned about that last time.

154
00:07:20,240 --> 00:07:21,990
You just need lots of monolingual data,

155
00:07:21,990 --> 00:07:23,240
in this case, English data.

156
00:07:23,240 --> 00:07:25,520
[NOISE] So I'm going to tell you more about how we would learn

157
00:07:25,520 --> 00:07:28,610
this translation model that needs to be learned from parallel data.

158
00:07:28,610 --> 00:07:30,860
[NOISE]

159
00:07:30,860 --> 00:07:36,565
So we need a large amount of parallel data in order to learn this translation model.

160
00:07:36,565 --> 00:07:39,700
And an early example of a parallel corpus,

161
00:07:39,700 --> 00:07:41,260
is the Rosetta Stone.

162
00:07:41,260 --> 00:07:46,045
So this is a stone that has the same text written in three different languages.

163
00:07:46,045 --> 00:07:48,985
And this is a hugely important artifact

164
00:07:48,985 --> 00:07:53,590
for the people who were trying to understand ancient Egyptian.

165
00:07:53,590 --> 00:07:55,060
So in the 19th century,

166
00:07:55,060 --> 00:07:56,680
uh, scholars discovered this stone,

167
00:07:56,680 --> 00:07:59,590
and it helped them to figure out ancient Egyptian because there was

168
00:07:59,590 --> 00:08:04,340
this parallel text that had the the same text in other languages that they did know.

169
00:08:04,340 --> 00:08:07,605
So this is, this is a really important parallel corpus,

170
00:08:07,605 --> 00:08:09,210
and if you're ever in London,

171
00:08:09,210 --> 00:08:10,410
you can go to the British Museum,

172
00:08:10,410 --> 00:08:12,320
and see this in person.

173
00:08:12,320 --> 00:08:15,235
So the idea is that you get your parallel data.

174
00:08:15,235 --> 00:08:17,200
Obviously, you need a larger amount that is on the stone,

175
00:08:17,200 --> 00:08:19,615
and hopefully it shouldn't be written on a stone either.

176
00:08:19,615 --> 00:08:25,105
But you can use this to learn your statistical machine translation model.

177
00:08:25,105 --> 00:08:27,430
So the idea is that, you are trying to learn

178
00:08:27,430 --> 00:08:30,550
this conditional probability distribution of x given y.

179
00:08:30,550 --> 00:08:33,520
So what we do is we actually break this down even further.

180
00:08:33,520 --> 00:08:37,795
We actually want to consider the probability of x and a given y.

181
00:08:37,795 --> 00:08:39,685
Where a is the alignment.

182
00:08:39,685 --> 00:08:41,125
So the idea of alignment,

183
00:08:41,125 --> 00:08:43,300
is this is how the words in

184
00:08:43,300 --> 00:08:47,110
the English sentence and the French sentence correspond to each other.

185
00:08:47,110 --> 00:08:50,690
So I'm gonna, uh, demonstrate this by an example.

186
00:08:50,690 --> 00:08:53,190
So in this example,

187
00:08:53,190 --> 00:08:57,315
while we're translating the sentence 'Japan shaken by two new quakes' to French.

188
00:08:57,315 --> 00:09:01,080
Then you can see there is a pretty simple one-to-one alignment here,

189
00:09:01,080 --> 00:09:02,595
uh, of English words to French words,

190
00:09:02,595 --> 00:09:05,150
and also they appear in the exact same order.

191
00:09:05,150 --> 00:09:10,510
The only thing that doesn't conform to that is the word 'Le' in French,

192
00:09:10,510 --> 00:09:12,490
which we call a spurious word because it doesn't

193
00:09:12,490 --> 00:09:15,205
have a direct counterpart in the English sentence,

194
00:09:15,205 --> 00:09:16,630
and that's because in English we just say,

195
00:09:16,630 --> 00:09:19,670
'Japan', but in French we say, 'Le Japon'.

196
00:09:19,680 --> 00:09:22,915
So alignment can be a bit more complicated than that.

197
00:09:22,915 --> 00:09:25,480
For example, alignment can be many-to-one.

198
00:09:25,480 --> 00:09:28,150
In this example, you have, uh,

199
00:09:28,150 --> 00:09:32,740
several French words that have multiple English words that correspond to them.

200
00:09:32,740 --> 00:09:35,050
So this is what we call many-to-one alignment.

201
00:09:35,050 --> 00:09:38,455
Uh, it can go in the other direction too.

202
00:09:38,455 --> 00:09:40,075
Alignment can be one-to-many.

203
00:09:40,075 --> 00:09:43,015
So here we have a single English word implemented,

204
00:09:43,015 --> 00:09:44,950
which has a one-to-many alignment because there is

205
00:09:44,950 --> 00:09:48,355
a three-word French phra-phrase that corresponds to it.

206
00:09:48,355 --> 00:09:49,900
So on the left and the right,

207
00:09:49,900 --> 00:09:52,390
we have two ways of depicting the same alignments.

208
00:09:52,390 --> 00:09:57,410
It's either a kind of chart or it can be a, a graph.

209
00:09:58,020 --> 00:10:01,345
So here's another example, um,

210
00:10:01,345 --> 00:10:04,960
of a one-to-many, well, sorry, right.

211
00:10:04,960 --> 00:10:08,200
So we call, uh, this word implemented, that is one-to-many.

212
00:10:08,200 --> 00:10:12,565
We call it a fertile word because the idea is that it has many children in the,

213
00:10:12,565 --> 00:10:14,095
in the target sentence.

214
00:10:14,095 --> 00:10:17,065
So in fact, there are some words which are very fertile.

215
00:10:17,065 --> 00:10:19,690
Here's an example where the source sentence,

216
00:10:19,690 --> 00:10:22,060
'il m'a entarte', means,

217
00:10:22,060 --> 00:10:23,455
'he hit me with a pie',

218
00:10:23,455 --> 00:10:25,090
and here in French,

219
00:10:25,090 --> 00:10:28,630
this verb, 'entarte' means, uh, to hit someone with a pie,

220
00:10:28,630 --> 00:10:29,020
[LAUGHTER]

221
00:10:29,020 --> 00:10:35,845
and this word has no single word equivalent in English.

222
00:10:35,845 --> 00:10:38,560
We don't have a single verb that means to hit someone with a pie.

223
00:10:38,560 --> 00:10:39,910
[LAUGHTER]

224
00:10:39,910 --> 00:10:41,695
Which I think that is really fun, that French has a word.

225
00:10:41,695 --> 00:10:43,270
You wonder, maybe they do it so

226
00:10:43,270 --> 00:10:45,100
often that they need a single word for that. I don't know.

227
00:10:45,100 --> 00:10:46,750
[LAUGHTER]

228
00:10:46,750 --> 00:10:48,790
So this is an example of a fertile word, right?

229
00:10:48,790 --> 00:10:53,360
Because it needs to have several corresponding English words to translate it.

230
00:10:54,000 --> 00:10:57,400
So we can have one-to-many, and many-to-one.

231
00:10:57,400 --> 00:10:59,650
You can also have many-to-many alignments.

232
00:10:59,650 --> 00:11:03,085
You could call that kind of phrase level translation, or phrase-to-phrase.

233
00:11:03,085 --> 00:11:06,100
So here, uh, the English sentence says,

234
00:11:06,100 --> 00:11:08,035
'The poor doesn't- don't have any money',

235
00:11:08,035 --> 00:11:11,350
and here don't have any money corresponds to the French phrase,

236
00:11:11,350 --> 00:11:14,860
'sont demunis', and this is a many-to-many alignment because there is

237
00:11:14,860 --> 00:11:18,685
no obvious way to break down this phrase-to-phrase alignment into,

238
00:11:18,685 --> 00:11:21,770
um, smaller word-to-word alignments.

239
00:11:22,860 --> 00:11:25,495
Okay. So that's what alignment is.

240
00:11:25,495 --> 00:11:27,880
And if you remember, we were thinking about how would you

241
00:11:27,880 --> 00:11:30,730
learn this probability distribution of what the alignment is,

242
00:11:30,730 --> 00:11:33,940
uh, in order to do statistical machine translation.

243
00:11:33,940 --> 00:11:37,180
So the idea is that you learn probability of x and a,

244
00:11:37,180 --> 00:11:41,215
given y as a combination of many factors or many features.

245
00:11:41,215 --> 00:11:43,359
So you consider for example,

246
00:11:43,359 --> 00:11:47,140
what's the probability of a particular word aligning to another particular word?

247
00:11:47,140 --> 00:11:48,940
Like you know, this English word and this French word,

248
00:11:48,940 --> 00:11:50,065
how often do they align?

249
00:11:50,065 --> 00:11:52,090
But then, this also depends on for example,

250
00:11:52,090 --> 00:11:53,740
what's their position in the sentence?

251
00:11:53,740 --> 00:11:56,739
Like if they both appear near the end of the sentences,

252
00:11:56,739 --> 00:11:58,810
then it's more likely that they align, whereas,

253
00:11:58,810 --> 00:12:01,855
if one's at the beginning and one's at the end, that's less likely.

254
00:12:01,855 --> 00:12:04,240
You would also consider things like, uh,

255
00:12:04,240 --> 00:12:08,080
what's the probability of this particular French word having this particular fertility?

256
00:12:08,080 --> 00:12:10,630
Like, what's the probability of this word having

257
00:12:10,630 --> 00:12:13,475
three corresponding English words and so on?

258
00:12:13,475 --> 00:12:17,070
So all of these statistics are learned from your parallel data,

259
00:12:17,070 --> 00:12:20,240
and there's many other things that you would take into consideration.

260
00:12:20,240 --> 00:12:23,890
So we're looking at a kind of overview of statistical machine translation today.

261
00:12:23,890 --> 00:12:26,020
You're not going to understand it in full detail,

262
00:12:26,020 --> 00:12:28,150
but we're understanding an overview of how it works,

263
00:12:28,150 --> 00:12:29,725
because we're going to be, uh,

264
00:12:29,725 --> 00:12:32,780
comparing it to neural machine translation.

265
00:12:33,510 --> 00:12:37,660
Okay. So we're learning this SMT system,

266
00:12:37,660 --> 00:12:40,825
and so far, we've broken it down into these two main components.

267
00:12:40,825 --> 00:12:42,295
We've got the translation model,

268
00:12:42,295 --> 00:12:43,900
and we've got the language model,

269
00:12:43,900 --> 00:12:46,690
and we understand a little bit about how you might

270
00:12:46,690 --> 00:12:50,365
learn this translation model by breaking it down into alignments.

271
00:12:50,365 --> 00:12:54,040
So the question remains, how do you do the argmax over y?

272
00:12:54,040 --> 00:12:59,560
How do you find your French sentence y that maximizes this probability?

273
00:12:59,560 --> 00:13:03,340
So one kind of brute force solution is you could say,

274
00:13:03,340 --> 00:13:05,830
"'let's enumerate every possible y."

275
00:13:05,830 --> 00:13:08,305
That's kind of every possible sequence of French words,

276
00:13:08,305 --> 00:13:10,270
maybe up to some length, and, uh,

277
00:13:10,270 --> 00:13:12,460
we'll calculate this probability for all of them,

278
00:13:12,460 --> 00:13:14,980
and it should be pretty clear that that is just a no go.

279
00:13:14,980 --> 00:13:16,405
That's way too expensive,

280
00:13:16,405 --> 00:13:19,525
and we're not going to be able to get anywhere with that.

281
00:13:19,525 --> 00:13:22,900
So the answer for how you actually do this in practice is,

282
00:13:22,900 --> 00:13:25,690
you are going to use some kind of heuristic search algorithm,

283
00:13:25,690 --> 00:13:28,150
to search for the best translation, y.

284
00:13:28,150 --> 00:13:33,115
Uh, but along the way, you're going to discard hypotheses that are too low probability.

285
00:13:33,115 --> 00:13:34,555
So you're gonna search,

286
00:13:34,555 --> 00:13:35,785
you're going to discard,

287
00:13:35,785 --> 00:13:37,990
and prune the trees as you go to make sure that you're not

288
00:13:37,990 --> 00:13:41,540
keeping too many hypotheses, uh, on each step.

289
00:13:42,090 --> 00:13:47,695
So this process of finding your best sequence is also called decoding.

290
00:13:47,695 --> 00:13:50,980
So here is an overview of how that works for SMT.

291
00:13:50,980 --> 00:13:55,750
This an example where you have this German sentence that translates to,

292
00:13:55,750 --> 00:13:57,565
'He does not go home',

293
00:13:57,565 --> 00:14:02,095
and you can see that there is some kind of phrase-to-phrase alignment here.

294
00:14:02,095 --> 00:14:06,550
So, uh, an overview of how this decoding would work in SMT,

295
00:14:06,550 --> 00:14:10,270
is that you kind of consider lots of different hypotheses,

296
00:14:10,270 --> 00:14:13,795
for how you might translate these individual words, uh,

297
00:14:13,795 --> 00:14:18,430
and then you build it up to consider how you might translate,

298
00:14:18,430 --> 00:14:21,235
uh, individual phrases, and the phrases get bigger.

299
00:14:21,235 --> 00:14:23,740
So for example, you can see that on the top right,

300
00:14:23,740 --> 00:14:25,855
if it's not too small, you can see that the, uh,

301
00:14:25,855 --> 00:14:27,415
the German word for house, uh,

302
00:14:27,415 --> 00:14:29,365
could be translated into the English word,

303
00:14:29,365 --> 00:14:32,110
'house' or 'home', or 'chamber', and so on.

304
00:14:32,110 --> 00:14:34,690
Uh, so we consider all of these different hypotheses,

305
00:14:34,690 --> 00:14:37,210
and look into how we might put those together to

306
00:14:37,210 --> 00:14:40,390
translate phrases but you don't keep all of them all the time.

307
00:14:40,390 --> 00:14:43,180
You get rid of the ones that are too low probability.

308
00:14:43,180 --> 00:14:46,225
So this can also be depicted as a kind of a tree,

309
00:14:46,225 --> 00:14:49,000
where you are exploring different options.

310
00:14:49,000 --> 00:14:51,265
You are searching through the space of options,

311
00:14:51,265 --> 00:14:53,125
but then you prune the tree as you go.

312
00:14:53,125 --> 00:14:54,790
So I know this is a very,

313
00:14:54,790 --> 00:14:56,005
very high level, uh,

314
00:14:56,005 --> 00:14:57,580
description of how decoding might walk.

315
00:14:57,580 --> 00:14:59,080
And in fact, later in this lecture,

316
00:14:59,080 --> 00:15:02,150
you're going to see a detailed explanation

317
00:15:02,150 --> 00:15:05,970
of how this kind of decoding works for neural machine translation.

318
00:15:07,320 --> 00:15:10,300
Okay. So what's our, um,

319
00:15:10,300 --> 00:15:12,460
overview of statistical machine translation,

320
00:15:12,460 --> 00:15:14,080
uh, was it effective?

321
00:15:14,080 --> 00:15:17,080
Uh, so SMT was a huge research field,

322
00:15:17,080 --> 00:15:20,425
uh, from the 1990s to about, maybe, uh, 2013.

323
00:15:20,425 --> 00:15:23,800
And the best systems during this time were extremely complex.

324
00:15:23,800 --> 00:15:26,725
They were extremely sophisticated and impressive systems and, uh,

325
00:15:26,725 --> 00:15:30,130
SMT made the best machine translation systems in the world.

326
00:15:30,130 --> 00:15:31,840
But they were very complex.

327
00:15:31,840 --> 00:15:32,980
So for example, you know,

328
00:15:32,980 --> 00:15:36,610
there were hundreds of important details that we haven't mentioned here at all.

329
00:15:36,610 --> 00:15:38,740
There were many, many techniques to make it, uh,

330
00:15:38,740 --> 00:15:40,435
more complex and more,

331
00:15:40,435 --> 00:15:43,315
um, sophisticated than what I've described today.

332
00:15:43,315 --> 00:15:48,565
In particular, the systems had to have many separately designed, uh, sub-components.

333
00:15:48,565 --> 00:15:50,035
So we already saw how you, uh,

334
00:15:50,035 --> 00:15:53,155
break down the translation model into two separate parts.

335
00:15:53,155 --> 00:15:55,645
Uh, but there was, you know, many more sub-components than that,

336
00:15:55,645 --> 00:15:57,970
and often they had to be learned separately.

337
00:15:57,970 --> 00:16:01,795
This meant the engineers had to do a lot of feature engineering.

338
00:16:01,795 --> 00:16:03,730
Uh, you have to design features to capture

339
00:16:03,730 --> 00:16:07,225
the particular language phenomena that you were interested in.

340
00:16:07,225 --> 00:16:10,330
So this meant that they had to require a lot of

341
00:16:10,330 --> 00:16:12,865
compiling and maintaining of extra resources,

342
00:16:12,865 --> 00:16:14,455
and in fact, you had to have, uh,

343
00:16:14,455 --> 00:16:16,495
different resources for different languages.

344
00:16:16,495 --> 00:16:19,900
So the work kind of multiplied the more languages you had.

345
00:16:19,900 --> 00:16:22,120
An example of this, is you had to have,

346
00:16:22,120 --> 00:16:23,920
uh, tables of equivalent phrases.

347
00:16:23,920 --> 00:16:27,175
So for example if you're doing French and English translation, then, uh,

348
00:16:27,175 --> 00:16:30,055
they would be collecting these phrases of, uh,

349
00:16:30,055 --> 00:16:32,590
sorry these tables of phrases that they considered similar,

350
00:16:32,590 --> 00:16:33,925
and those were learned from the data.

351
00:16:33,925 --> 00:16:37,360
But this was a lot of information that had to be stored and maintained.

352
00:16:37,360 --> 00:16:40,930
So overall, this was just a lot of human effort to maintain.

353
00:16:40,930 --> 00:16:42,550
Uh, and again, yes,

354
00:16:42,550 --> 00:16:45,130
you had to put more human effort in if you wanted to

355
00:16:45,130 --> 00:16:48,490
learn an SMT system for a new language pair.

356
00:16:48,490 --> 00:16:51,550
Okay, are there any questions here about, uh, SMT?

357
00:16:51,550 --> 00:16:59,290
[NOISE] Okay.

358
00:16:59,290 --> 00:17:01,480
Uh, so moving on, that's SMT.

359
00:17:01,480 --> 00:17:02,815
[NOISE] Now, we're gonna move on to,

360
00:17:02,815 --> 00:17:04,660
uh, section two of this lecture.

361
00:17:04,660 --> 00:17:10,030
So I want to take you back to the year 2014,

362
00:17:10,030 --> 00:17:12,580
for a dramatic re-enactment of what happened in

363
00:17:12,580 --> 00:17:15,415
the world of machine translation research.

364
00:17:15,415 --> 00:17:17,980
So in 2014, something very dramatic happened and

365
00:17:17,980 --> 00:17:20,800
that thing that happened is called neural machine translation,

366
00:17:20,800 --> 00:17:22,810
and [LAUGHTER] I think it looks a little

367
00:17:22,810 --> 00:17:25,855
bit like this [NOISE] if I'm not being too dramatic.

368
00:17:25,855 --> 00:17:28,825
So what is neural machine translation?

369
00:17:28,825 --> 00:17:31,450
The idea is that NMT is a way to do

370
00:17:31,450 --> 00:17:35,260
machine translation but using just a single neural network.

371
00:17:35,260 --> 00:17:38,115
[NOISE] And the neural network architecture that they use

372
00:17:38,115 --> 00:17:41,220
is called Sequence-to-Sequence or sometimes just called seq2seq,

373
00:17:41,220 --> 00:17:44,210
uh, and involves two RNNs.

374
00:17:44,210 --> 00:17:46,180
So, uh, it's called sequence-to-sequence,

375
00:17:46,180 --> 00:17:48,385
because you're mapping one sequence to the other.

376
00:17:48,385 --> 00:17:52,000
The source sentence [NOISE] to the target sentence and you need two RNNs,

377
00:17:52,000 --> 00:17:54,685
basically to handle those two different sentences.

378
00:17:54,685 --> 00:17:59,110
All right, lets look at the diagram to see what sequences-to-sequence is in detail.

379
00:17:59,110 --> 00:18:01,690
So we start off with our source sentence,

380
00:18:01,690 --> 00:18:03,400
and we're gonna use our example from before

381
00:18:03,400 --> 00:18:07,300
il a m'entarte, which means, he hit me with a pie.

382
00:18:07,300 --> 00:18:10,810
So we, uh, feed this into our encoder RNN,

383
00:18:10,810 --> 00:18:12,835
and this is as you've seen before,

384
00:18:12,835 --> 00:18:15,700
I've drawn a uni-directional RNN,

385
00:18:15,700 --> 00:18:17,275
but this could be bi-directional.

386
00:18:17,275 --> 00:18:19,045
It also could be multi-layer.

387
00:18:19,045 --> 00:18:22,675
It could be vanilla, or it could be LSTM, and so on.

388
00:18:22,675 --> 00:18:25,570
Uh, another thing to note is that [NOISE] we are

389
00:18:25,570 --> 00:18:28,240
passing word embeddings into this encoder RNN,

390
00:18:28,240 --> 00:18:31,000
but I'm just not explicitly depicting that step.

391
00:18:31,000 --> 00:18:33,280
[NOISE] Okay.

392
00:18:33,280 --> 00:18:36,430
So the idea of the encoder RNN is that it's going to

393
00:18:36,430 --> 00:18:39,820
produce some kind of encoding of this source sentence.

394
00:18:39,820 --> 00:18:43,690
So for now, let's assume that the encoding of the source sentence is going to be,

395
00:18:43,690 --> 00:18:47,305
uh, the final hidden state of this encoder RNN.

396
00:18:47,305 --> 00:18:51,700
So what happens next is we pass this encoding of the source sentence.

397
00:18:51,700 --> 00:18:54,055
We pass it over to the decoder RNN,

398
00:18:54,055 --> 00:18:56,635
which is going to translate into English.

399
00:18:56,635 --> 00:18:59,635
So the decoder RNN is a language model.

400
00:18:59,635 --> 00:19:01,795
In particular, it's a conditional language model,

401
00:19:01,795 --> 00:19:03,340
like we talked about last time.

402
00:19:03,340 --> 00:19:06,430
So it's conditional because it's going to produce the target sentence,

403
00:19:06,430 --> 00:19:08,425
but conditioned on this encoding,

404
00:19:08,425 --> 00:19:12,445
and the encoding is that vector that has the orange box around it.

405
00:19:12,445 --> 00:19:15,805
So how does this work? Uh, we start off by feeding, uh,

406
00:19:15,805 --> 00:19:20,500
the start token into the decoder, and then, uh,

407
00:19:20,500 --> 00:19:22,765
we can get the first state of the decoder,

408
00:19:22,765 --> 00:19:24,025
because we're using, uh,

409
00:19:24,025 --> 00:19:28,745
the encoding of the source sentence as the initial hidden state for the decoder.

410
00:19:28,745 --> 00:19:31,800
So then we get our first output from the decoder,

411
00:19:31,800 --> 00:19:34,725
which is a probability distribution of what word might come next,

412
00:19:34,725 --> 00:19:37,140
and let's suppose that we take the argmax over that,

413
00:19:37,140 --> 00:19:39,420
and then that gets us the word, uh, he.

414
00:19:39,420 --> 00:19:41,070
Which is in this case is correct,

415
00:19:41,070 --> 00:19:43,355
because that's probably the word you should start with.

416
00:19:43,355 --> 00:19:45,280
Okay, so then we just take the word,

417
00:19:45,280 --> 00:19:48,730
he and then we feed it back into the decoder on the next step,

418
00:19:48,730 --> 00:19:50,740
and then we do the same thing again.

419
00:19:50,740 --> 00:19:53,965
We take argmax and we get a new word and we get he hit.

420
00:19:53,965 --> 00:19:58,750
So the idea here is you can co- uh, continue doing this operation and in that way,

421
00:19:58,750 --> 00:20:00,220
you're going to generate, uh,

422
00:20:00,220 --> 00:20:02,335
your target sentence, uh,

423
00:20:02,335 --> 00:20:05,215
which will be something like he hit me with a pie,

424
00:20:05,215 --> 00:20:09,620
and you stop once your decoder produces the end token.

425
00:20:09,780 --> 00:20:12,715
So an important thing to note here,

426
00:20:12,715 --> 00:20:16,510
is that this picture is showing you what happens at test time.

427
00:20:16,510 --> 00:20:18,610
This shows you how to generate text.

428
00:20:18,610 --> 00:20:20,365
Uh, this isn't what happens during training.

429
00:20:20,365 --> 00:20:21,955
I'll show you what happens [NOISE] during training later.

430
00:20:21,955 --> 00:20:23,635
Uh, but this thing with the,

431
00:20:23,635 --> 00:20:26,005
the pink dotted arrows where you feed the word back in.

432
00:20:26,005 --> 00:20:29,275
This is what you do to generate text at test time.

433
00:20:29,275 --> 00:20:35,365
Any questions on this? Uh, oh,

434
00:20:35,365 --> 00:20:39,940
another thing I should note is that you need two separate sets of word embeddings, right?

435
00:20:39,940 --> 00:20:43,360
You need word embeddings for French words, and you need English word embeddings,

436
00:20:43,360 --> 00:20:44,680
so that's kind of two separate sets,

437
00:20:44,680 --> 00:20:48,925
two separate vocabularies. Um, yeah.

438
00:20:48,925 --> 00:20:51,790
Okay. So as a side note, uh,

439
00:20:51,790 --> 00:20:54,595
this architecture called sequence-to-sequence is actually pretty versatile.

440
00:20:54,595 --> 00:20:56,800
It's not just a machine translation architecture.

441
00:20:56,800 --> 00:20:59,410
Uh, you can, uh, uh,

442
00:20:59,410 --> 00:21:03,535
phrase quite a few NLP tasks as sequence-to-sequence tasks.

443
00:21:03,535 --> 00:21:05,590
Uh, so for example a summarization is

444
00:21:05,590 --> 00:21:10,120
a sequence-to-sequence task because in goes your long text and out comes your short text.

445
00:21:10,120 --> 00:21:12,640
Uh, dialogue can [NOISE] be seq2seq because in

446
00:21:12,640 --> 00:21:15,730
goes the previous utterance and out comes your next utterance, uh,

447
00:21:15,730 --> 00:21:19,345
parsing can even be thought of as a sequence-to-sequence task,

448
00:21:19,345 --> 00:21:21,910
because you could say in goes the input text and

449
00:21:21,910 --> 00:21:24,730
the output parse is going to be expressed as a sequence.

450
00:21:24,730 --> 00:21:28,030
This might not be the best way to do parsing but it is a way you can try.

451
00:21:28,030 --> 00:21:31,480
Lastly, you could even do something like code generation.

452
00:21:31,480 --> 00:21:34,150
So suppose you want to build a system that takes some kind of,

453
00:21:34,150 --> 00:21:35,440
uh, natural language input,

454
00:21:35,440 --> 00:21:39,520
such as sum up the numbers from 1-10 and then it outputs,

455
00:21:39,520 --> 00:21:41,665
let's say some Python code that says,

456
00:21:41,665 --> 00:21:44,920
sum open brackets range 10 or something like that.

457
00:21:44,920 --> 00:21:47,140
Uh, so if you wanted to train,

458
00:21:47,140 --> 00:21:48,880
um, an assistant to do this.

459
00:21:48,880 --> 00:21:51,655
You could in a way view that as a translation task,

460
00:21:51,655 --> 00:21:53,740
where you're translating from English to Python.

461
00:21:53,740 --> 00:21:55,960
It's a pretty challenging translation task.

462
00:21:55,960 --> 00:21:58,510
It probably requires a lot more logic than just uh, you know,

463
00:21:58,510 --> 00:22:01,390
French to English [NOISE] but you can try and people have tried.

464
00:22:01,390 --> 00:22:05,930
There are research papers where people have used seq2seq to do this kind of task.

465
00:22:09,510 --> 00:22:12,370
Okay. So to recap, uh,

466
00:22:12,370 --> 00:22:14,950
seq2seq is an example of a conditional language model.

467
00:22:14,950 --> 00:22:17,695
Uh, it's a language model because the decoder

468
00:22:17,695 --> 00:22:20,905
is a language model that's predicting the next target words.

469
00:22:20,905 --> 00:22:24,280
But it's a conditional language model because it's also conditioning on

470
00:22:24,280 --> 00:22:29,390
your source sentence which is represented by the encoding of the source sentence.

471
00:22:30,150 --> 00:22:32,305
So you could look,

472
00:22:32,305 --> 00:22:33,700
you could view it like this.

473
00:22:33,700 --> 00:22:36,700
NMT is directly calculating the probability

474
00:22:36,700 --> 00:22:39,835
of the target sentence y given the source sentence x.

475
00:22:39,835 --> 00:22:42,445
So if you look at this, you see that this is just, uh,

476
00:22:42,445 --> 00:22:45,085
breaking down the probability of the sequence y,

477
00:22:45,085 --> 00:22:46,450
which we suppose is of length, uh,

478
00:22:46,450 --> 00:22:50,110
capital T. You can break it down into the being the probability of

479
00:22:50,110 --> 00:22:54,280
the first word of y given x and then the probability of the second word of y given,

480
00:22:54,280 --> 00:22:56,920
uh, the words that came before, and x, and so on.

481
00:22:56,920 --> 00:23:00,370
So in fact, you can see that each of the terms in this product on the right,

482
00:23:00,370 --> 00:23:02,710
those are probabilities of the next target word

483
00:23:02,710 --> 00:23:05,365
given all the ones so far, and also the source sentence,

484
00:23:05,365 --> 00:23:09,670
and that's exactly the conditional probability that your language model produces.

485
00:23:09,670 --> 00:23:14,020
So the reason I'm highlighting this is because if you remember in SMT, uh,

486
00:23:14,020 --> 00:23:18,190
we didn't directly learn the translation model p of y given x,

487
00:23:18,190 --> 00:23:19,765
we broke it down into,

488
00:23:19,765 --> 00:23:22,210
uh, uh, smaller components.

489
00:23:22,210 --> 00:23:23,995
Whereas here in NMT,

490
00:23:23,995 --> 00:23:26,440
we are directly learning this model.

491
00:23:26,440 --> 00:23:29,335
And this is in some ways an advantage because it's simpler to do.

492
00:23:29,335 --> 00:23:32,545
You don't have to learn all of these different systems and optimize them separately.

493
00:23:32,545 --> 00:23:35,720
It's, uh, kind of, simpler and easier.

494
00:23:36,660 --> 00:23:38,710
So, uh, this is,

495
00:23:38,710 --> 00:23:40,150
this is the model that we're learning.

496
00:23:40,150 --> 00:23:43,165
Uh, the question is, how do we train this NMT system?

497
00:23:43,165 --> 00:23:46,195
So hopefully, you should already have a good idea of how this would work,

498
00:23:46,195 --> 00:23:49,075
given that we've already seen how you would train a language model.

499
00:23:49,075 --> 00:23:50,830
But here are the details just in case.

500
00:23:50,830 --> 00:23:53,080
So you get your big, uh, parallel corpus.

501
00:23:53,080 --> 00:23:55,075
Uh, and then, uh,

502
00:23:55,075 --> 00:23:58,930
let's say you have your sentence pair from your parallel corpus.

503
00:23:58,930 --> 00:24:01,915
Uh, so this is what happens during training.

504
00:24:01,915 --> 00:24:05,005
Uh, you feed your source sentence into the encoder RNN, uh,

505
00:24:05,005 --> 00:24:09,475
and then you feed your target sentence into the decoder RNN,

506
00:24:09,475 --> 00:24:10,900
and you're going to pass over that

507
00:24:10,900 --> 00:24:14,005
final hidden state to be the initial hidden state of the decoder.

508
00:24:14,005 --> 00:24:18,445
And then, uh, for every step of the decoder RNN,

509
00:24:18,445 --> 00:24:20,065
you're going to produce the, uh,

510
00:24:20,065 --> 00:24:22,060
probability distribution of what comes next,

511
00:24:22,060 --> 00:24:23,905
which is the, the y hats.

512
00:24:23,905 --> 00:24:25,375
And then from those,

513
00:24:25,375 --> 00:24:26,875
you can compute your loss.

514
00:24:26,875 --> 00:24:29,410
And the loss is just the same as we saw for,

515
00:24:29,410 --> 00:24:31,165
u h, unconditional language models.

516
00:24:31,165 --> 00:24:33,580
It's, uh, the cross entropy or you could also

517
00:24:33,580 --> 00:24:37,330
say negative log-likelihood of the true next word.

518
00:24:37,330 --> 00:24:39,730
So for example, on those selected ones, uh,

519
00:24:39,730 --> 00:24:44,335
the loss is the negative log probability of the correct next word.

520
00:24:44,335 --> 00:24:47,680
And then as before, we're going to average all of

521
00:24:47,680 --> 00:24:50,995
these losses to get the total loss for the example.

522
00:24:50,995 --> 00:24:55,440
Uh, so a thing you might notice people saying in,

523
00:24:55,440 --> 00:24:58,770
for example, research papers is this phrase end-to-end.

524
00:24:58,770 --> 00:25:02,415
And this is an example of learning a system end-to-end.

525
00:25:02,415 --> 00:25:07,275
And what we mean by this is that the backpropagation is happening end-to-end, one end is,

526
00:25:07,275 --> 00:25:09,000
is losses, the loss functions,

527
00:25:09,000 --> 00:25:11,160
and the other end I guess is kind of like the,

528
00:25:11,160 --> 00:25:12,930
the beginning of the encoder RNN.

529
00:25:12,930 --> 00:25:14,310
The point is that you, uh,

530
00:25:14,310 --> 00:25:18,045
backpropagation, uh, flows throughout the entire system,

531
00:25:18,045 --> 00:25:23,280
and you learn the entire system with respect to this single, uh, loss. Yep?

532
00:25:23,280 --> 00:25:24,030
[inaudible]

533
00:25:24,030 --> 00:25:36,280
The question is,

534
00:25:36,280 --> 00:25:41,035
if the decoder RNN outputs the end token too early,

535
00:25:41,035 --> 00:25:43,840
then how can you measure the loss on,

536
00:25:43,840 --> 00:25:45,700
uh, the words that came after that?

537
00:25:45,700 --> 00:25:48,850
So this is the difference between training time and test time,

538
00:25:48,850 --> 00:25:50,020
which is pretty confusing.

539
00:25:50,020 --> 00:25:51,880
So, uh, during training,

540
00:25:51,880 --> 00:25:55,990
we have this picture where you feed the token back in.

541
00:25:55,990 --> 00:25:57,160
So in this scenario,

542
00:25:57,160 --> 00:25:58,195
once you produce end,

543
00:25:58,195 --> 00:26:02,080
then you have to stop because you can't feed end in as the initial next step.

544
00:26:02,080 --> 00:26:07,060
But in training, you don't feed the thing that you produced into the next step.

545
00:26:07,060 --> 00:26:11,080
During training, you feed the target sentence from the corpus.

546
00:26:11,080 --> 00:26:14,470
So like the gold target sentence into the model.

547
00:26:14,470 --> 00:26:16,750
So no matter what the, uh,

548
00:26:16,750 --> 00:26:19,225
the decoder predicts on a step,

549
00:26:19,225 --> 00:26:23,780
you kind of, you don't use that for anything other than computing loss.

550
00:26:24,150 --> 00:26:26,995
Any other questions? Yeah.

551
00:26:26,995 --> 00:26:31,405
Is there a reason why you would, uh, backpropagation end-to-end instead of maybe

552
00:26:31,405 --> 00:26:37,720
training an encoder like [inaudible] model and then [inaudible] together?

553
00:26:37,720 --> 00:26:41,530
The question is, is there a reason why you would want to train end-to-end when,

554
00:26:41,530 --> 00:26:45,205
for example, you might want to train the encoder and the decoder separately?

555
00:26:45,205 --> 00:26:48,580
Uh, so I think, uh, people view training end-to-end as favorable

556
00:26:48,580 --> 00:26:52,000
because the idea is that you can optimize the system as a whole.

557
00:26:52,000 --> 00:26:54,625
You might think that if you optimize the part separately,

558
00:26:54,625 --> 00:26:56,065
then when you put them together,

559
00:26:56,065 --> 00:26:58,090
they will not be optimal together necessarily.

560
00:26:58,090 --> 00:27:01,405
So if possible, directly optimizing the thing that you care abou-

561
00:27:01,405 --> 00:27:05,110
about with respect to all of the parameters is more likely to succeed.

562
00:27:05,110 --> 00:27:07,405
However, there is a notion of pre-training.

563
00:27:07,405 --> 00:27:09,865
And as you said, maybe you'd want to learn your, um,

564
00:27:09,865 --> 00:27:13,570
decoder RNN as a kind of a language model,

565
00:27:13,570 --> 00:27:15,325
an unconditional language model by itself.

566
00:27:15,325 --> 00:27:16,825
And that's something that people do.

567
00:27:16,825 --> 00:27:19,645
You might, uh, learn a very strong language model,

568
00:27:19,645 --> 00:27:22,870
and then use that to initialize your decoder RNN,

569
00:27:22,870 --> 00:27:24,625
and then fine-tune it on your task.

570
00:27:24,625 --> 00:27:27,430
That's a, a valid thing you might try to do.

571
00:27:27,430 --> 00:27:31,230
Yep.

572
00:27:31,230 --> 00:27:37,990
Are you always [inaudible]

573
00:27:37,990 --> 00:27:39,760
The question is, is the length of

574
00:27:39,760 --> 00:27:42,310
the source sentence and the length of the target sentence fixed?

575
00:27:42,310 --> 00:27:44,530
So for example, is the source sentence always length 4?

576
00:27:44,530 --> 00:27:45,745
Uh, no.

577
00:27:45,745 --> 00:27:48,310
That's definitely not true because in your parallel corpus,

578
00:27:48,310 --> 00:27:50,125
you're going to have sentences of all lengths.

579
00:27:50,125 --> 00:27:53,725
Uh, so this is more kind of an implementation or a practicality question.

580
00:27:53,725 --> 00:27:56,620
Uh, the idea is that this is what you mathematically want

581
00:27:56,620 --> 00:27:59,290
to be computing during training for each example,

582
00:27:59,290 --> 00:28:01,045
and you're going to have batches of examples.

583
00:28:01,045 --> 00:28:04,900
But the question is, how do you actually implement them in, uh, in practice?

584
00:28:04,900 --> 00:28:08,380
So what you usually do just because it's easier to assume that your

585
00:28:08,380 --> 00:28:12,535
batch is this kind of even-sized tensor where everything is the same length,

586
00:28:12,535 --> 00:28:17,440
is you pad any short sentences up to some predefined maximum length,

587
00:28:17,440 --> 00:28:21,220
or maybe the length of the maximum example in your batch, uh,

588
00:28:21,220 --> 00:28:23,755
and then you make sure that you don't

589
00:28:23,755 --> 00:28:27,420
use any hidden states that came from the padding. Yep.

590
00:28:27,420 --> 00:28:37,210
I believe two languages together [inaudible]

591
00:28:37,210 --> 00:28:38,410
possible to have a system

592
00:28:38,410 --> 00:28:48,730
[inaudible] that will be kind of universal with similar languages or something like that?

593
00:28:48,730 --> 00:28:51,340
Okay. So the question I think is,

594
00:28:51,340 --> 00:28:54,820
uh, it seems like sometimes you wouldn't want to train things end-to-end,

595
00:28:54,820 --> 00:28:58,120
and there are circumstances in which you might want to train things separately,

596
00:28:58,120 --> 00:28:59,575
and you mentioned, for example,

597
00:28:59,575 --> 00:29:01,825
having, uh, different languages mapped to each other.

598
00:29:01,825 --> 00:29:03,355
So this is a totally valid point,

599
00:29:03,355 --> 00:29:05,170
and in fact, uh, so far we've, kind of,

600
00:29:05,170 --> 00:29:09,430
assumed that you want to learn language A to language B as a pair, right?

601
00:29:09,430 --> 00:29:13,075
And that's different to language A to language C or even language B to language A.

602
00:29:13,075 --> 00:29:17,350
And, um, that does mean you have kind of n-squared many systems in the number of,

603
00:29:17,350 --> 00:29:18,910
uh, languages you're considering.

604
00:29:18,910 --> 00:29:20,830
So, yeah, that's actually a valid idea,

605
00:29:20,830 --> 00:29:22,540
and this is something that people have researched.

606
00:29:22,540 --> 00:29:24,370
The idea that maybe you could have a, kind of,

607
00:29:24,370 --> 00:29:26,815
mix and match with your encoders and decoders.

608
00:29:26,815 --> 00:29:29,920
And you could try to, uh, train a kind of general purpose,

609
00:29:29,920 --> 00:29:34,345
let's say English decoder and then match it up with your different encoders.

610
00:29:34,345 --> 00:29:37,315
Uh, but this is, I think fairly complex to train to,

611
00:29:37,315 --> 00:29:38,965
to make sure that they all work together.

612
00:29:38,965 --> 00:29:41,290
But that, that is certainly something that people have done.

613
00:29:41,290 --> 00:29:43,850
Let me just check on the time.

614
00:29:43,890 --> 00:29:47,335
Okay. Let's take one more question. Yep.

615
00:29:47,335 --> 00:29:53,300
So does the word embedding also come from the same corpus that we are training on?

616
00:29:53,340 --> 00:29:56,020
The question is, does the word embedding

617
00:29:56,020 --> 00:29:57,970
also come from the corpus that you're training on?

618
00:29:57,970 --> 00:30:02,109
So I think there's a few options just as we saw with language models; you could download,

619
00:30:02,109 --> 00:30:04,900
uh, pretrained word vectors like Word2Vec or GloVe,

620
00:30:04,900 --> 00:30:06,265
and you could use those.

621
00:30:06,265 --> 00:30:08,410
And then you can either, kind of, freeze them or you could

622
00:30:08,410 --> 00:30:10,825
fine-tune them as part of the end-to-end training,

623
00:30:10,825 --> 00:30:13,540
or you could just initialize your word vectors as,

624
00:30:13,540 --> 00:30:17,140
uh, you know, close to zero random and then learn them from scratch.

625
00:30:17,140 --> 00:30:19,585
All right. Okay, moving on.

626
00:30:19,585 --> 00:30:24,670
Uh, so now we understand how you would train a neural machine translation system.

627
00:30:24,670 --> 00:30:26,800
And we talked briefly about how you might,

628
00:30:26,800 --> 00:30:28,795
uh, do decoding or generation.

629
00:30:28,795 --> 00:30:32,020
So what I showed you before is something called, uh, greedy decoding,

630
00:30:32,020 --> 00:30:33,925
which is this idea that on each step,

631
00:30:33,925 --> 00:30:35,110
you just choose the argmax,

632
00:30:35,110 --> 00:30:36,415
the top one best word,

633
00:30:36,415 --> 00:30:38,755
and then you feed that in on the next step.

634
00:30:38,755 --> 00:30:42,400
So this is called greedy decoding because you're just taking the best, uh,

635
00:30:42,400 --> 00:30:44,815
the best option that you can see right now,

636
00:30:44,815 --> 00:30:47,200
and then you really don't have a way to go back.

637
00:30:47,200 --> 00:30:52,330
So can anyone see a problem with this method? Maybe I've kind of given it away but, uh, yeah.

638
00:30:52,330 --> 00:30:59,665
[inaudible].

639
00:30:59,665 --> 00:31:01,525
You said too expensive.

640
00:31:01,525 --> 00:31:04,390
Um, I guess I mean it is expensive in that you have to do

641
00:31:04,390 --> 00:31:07,450
a sequence and the sequence is usually worse than something you can do in parallel.

642
00:31:07,450 --> 00:31:10,030
But I suppose, um, maybe what's wrong with the greediness?

643
00:31:10,030 --> 00:31:11,650
Can anyone suggest what's wrong with the greediness, yeah?

644
00:31:11,650 --> 00:31:19,210
[inaudible] [NOISE] That's not

645
00:31:19,210 --> 00:31:21,820
necessarily gonna give you the argmax over the entire sentence.

646
00:31:21,820 --> 00:31:23,335
That's exactly right. That's, uh,

647
00:31:23,335 --> 00:31:26,080
kind of what, uh, what greediness means.

648
00:31:26,080 --> 00:31:29,035
So in practice, this might give you something like this.

649
00:31:29,035 --> 00:31:30,715
Uh, we're trying to translate

650
00:31:30,715 --> 00:31:34,000
our running example sentence and let's suppose on the first step we say,

651
00:31:34,000 --> 00:31:35,320
"He," and then we say,

652
00:31:35,320 --> 00:31:37,000
"He hit," and then we say,

653
00:31:37,000 --> 00:31:39,190
"He hit a," oh no, that wasn't right.

654
00:31:39,190 --> 00:31:42,880
That wasn't the best thing to choose but we kinda have no way to go back now, right.

655
00:31:42,880 --> 00:31:45,430
We just have to continue and try to make the best of it after saying,

656
00:31:45,430 --> 00:31:48,760
"He hit a," which isn't gonna work out well.

657
00:31:48,760 --> 00:31:50,920
So that's the main problem with greedy decoding.

658
00:31:50,920 --> 00:31:53,965
There's kind of no way to backtrack, no way to go back.

659
00:31:53,965 --> 00:31:56,290
So how can we fix this?

660
00:31:56,290 --> 00:31:57,940
And this relates back to, uh,

661
00:31:57,940 --> 00:32:00,100
what I told you earlier about how we might use, uh,

662
00:32:00,100 --> 00:32:04,220
a kind of searching algorithm to do decoding and SMT.

663
00:32:04,310 --> 00:32:07,785
Uh, but first, you might,

664
00:32:07,785 --> 00:32:09,990
uh, think exhaustive search is a good idea.

665
00:32:09,990 --> 00:32:13,125
Well, probably not because it's still a bad idea for the same reasons as before.

666
00:32:13,125 --> 00:32:14,955
So if you did want to do exhaustive search,

667
00:32:14,955 --> 00:32:18,615
and search through the space of all possible French translations, uh,

668
00:32:18,615 --> 00:32:19,890
then you would be again,

669
00:32:19,890 --> 00:32:23,010
trying to consider which Y maximizes,

670
00:32:23,010 --> 00:32:26,885
uh, this product of all of these individual probability distributions.

671
00:32:26,885 --> 00:32:29,740
So as before, if you try to do this,

672
00:32:29,740 --> 00:32:32,229
uh, then on each step T of the decoder,

673
00:32:32,229 --> 00:32:37,750
you're gonna be having to track V to the power of T possible partial translations,

674
00:32:37,750 --> 00:32:40,330
uh, where V is your vocabulary size.

675
00:32:40,330 --> 00:32:42,130
So here when I say partial translation,

676
00:32:42,130 --> 00:32:43,435
I just mean, uh, a kinda,

677
00:32:43,435 --> 00:32:46,945
you know, like, half of a sentence so far, or something like that.

678
00:32:46,945 --> 00:32:48,850
So, of course, this, uh,

679
00:32:48,850 --> 00:32:52,000
exponential in V complexity is just far too expensive.

680
00:32:52,000 --> 00:32:54,790
So yes, we're gonna use some kind of search algorithm,

681
00:32:54,790 --> 00:32:57,970
and in particular, we're gonna use a beam search decoding.

682
00:32:57,970 --> 00:33:03,550
So the core idea of beam search decoding is that on each step of the decoder,

683
00:33:03,550 --> 00:33:08,515
you're gonna be keeping track of the K most probable partial translations,

684
00:33:08,515 --> 00:33:10,975
and we call partial translations hypotheses,

685
00:33:10,975 --> 00:33:14,410
because we're kind of tracking multiple of them and we're not sure which one is best.

686
00:33:14,410 --> 00:33:16,550
So we're thinking about several.

687
00:33:16,550 --> 00:33:21,240
Here K is an integer and we call this the beam size,

688
00:33:21,240 --> 00:33:25,095
and in practice for NMT this is usually maybe 5-10.

689
00:33:25,095 --> 00:33:26,745
So you can think of, uh,

690
00:33:26,745 --> 00:33:30,330
K kind of as how big is your search space at any one time.

691
00:33:30,330 --> 00:33:32,190
So if you increase K, then you're going to be

692
00:33:32,190 --> 00:33:34,860
considering more different options on each step

693
00:33:34,860 --> 00:33:36,810
and you might hope that this will mean that you get

694
00:33:36,810 --> 00:33:40,930
the best quality solution in the end though of course it'll be more expensive.

695
00:33:41,030 --> 00:33:44,010
So I said that we want to keep track of

696
00:33:44,010 --> 00:33:47,865
the K most probable partial translations, that is, hypotheses.

697
00:33:47,865 --> 00:33:50,205
So this means that we need some kind of notion of, you know,

698
00:33:50,205 --> 00:33:53,195
how probable is this hypothesis or what's its score.

699
00:33:53,195 --> 00:33:56,065
So the score of a hypothesis and, uh,

700
00:33:56,065 --> 00:33:59,110
we're representing that as Y_1 up to Y_T,

701
00:33:59,110 --> 00:34:02,995
um, is just its log probability.

702
00:34:02,995 --> 00:34:07,180
So, uh, the log probability of this partial translation, uh,

703
00:34:07,180 --> 00:34:11,200
according to the language model can be broken down as we saw before into the sum of

704
00:34:11,200 --> 00:34:16,220
the individual log probabilities of the words given everything that came before.

705
00:34:16,920 --> 00:34:19,390
So it's, if it's not obvious, uh,

706
00:34:19,390 --> 00:34:22,090
these scores are all negative because we're taking log of,

707
00:34:22,090 --> 00:34:24,100
uh, of a number between 0 and 1.

708
00:34:24,100 --> 00:34:29,230
Uh, and a higher score is better.

709
00:34:29,230 --> 00:34:32,830
Yes, because you want a higher probability of,

710
00:34:32,830 --> 00:34:37,465
uh, of the hypothesis according to the language model.

711
00:34:37,465 --> 00:34:40,660
So the idea is that we're gonna use this score, uh,

712
00:34:40,660 --> 00:34:42,220
and the search algorithm to search for

713
00:34:42,220 --> 00:34:46,255
high-scoring hypotheses and we're gonna track the top K on each step.

714
00:34:46,255 --> 00:34:49,285
So I'm gonna show you a detailed example in a moment,

715
00:34:49,285 --> 00:34:51,250
but the important thing is to know

716
00:34:51,250 --> 00:34:54,910
that beam search is not guaranteed to find an optimal solution.

717
00:34:54,910 --> 00:34:57,295
Uh, exhaustive search, the one where you enumerate,

718
00:34:57,295 --> 00:35:00,820
enumerate all V to the T possible translations, that is guaranteed to find

719
00:35:00,820 --> 00:35:04,960
the optimal solution but it is just completely infeasible because it's so expensive.

720
00:35:04,960 --> 00:35:08,080
So beam search is not guaranteed to find the optimal solution,

721
00:35:08,080 --> 00:35:11,860
but it is much more efficient than exhaustive search of course.

722
00:35:11,860 --> 00:35:16,945
Okay. So, um, here's an example of beam search decoding in action.

723
00:35:16,945 --> 00:35:19,420
Uh, so let's suppose the beam size equals K, uh,

724
00:35:19,420 --> 00:35:22,795
is 2 and then as a reminder, we have, uh,

725
00:35:22,795 --> 00:35:25,630
this is the score that you apply to a partial, uh,

726
00:35:25,630 --> 00:35:28,540
hypothesis, uh, a partial translation,

727
00:35:28,540 --> 00:35:29,935
which is a hypothesis.

728
00:35:29,935 --> 00:35:32,590
So we start off with our starting token,

729
00:35:32,590 --> 00:35:34,570
and the idea is that we're going to compute

730
00:35:34,570 --> 00:35:37,765
the probability distribution of what word might come next.

731
00:35:37,765 --> 00:35:41,880
So having computed that probability distribution using our seq2seq model,

732
00:35:41,880 --> 00:35:45,435
then we just take the top K, that is top two possible options.

733
00:35:45,435 --> 00:35:48,695
So let's suppose that the top two are the words "He" and "I".

734
00:35:48,695 --> 00:35:52,930
So the idea is that we can compute the score of these two hypotheses,

735
00:35:52,930 --> 00:35:55,540
uh, by using the formula above.

736
00:35:55,540 --> 00:35:59,845
It's just the log probability of this word given the context so far.

737
00:35:59,845 --> 00:36:05,125
So here, let's say that "He" has a score of -0,7 and "I" has a score of -0,9.

738
00:36:05,125 --> 00:36:07,750
So this means that he is currently the best one.

739
00:36:07,750 --> 00:36:09,640
Okay. So what we do is, uh,

740
00:36:09,640 --> 00:36:12,655
we have our two, uh, K hypotheses,

741
00:36:12,655 --> 00:36:14,710
and then for each of those,

742
00:36:14,710 --> 00:36:18,160
we find the top K words that could come next.

743
00:36:18,160 --> 00:36:20,005
And we calculate their scores.

744
00:36:20,005 --> 00:36:24,055
So this means that for both "He" and "I" we find the top two words that could come next.

745
00:36:24,055 --> 00:36:26,440
And for each of these four possibilities, uh,

746
00:36:26,440 --> 00:36:29,395
the score of the hypothesis is equal to, uh,

747
00:36:29,395 --> 00:36:32,980
the log probability of this new word given the context so far plus

748
00:36:32,980 --> 00:36:36,820
the score so far because you can accumulate the sum of low probabilities.

749
00:36:36,820 --> 00:36:39,620
You don't have to compute it from scratch each time.

750
00:36:40,170 --> 00:36:44,350
So here you can see that we have these four possibilities and that

751
00:36:44,350 --> 00:36:48,610
the top two scores are -1,6 and -1,7.

752
00:36:48,610 --> 00:36:52,180
So this means that hit and was are the two best ones.

753
00:36:52,180 --> 00:36:55,570
So the idea is that of these K squared equals 4 hypotheses,

754
00:36:55,570 --> 00:36:58,795
we're just gonna keep the K equals 2 top ones.

755
00:36:58,795 --> 00:37:00,835
And then we just keep doing the same thing.

756
00:37:00,835 --> 00:37:03,955
For these two, we expand to get the two next ones.

757
00:37:03,955 --> 00:37:06,295
And then of those we compute the scores,

758
00:37:06,295 --> 00:37:11,905
and then we keep the two best ones and discard the others and then of those, we expand.

759
00:37:11,905 --> 00:37:13,675
So we keep doing this again and again,

760
00:37:13,675 --> 00:37:18,700
expanding and then just keeping the top K and expanding like this until,

761
00:37:18,700 --> 00:37:22,045
uh, you get some kinda, uh, finished translation.

762
00:37:22,045 --> 00:37:25,570
I'm going to tell you more in a moment about what exactly the stopping criterion is.

763
00:37:25,570 --> 00:37:27,550
But let's suppose that we stop here.

764
00:37:27,550 --> 00:37:31,150
Uh, looking at the four hypotheses that we have on the far right,

765
00:37:31,150 --> 00:37:33,220
the one with the top score is, uh,

766
00:37:33,220 --> 00:37:36,265
the top pie one with -4,3.

767
00:37:36,265 --> 00:37:38,170
So let's suppose that we are gonna stop now when we

768
00:37:38,170 --> 00:37:39,955
decide that this is the top hypothesis,

769
00:37:39,955 --> 00:37:42,340
then all we need to do is just backtrack

770
00:37:42,340 --> 00:37:45,070
through this tree in order to find the full translation,

771
00:37:45,070 --> 00:37:48,790
which is "He hit me with the pie."

772
00:37:48,790 --> 00:37:53,185
All right. So, um, let me tell you more detail about how exactly we decide when to stop.

773
00:37:53,185 --> 00:37:55,524
So if you remember in greedy decoding,

774
00:37:55,524 --> 00:37:59,350
usually we just keep decoding until the model produces the END token.

775
00:37:59,350 --> 00:38:03,460
So for example, this means that your model is actually producing the sequence, uh,

776
00:38:03,460 --> 00:38:04,525
I guess it doesn't produce START,

777
00:38:04,525 --> 00:38:09,265
you give it START but then it produces the sequence "He hit me with a pie" END.

778
00:38:09,265 --> 00:38:12,310
So the problem in beam search decoding is

779
00:38:12,310 --> 00:38:14,920
that you're considering all these different hypotheses,

780
00:38:14,920 --> 00:38:17,785
K different hypotheses at once and the thing is

781
00:38:17,785 --> 00:38:21,250
those hypotheses might produce END tokens at different times.

782
00:38:21,250 --> 00:38:24,100
So there's no one obvious place to stop.

783
00:38:24,100 --> 00:38:26,005
So what we do in practice,

784
00:38:26,005 --> 00:38:28,510
is when a hypothesis produces the END token,

785
00:38:28,510 --> 00:38:32,935
then we regard this hypothesis as complete and we kind of place it aside.

786
00:38:32,935 --> 00:38:35,425
We have a collection of completed hypothesis.

787
00:38:35,425 --> 00:38:37,000
So we kind of take it out of beam search,

788
00:38:37,000 --> 00:38:39,565
we no longer keep exploring it because it's finished,

789
00:38:39,565 --> 00:38:41,800
uh, and we, yeah, place it aside.

790
00:38:41,800 --> 00:38:45,685
And you continue exploring other hypotheses with beam search.

791
00:38:45,685 --> 00:38:49,120
So the remaining question is when do you stop doing beam search?

792
00:38:49,120 --> 00:38:51,535
When do you stop iterating through this algorithm?

793
00:38:51,535 --> 00:38:55,105
So there's, uh, uh, multiple possible stopping criterion

794
00:38:55,105 --> 00:38:57,910
but two common ones are you might say, uh,

795
00:38:57,910 --> 00:39:01,150
we're gonna stop doing beam search once we reach time step T,

796
00:39:01,150 --> 00:39:02,545
where T is some, uh,

797
00:39:02,545 --> 00:39:04,120
predefined threshold that you choose.

798
00:39:04,120 --> 00:39:05,230
So you might say, uh,

799
00:39:05,230 --> 00:39:08,380
we're gonna stop beam search after 30 steps because we don't want

800
00:39:08,380 --> 00:39:11,755
any output sentences that are longer than 30 words for example,

801
00:39:11,755 --> 00:39:13,120
or you might say, "Uh,

802
00:39:13,120 --> 00:39:17,410
we're gonna stop doing beam search once we've collected at least N completed hypotheses."

803
00:39:17,410 --> 00:39:18,820
So you might say, "Uh, I want

804
00:39:18,820 --> 00:39:23,510
at least 10 complete translations before I stop doing beam search."

805
00:39:24,510 --> 00:39:27,550
Okay. So what's the final thing you have to do?

806
00:39:27,550 --> 00:39:29,350
Uh, we finished doing beam search, um,

807
00:39:29,350 --> 00:39:32,350
we have this collection of completed hypotheses.

808
00:39:32,350 --> 00:39:34,525
Uh, we want to choose the top one.

809
00:39:34,525 --> 00:39:36,835
Uh, the one that we're going to use is our translation.

810
00:39:36,835 --> 00:39:41,380
So, uh, how do we select the top one that has the highest score?

811
00:39:41,380 --> 00:39:43,360
Uh, you might think this is simple given that all of

812
00:39:43,360 --> 00:39:45,850
these hypotheses already have scores attached.

813
00:39:45,850 --> 00:39:47,440
But if we just look at this, uh,

814
00:39:47,440 --> 00:39:51,745
formula again, uh, for what the score is of each hypothesis.

815
00:39:51,745 --> 00:39:55,120
Uh, can anyone see a problem with this?

816
00:39:55,120 --> 00:39:57,115
If we have our sets of hypotheses,

817
00:39:57,115 --> 00:39:59,290
and then we're choosing the top one

818
00:39:59,290 --> 00:40:02,750
based on the one that has the best score, can anyone see a problem?

819
00:40:05,130 --> 00:40:09,460
Yeah. [NOISE] So the answer was you're gonna end up choosing the shortest one.

820
00:40:09,460 --> 00:40:13,630
The problem here is that longer hypotheses have lower scores in

821
00:40:13,630 --> 00:40:18,190
general because you're multiplying more probabilities so you're getting a smaller,

822
00:40:18,190 --> 00:40:20,440
a smaller overall value or I guess if we're adding

823
00:40:20,440 --> 00:40:22,855
low probabilities we're gonna get more negative values.

824
00:40:22,855 --> 00:40:25,255
So it's not quite that you will definitely choose

825
00:40:25,255 --> 00:40:28,690
the shortest hypothesis because if you could overall have,

826
00:40:28,690 --> 00:40:33,550
uh, a lower score but there's definitely going to be a bias towards shorter translations,

827
00:40:33,550 --> 00:40:35,995
uh, because they'll in general have lower scores.

828
00:40:35,995 --> 00:40:38,620
So the way you can fix this is pretty simple,

829
00:40:38,620 --> 00:40:40,210
you just normalize by length.

830
00:40:40,210 --> 00:40:43,720
So instead of using the tools we have above, you're going to use, uh,

831
00:40:43,720 --> 00:40:48,490
the score divided by [inaudible].

832
00:40:48,490 --> 00:40:51,010
And then you use this to select the top one.

833
00:40:51,010 --> 00:41:00,235
Any questions on this? [NOISE].

834
00:41:00,235 --> 00:41:02,680
Yeah.

835
00:41:02,680 --> 00:41:06,970
Can we train with the END token so that it is possible to [inaudible]

836
00:41:06,970 --> 00:41:09,475
I didn't quite hear that, can you train with the END token?

837
00:41:09,475 --> 00:41:12,230
Yeah, like we had an END token.

838
00:41:12,690 --> 00:41:16,435
Yes. So you train with the END token, if that's your question.

839
00:41:16,435 --> 00:41:19,930
Um, because the whole point is you're relying on your language model,

840
00:41:19,930 --> 00:41:24,145
your decoder to produce the END token in order to know when to stop.

841
00:41:24,145 --> 00:41:27,370
So you need to train it to produce the END token by giving it examples of

842
00:41:27,370 --> 00:41:33,490
training sentences with END tokens. Yeah.

843
00:41:33,490 --> 00:41:37,270
Why don't we use this score being changed [inaudible]

844
00:41:37,270 --> 00:41:38,650
Great question. The question is,

845
00:41:38,650 --> 00:41:40,360
why don't we use this normalized score,

846
00:41:40,360 --> 00:41:43,315
the one at the bottom of the screen during beam search in the first place?

847
00:41:43,315 --> 00:41:45,205
So the reason why that's not necessary,

848
00:41:45,205 --> 00:41:46,825
you could, but it's not necessary,

849
00:41:46,825 --> 00:41:48,790
is because during beam search,

850
00:41:48,790 --> 00:41:54,100
we only ever compare the scores of hypotheses that have the same length, right?

851
00:41:54,100 --> 00:41:55,990
So in each of these steps, the way we look at,

852
00:41:55,990 --> 00:42:00,040
let's say the top k squared and we want to choose which ones are the top k,

853
00:42:00,040 --> 00:42:04,030
we're comparing the scores of four different hypotheses that are of length,

854
00:42:04,030 --> 00:42:05,755
one, two, three, four, five.

855
00:42:05,755 --> 00:42:10,135
So, um, it's true that these scores are getting lower and lower,

856
00:42:10,135 --> 00:42:12,760
but in the same way because they're all length five right now.

857
00:42:12,760 --> 00:42:19,095
[NOISE] Okay.

858
00:42:19,095 --> 00:42:23,580
So we now understand how you would train an NMT system and how would you- you

859
00:42:23,580 --> 00:42:27,900
would use your trained NMT system to generate your translations using,

860
00:42:27,900 --> 00:42:29,585
let's say, beam search.

861
00:42:29,585 --> 00:42:32,290
So let's all take a step back and think about,

862
00:42:32,290 --> 00:42:36,730
what are the overall advantages of NMT in comparison to SMT?

863
00:42:36,730 --> 00:42:41,020
Uh, so the first advantage is just better performance.

864
00:42:41,020 --> 00:42:45,715
Uh, NMT systems tend to give better output than SMT systems in several ways.

865
00:42:45,715 --> 00:42:48,760
One is that the output often tends to be more fluent.

866
00:42:48,760 --> 00:42:51,325
Uh, this is probably because NMT, uh,

867
00:42:51,325 --> 00:42:53,425
this is probably because RNNs are particularly good at

868
00:42:53,425 --> 00:42:56,005
learning language models as you learned last week.

869
00:42:56,005 --> 00:42:58,510
Uh, another way that they're better is they often use,

870
00:42:58,510 --> 00:43:00,760
uh, the context better, that is,

871
00:43:00,760 --> 00:43:02,500
uh, they're better at conditioning on

872
00:43:02,500 --> 00:43:05,935
the source sentence and using that to change the output.

873
00:43:05,935 --> 00:43:09,040
Another way they're better is they often, uh,

874
00:43:09,040 --> 00:43:13,795
are more able to generalize what they learn about phrases and how to translate them.

875
00:43:13,795 --> 00:43:17,410
So for example, if it sees an example of how to translate a certain

876
00:43:17,410 --> 00:43:21,850
source phrase and then later it sees a slightly different version of that source phrase,

877
00:43:21,850 --> 00:43:24,490
it's, uh, more able to generalize what

878
00:43:24,490 --> 00:43:28,040
it learned about the first phrase than SMT systems will.

879
00:43:28,950 --> 00:43:33,580
Another big advantage of NMT systems compared to SMT that we talked about

880
00:43:33,580 --> 00:43:37,505
before is that it's a single neural network that can be optimized end-to-end.

881
00:43:37,505 --> 00:43:42,490
And the- the advantage here I suppose is primarily simplicity and convenience.

882
00:43:42,620 --> 00:43:47,140
So there's no subcomponents that need to be individually optimized.

883
00:43:47,250 --> 00:43:52,030
Another big advantage is that it requires much less human engineering efforts.

884
00:43:52,030 --> 00:43:54,130
When I told you earlier about all the different things that

885
00:43:54,130 --> 00:43:56,485
people had to do to build, uh, big,

886
00:43:56,485 --> 00:43:58,765
uh, powerful SMT systems, uh,

887
00:43:58,765 --> 00:44:01,555
there's relatively less engineering effort for NMT.

888
00:44:01,555 --> 00:44:03,010
And NMT is certainly not easy,

889
00:44:03,010 --> 00:44:05,485
but it's- is less complicated than SMT.

890
00:44:05,485 --> 00:44:08,245
In particular, there's no feature engineering.

891
00:44:08,245 --> 00:44:10,480
You don't have to define what features of,

892
00:44:10,480 --> 00:44:12,520
uh, linguistic phenomena that you want to capture.

893
00:44:12,520 --> 00:44:15,550
You can mostly just view it as a sequence of words although,

894
00:44:15,550 --> 00:44:18,530
uh, there are different views on that.

895
00:44:19,500 --> 00:44:22,930
Uh, lastly, a great thing about NMT is that you can

896
00:44:22,930 --> 00:44:26,005
use pretty much the same method for all language pairs.

897
00:44:26,005 --> 00:44:27,370
So if you've, uh, you know,

898
00:44:27,370 --> 00:44:29,200
built your French-to-English translation system

899
00:44:29,200 --> 00:44:31,075
and now you want to build a Spanish-to-English one, [NOISE] uh,

900
00:44:31,075 --> 00:44:34,375
you can probably use basically the same architecture and the same method

901
00:44:34,375 --> 00:44:38,720
as long as you can go find a big enough parallel corpus of Spanish-to-English.

902
00:44:38,880 --> 00:44:43,060
All right. So what are the disadvantages of NMT, uh, remaining?

903
00:44:43,060 --> 00:44:44,560
So compared to SMT,

904
00:44:44,560 --> 00:44:46,090
there are some disadvantages.

905
00:44:46,090 --> 00:44:48,970
One is that NMT is less interpretable.

906
00:44:48,970 --> 00:44:51,670
Uh, what I mean by this is you

907
00:44:51,670 --> 00:44:54,385
feed in your source sentence into the neural network and then it feeds

908
00:44:54,385 --> 00:44:57,310
out some target sentence and you didn't

909
00:44:57,310 --> 00:45:00,430
really have any way to figure out why that happened, right?

910
00:45:00,430 --> 00:45:03,430
So in particular, if the target sentence con- contains some kind of error,

911
00:45:03,430 --> 00:45:06,985
um, you can't really look at the neurons and understand what happened.

912
00:45:06,985 --> 00:45:08,800
It's pretty hard to attribute errors.

913
00:45:08,800 --> 00:45:10,240
So this means that, uh,

914
00:45:10,240 --> 00:45:12,235
NMT systems are pretty hard to debug.

915
00:45:12,235 --> 00:45:15,850
So by comparison, SMT systems were more

916
00:45:15,850 --> 00:45:17,860
interpretable in that you had all of

917
00:45:17,860 --> 00:45:20,500
these different sub-components that were doing different jobs.

918
00:45:20,500 --> 00:45:22,600
And, uh, you were more able to look at those.

919
00:45:22,600 --> 00:45:24,910
They weren't, you know, neurons often would be, uh,

920
00:45:24,910 --> 00:45:27,880
you know, probabilities of certain words given other words and so on.

921
00:45:27,880 --> 00:45:30,070
And, you know, that's by no means easy to

922
00:45:30,070 --> 00:45:33,295
interpret but it was at least more interpretable than NMT.

923
00:45:33,295 --> 00:45:38,350
Uh, another disadvantage is NMT is pretty difficult to control.

924
00:45:38,350 --> 00:45:40,210
So, uh, for example,

925
00:45:40,210 --> 00:45:42,490
if your NMT system is,

926
00:45:42,490 --> 00:45:43,990
uh, doing a particular error,

927
00:45:43,990 --> 00:45:46,405
it's not very easy for you, the, uh,

928
00:45:46,405 --> 00:45:48,490
programmer to specify some kind of

929
00:45:48,490 --> 00:45:51,280
rule or guideline that you want the NMT system to follow.

930
00:45:51,280 --> 00:45:52,915
So for example, if you want to say,

931
00:45:52,915 --> 00:45:56,380
I want to always translate this word in this way.

932
00:45:56,380 --> 00:45:59,290
Um, when- when this other thing is present,

933
00:45:59,290 --> 00:46:02,725
like that's not particularly easy to, uh,

934
00:46:02,725 --> 00:46:05,635
to impose as a rule on the NMT system, uh,

935
00:46:05,635 --> 00:46:07,225
because you can't, uh,

936
00:46:07,225 --> 00:46:10,540
easily control what it's doing on a step-by-step basis.

937
00:46:10,540 --> 00:46:12,385
So sometimes you have some kind of, uh,

938
00:46:12,385 --> 00:46:14,320
post-processing rules you might try to do,

939
00:46:14,320 --> 00:46:16,255
but overall you can't.

940
00:46:16,255 --> 00:46:18,985
It- it's- it's harder than you'd expect to try to, um,

941
00:46:18,985 --> 00:46:22,870
impose a fairly simple form.

942
00:46:22,870 --> 00:46:25,360
[NOISE] So this means it has some kind of safety concerns in fact.

943
00:46:25,360 --> 00:46:26,590
Because, uh, let's say, you know,

944
00:46:26,590 --> 00:46:29,650
you don't want your NMT system to say bad things, right?

945
00:46:29,650 --> 00:46:31,750
It's- it's pretty hard to actually put, um,

946
00:46:31,750 --> 00:46:34,180
these, uh, controls in

947
00:46:34,180 --> 00:46:36,745
place to stop it from saying these things that you don't want it to say.

948
00:46:36,745 --> 00:46:39,640
I mean, on the level of maybe just never saying particular bad words,

949
00:46:39,640 --> 00:46:41,815
then sure you can remove them from the vocabulary.

950
00:46:41,815 --> 00:46:44,005
But overall they're pretty hard to control,

951
00:46:44,005 --> 00:46:47,635
and we're actually gonna see some examples of NMT systems being,

952
00:46:47,635 --> 00:46:48,760
you know, doing things that their

953
00:46:48,760 --> 00:46:51,620
uh, designers certainly didn't intend.

954
00:46:52,350 --> 00:46:57,250
Okay. So, uh, how do we evaluate MT?

955
00:46:57,250 --> 00:46:59,500
Uh, every good NLP task needs to have

956
00:46:59,500 --> 00:47:02,395
an automatic metric so that we can, uh, measure our progress.

957
00:47:02,395 --> 00:47:05,980
So the, uh, most commonly used evaluation metric for MT is

958
00:47:05,980 --> 00:47:10,240
called BLEU and that stands for Bilingual Evaluation Understudy.

959
00:47:10,240 --> 00:47:13,090
So the main idea is that BLEU is gonna

960
00:47:13,090 --> 00:47:17,575
compare the translation that's produced by your machine translation system.

961
00:47:17,575 --> 00:47:19,150
It's gonna compare that to

962
00:47:19,150 --> 00:47:22,885
one or maybe several human written translations of the same sentence.

963
00:47:22,885 --> 00:47:27,685
And then it's gonna compute a similarity score that's based on n-gram precision.

964
00:47:27,685 --> 00:47:29,545
So when I say n-gram precision,

965
00:47:29,545 --> 00:47:31,750
I mean you're gonna look at all the one, two, three,

966
00:47:31,750 --> 00:47:34,150
and four grams that appear in your, uh,

967
00:47:34,150 --> 00:47:37,345
machine written translation and your human written translation.

968
00:47:37,345 --> 00:47:39,865
And then n-gram precision is basically saying,

969
00:47:39,865 --> 00:47:42,895
for all of the n-grams that appeared in the machine-written translation,

970
00:47:42,895 --> 00:47:44,920
how many of those appeared in, you know,

971
00:47:44,920 --> 00:47:48,460
at least one of the human-written translations?

972
00:47:48,460 --> 00:47:53,170
Another thing that you need to add to BLEU is a brevity penalty.

973
00:47:53,170 --> 00:47:56,050
Uh, so you're saying that you get a lower BLEU score if

974
00:47:56,050 --> 00:47:57,970
your system translation is significantly

975
00:47:57,970 --> 00:48:00,925
shorter than all of the human-written translations.

976
00:48:00,925 --> 00:48:03,610
And the reason why you need to add this is because

977
00:48:03,610 --> 00:48:07,750
n-gram precision alone doesn't [NOISE] really punish using, uh, fewer words.

978
00:48:07,750 --> 00:48:12,640
So you might try to maximize n-gram precision by being very conservative and writing,

979
00:48:12,640 --> 00:48:15,790
uh, short sentences that only contain words that you're really sure about,

980
00:48:15,790 --> 00:48:17,425
and then you get a good precision score.

981
00:48:17,425 --> 00:48:20,260
But this doesn't make a good translation because you're probably missing a bunch of

982
00:48:20,260 --> 00:48:23,125
information that you needed to translate from the source sentence.

983
00:48:23,125 --> 00:48:26,260
So that's why you need to add the brevity, uh, penalty.

984
00:48:26,260 --> 00:48:31,045
So overall, um, BLEU is very useful because, uh,

985
00:48:31,045 --> 00:48:34,180
we need an automatic metric in order to, uh, measure progress,

986
00:48:34,180 --> 00:48:36,160
you can't measure progress on human evaluation

987
00:48:36,160 --> 00:48:38,560
alone because it takes too long [NOISE] to compute.

988
00:48:38,560 --> 00:48:41,470
Um, but of course it's pretty and perfect.

989
00:48:41,470 --> 00:48:44,350
So for example, you can think about how there

990
00:48:44,350 --> 00:48:47,170
are many ways- many valid ways to translate a sentence.

991
00:48:47,170 --> 00:48:48,700
At the very beginning of this lecture,

992
00:48:48,700 --> 00:48:50,995
I asked how do we translate that sentence, uh,

993
00:48:50,995 --> 00:48:54,025
by Rousseau and there were at least a few different options that came up.

994
00:48:54,025 --> 00:48:57,744
Uh, so if there's many valid ways to translate a sentence,

995
00:48:57,744 --> 00:48:59,575
how does BLEU recognize that?

996
00:48:59,575 --> 00:49:03,820
BLEU is [NOISE] rewarding sentences that have a high n-gram overlap with,

997
00:49:03,820 --> 00:49:07,315
uh, one or some of the human-written translations.

998
00:49:07,315 --> 00:49:08,815
But, if, uh, you write one,

999
00:49:08,815 --> 00:49:11,650
if your model writes one valid translation and the humans wrote

1000
00:49:11,650 --> 00:49:15,040
a different valid translation and they don't have high n-gram overlap,

1001
00:49:15,040 --> 00:49:18,520
then BLEU is going to, uh, give you a low score.

1002
00:49:18,520 --> 00:49:23,425
So, um, you're going to learn about BLEU in detail in Assignment 4,

1003
00:49:23,425 --> 00:49:24,850
and in fact Assignment 4 has

1004
00:49:24,850 --> 00:49:28,225
a full description- mathematical description of what the BLEU score is.

1005
00:49:28,225 --> 00:49:31,390
So I'm not gonna tell you about that now, uh, yes,

1006
00:49:31,390 --> 00:49:34,270
so you're gonna think about BLEU and the- the ways in which it's

1007
00:49:34,270 --> 00:49:38,810
imperfect but useful. Yeah.

1008
00:49:38,940 --> 00:49:43,660
So would one- one n-gram, be a one to one equivalency?

1009
00:49:43,660 --> 00:49:44,680
What?

1010
00:49:44,680 --> 00:49:47,420
Would a one n-gram be a one to one equivalency?

1011
00:49:47,420 --> 00:49:51,250
The question is, would a one n-gram be a one-to-one equivalency?

1012
00:49:51,250 --> 00:49:54,355
I'm not sure I understand the question. You're asking about alignment or something else?

1013
00:49:54,355 --> 00:49:58,300
Uh, just trying to get an idea about how they're doing n-gram checks,

1014
00:49:58,300 --> 00:50:03,520
is it doing all n-gram permutations or is it doing like window size of one?

1015
00:50:03,520 --> 00:50:05,980
Well, I guess one- one n-gram it doesn't

1016
00:50:05,980 --> 00:50:07,780
make a difference because you can't permute a one-gram.

1017
00:50:07,780 --> 00:50:08,830
Okay. So you're asking for examples,

1018
00:50:08,830 --> 00:50:10,990
for four grams are they checking, uh,

1019
00:50:10,990 --> 00:50:13,090
whether this exact sequence of four paired or

1020
00:50:13,090 --> 00:50:15,280
any permutation of it, its exact sequences?

1021
00:50:15,280 --> 00:50:19,660
So by definition, n-grams are sequences where the order matters.

1022
00:50:19,660 --> 00:50:23,875
Okay. All right.

1023
00:50:23,875 --> 00:50:26,395
So, uh, that's how you evaluate machine translation.

1024
00:50:26,395 --> 00:50:28,450
So now you can understand this metric of how we

1025
00:50:28,450 --> 00:50:30,550
evaluate our progress on machine translation,

1026
00:50:30,550 --> 00:50:34,030
um, I can show you this graph and you might understand what it means.

1027
00:50:34,030 --> 00:50:35,800
So this is a, uh,

1028
00:50:35,800 --> 00:50:41,860
bar plot which shows in a nutshell how NMT changed the machine translation,

1029
00:50:41,860 --> 00:50:44,110
uh, landscape in just a few years.

1030
00:50:44,110 --> 00:50:48,070
So in this plot, we've got BLEU score is the Y-axis.

1031
00:50:48,070 --> 00:50:50,860
Uh, and you have two different types of SMT

1032
00:50:50,860 --> 00:50:53,560
which is the red and the dark blue, uh, bar plots.

1033
00:50:53,560 --> 00:50:55,165
And what's happening is,

1034
00:50:55,165 --> 00:50:56,725
uh, in 2015, uh,

1035
00:50:56,725 --> 00:51:01,630
Neural MT enters the scene for the first time and it isn't doi- doing as well as SMT,

1036
00:51:01,630 --> 00:51:04,735
and then the next year it's suddenly outperforming SMT.

1037
00:51:04,735 --> 00:51:08,380
And here these are BLEU scores on some particular fixed dataset like,

1038
00:51:08,380 --> 00:51:10,450
uh, a shared task that many people were,

1039
00:51:10,450 --> 00:51:12,100
um, submitting systems for.

1040
00:51:12,100 --> 00:51:15,130
[NOISE] So the main thing to notice here is

1041
00:51:15,130 --> 00:51:18,550
that the progress that was being made by SMT systems was,

1042
00:51:18,550 --> 00:51:21,490
you know, a fairly gentle increase in BLEU year-by-year.

1043
00:51:21,490 --> 00:51:23,305
And then in just one year,

1044
00:51:23,305 --> 00:51:25,435
NMT arrives and is suddenly doing,

1045
00:51:25,435 --> 00:51:27,040
uh, much more rapid progress.

1046
00:51:27,040 --> 00:51:31,790
So I think this justifies why the picture of the meteor maybe isn't too Jurassic here.

1047
00:51:33,000 --> 00:51:38,545
So you could in fact call NMT the biggest success story of NLP in deep learning.

1048
00:51:38,545 --> 00:51:40,930
Uh, because if you think about the history of this,

1049
00:51:40,930 --> 00:51:45,850
NMT went from being a fringe research activity in 2014 to being actually

1050
00:51:45,850 --> 00:51:51,250
the leading standard methodfor machine translation in the world in 2016.

1051
00:51:51,250 --> 00:51:53,109
In particular, in 2014,

1052
00:51:53,109 --> 00:51:54,880
the first seq2seq paper was published.

1053
00:51:54,880 --> 00:51:59,185
And in 2016, Google Translate switches from SMT to NMT.

1054
00:51:59,185 --> 00:52:03,175
This is a pretty remarkable turnaround for just two years.

1055
00:52:03,175 --> 00:52:05,245
So this is amazing,

1056
00:52:05,245 --> 00:52:07,060
not just because it was a quick turnaround,

1057
00:52:07,060 --> 00:52:10,180
but also if you think about the level of human effort involved.

1058
00:52:10,180 --> 00:52:12,310
Uh, these SMT systems, for example

1059
00:52:12,310 --> 00:52:15,010
the Google Translate SMT system was built by

1060
00:52:15,010 --> 00:52:17,965
doubtless hundreds of engineers over many years.

1061
00:52:17,965 --> 00:52:23,875
And this, uh, this SMT system was outperformed by an NMT system that was trained by,

1062
00:52:23,875 --> 00:52:27,265
uh, you know, relatively few like a handful of engineers in a few months.

1063
00:52:27,265 --> 00:52:29,680
So I'm not- I'm not diminishing how difficult it is to,

1064
00:52:29,680 --> 00:52:31,165
um, build NMT systems,

1065
00:52:31,165 --> 00:52:33,670
and certainly I'm sure Google's NMT system

1066
00:52:33,670 --> 00:52:36,745
today is built by more than a handful of engineers in a few months.

1067
00:52:36,745 --> 00:52:38,650
I'm sure it's a very big operation now.

1068
00:52:38,650 --> 00:52:40,509
Uh, but when NMT,

1069
00:52:40,509 --> 00:52:42,355
uh, began to outperform SMT,

1070
00:52:42,355 --> 00:52:45,220
it was pretty remarkable how it was able to do that,

1071
00:52:45,220 --> 00:52:48,550
uh, based on the amount of effort involved. Yeah.

1072
00:52:48,550 --> 00:52:55,785
Given the [inaudible] cons of NMT has there been research on  combining the two and if there is, what does that look like?

1073
00:52:55,785 --> 00:53:00,010
Yeah, great. The question is given that we know that there are

1074
00:53:00,010 --> 00:53:03,865
some disadvantages of NMT even in comparison to SMT,

1075
00:53:03,865 --> 00:53:05,770
is there any work on combining the two?

1076
00:53:05,770 --> 00:53:07,300
So, yes. I think there is.

1077
00:53:07,300 --> 00:53:10,390
Uh, there's a lot of NMT research ongoing and in particular,

1078
00:53:10,390 --> 00:53:13,195
people sometimes focus on these particular shortcomings.

1079
00:53:13,195 --> 00:53:16,660
And, uh, there's a lot of work in kind of taking techniques

1080
00:53:16,660 --> 00:53:20,200
and ideas and wisdom from the many decades of SMT research,

1081
00:53:20,200 --> 00:53:23,005
and then integrating them into the new NMT paradigm.

1082
00:53:23,005 --> 00:53:30,590
So yes. [NOISE]. Okay.

1083
00:53:31,050 --> 00:53:35,290
So is machine translation solved?

1084
00:53:35,290 --> 00:53:38,470
Can we all go home? I think the answer is clearly no.

1085
00:53:38,470 --> 00:53:42,085
Uh, NMT definitely is not doing machine translation perfectly.

1086
00:53:42,085 --> 00:53:46,480
So, um, just to highlight some of the difficulties that remained with NMT.

1087
00:53:46,480 --> 00:53:48,610
Uh, one is out-of-vocabulary words.

1088
00:53:48,610 --> 00:53:52,090
Uh, this is a kind of basic problem but it is pretty tricky.

1089
00:53:52,090 --> 00:53:54,445
You know, what do you do if you're trying to translate

1090
00:53:54,445 --> 00:53:57,610
a sentence that contains a word that is not in your source vocabulary,

1091
00:53:57,610 --> 00:54:00,925
or what if you're trying to produce a word that's not in your target vocabulary?

1092
00:54:00,925 --> 00:54:03,625
Um, there's certainly been lots of work on doing this,

1093
00:54:03,625 --> 00:54:05,920
and you're going to hear later in the class how you

1094
00:54:05,920 --> 00:54:08,305
might try to attack this with for example,

1095
00:54:08,305 --> 00:54:10,435
uh, sub-word modeling can make it easier.

1096
00:54:10,435 --> 00:54:13,435
Uh, but this is a significant problem.

1097
00:54:13,435 --> 00:54:15,775
Another one is domain mismatch.

1098
00:54:15,775 --> 00:54:20,170
So let's suppose that you train your machine translation system on a bunch of fairly,

1099
00:54:20,170 --> 00:54:21,760
uh, formal text, like let's say,

1100
00:54:21,760 --> 00:54:23,785
uh, Wikipedia or something like that.

1101
00:54:23,785 --> 00:54:26,560
Uh, but then you try to deploy it to translate

1102
00:54:26,560 --> 00:54:29,590
informal text, like people chatting on Twitter or something.

1103
00:54:29,590 --> 00:54:32,080
Then often, you'll find that it doesn't perform very well

1104
00:54:32,080 --> 00:54:34,480
on this different domain because you've got a domain mismatch.

1105
00:54:34,480 --> 00:54:37,060
Uh, so that's quite a big problem.

1106
00:54:37,060 --> 00:54:40,465
Another one is maintaining context over longer text.

1107
00:54:40,465 --> 00:54:43,120
So everything we've talked about so far has assumed that you were

1108
00:54:43,120 --> 00:54:45,805
just translating a single sentence to a single sentence,

1109
00:54:45,805 --> 00:54:48,655
and there's no other wider context.

1110
00:54:48,655 --> 00:54:50,560
Uh, but, you know if you want to use

1111
00:54:50,560 --> 00:54:54,640
a machine translation system to translate a whole news article and maybe even a book,

1112
00:54:54,640 --> 00:54:58,270
then you're probably going to want to use the context that came in

1113
00:54:58,270 --> 00:55:03,475
previous sentences in order to translate things correctly in the current sentence.

1114
00:55:03,475 --> 00:55:06,070
So, uh, this is an active area of research,

1115
00:55:06,070 --> 00:55:08,410
how can you get an NMT system to condition on

1116
00:55:08,410 --> 00:55:12,620
larger pieces of context without it becoming too expensive and so on?

1117
00:55:13,200 --> 00:55:16,600
Another difficulty is low-resource language pairs.

1118
00:55:16,600 --> 00:55:19,690
Um, everything we've talked about so far has assumed that you have

1119
00:55:19,690 --> 00:55:22,915
access to a very large parallel corpus, but what if you don't?

1120
00:55:22,915 --> 00:55:25,180
What if you are trying to translate to or from a language that

1121
00:55:25,180 --> 00:55:27,430
has relatively little text available,

1122
00:55:27,430 --> 00:55:29,380
um, online for example?

1123
00:55:29,380 --> 00:55:31,630
So that can be pretty difficult.

1124
00:55:31,630 --> 00:55:35,170
Here a few examples of machine translation screwing up,

1125
00:55:35,170 --> 00:55:37,525
uh, with specific errors.

1126
00:55:37,525 --> 00:55:43,195
So, here's an example of how common sense is really difficult for NMT systems.

1127
00:55:43,195 --> 00:55:45,640
On the left, we have the English phrase paper jam,

1128
00:55:45,640 --> 00:55:47,380
which means when your printer

1129
00:55:47,380 --> 00:55:50,830
gets jammed up with paper and it's all, uh, tangled inside.

1130
00:55:50,830 --> 00:55:52,255
And then on the right,

1131
00:55:52,255 --> 00:55:54,940
we have a very literal translation of that into Spanish,

1132
00:55:54,940 --> 00:55:56,395
and it's essentially saying jam,

1133
00:55:56,395 --> 00:55:58,420
edible jam made of paper,

1134
00:55:58,420 --> 00:56:01,240
which clearly isn't the right interpretation.

1135
00:56:01,240 --> 00:56:04,060
So here, we have an NMT system that's just doing

1136
00:56:04,060 --> 00:56:07,120
very literal translation and clearly doesn't have any notion of common sense.

1137
00:56:07,120 --> 00:56:11,365
You can't make jam from paper. Uh, here's another example.

1138
00:56:11,365 --> 00:56:14,350
NMT can pick up biases in the training data.

1139
00:56:14,350 --> 00:56:16,585
We already talked about this at the,

1140
00:56:16,585 --> 00:56:18,310
uh, the word embedding level,

1141
00:56:18,310 --> 00:56:19,885
the representation of words.

1142
00:56:19,885 --> 00:56:22,255
Uh, but it can also be a problem at the you know,

1143
00:56:22,255 --> 00:56:24,175
the sentence level when you're translating things.

1144
00:56:24,175 --> 00:56:26,170
So here in this example,

1145
00:56:26,170 --> 00:56:27,340
uh, on the left,

1146
00:56:27,340 --> 00:56:30,460
we have two sentences in Malay that roughly mean,

1147
00:56:30,460 --> 00:56:34,090
uh, they work as a nurse, and they work as a programmer.

1148
00:56:34,090 --> 00:56:35,350
The point is on the left,

1149
00:56:35,350 --> 00:56:38,260
there is no information about gender in the pronouns.

1150
00:56:38,260 --> 00:56:40,645
But when it gets translated to English,

1151
00:56:40,645 --> 00:56:43,150
then we've suddenly got gender coming out of nowhere,

1152
00:56:43,150 --> 00:56:46,105
she works as a nurse, and he works as a programmer.

1153
00:56:46,105 --> 00:56:48,430
This is likely happening because in our training data,

1154
00:56:48,430 --> 00:56:52,090
we had more examples of female nurses and male programmers.

1155
00:56:52,090 --> 00:56:55,600
So you can understand why from a machine learning, uh,

1156
00:56:55,600 --> 00:56:57,880
maximizing the objective point of view the,

1157
00:56:57,880 --> 00:56:59,995
uh, English language model has learned to do that.

1158
00:56:59,995 --> 00:57:02,530
But the problem here is this isn't good machine translation.

1159
00:57:02,530 --> 00:57:08,590
Uh, here the system is making up information that was not present in the source sentence.

1160
00:57:08,590 --> 00:57:10,720
So this is certainly an error that

1161
00:57:10,720 --> 00:57:13,780
the machine translation shouldn't be doing because it's just simply inaccurate.

1162
00:57:13,780 --> 00:57:17,575
And even worse, it's propagating, uh, gender roles.

1163
00:57:17,575 --> 00:57:20,860
Here's another pretty weird example.

1164
00:57:20,860 --> 00:57:29,680
[LAUGHTER] What is happening here?

1165
00:57:29,680 --> 00:57:30,880
Uh, on the left,

1166
00:57:30,880 --> 00:57:33,100
we have a nonsense sentence,

1167
00:57:33,100 --> 00:57:35,970
this is just kind of a syllable repeated.

1168
00:57:35,970 --> 00:57:38,610
And we're supposedly translating from Somali.

1169
00:57:38,610 --> 00:57:40,590
Uh, and then we're asking to translate this into

1170
00:57:40,590 --> 00:57:43,350
English and then we're getting this out of nowhere.

1171
00:57:43,350 --> 00:57:46,665
Um, as the name of the Lord was written in the Hebrew language,

1172
00:57:46,665 --> 00:57:48,675
it was written in the language of the Hebrew nation,

1173
00:57:48,675 --> 00:57:51,720
and you might be thinking "Where on earth did that come from?"

1174
00:57:51,720 --> 00:57:54,675
And in fact, this got reported in the media as you know,

1175
00:57:54,675 --> 00:57:57,390
Google Translate wants to convert you to its religion or whatever.

1176
00:57:57,390 --> 00:58:00,960
[LAUGHTER] Um, so for sure,

1177
00:58:00,960 --> 00:58:02,175
it is very startling.

1178
00:58:02,175 --> 00:58:05,860
But the thing is there's actually quite a reasonable explanation.

1179
00:58:05,860 --> 00:58:08,830
So what's going on here is that,

1180
00:58:08,830 --> 00:58:12,175
um, often for low resource languages,

1181
00:58:12,175 --> 00:58:14,095
such as for example Somali, um,

1182
00:58:14,095 --> 00:58:19,420
one of the best resources of parallel text is the Bible.

1183
00:58:19,420 --> 00:58:24,040
So you train for example Somali to English using the Bible as a training text,

1184
00:58:24,040 --> 00:58:25,615
maybe among other texts.

1185
00:58:25,615 --> 00:58:27,520
Okay, that's the first puzzle piece.

1186
00:58:27,520 --> 00:58:30,460
But the other puzzle piece is the nonsensical input.

1187
00:58:30,460 --> 00:58:35,020
So when the input isn't really Somali or any kind of text, right?

1188
00:58:35,020 --> 00:58:36,895
It's just the same syllable over and over.

1189
00:58:36,895 --> 00:58:40,615
Then the NMT system doesn't really have anything sensible to condition on.

1190
00:58:40,615 --> 00:58:42,595
Its basically nonsense, it's just noise.

1191
00:58:42,595 --> 00:58:44,545
So what does the NMT system do?

1192
00:58:44,545 --> 00:58:48,100
Right? It can't really use, it can't really  condition on the source sentence.

1193
00:58:48,100 --> 00:58:51,475
So what it does, is it just uses the English language model, right?

1194
00:58:51,475 --> 00:58:54,460
You can think of it as like the English language model of the decoder RNN

1195
00:58:54,460 --> 00:58:57,835
just kind of goes into autopilot and starts generating random text,

1196
00:58:57,835 --> 00:59:00,760
kind of like we saw last week when we saw, uh,

1197
00:59:00,760 --> 00:59:02,380
a language model trained on Obama's speeches or

1198
00:59:02,380 --> 00:59:04,720
Harry Potter would just generate texts in that style.

1199
00:59:04,720 --> 00:59:06,760
That's kind of what's happening here with the Bible,

1200
00:59:06,760 --> 00:59:08,740
because we don't have any useful information,

1201
00:59:08,740 --> 00:59:11,320
um, from the sentence on the left.

1202
00:59:11,320 --> 00:59:15,700
Um, so, this is an example why, uh,

1203
00:59:15,700 --> 00:59:18,730
neural machine translation in particular makes these kinds of errors,

1204
00:59:18,730 --> 00:59:20,890
uh, because the system is uninterpretable.

1205
00:59:20,890 --> 00:59:23,665
So you don't know that this is going to happen until it happens,

1206
00:59:23,665 --> 00:59:25,450
and perhaps Google didn't know this was going to

1207
00:59:25,450 --> 00:59:27,640
happen until it happened and it got reported.

1208
00:59:27,640 --> 00:59:32,410
Um, so this is one downside of uninterpretability is that really weird effects can

1209
00:59:32,410 --> 00:59:34,690
happen and you don't see them coming and it's not

1210
00:59:34,690 --> 00:59:37,210
always even easy to explain why they happened. Yeah?

1211
00:59:37,210 --> 00:59:38,830
[inaudible].

1212
00:59:38,830 --> 00:59:47,530
Ah, the question is what happens if you did translate from Irish?

1213
00:59:47,530 --> 00:59:49,810
I suppose that's the part where Google tries to autodetect

1214
00:59:49,810 --> 00:59:53,050
the language, maybe it thinks that ag ag ag is more like Irish than Somali,

1215
00:59:53,050 --> 00:59:56,530
[LAUGHTER] I imagine if you did put Irish to English,

1216
00:59:56,530 --> 00:59:58,615
there's probably more, uh,

1217
00:59:58,615 --> 01:00:00,130
training data for Irish to English.

1218
01:00:00,130 --> 01:00:02,725
So maybe it wouldn't be so Bible-focused.

1219
01:00:02,725 --> 01:00:05,680
Um, yeah, and there's a lot of examples of these online where

1220
01:00:05,680 --> 01:00:09,260
you do different kinds of nonsense syllables in different languages.

1221
01:00:09,870 --> 01:00:14,035
So there's a lot of, uh, challenges remaining in NMT.

1222
01:00:14,035 --> 01:00:16,360
And, uh, the research continues.

1223
01:00:16,360 --> 01:00:21,370
So NMT, I think remains one of the flagship tasks for NLP Deep Learning.

1224
01:00:21,370 --> 01:00:24,400
In fact, NMT research has pioneered many of

1225
01:00:24,400 --> 01:00:27,205
the successful innovations of NLP Deep Learning in general.

1226
01:00:27,205 --> 01:00:30,400
Uh, so today in 2019, uh,

1227
01:00:30,400 --> 01:00:32,710
NMT research continues to thrive, there's still many,

1228
01:00:32,710 --> 01:00:35,530
many papers, uh, published all the time on NMT.

1229
01:00:35,530 --> 01:00:38,260
And in fact, uh, researchers have found lots of

1230
01:00:38,260 --> 01:00:41,770
improvements to the fairly vanilla seq2seq models that I've shown you today.

1231
01:00:41,770 --> 01:00:43,285
Uh, but in fact,

1232
01:00:43,285 --> 01:00:45,849
there is one improvement that is so integral

1233
01:00:45,849 --> 01:00:48,655
to seq2seq that you could regard it as the new vanilla.

1234
01:00:48,655 --> 01:00:50,785
And that's the improvement we're going to learn about today,

1235
01:00:50,785 --> 01:00:53,050
and it's called attention.

1236
01:00:53,050 --> 01:00:58,225
Okay. So section three is on attention. What is attention?

1237
01:00:58,225 --> 01:01:01,855
First, I'm going to motivate why we need this thing called attention.

1238
01:01:01,855 --> 01:01:05,440
So let's look at this diagram that we saw before of sequence-to-sequence.

1239
01:01:05,440 --> 01:01:07,705
And remember when we assumed that this,

1240
01:01:07,705 --> 01:01:09,370
uh, encoding of the source sentence,

1241
01:01:09,370 --> 01:01:13,315
the, t he one in the orange box is going to represent the whole sentence.

1242
01:01:13,315 --> 01:01:18,320
Uh, can anyone volunteer a problem you can see with this architecture?

1243
01:01:19,230 --> 01:01:23,530
In particular perhaps, a problem with this idea that that single vector is

1244
01:01:23,530 --> 01:01:27,100
the encoding of the source sentence. Yeah?

1245
01:01:27,100 --> 01:01:39,385
[inaudible]

1246
01:01:39,385 --> 01:01:41,650
Okay, so the answer is something like, um, you're only looking at

1247
01:01:41,650 --> 01:01:44,095
one word, you mean like the last word in the source sentence?

1248
01:01:44,095 --> 01:01:45,955
And you're not seeing more information.

1249
01:01:45,955 --> 01:01:47,710
Yeah some- it's, it's something like that.

1250
01:01:47,710 --> 01:01:49,045
Any other ideas? Yep.

1251
01:01:49,045 --> 01:01:50,905
We might have lost information in

1252
01:01:50,905 --> 01:01:53,575
the beginning of the sentence by the time you get to the end.

1253
01:01:53,575 --> 01:01:56,530
Yeah. You might have lost information from [NOISE] the beginning of the sentence by,

1254
01:01:56,530 --> 01:01:59,740
by the time you get to the end, especially if it was longer than four words.

1255
01:01:59,740 --> 01:02:01,561
Right. I think these are different ways of saying a

1256
01:02:01,561 --> 01:02:06,730
similar idea [NOISE] which is that we have a kind of informational bottleneck.

1257
01:02:06,730 --> 01:02:10,600
Uh, we're forcing all of the information about the source sentence to be captured

1258
01:02:10,600 --> 01:02:14,675
in this single vector because that's the only thing that gets given to the decoder.

1259
01:02:14,675 --> 01:02:17,265
If some information about source sentence isn't in our vector,

1260
01:02:17,265 --> 01:02:20,610
then there's no way the decoder is gonna be able to translate it correctly.

1261
01:02:20,610 --> 01:02:21,930
So this is the, yeah,

1262
01:02:21,930 --> 01:02:23,310
this is an informational bottleneck.

1263
01:02:23,310 --> 01:02:25,650
[NOISE] It's putting kind of too much pressure on

1264
01:02:25,650 --> 01:02:29,180
this single vector to be a good representation [NOISE] of the encoder.

1265
01:02:29,180 --> 01:02:32,290
So this is the motivation for attention.

1266
01:02:32,290 --> 01:02:36,445
Attention is a neural technique and it provides a solution to the bottleneck problem.

1267
01:02:36,445 --> 01:02:39,520
The core idea is that on each step [NOISE] of the decoder,

1268
01:02:39,520 --> 01:02:42,459
you're gonna use a direct connection to the encoder

1269
01:02:42,459 --> 01:02:46,520
to focus on a particular part of the source sequence.

1270
01:02:47,100 --> 01:02:50,560
So first I'm gonna show you what attention is

1271
01:02:50,560 --> 01:02:53,140
via a diagram so that's kind of an intuitive explanation.

1272
01:02:53,140 --> 01:02:55,405
And then I'm gonna show you the equations later.

1273
01:02:55,405 --> 01:02:59,455
So here's how seq- sequence-to-sequence with attention works.

1274
01:02:59,455 --> 01:03:01,960
So on the first step of our decoder,

1275
01:03:01,960 --> 01:03:05,095
uh, we have our first decoder hidden state.

1276
01:03:05,095 --> 01:03:08,320
So what we do, is we take the dot-product between

1277
01:03:08,320 --> 01:03:11,590
that decoder hidden state and the first [NOISE] encoder hidden state.

1278
01:03:11,590 --> 01:03:13,090
And then we get something called

1279
01:03:13,090 --> 01:03:16,570
an attention score which I'm representing by a dot. So that's a scalar.

1280
01:03:16,570 --> 01:03:18,070
[NOISE] And in fact,

1281
01:03:18,070 --> 01:03:20,305
we take the dot-product between the decoder hidden state

1282
01:03:20,305 --> 01:03:23,050
and all of the encoder hidden states.

1283
01:03:23,050 --> 01:03:27,550
So this means that we get one attention score or one scalar for each of these,

1284
01:03:27,550 --> 01:03:29,980
uh, source words effectively.

1285
01:03:29,980 --> 01:03:35,980
So next what we do, is we take those four number scores and we apply the softmax,

1286
01:03:35,980 --> 01:03:38,020
uh, distribution, uh, the softmax function to

1287
01:03:38,020 --> 01:03:41,155
them and then we get a probability distribution.

1288
01:03:41,155 --> 01:03:45,685
So here, I'm going to represent that probability distribution as a bar chart.

1289
01:03:45,685 --> 01:03:50,200
Um, and we call this the attention distribution and this one sums up to 1.

1290
01:03:50,200 --> 01:03:54,805
So here, you can see that most of the probability mass is on the first word.

1291
01:03:54,805 --> 01:03:58,300
And that kinda makes sense because our first word essentially means "he" and,

1292
01:03:58,300 --> 01:04:02,375
uh, were gonna be producing the word "he" first in our target sentence.

1293
01:04:02,375 --> 01:04:04,965
So once we've got this attention distribution,

1294
01:04:04,965 --> 01:04:11,295
uh, we're going to use it to produce something called the attention output.

1295
01:04:11,295 --> 01:04:15,400
So the idea is that the attention output is a weighted sum of

1296
01:04:15,400 --> 01:04:20,515
the encoder hidden states and the weighting is the attention distribution.

1297
01:04:20,515 --> 01:04:22,450
So I've got these dotted arrows that go from

1298
01:04:22,450 --> 01:04:24,400
the attention distribution to the attention output,

1299
01:04:24,400 --> 01:04:26,380
probably there should be dotted arrows also from

1300
01:04:26,380 --> 01:04:28,270
the encoder RNN but that's hard to depict.

1301
01:04:28,270 --> 01:04:32,215
[NOISE] But the idea is that you're summing up these encoder RNN, uh,

1302
01:04:32,215 --> 01:04:34,480
hidden states, [NOISE] but you're gonna weight each

1303
01:04:34,480 --> 01:04:37,555
one according to how much attention distribution it has on them.

1304
01:04:37,555 --> 01:04:41,620
So this means that your attention output which is a single vector is going to be

1305
01:04:41,620 --> 01:04:45,370
mostly containing information from the hidden states that had high attention.

1306
01:04:45,370 --> 01:04:49,730
In this case, it's gonna be mostly information from the first hidden state.

1307
01:04:52,230 --> 01:04:54,370
So after you do this,

1308
01:04:54,370 --> 01:04:58,390
you're going to use the attention output to influence your prediction of the next word.

1309
01:04:58,390 --> 01:05:00,520
So what you usually do is you concatenate

1310
01:05:00,520 --> 01:05:03,610
the attention output with your decoder hidden state and then,

1311
01:05:03,610 --> 01:05:06,070
uh, use that kind of concatenated pair in the way

1312
01:05:06,070 --> 01:05:08,905
you would have used the decoder [NOISE] hidden state alone before.

1313
01:05:08,905 --> 01:05:11,590
So that way you can get your probability distribution,

1314
01:05:11,590 --> 01:05:14,215
uh, y hat 1 of what's coming next.

1315
01:05:14,215 --> 01:05:17,170
So as before, we can use that to sample your next word.

1316
01:05:17,170 --> 01:05:19,360
[NOISE] So on the next step,

1317
01:05:19,360 --> 01:05:20,740
you just do the same thing again.

1318
01:05:20,740 --> 01:05:22,690
You've got your second decoder hidden state.

1319
01:05:22,690 --> 01:05:25,480
Again, you take dot-product with all of the encoder hidden states.

1320
01:05:25,480 --> 01:05:28,225
You take softmax over that to the get attention distribution.

1321
01:05:28,225 --> 01:05:30,685
And here, you can see the attention distribution is different.

1322
01:05:30,685 --> 01:05:33,595
We're putting more attention on, uh, the,

1323
01:05:33,595 --> 01:05:36,520
the word entart because we're about to produce the word hit.

1324
01:05:36,520 --> 01:05:38,590
Uh, but we're also attending a little bit to

1325
01:05:38,590 --> 01:05:42,295
the second word a because that's telling us that hit is a past tense.

1326
01:05:42,295 --> 01:05:46,495
So a cool thing that's happening here is we're getting [NOISE] a soft alignment.

1327
01:05:46,495 --> 01:05:49,555
If you remember when we looked at alignment in SMT systems,

1328
01:05:49,555 --> 01:05:50,665
it was mostly this, uh,

1329
01:05:50,665 --> 01:05:54,595
hard binary thing with on or off, either these words are aligned or they're not.

1330
01:05:54,595 --> 01:05:59,140
Here, you have a much more flexible soft notion of alignments where,

1331
01:05:59,140 --> 01:06:01,300
uh, each word kind of has a distribution over

1332
01:06:01,300 --> 01:06:04,135
the corresponding words in the source sentence.

1333
01:06:04,135 --> 01:06:06,805
So another thing to note kind of a side note,

1334
01:06:06,805 --> 01:06:08,440
is that sometimes, uh,

1335
01:06:08,440 --> 01:06:11,920
we take the attention output from the previous hidden state, uh,

1336
01:06:11,920 --> 01:06:16,495
and we kind of feed it into the decoder again along with the usual word.

1337
01:06:16,495 --> 01:06:19,510
So that would mean you take the attention output from the first step and kind of

1338
01:06:19,510 --> 01:06:23,080
concatenate it to the word vector for he and then use it in the decoder.

1339
01:06:23,080 --> 01:06:26,515
Uh, the reason for this is sometimes it's useful to have this, uh,

1340
01:06:26,515 --> 01:06:30,715
information from the, the attention on the previous step on the next step.

1341
01:06:30,715 --> 01:06:33,475
So I'm telling you this because this is something we do in Assignment 4

1342
01:06:33,475 --> 01:06:37,130
and it's a fairly common technique but also sometimes people don't do it.

1343
01:06:37,170 --> 01:06:40,390
Okay. So, um, the theory is,

1344
01:06:40,390 --> 01:06:42,010
that you just do this attention,

1345
01:06:42,010 --> 01:06:44,245
uh, computation on every step.

1346
01:06:44,245 --> 01:06:46,690
And on each step, you're going to be attending to different things.

1347
01:06:46,690 --> 01:06:48,850
So in our example on this third step,

1348
01:06:48,850 --> 01:06:51,730
we look at m' which means me when we

1349
01:06:51,730 --> 01:06:53,350
produce me and then on the last

1350
01:06:53,350 --> 01:06:55,105
three [NOISE] we're probably mostly just gonna be looking at this,

1351
01:06:55,105 --> 01:06:58,600
uh, fertile word entart to produce hit me with a pie.

1352
01:06:58,600 --> 01:07:02,005
[NOISE] I'm gonna keep going because we don't have a lot of time.

1353
01:07:02,005 --> 01:07:04,690
Uh, so here are the equations to describe attention.

1354
01:07:04,690 --> 01:07:06,520
Uh, I think it's probably easier to look at these in

1355
01:07:06,520 --> 01:07:09,025
your own time later rather than look at them in the lecture now.

1356
01:07:09,025 --> 01:07:10,870
But these are the equations that essentially

1357
01:07:10,870 --> 01:07:13,330
say the same thing as what the diagram just said.

1358
01:07:13,330 --> 01:07:17,110
So you have your encoder hidden states h_1 up to h_N.

1359
01:07:17,110 --> 01:07:19,795
And then on timestep t of the decoder,

1360
01:07:19,795 --> 01:07:22,855
we also have a decoder hidden state, uh, S_t.

1361
01:07:22,855 --> 01:07:27,100
So we're gonna get the attention score which we're gonna call et by taking

1362
01:07:27,100 --> 01:07:30,730
the dot-product of your decoder hidden state with each of the encoder hidden states.

1363
01:07:30,730 --> 01:07:32,425
[NOISE] And that gives you, uh,

1364
01:07:32,425 --> 01:07:34,900
a vector of same length as the, uh,

1365
01:07:34,900 --> 01:07:39,355
encoder [NOISE] sentence because you've got one score per source word.

1366
01:07:39,355 --> 01:07:42,580
Next you take softmax over these scores

1367
01:07:42,580 --> 01:07:45,280
to get attention distribution that sums up to 1,

1368
01:07:45,280 --> 01:07:47,635
and we call that alpha.

1369
01:07:47,635 --> 01:07:50,860
And then you use alpha to take a weighted sum of

1370
01:07:50,860 --> 01:07:54,535
the encoder hidden states and that gives you your attention output.

1371
01:07:54,535 --> 01:07:57,490
So [NOISE] the attention output which we call a is a vector that's

1372
01:07:57,490 --> 01:08:01,645
the same size as your encoder hidden states.

1373
01:08:01,645 --> 01:08:06,610
Lastly, you take your attention output a and then you, [NOISE] uh,

1374
01:08:06,610 --> 01:08:09,640
concatenate it with your decoder hidden states and then

1375
01:08:09,640 --> 01:08:13,940
proceed with that as you were taught before in the no attention model.

1376
01:08:14,520 --> 01:08:17,770
So attention, if it's not clear, it's pretty cool.

1377
01:08:17,770 --> 01:08:19,510
It has a number of advantages.

1378
01:08:19,510 --> 01:08:23,905
So one advantage is that attention just significantly improves NMT performance.

1379
01:08:23,905 --> 01:08:25,675
And the main reason why it improves it,

1380
01:08:25,675 --> 01:08:28,690
is because it turns out it's super useful to allow the decoder [NOISE] to

1381
01:08:28,690 --> 01:08:32,215
focus on certain parts of the source sentence when it's translating.

1382
01:08:32,215 --> 01:08:33,940
And you can see why this makes sense, right?

1383
01:08:33,940 --> 01:08:35,740
Because there's a very natural notion of alignment,

1384
01:08:35,740 --> 01:08:38,619
and if you can focus on the specific word or words you're translating,

1385
01:08:38,619 --> 01:08:40,585
you can probably do a better job.

1386
01:08:40,585 --> 01:08:44,410
Another reason why attention is cool is that [NOISE] it solves the bottleneck problem.

1387
01:08:44,410 --> 01:08:46,540
Uh, we were noting that the problem with having

1388
01:08:46,540 --> 01:08:49,870
a single vector that has to represent the entire source sentence [NOISE] and that's

1389
01:08:49,870 --> 01:08:52,780
the only way information can pass from encoder to decoder

1390
01:08:52,780 --> 01:08:56,590
means that if that encoding isn't very good then, uh, you're not gonna do well.

1391
01:08:56,590 --> 01:08:59,140
So by contrast in, uh, with attention,

1392
01:08:59,140 --> 01:09:01,690
the decoder can look directly at the encoder and

1393
01:09:01,690 --> 01:09:04,240
the source sentence and translate without the bottleneck.

1394
01:09:04,240 --> 01:09:07,060
[NOISE]

1395
01:09:07,060 --> 01:09:10,915
Another great thing about attention is that it helps with the vanishing gradient problem,

1396
01:09:10,915 --> 01:09:13,240
especially if your sentences are quite long.

1397
01:09:13,240 --> 01:09:16,060
Uh, the reason why attention helps is because you have

1398
01:09:16,060 --> 01:09:19,390
these direct connections between the decoder and the encoder,

1399
01:09:19,390 --> 01:09:21,145
kind of over many time steps.

1400
01:09:21,145 --> 01:09:22,825
So it's like a shortcut connection.

1401
01:09:22,825 --> 01:09:24,730
And just as we learned last time about, uh,

1402
01:09:24,730 --> 01:09:28,255
skip connections being [NOISE] useful for reducing vanishing gradient.

1403
01:09:28,255 --> 01:09:29,590
Here it's the same notion.

1404
01:09:29,590 --> 01:09:31,270
We have these, uh, long distance

1405
01:09:31,270 --> 01:09:34,580
[NOISE] direct connections that help the gradients flow better.

1406
01:09:34,620 --> 01:09:38,230
Another great thing about attention is it provides some interpretability.

1407
01:09:38,230 --> 01:09:41,155
Uh, if you look at the attention distribution,

1408
01:09:41,155 --> 01:09:42,880
often you've produced your translation.

1409
01:09:42,880 --> 01:09:46,360
Uh, you can see what the decoder was focusing on on each step.

1410
01:09:46,360 --> 01:09:48,820
So for example if we run our system and we translate our,

1411
01:09:48,820 --> 01:09:50,125
our running example here,

1412
01:09:50,125 --> 01:09:51,670
then we can produce a plot,

1413
01:09:51,670 --> 01:09:54,490
kind of like this that shows the attention distribution.

1414
01:09:54,490 --> 01:09:58,255
So here, dark means high attention and white means low attention.

1415
01:09:58,255 --> 01:10:00,295
So you might see something like this where,

1416
01:10:00,295 --> 01:10:03,550
um, it was, it was focusing on the different words and different steps.

1417
01:10:03,550 --> 01:10:06,160
And this is basically the same kind of

1418
01:10:06,160 --> 01:10:08,710
plot that we had earlier with a hard notion of alignment,

1419
01:10:08,710 --> 01:10:11,245
uh, in SNT except that we, uh,

1420
01:10:11,245 --> 01:10:14,380
we have more flexibility to have a more soft version of alignment

1421
01:10:14,380 --> 01:10:17,995
like for example when we produce the English word hit,

1422
01:10:17,995 --> 01:10:22,400
perhaps we were mostly looking at entarte, but we're also looking at a little bit of A.

1423
01:10:22,880 --> 01:10:25,980
So this, uh, means that we're getting,

1424
01:10:25,980 --> 01:10:27,315
uh, alignment for free.

1425
01:10:27,315 --> 01:10:31,424
And the reason I say for free is because when you remember the SNT systems,

1426
01:10:31,424 --> 01:10:33,210
the whole point there is that you had to learn

1427
01:10:33,210 --> 01:10:36,580
an alignment system deliberately and separately.

1428
01:10:36,580 --> 01:10:38,410
You had to define the notion of alignment,

1429
01:10:38,410 --> 01:10:40,060
you had to define the model of calculating,

1430
01:10:40,060 --> 01:10:42,865
what the probability of different alignments were and train it.

1431
01:10:42,865 --> 01:10:47,035
Whereas here, we never told the NMT system about alignments.

1432
01:10:47,035 --> 01:10:49,105
We never explicitly trained an alignment system.

1433
01:10:49,105 --> 01:10:52,225
We never had a loss function that tells you how good your alignment was.

1434
01:10:52,225 --> 01:10:55,600
We just gave the NMT system the apparatus to

1435
01:10:55,600 --> 01:10:59,245
do something like alignments and told it to maximize the,

1436
01:10:59,245 --> 01:11:02,380
uh, the cross-entropy loss for doing machine translation.

1437
01:11:02,380 --> 01:11:05,560
And then the network just learned alignment by itself.

1438
01:11:05,560 --> 01:11:07,900
I think this is the coolest thing about attention,

1439
01:11:07,900 --> 01:11:12,410
is that it's learned some structure in a somewhat unsupervised way.

1440
01:11:13,290 --> 01:11:15,520
Okay, so in the last few minutes,

1441
01:11:15,520 --> 01:11:18,340
I'm just going to, uh, generalize the notion of attention.

1442
01:11:18,340 --> 01:11:21,280
Because it turns out that attention is actually a very general, uh,

1443
01:11:21,280 --> 01:11:25,315
deep learning technique that you can apply in lots of different circumstances.

1444
01:11:25,315 --> 01:11:27,610
So you've seen that attention is a great way to improve

1445
01:11:27,610 --> 01:11:29,875
the sequence-to-sequence model for MT,

1446
01:11:29,875 --> 01:11:32,710
but you can actually use attention for other architectures that aren't

1447
01:11:32,710 --> 01:11:36,220
seq2seq and also tasks that aren't MT.

1448
01:11:36,220 --> 01:11:37,975
So to understand this,

1449
01:11:37,975 --> 01:11:42,085
I'm going to somewhat redefine attention to a more general definition.

1450
01:11:42,085 --> 01:11:44,185
So here's our more general definition.

1451
01:11:44,185 --> 01:11:46,885
Suppose you have a set of values,

1452
01:11:46,885 --> 01:11:48,190
each of which is a vector,

1453
01:11:48,190 --> 01:11:51,370
and you also have a single vector in which you're calling the query.

1454
01:11:51,370 --> 01:11:53,650
Then attention is a way, uh,

1455
01:11:53,650 --> 01:11:56,350
to compute a weighted sum of the values.

1456
01:11:56,350 --> 01:11:59,380
But the way you weight it is dependent on the query.

1457
01:11:59,380 --> 01:12:03,925
[NOISE] So we often phrase this,

1458
01:12:03,925 --> 01:12:06,655
uh, as saying that the query is attending to the values.

1459
01:12:06,655 --> 01:12:09,520
The idea being that you have all this information that's in the values and

1460
01:12:09,520 --> 01:12:13,810
the query is somehow determining how it's going to pay attention to the values.

1461
01:12:13,810 --> 01:12:16,345
So for example in seq2seq, uh,

1462
01:12:16,345 --> 01:12:19,435
the decoder hidden state is the query.

1463
01:12:19,435 --> 01:12:22,450
Uh the decoder hidden state on a particular time step is the query

1464
01:12:22,450 --> 01:12:27,050
and is attending to all the encoder hidden states which are the values.

1465
01:12:27,090 --> 01:12:29,515
All right, here's our definition again.

1466
01:12:29,515 --> 01:12:33,445
So here's a way to kind of understand this intuitively, two alternative ways.

1467
01:12:33,445 --> 01:12:35,530
One is to think of it like this.

1468
01:12:35,530 --> 01:12:38,290
You could think of it as the weighted sum is like

1469
01:12:38,290 --> 01:12:42,235
a selective summary of the information in the values.

1470
01:12:42,235 --> 01:12:45,760
And I say selective because your choice of how much you choose

1471
01:12:45,760 --> 01:12:49,345
to draw from each value depends on the attention distribution.

1472
01:12:49,345 --> 01:12:53,095
Uh, so the distribution, uh, depends on the query.

1473
01:12:53,095 --> 01:12:57,595
So the query is determining how much you're going to select from different, uh, values.

1474
01:12:57,595 --> 01:13:01,555
And this is kind of similar to LSTM that learned about earlier this week.

1475
01:13:01,555 --> 01:13:04,300
LSTMs rule based on the idea of a gate that, uh,

1476
01:13:04,300 --> 01:13:07,150
[NOISE] that defines how much information shou-

1477
01:13:07,150 --> 01:13:08,845
should [NOISE] come from different elements.

1478
01:13:08,845 --> 01:13:11,395
And the gate depends on the context.

1479
01:13:11,395 --> 01:13:14,710
So the strength of LSTMs came from the idea that based on the context,

1480
01:13:14,710 --> 01:13:17,185
you decide where you're going to draw information from.

1481
01:13:17,185 --> 01:13:19,435
And this is kind of like the same idea.

1482
01:13:19,435 --> 01:13:24,070
The second way to think about attention is you could say that it's a way to obtain

1483
01:13:24,070 --> 01:13:28,420
a fixed-size representation from an arbitrary set of representations.

1484
01:13:28,420 --> 01:13:29,965
So when I say arbitrary sets,

1485
01:13:29,965 --> 01:13:32,875
I'm saying we have this set of vectors called the values, right?

1486
01:13:32,875 --> 01:13:34,090
And you could have 10 values.

1487
01:13:34,090 --> 01:13:35,380
You could have 100 values.

1488
01:13:35,380 --> 01:13:36,400
You can have, uh,

1489
01:13:36,400 --> 01:13:38,305
any [NOISE] arbitrary number of these vectors.

1490
01:13:38,305 --> 01:13:42,460
But attention gives you a way to get a single vector,

1491
01:13:42,460 --> 01:13:48,350
um, summary of that which is the attention output, uh, using your query.

1492
01:13:48,420 --> 01:13:51,055
Okay, uh, so the last thing, uh,

1493
01:13:51,055 --> 01:13:53,710
is that there's actually several variants of

1494
01:13:53,710 --> 01:13:57,655
attention and this is something were are going to look at a little in Assignment 4.

1495
01:13:57,655 --> 01:13:59,920
So in our more general setting,

1496
01:13:59,920 --> 01:14:02,290
we've seen that we have some values in the query.

1497
01:14:02,290 --> 01:14:06,265
Doing attention always involves computing the attention [NOISE] scores,

1498
01:14:06,265 --> 01:14:09,880
and then you apply softmax to get the attention distribution.

1499
01:14:09,880 --> 01:14:13,870
And then you use that attention distribution to take a weighted sum.

1500
01:14:13,870 --> 01:14:17,240
So this is, uh, always the outline of how attention works.

1501
01:14:17,240 --> 01:14:19,920
The part that can be different is this, uh, number one.

1502
01:14:19,920 --> 01:14:23,520
There are multiple ways you can compute the scores.

1503
01:14:23,520 --> 01:14:27,135
So, uh, last slide,

1504
01:14:27,135 --> 01:14:30,035
here all the different ways you can repeat the scores.

1505
01:14:30,035 --> 01:14:34,615
So the first one which you've already seen today is basic dot-product attention.

1506
01:14:34,615 --> 01:14:38,575
And the idea here is that [NOISE] the score for a particular, a particular value,

1507
01:14:38,575 --> 01:14:42,850
HI, is just the dot-product of the query and that particular value.

1508
01:14:42,850 --> 01:14:46,720
[NOISE] And, uh, in particular this assumes that the size

1509
01:14:46,720 --> 01:14:48,190
of your query vector and the size of

1510
01:14:48,190 --> 01:14:50,740
your value vector has to be the same because you're taking dot-product.

1511
01:14:50,740 --> 01:14:54,865
[NOISE] Another, uh, version of,

1512
01:14:54,865 --> 01:14:57,685
uh, attention is called multiplicative attention.

1513
01:14:57,685 --> 01:15:00,385
And here, the idea is that the score of your, uh,

1514
01:15:00,385 --> 01:15:03,220
value HI, is going to be this, uh,

1515
01:15:03,220 --> 01:15:06,625
bi-linear function of your query and that value.

1516
01:15:06,625 --> 01:15:08,500
So in particular, we're pushing this weight matrix in

1517
01:15:08,500 --> 01:15:10,690
the middle and that's a learnable parameter.

1518
01:15:10,690 --> 01:15:14,650
You're learning the best way matric- ma- weight matrix in order to get the scores,

1519
01:15:14,650 --> 01:15:16,885
the attention scores that are useful.

1520
01:15:16,885 --> 01:15:19,855
The last one is called additive attention.

1521
01:15:19,855 --> 01:15:26,215
So what's happening here is that the score of the value HI is, uh,

1522
01:15:26,215 --> 01:15:28,435
you get it by applying

1523
01:15:28,435 --> 01:15:33,400
a linear transformation to both the value and the query and then you add them together.

1524
01:15:33,400 --> 01:15:35,875
And then you put them through a non-linearity like tanh.

1525
01:15:35,875 --> 01:15:37,390
And then lastly, uh,

1526
01:15:37,390 --> 01:15:40,270
you take that vector and you take the dot-product with

1527
01:15:40,270 --> 01:15:43,720
a weight vector to give you a single number that is the score.

1528
01:15:43,720 --> 01:15:46,450
[NOISE] So here, you've got

1529
01:15:46,450 --> 01:15:47,980
two different weight matrices and [NOISE]

1530
01:15:47,980 --> 01:15:51,190
also a weight vector which are the learnable parameters.

1531
01:15:51,190 --> 01:15:55,765
One thing that's different here is that there's kind of an additional hyperparameter,

1532
01:15:55,765 --> 01:15:58,000
which is the attention dimensionality.

1533
01:15:58,000 --> 01:16:00,295
So [NOISE] that's kind of, uh, the,

1534
01:16:00,295 --> 01:16:04,810
I think it's the heights of the W1 and W2 and this is the length of V, right?

1535
01:16:04,810 --> 01:16:07,420
You can choose what size that dimension is.

1536
01:16:07,420 --> 01:16:10,225
It's kind of like a hidden layer in the computation.

1537
01:16:10,225 --> 01:16:14,995
So, um, you can decide how big you want that intermediate representation to be.

1538
01:16:14,995 --> 01:16:17,470
Okay, so I'm not going to tell you any more about that because that's

1539
01:16:17,470 --> 01:16:19,510
actually one of the questions in the assignment, uh,

1540
01:16:19,510 --> 01:16:20,860
Assignment 4 is to think about

1541
01:16:20,860 --> 01:16:24,010
the relative advantages and disadvantages of these models.

1542
01:16:24,010 --> 01:16:25,645
[NOISE] Okay.

1543
01:16:25,645 --> 01:16:26,890
So here's a summary of today.

1544
01:16:26,890 --> 01:16:28,150
[NOISE] It really is the last slide,

1545
01:16:28,150 --> 01:16:29,800
[BACKGROUND] second last, last time,

1546
01:16:29,800 --> 01:16:30,820
but this was the last slide.

1547
01:16:30,820 --> 01:16:33,010
[BACKGROUND] So we learned about the history of MT.

1548
01:16:33,010 --> 01:16:37,960
[NOISE] We learned about how in 2014 [NOISE] Neural MT revolutionized MT.

1549
01:16:37,960 --> 01:16:41,005
[NOISE] We learned about how sequence-to-sequence

1550
01:16:41,005 --> 01:16:44,395
is the right architecture for NMT and it uses two RNNs.

1551
01:16:44,395 --> 01:16:47,080
And lastly, we learned about how attention [NOISE] is a way to focus

1552
01:16:47,080 --> 01:16:50,660
on particular parts of the input. All right, thanks.

