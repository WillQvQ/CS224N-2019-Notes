1
00:00:04,790 --> 00:00:11,940
Okay. So great to see everyone back for lecture four of the class.

2
00:00:11,940 --> 00:00:14,160
Um, so, for lec,

3
00:00:14,160 --> 00:00:16,500
for today's lecture, um,

4
00:00:16,500 --> 00:00:20,040
what I want to do for most of the time is actually

5
00:00:20,040 --> 00:00:23,730
get into the heart of these ideas of having

6
00:00:23,730 --> 00:00:28,290
the backpropagation algorithm for neural nets and how we can construct

7
00:00:28,290 --> 00:00:33,690
computation graphs that allow sufficiently to do backpropagation,

8
00:00:33,690 --> 00:00:36,195
neural nets to train the neural nets.

9
00:00:36,195 --> 00:00:41,330
So, overall, um, this is sort of what I plan to do it today.

10
00:00:41,330 --> 00:00:43,160
So, at the end of last lecture,

11
00:00:43,160 --> 00:00:47,870
I slightly ran out of time and I started mumbling and waving my hands about the,

12
00:00:47,870 --> 00:00:51,620
um, doing the derivatives with respect to the weight gradients.

13
00:00:51,620 --> 00:00:54,265
So, I kinda of wanted to do that but again.

14
00:00:54,265 --> 00:00:57,665
So hopefully it actually communicates slightly better.

15
00:00:57,665 --> 00:01:04,070
So, we'll do that and talk a bit more about sort of just tips for doing matrix gradients,

16
00:01:04,070 --> 00:01:07,835
um, and a particular issue that comes up with word vectors.

17
00:01:07,835 --> 00:01:10,010
And so then the main part of the class,

18
00:01:10,010 --> 00:01:12,619
we'll be talking about the backpropagation algorithm

19
00:01:12,619 --> 00:01:15,770
and how it runs over computation graphs.

20
00:01:15,770 --> 00:01:18,870
Um, and then for the last part of the class,

21
00:01:18,870 --> 00:01:22,380
um, is I'm not going to hide that, um,

22
00:01:22,380 --> 00:01:26,750
this is sort of just a grab bag of miscellaneous stuff you should

23
00:01:26,750 --> 00:01:32,005
know about neural networks and training neural networks.

24
00:01:32,005 --> 00:01:33,940
Um, like, I think,

25
00:01:33,940 --> 00:01:38,720
you know we dream of a future of artificial intelligence where our machines are

26
00:01:38,720 --> 00:01:43,670
really intelligent and you can just say to them this is the data and this is my problem,

27
00:01:43,670 --> 00:01:46,850
go and train me a model and it might work.

28
00:01:46,850 --> 00:01:48,560
Um, and in some future world,

29
00:01:48,560 --> 00:01:50,720
that may be  [NOISE] that comes along.

30
00:01:50,720 --> 00:01:53,150
It's something that's certainly being actively

31
00:01:53,150 --> 00:01:56,840
researched at the moment under the topic of Auto ML.

32
00:01:56,840 --> 00:02:02,240
I guess the question is whether it turns out that Auto ML was a scalable solution or

33
00:02:02,240 --> 00:02:06,110
the climate change consequences of Auto ML techniques are

34
00:02:06,110 --> 00:02:11,000
sufficiently bad that someone actually decides that these much lower power,

35
00:02:11,000 --> 00:02:17,135
um, neural systems might actually be better still for doing some parts of the problem.

36
00:02:17,135 --> 00:02:20,240
But anyway, either way we're not really there yet.

37
00:02:20,240 --> 00:02:22,235
And the fact of the matter is,

38
00:02:22,235 --> 00:02:24,320
when you're training neural networks,

39
00:02:24,320 --> 00:02:27,860
there's just a whole bunch of stuff you have to know about

40
00:02:27,860 --> 00:02:31,565
initialization and nonlinearities and learning rates and so on.

41
00:02:31,565 --> 00:02:34,440
And, you know, when I taught this class

42
00:02:34,440 --> 00:02:41,410
last time I somehow thought that people would pick this up by osmosis.

43
00:02:41,410 --> 00:02:43,689
That if we gave starter,

44
00:02:43,689 --> 00:02:47,440
cut code to people and now start

45
00:02:47,440 --> 00:02:52,760
a code we initialized how matrices and we set our learning rates,

46
00:02:52,760 --> 00:02:58,445
that by osmosis people would understand that's what you have to do and do it.

47
00:02:58,445 --> 00:03:03,859
Um, it didn't really sort of teach in class the practical tips and tricks enough,

48
00:03:03,859 --> 00:03:05,840
but it was perfectly obvious that when we got to

49
00:03:05,840 --> 00:03:12,035
final project time that at least for quite a few people, osmosis hadn't worked.

50
00:03:12,035 --> 00:03:13,789
Um, so this time,

51
00:03:13,789 --> 00:03:16,625
[LAUGHTER] I'm at least wanting to spend a few minutes on that

52
00:03:16,625 --> 00:03:20,510
and at least point out some other things that are important.

53
00:03:20,510 --> 00:03:22,729
And, I mean just in general,

54
00:03:22,729 --> 00:03:27,330
you know the reality of 2018, deep learning, no,

55
00:03:27,330 --> 00:03:30,510
wait, it's 2019 now, 2019, um,

56
00:03:30,510 --> 00:03:34,430
deep learning, is deep learning is still kind of a craft.

57
00:03:34,430 --> 00:03:39,620
There's quite a bit you have to know of techniques of doing things that lead

58
00:03:39,620 --> 00:03:42,440
neural net training to work successfully as

59
00:03:42,440 --> 00:03:46,050
opposed to your models failing to work successfully.

60
00:03:46,050 --> 00:03:50,390
Okay. One final announcement and I go in to it.

61
00:03:50,390 --> 00:03:56,345
Um, so, we've sort of been doing some further working on Office,

62
00:03:56,345 --> 00:03:59,840
our placement and I guess there are sort of multiple issues which

63
00:03:59,840 --> 00:04:04,460
include the opportunities for local ICPD students without Stanford IDs.

64
00:04:04,460 --> 00:04:05,795
We have to, um,

65
00:04:05,795 --> 00:04:07,880
get, um, to office hours.

66
00:04:07,880 --> 00:04:10,670
So for the Thursday night office hour,

67
00:04:10,670 --> 00:04:12,830
um, that's after this class,

68
00:04:12,830 --> 00:04:14,655
if you'd like to go and talk about,

69
00:04:14,655 --> 00:04:16,705
um, the second homework, um,

70
00:04:16,705 --> 00:04:21,335
the Thursday night office hour is going to be in Thorton- Thornton 110.

71
00:04:21,335 --> 00:04:24,260
Um, now I didn't know where Thornton was.

72
00:04:24,260 --> 00:04:29,390
It made more sense to me when I translated that as that's the old terman annex,

73
00:04:29,390 --> 00:04:32,450
but that's probably just showing my age since probably none

74
00:04:32,450 --> 00:04:35,620
of you remember when they used to be a building called Terman.

75
00:04:35,620 --> 00:04:37,695
So that probably doesn't help you either.

76
00:04:37,695 --> 00:04:38,980
Um, but you know,

77
00:04:38,980 --> 00:04:40,460
if you're heading, right,

78
00:04:40,460 --> 00:04:42,230
I don't know which direction we're facing.

79
00:04:42,230 --> 00:04:44,855
If you're heading that way I guess

80
00:04:44,855 --> 00:04:49,520
and if you know where the Papua New Guinea Sculpture Garden is, um, the,

81
00:04:49,520 --> 00:04:54,650
the sort of open grassy area before you get to the Papua New Guinea Sculpture Garden,

82
00:04:54,650 --> 00:04:59,695
that's where Terman used to be and the building that still stands in there is Thornton.

83
00:04:59,695 --> 00:05:02,700
Um, Thornton 110 um tonight.

84
00:05:02,700 --> 00:05:04,349
I think it starts at 6:30,

85
00:05:04,349 --> 00:05:06,210
right? 6:30 to nine.

86
00:05:06,210 --> 00:05:11,120
Okay. Right. So, let me just finish off where we were last time.

87
00:05:11,120 --> 00:05:15,195
So remember we had this window of five words and then we're

88
00:05:15,195 --> 00:05:19,700
putting it through a neural net layer of C equals WX plus B,

89
00:05:19,700 --> 00:05:22,160
non-linearity of H equals F of X,

90
00:05:22,160 --> 00:05:23,480
and then we're, um,

91
00:05:23,480 --> 00:05:29,150
going to just get a score as to whether this has in its center [NOISE]

92
00:05:29,150 --> 00:05:31,955
named entity like Paris which is sort of

93
00:05:31,955 --> 00:05:35,090
taking this dot product of a vector times the hidden layer.

94
00:05:35,090 --> 00:05:36,774
So this was our model,

95
00:05:36,774 --> 00:05:41,120
and then we are wanting to work out partial derivatives of S with

96
00:05:41,120 --> 00:05:45,680
respect to all of our variables and we did various of the cases,

97
00:05:45,680 --> 00:05:48,110
but one we hadn't yet done is the weights,

98
00:05:48,110 --> 00:05:50,645
and the weight through all of this neural net layer here.

99
00:05:50,645 --> 00:05:53,285
Okay. So, chain rule, um,

100
00:05:53,285 --> 00:05:58,005
the partial of ds dw is DS times HD,

101
00:05:58,005 --> 00:06:01,695
um, dHDZ times DZ, DW.

102
00:06:01,695 --> 00:06:05,055
And well, if you remember last time,

103
00:06:05,055 --> 00:06:09,945
we had sort of done some computation of what those first two,

104
00:06:09,945 --> 00:06:12,125
um, partial derivatives were.

105
00:06:12,125 --> 00:06:14,780
And we could say that we could just call

106
00:06:14,780 --> 00:06:19,145
those delta which is our error signal coming from above.

107
00:06:19,145 --> 00:06:23,060
And that concept of having an error signal coming from above is

108
00:06:23,060 --> 00:06:24,920
something I'll get back to in the main part of

109
00:06:24,920 --> 00:06:27,020
the lecture and a sort of a central notion.

110
00:06:27,020 --> 00:06:29,630
But the bit we hadn't dealt with is this dz,

111
00:06:29,630 --> 00:06:35,540
dw and we started to look at that and I made the argument, um,

112
00:06:35,540 --> 00:06:39,050
based on our shape convention that the shape of

113
00:06:39,050 --> 00:06:42,320
that should be the same shape as our W matrix.

114
00:06:42,320 --> 00:06:43,835
So it should be, um,

115
00:06:43,835 --> 00:06:47,360
same in times M shape as this W matrix.

116
00:06:47,360 --> 00:06:55,590
So we want to work out the partial of Z by W which is the same as this,

117
00:06:55,590 --> 00:06:59,335
um, [NOISE] dwx plus b, dw.

118
00:06:59,335 --> 00:07:03,065
And so we want to work out what that derivative is.

119
00:07:03,065 --> 00:07:04,940
Um, and if that's not obvious,

120
00:07:04,940 --> 00:07:09,740
one way to think about it is to go back to this elements of the matrix

121
00:07:09,740 --> 00:07:14,830
and actually first off work it out element-wise and think out what it should be,

122
00:07:14,830 --> 00:07:17,795
and then once you've thought out what it should be, um,

123
00:07:17,795 --> 00:07:22,090
to rewrite it back in matrix form to give the compact answer.

124
00:07:22,090 --> 00:07:26,750
So what we have is we have the inputs here and a biased term

125
00:07:26,750 --> 00:07:31,730
and we're going to do the matrix multiply it this vector to produce these.

126
00:07:31,730 --> 00:07:34,400
And if you think about what's happening there,

127
00:07:34,400 --> 00:07:39,200
so we've got this matrix of weights and for a particular weight,

128
00:07:39,200 --> 00:07:44,885
a weight is first index is going to correspond to a position in

129
00:07:44,885 --> 00:07:48,950
the hidden layer and its second index is going to

130
00:07:48,950 --> 00:07:53,240
correspond to a position in the input vector.

131
00:07:53,240 --> 00:07:56,870
And one weight in the matrix ends up being

132
00:07:56,870 --> 00:08:01,100
part of what's used to compute one element of the hidden layer.

133
00:08:01,100 --> 00:08:04,939
So, the one element of the hidden layer you're taking, um,

134
00:08:04,939 --> 00:08:08,270
a row of the matrix and you're multiplying it by

135
00:08:08,270 --> 00:08:11,600
the components of this vector so they sum together when the bias

136
00:08:11,600 --> 00:08:15,050
is added on but one element of the matrix is sort of only being

137
00:08:15,050 --> 00:08:19,020
used in the computation between one element of the,

138
00:08:19,020 --> 00:08:22,150
um, important one element of the hidden vector.

139
00:08:22,150 --> 00:08:25,170
Okay. So, well, that means, um,

140
00:08:25,170 --> 00:08:31,040
if we're thinking about what's the partial derivative with respect to WIJ, well,

141
00:08:31,040 --> 00:08:38,535
it's only contributing to ZI and it's only,

142
00:08:38,535 --> 00:08:42,750
it's only doing anything with XJ.

143
00:08:42,750 --> 00:08:44,850
So, that we end up with,

144
00:08:44,850 --> 00:08:47,870
we're getting the partial with respect to WIJ,

145
00:08:47,870 --> 00:08:50,570
we can work that out with respect to,

146
00:08:50,570 --> 00:08:53,690
just to respect to ZI.

147
00:08:53,690 --> 00:08:57,080
And when we're going to look at this multiplication here,

148
00:08:57,080 --> 00:09:01,370
what we're ending up is this sort of sum of terms WIK times

149
00:09:01,370 --> 00:09:04,490
Xk where there's sort of weights in that row

150
00:09:04,490 --> 00:09:07,685
of the matrix going across the positions of the vector.

151
00:09:07,685 --> 00:09:17,105
So the only position in which WIJ is used is multiplying, um, by XJ.

152
00:09:17,105 --> 00:09:18,485
And at that point,

153
00:09:18,485 --> 00:09:20,570
what we have in terms of sort of,

154
00:09:20,570 --> 00:09:24,440
in our basic one variable doing a differentiation,

155
00:09:24,440 --> 00:09:27,035
this is just like we have 3x,

156
00:09:27,035 --> 00:09:30,260
um, and we say what's the derivative of 3x?

157
00:09:30,260 --> 00:09:31,700
Actually X is confusing,

158
00:09:31,700 --> 00:09:32,795
so I shouldn't say that.

159
00:09:32,795 --> 00:09:38,450
Is like we have three W and what's the derivative of three W with respect to W?

160
00:09:38,450 --> 00:09:39,890
It's three, right?

161
00:09:39,890 --> 00:09:44,660
So, that we've have a term here which is what would have been W,

162
00:09:44,660 --> 00:09:47,805
will be WIJ times XJ,

163
00:09:47,805 --> 00:09:51,725
and its derivative with respect to WIJ is just XJ.

164
00:09:51,725 --> 00:09:53,825
Does that makes sense?

165
00:09:53,825 --> 00:09:55,310
Everyone believe it?

166
00:09:55,310 --> 00:09:57,270
[NOISE] Fingers crossed.

167
00:09:57,270 --> 00:10:01,920
Okay. Um, so, so for one element of this matrix,

168
00:10:01,920 --> 00:10:04,290
we're just getting out XJ.

169
00:10:04,290 --> 00:10:05,715
And at that point,

170
00:10:05,715 --> 00:10:07,910
um, we say, um,

171
00:10:07,910 --> 00:10:14,060
well of course we want to know what the Jacobian is for the full matrix W. Well,

172
00:10:14,060 --> 00:10:15,410
if you start thinking about it,

173
00:10:15,410 --> 00:10:18,545
this argument applies to every cell.

174
00:10:18,545 --> 00:10:20,210
So, that for every,

175
00:10:20,210 --> 00:10:22,175
um, cell of, um,

176
00:10:22,175 --> 00:10:24,260
the Jacobian for W,

177
00:10:24,260 --> 00:10:26,900
um, it's going to be XJ.

178
00:10:26,900 --> 00:10:30,530
So, that means, um,

179
00:10:30,530 --> 00:10:34,850
we're just going to be able to make use of that in calculating our Jacobian.

180
00:10:34,850 --> 00:10:42,650
So, the derivative for a single WIJ is delta IXJ and that's true for all cells.

181
00:10:42,650 --> 00:10:48,430
So we wanted to have a matrix for our Jacobian which has delta I,

182
00:10:48,430 --> 00:10:51,560
um, XJ in every cell of it.

183
00:10:51,560 --> 00:10:55,850
And the way we can create that is by using an outer products.

184
00:10:55,850 --> 00:11:00,049
So, if we have a row vector of the deltas,

185
00:11:00,049 --> 00:11:03,325
the error signals from above and a column,

186
00:11:03,325 --> 00:11:05,795
right, I said that wrong, sorry.

187
00:11:05,795 --> 00:11:11,420
If we have a column of the delta error signals

188
00:11:11,420 --> 00:11:17,300
from above and we have a row of X transfers vectors,

189
00:11:17,300 --> 00:11:21,995
um, when we multiply those together we get the outer product

190
00:11:21,995 --> 00:11:26,840
and we get delta IXJ in each cell and that is our Jacobian answer,

191
00:11:26,840 --> 00:11:29,615
um, for working out,

192
00:11:29,615 --> 00:11:34,070
um, the delta S delta W that we started off with at the beginning.

193
00:11:34,070 --> 00:11:36,710
Okay. And this, um,

194
00:11:36,710 --> 00:11:40,100
and we get this form where it's a multiplication of

195
00:11:40,100 --> 00:11:44,855
an error signal from above and our computed local gradient signal.

196
00:11:44,855 --> 00:11:47,720
And that's the pattern that we're going to see over and over

197
00:11:47,720 --> 00:11:51,260
again and that will exploit and our computation graphs.

198
00:11:51,260 --> 00:11:53,315
Okay, all good?

199
00:11:53,315 --> 00:11:56,810
Okay. Um, so, here's just,

200
00:11:56,810 --> 00:12:01,070
um, here's homework two.

201
00:12:01,070 --> 00:12:03,110
You're meant to do some of this stuff.

202
00:12:03,110 --> 00:12:06,845
Um, here are just over a couple of collected tips,

203
00:12:06,845 --> 00:12:10,010
um, which I hope will help.

204
00:12:10,010 --> 00:12:13,480
I mean keeping here track of your variables and

205
00:12:13,480 --> 00:12:16,150
their dimensionality is really useful because we

206
00:12:16,150 --> 00:12:19,255
just can work out what the dimensionality of things should be.

207
00:12:19,255 --> 00:12:21,385
You're often kind of halfway there.

208
00:12:21,385 --> 00:12:24,190
I mean basically what you're doing is sort of

209
00:12:24,190 --> 00:12:27,100
applying the chain rule over and over again.

210
00:12:27,100 --> 00:12:28,800
It always looks like this.

211
00:12:28,800 --> 00:12:33,920
Um, but doing it in this sort of matrix calculus sense of the chain rule.

212
00:12:33,920 --> 00:12:37,610
Um, in the homework you have to do a softmax,

213
00:12:37,610 --> 00:12:39,410
which we haven't done in class.

214
00:12:39,410 --> 00:12:42,305
Um, something that I think you'll find useful,

215
00:12:42,305 --> 00:12:46,985
if you want to break apart the softmax is to consider two cases.

216
00:12:46,985 --> 00:12:51,650
One, the case is to when you're working it out for the correct class.

217
00:12:51,650 --> 00:12:56,795
And then, the other case is for all the other incorrect classes.

218
00:12:56,795 --> 00:12:58,505
Um, yeah.

219
00:12:58,505 --> 00:13:00,860
Um, in the the little derivation,

220
00:13:00,860 --> 00:13:03,035
I did before, I said well,

221
00:13:03,035 --> 00:13:05,360
let's work out an element-wise partial

222
00:13:05,360 --> 00:13:08,660
derivative because that should give me some sense of what's going on,

223
00:13:08,660 --> 00:13:09,860
what the answer is.

224
00:13:09,860 --> 00:13:12,230
I think that can be a really good thing to do

225
00:13:12,230 --> 00:13:14,780
if you're getting confused by matrix calculus.

226
00:13:14,780 --> 00:13:16,565
And I sort of,

227
00:13:16,565 --> 00:13:20,660
um, slightly skipped past another slide.

228
00:13:20,660 --> 00:13:22,295
Last time that was talking about

229
00:13:22,295 --> 00:13:25,400
the shape convention that I talked about it for a moment that

230
00:13:25,400 --> 00:13:31,550
for the homeworks you can work out your answer however you want,

231
00:13:31,550 --> 00:13:33,380
you can work it out in terms of;

232
00:13:33,380 --> 00:13:35,630
you know numerator ordered Jacobians,

233
00:13:35,630 --> 00:13:37,280
if that seems best to you.

234
00:13:37,280 --> 00:13:40,130
But we'd like you to give the final answer to

235
00:13:40,130 --> 00:13:43,730
your assignment questions following the shape convention.

236
00:13:43,730 --> 00:13:46,760
So, that the derivative should be shaped in

237
00:13:46,760 --> 00:13:50,825
a vector matrix in the same way as the variable,

238
00:13:50,825 --> 00:13:54,920
with respect to which you're working out your derivatives.

239
00:13:54,920 --> 00:14:00,320
Okay. Um, the last little bit for finishing up this example from last time,

240
00:14:00,320 --> 00:14:01,700
I want to say a little bit about,

241
00:14:01,700 --> 00:14:05,150
is what happens with words.

242
00:14:05,150 --> 00:14:08,300
And one answer is nothing different.

243
00:14:08,300 --> 00:14:12,785
But another answer is they are a little bit of a special case here because,

244
00:14:12,785 --> 00:14:16,610
you know, really we have a matrix of word vectors, right?

245
00:14:16,610 --> 00:14:19,535
We have a vector for each word.

246
00:14:19,535 --> 00:14:23,540
And so then you can think of that as sort of this matrix of word vectors,

247
00:14:23,540 --> 00:14:25,310
which row has a different word.

248
00:14:25,310 --> 00:14:28,760
But we're not actually kind of connecting up

249
00:14:28,760 --> 00:14:33,170
that matrix directly to our classifier system.

250
00:14:33,170 --> 00:14:37,190
Instead of that, what we're connect connecting up to the classifier system is

251
00:14:37,190 --> 00:14:41,630
this window and the window will have it in at five words.

252
00:14:41,630 --> 00:14:43,490
Most commonly they're different words.

253
00:14:43,490 --> 00:14:46,145
But you know occasionally the same word might appear,

254
00:14:46,145 --> 00:14:49,160
um, in two positions in that window.

255
00:14:49,160 --> 00:14:52,190
And so, we can nevertheless do

256
00:14:52,190 --> 00:14:57,349
exactly the same thing and continue our gradients down and say okay,

257
00:14:57,349 --> 00:14:59,435
um, let's work out, um,

258
00:14:59,435 --> 00:15:03,335
the gradients of this word window vector.

259
00:15:03,335 --> 00:15:09,755
And if, um, these are of dimension D we'll have this sort of 5-D, um, vector.

260
00:15:09,755 --> 00:15:13,025
But, you know then what do we do about it,

261
00:15:13,025 --> 00:15:15,140
and the answer of what we do about it.

262
00:15:15,140 --> 00:15:21,980
Is we can just sort of split this window vector into five pieces and say aha,

263
00:15:21,980 --> 00:15:25,190
we have five updates to word vectors.

264
00:15:25,190 --> 00:15:30,335
We're just going to go off and apply them to the word Vector Matrix.

265
00:15:30,335 --> 00:15:34,565
Um, and you know if we if the same word occurs twice,

266
00:15:34,565 --> 00:15:38,990
um, in that window we literally apply both of the updates.

267
00:15:38,990 --> 00:15:41,810
So, it gets updated twice or maybe

268
00:15:41,810 --> 00:15:44,810
actually you want to sum them first and then do the update once but yeah,

269
00:15:44,810 --> 00:15:46,955
that's a technical issue.

270
00:15:46,955 --> 00:15:53,345
Um, so what that actually means is that we're extremely sparsely

271
00:15:53,345 --> 00:15:57,180
updating the word Vector Matrix because most of

272
00:15:57,180 --> 00:16:01,655
the word Vector Matrix will be unchanged and just a few rows of that,

273
00:16:01,655 --> 00:16:03,440
um, will be being updated.

274
00:16:03,440 --> 00:16:07,880
And if- um, soon we're going to be here doing stuff with PyTorch

275
00:16:07,880 --> 00:16:11,990
Um, and if you poke around Pytorch it even has some special stuff.

276
00:16:11,990 --> 00:16:15,440
Um, look for things like Sparse SGD for meaning

277
00:16:15,440 --> 00:16:19,100
that you're sort of doing a very sparse updating like that.

278
00:16:19,100 --> 00:16:24,590
Um, but there's one other sort of interesting thing that you should know about.

279
00:16:24,590 --> 00:16:26,285
For a lot of um,

280
00:16:26,285 --> 00:16:29,600
things that you do is just what actually happens if we push

281
00:16:29,600 --> 00:16:33,380
down these gradients into our word vectors.

282
00:16:33,380 --> 00:16:35,645
Well, the idea is no,

283
00:16:35,645 --> 00:16:39,094
if we do that would be just like all other neural net learning,

284
00:16:39,094 --> 00:16:46,655
that we will sort of in principle say move the word vectors around in such a way

285
00:16:46,655 --> 00:16:50,134
as they're more useful in helping determine

286
00:16:50,134 --> 00:16:54,665
named entity classification in this case because that was our motivating example.

287
00:16:54,665 --> 00:16:59,150
Um, so you know it might for example learn that the word in is

288
00:16:59,150 --> 00:17:04,970
a very good indicator of a named entity fall or sorry the place name following.

289
00:17:04,970 --> 00:17:08,090
So, after n you often get London, Paris et cetera.

290
00:17:08,090 --> 00:17:11,030
Right, so it's sort of got a special behavior that

291
00:17:11,030 --> 00:17:14,360
other prepositions don't as being a good location indicator.

292
00:17:14,360 --> 00:17:16,040
And so, it could sort of um,

293
00:17:16,040 --> 00:17:19,940
move it's location around and say here are words that are

294
00:17:19,940 --> 00:17:26,135
good location indicators and therefore help our classifier work even better.

295
00:17:26,135 --> 00:17:30,020
So, in principle that's good and it's a good thing to do,

296
00:17:30,020 --> 00:17:34,100
to update word vectors to help you perform better on

297
00:17:34,100 --> 00:17:39,710
a supervised task such as this Named Entity Recognition classification.

298
00:17:39,710 --> 00:17:45,050
But, there's a catch which is that it doesn't always work actually.

299
00:17:45,050 --> 00:17:47,120
And so, why doesn't it always work?

300
00:17:47,120 --> 00:17:50,750
Well, suppose that we're training a classifier.

301
00:17:50,750 --> 00:17:56,360
Um, you know it could be the one I just did or a softmax or logistic regression.

302
00:17:56,360 --> 00:17:59,525
And we wanting to classify um,

303
00:17:59,525 --> 00:18:02,855
movie reviews sentiment for positive or negative.

304
00:18:02,855 --> 00:18:07,730
Well, you know if we have trained our word vectors,

305
00:18:07,730 --> 00:18:13,310
we've got some word vector space and maybe in the word vector space, um, TV,

306
00:18:13,310 --> 00:18:15,860
telly and television are all very close

307
00:18:15,860 --> 00:18:19,520
together because they mean basically the same thing.

308
00:18:19,520 --> 00:18:22,400
So, that's great, our word vectors are good.

309
00:18:22,400 --> 00:18:25,250
But, well suppose it was the case,

310
00:18:25,250 --> 00:18:28,820
that in our training data for our classifier.

311
00:18:28,820 --> 00:18:32,630
So, this is our training data for movie sentiment review.

312
00:18:32,630 --> 00:18:38,465
We had the word TV and telly but we didn't have the word television.

313
00:18:38,465 --> 00:18:40,400
Well, then what's going to happen,

314
00:18:40,400 --> 00:18:45,350
is well while we try and train our sentiment classifier,

315
00:18:45,350 --> 00:18:51,200
if we push gradient back down into the word vectors what's likely to happen

316
00:18:51,200 --> 00:18:58,040
is that it will move around the word vectors of the words we saw in the training data.

317
00:18:58,040 --> 00:19:01,070
But, necessarily television's not moving, right?

318
00:19:01,070 --> 00:19:05,090
Because we're only pushing gradient down to words that are in our training data.

319
00:19:05,090 --> 00:19:06,890
So, this word goes nowhere,

320
00:19:06,890 --> 00:19:09,740
so it just stays where it was all along.

321
00:19:09,740 --> 00:19:13,760
So, if the result of our training is words get moved around.

322
00:19:13,760 --> 00:19:17,525
So, here a good words for indicating negative sentiment, um,

323
00:19:17,525 --> 00:19:20,839
will actually if at test time,

324
00:19:20,839 --> 00:19:22,400
when we're running our model,

325
00:19:22,400 --> 00:19:25,385
if we evaluate on a sentence with television in it,

326
00:19:25,385 --> 00:19:27,620
it's actually going to give the wrong answer.

327
00:19:27,620 --> 00:19:32,510
Whereas if we haven't changed the word vectors at all and had just left

328
00:19:32,510 --> 00:19:37,610
them where our word embedding learning system put them.

329
00:19:37,610 --> 00:19:39,500
Then it would have said television,

330
00:19:39,500 --> 00:19:42,620
that's a word that means about the same as TV or telly.

331
00:19:42,620 --> 00:19:43,850
I should treat it the same and

332
00:19:43,850 --> 00:19:47,840
my sentiment classifier and it would actually do a better job.

333
00:19:47,840 --> 00:19:54,740
So, it's sort of two-sided whether you gain by training word vectors.

334
00:19:54,740 --> 00:19:58,025
And so, this is a summary um, that says;

335
00:19:58,025 --> 00:20:01,715
that it's two sided and practically what you should do.

336
00:20:01,715 --> 00:20:09,080
So, the first choice is G is a good idea to use pre-trained word vectors like

337
00:20:09,080 --> 00:20:12,575
the word2vec vectors that you used in assignment one or

338
00:20:12,575 --> 00:20:17,110
using the training methods that you're doing right now for homework two.

339
00:20:17,110 --> 00:20:20,830
And the answer that is almost always yes.

340
00:20:20,830 --> 00:20:24,970
And the reason for that is this word vector training methods are

341
00:20:24,970 --> 00:20:29,500
extremely easy to run on billions of words of texts.

342
00:20:29,500 --> 00:20:35,750
So, we you know train these models like [inaudible] on billions or tens of billions of words.

343
00:20:35,750 --> 00:20:38,315
And it's easy to do that for two reasons.

344
00:20:38,315 --> 00:20:41,960
Firstly, because the training algorithms are very simple, right?

345
00:20:41,960 --> 00:20:46,925
That um, the word2vec training algorithms skip grams very simple algorithm.

346
00:20:46,925 --> 00:20:50,900
Secondly; because we don't need any expensive resources,

347
00:20:50,900 --> 00:20:54,470
all or we need as a big pile of text documents and we can run it on them.

348
00:20:54,470 --> 00:20:56,870
So, really easy to run it on,

349
00:20:56,870 --> 00:20:59,360
you know five or 50 billion words.

350
00:20:59,360 --> 00:21:03,350
Whereas, you know, we can't do that for most of the classifiers that we

351
00:21:03,350 --> 00:21:04,760
want to build because if it's something

352
00:21:04,760 --> 00:21:07,670
I sentiment classifier or a named entity recognizer,

353
00:21:07,670 --> 00:21:10,280
we need labeled training data to train

354
00:21:10,280 --> 00:21:15,605
our classifier and then we ask someone how many words have labeled training data,

355
00:21:15,605 --> 00:21:18,800
do you have for named entity recognition and they give this back

356
00:21:18,800 --> 00:21:22,130
a number like 300,000 words or one million words, right.

357
00:21:22,130 --> 00:21:24,655
It's orders a magnitude smaller.

358
00:21:24,655 --> 00:21:27,554
Okay. Um. So, therefore,

359
00:21:27,554 --> 00:21:30,340
we can gain using pre-trained word vectors,

360
00:21:30,340 --> 00:21:32,795
because they know about all the words that aren't

361
00:21:32,795 --> 00:21:35,705
now supervised, classifies training data.

362
00:21:35,705 --> 00:21:38,180
And they also know much more about the words that actually

363
00:21:38,180 --> 00:21:40,960
are in the training data, but only rarely.

364
00:21:40,960 --> 00:21:42,750
So, the exception to that is,

365
00:21:42,750 --> 00:21:45,490
if you have hundreds of millions of words of data,

366
00:21:45,490 --> 00:21:50,115
then you can start off with random word vectors and go from there.

367
00:21:50,115 --> 00:21:52,775
And so, a case where this is actually commonly done,

368
00:21:52,775 --> 00:21:54,450
is for machine translation,

369
00:21:54,450 --> 00:21:56,115
which we do later in the class.

370
00:21:56,115 --> 00:21:58,415
It's relatively easy for

371
00:21:58,415 --> 00:22:03,335
large languages to get hundreds of millions of words of translated text.

372
00:22:03,335 --> 00:22:04,635
If you wanted to build something,

373
00:22:04,635 --> 00:22:09,510
like a German- English or Chinese-English machine translation system.

374
00:22:09,510 --> 00:22:14,320
Not hard to get 150 million words of translated texts.

375
00:22:14,320 --> 00:22:16,760
And so, that's sort of sufficiently much data,

376
00:22:16,760 --> 00:22:21,015
that people commonly just start with word vectors, um,

377
00:22:21,015 --> 00:22:24,300
being randomly initialized and start training,

378
00:22:24,300 --> 00:22:27,000
um, their translation system.

379
00:22:27,000 --> 00:22:29,855
Okay. So then the second question is, okay.

380
00:22:29,855 --> 00:22:32,150
I'm using pre-trained word vectors.

381
00:22:32,150 --> 00:22:35,780
Um, when I train my supervised classifier,

382
00:22:35,780 --> 00:22:40,285
should I push gradients down into the word vectors and up, and update them?

383
00:22:40,285 --> 00:22:44,325
Which is often referred to as fine tuning the word vectors, um,

384
00:22:44,325 --> 00:22:45,900
or should I not,

385
00:22:45,900 --> 00:22:47,670
should I just sort of throw away

386
00:22:47,670 --> 00:22:51,090
those gradients and not push them down into the word vectors?

387
00:22:51,090 --> 00:22:53,710
And you know, the answer to that is it depends,

388
00:22:53,710 --> 00:22:55,470
and it just depends on the size.

389
00:22:55,470 --> 00:23:01,345
So, if you only have a small training data set, um, typically,

390
00:23:01,345 --> 00:23:06,065
it's best to just treat the pre-trained word vectors as fixed,

391
00:23:06,065 --> 00:23:08,805
um, and not do any updating of them at all.

392
00:23:08,805 --> 00:23:11,270
If you have a large data set,

393
00:23:11,270 --> 00:23:16,620
then you can normally gain by doing fine tuning of the word vectors.

394
00:23:16,620 --> 00:23:17,910
And of course, the answer here,

395
00:23:17,910 --> 00:23:19,955
is what counts as large.

396
00:23:19,955 --> 00:23:21,850
Um, you know, if certainly,

397
00:23:21,850 --> 00:23:24,370
if you're down in the regime of 100 thousand words,

398
00:23:24,370 --> 00:23:27,105
a couple of hundred thousand words, you're small.

399
00:23:27,105 --> 00:23:29,860
If you're starting to be over a million words,

400
00:23:29,860 --> 00:23:31,020
then maybe you're large.

401
00:23:31,020 --> 00:23:34,265
But you know, on practice, people do it both ways and see which number is higher,

402
00:23:34,265 --> 00:23:36,290
and that's what they stick with.

403
00:23:36,290 --> 00:23:39,955
Um. Yes. Um, then, the sort of,

404
00:23:39,955 --> 00:23:44,755
there's the sort of point here that is just worth underlying is " Yes",

405
00:23:44,755 --> 00:23:51,980
so on principle, we can back-propagate this gradient to every variable in our model.

406
00:23:51,980 --> 00:23:56,650
Um, it's actually a theorem that we can arbitrarily

407
00:23:56,650 --> 00:24:02,360
decide to throw any subset of those gradients away,

408
00:24:02,360 --> 00:24:07,960
and we are still improving the log-likelihood of our model, all right?

409
00:24:07,960 --> 00:24:09,815
It kind of can't be inconsistent.

410
00:24:09,815 --> 00:24:12,240
You can just sort of pick some subset and say only

411
00:24:12,240 --> 00:24:14,980
train those 37 and throw away all the rest.

412
00:24:14,980 --> 00:24:17,389
And the algorithm will still improve,

413
00:24:17,389 --> 00:24:19,185
um, the log-likelihood of the model.

414
00:24:19,185 --> 00:24:22,355
Perhaps not by as much as if you trained the rest of the variables,

415
00:24:22,355 --> 00:24:24,280
as well, um, but yes,

416
00:24:24,280 --> 00:24:27,145
it can't actually do any harm not to train anything.

417
00:24:27,145 --> 00:24:32,315
Um, that's one of the reasons why often people don't notice bugs in their code, as well.

418
00:24:32,315 --> 00:24:34,800
It is because if your code is kind of broken

419
00:24:34,800 --> 00:24:37,550
and only half of the variables are being updated,

420
00:24:37,550 --> 00:24:40,650
it will still seem to be training something and improving.

421
00:24:40,650 --> 00:24:43,415
Um. It's just not doing as well as it could be doing,

422
00:24:43,415 --> 00:24:45,455
if you've coded correctly.

423
00:24:45,455 --> 00:24:49,185
Okay. Um, so, at this point, um,

424
00:24:49,185 --> 00:24:51,035
that's sort of, um,

425
00:24:51,035 --> 00:24:53,730
almost shown you back propagation, right?

426
00:24:53,730 --> 00:24:59,030
So, back-propagation is really taking derivatives with a generalized chain rule,

427
00:24:59,030 --> 00:25:03,420
with the one further trick which we sort of represented with that delta,

428
00:25:03,420 --> 00:25:06,860
which is G. You want to be, um,

429
00:25:06,860 --> 00:25:08,560
clever in doing this, so,

430
00:25:08,560 --> 00:25:12,995
you minimize computation by reusing shared stuff.

431
00:25:12,995 --> 00:25:16,620
Um, but now what I want to move on is to sort of look at how we can do

432
00:25:16,620 --> 00:25:19,890
that much more systematically, which is this idea.

433
00:25:19,890 --> 00:25:22,540
We have a computation graph and we're going to run

434
00:25:22,540 --> 00:25:26,380
a back-propagation algorithm through the computation graph.

435
00:25:27,170 --> 00:25:33,730
So, this is kind of like an abstracts syntax tree,

436
00:25:33,730 --> 00:25:37,085
expression tree that you might see in a compiler's class,

437
00:25:37,085 --> 00:25:38,595
or something like that, right?

438
00:25:38,595 --> 00:25:44,150
So, when we have an arithmetic expression of the kind that we're going to compute,

439
00:25:44,150 --> 00:25:48,565
we can make this tipped over on its side tree representation.

440
00:25:48,565 --> 00:25:50,990
So, we've got the X and W variables,

441
00:25:50,990 --> 00:25:52,740
we're going to multiply them.

442
00:25:52,740 --> 00:25:53,990
There's the B variable,

443
00:25:53,990 --> 00:25:56,470
we're going to add it to the previous partial result.

444
00:25:56,470 --> 00:25:59,185
We're going to stick it through our non-linearity F

445
00:25:59,185 --> 00:26:01,315
and then we're going to multiply it by U.

446
00:26:01,315 --> 00:26:03,005
And that was the computation,

447
00:26:03,005 --> 00:26:05,455
that we're doing in our neural network.

448
00:26:05,455 --> 00:26:08,790
So, um the source nodes or inputs,

449
00:26:08,790 --> 00:26:12,530
the interior nodes of this tree are operations.

450
00:26:12,530 --> 00:26:17,255
And then we've got these edges that pass along the results of our computation.

451
00:26:17,255 --> 00:26:20,955
And so, this is the computation graph for precisely the example

452
00:26:20,955 --> 00:26:25,330
I've been doing for the last lecture [NOISE].

453
00:26:25,330 --> 00:26:28,660
Okay, so there are two things that we want to be able to do.

454
00:26:28,660 --> 00:26:30,090
The first one is,

455
00:26:30,090 --> 00:26:34,315
we want to be able to start with these variables and do this computation,

456
00:26:34,315 --> 00:26:36,130
and calculate what S is.

457
00:26:36,130 --> 00:26:38,485
That's the part that's dead simple,

458
00:26:38,485 --> 00:26:41,815
that's referred to as forward propagation.

459
00:26:41,815 --> 00:26:45,800
So, forward propagation is just expression evaluation,

460
00:26:45,800 --> 00:26:48,870
as you do in any any programming in language interpreter.

461
00:26:48,870 --> 00:26:51,220
Um, that's not hard at all.

462
00:26:51,220 --> 00:26:54,390
Um, but the difference here is, "Hey,

463
00:26:54,390 --> 00:26:59,705
we want to do a learning algorithm" so we're going to do the opposite of that, as well.

464
00:26:59,705 --> 00:27:04,070
What we want to be able to do is also backward propagation,

465
00:27:04,070 --> 00:27:07,805
or back-propagation or just back-prop, it's commonly called,

466
00:27:07,805 --> 00:27:10,170
which is we want to be able to go,

467
00:27:10,170 --> 00:27:12,210
um, from the final part.

468
00:27:12,210 --> 00:27:14,190
The final part here.

469
00:27:14,190 --> 00:27:16,395
And then at each step,

470
00:27:16,395 --> 00:27:18,315
we want to be calculating

471
00:27:18,315 --> 00:27:22,680
these partial derivatives and passing them back through the graph.

472
00:27:22,680 --> 00:27:27,210
And so, this was sort of the notion before that we had an error signal, right?

473
00:27:27,210 --> 00:27:28,860
So, we're starting from up here,

474
00:27:28,860 --> 00:27:32,190
we've calculated a partial of S by Z,

475
00:27:32,190 --> 00:27:34,920
which is this with respect to that.

476
00:27:34,920 --> 00:27:38,735
And so, that's sort of our calculated error signal, up to here,

477
00:27:38,735 --> 00:27:41,940
and then we want to pass that further back, to start, um,

478
00:27:41,940 --> 00:27:46,010
computing, um, um, gradients further back.

479
00:27:46,010 --> 00:27:49,570
Right? And we started off, um, right here,

480
00:27:49,570 --> 00:27:54,560
with the partial of S by S. What's the partial of S by S going to be?

481
00:27:54,560 --> 00:27:57,040
One. Okay, yes.

482
00:27:57,040 --> 00:28:00,240
So, the rate at which S changes is the rate at which S changes.

483
00:28:00,240 --> 00:28:02,130
So, we just start off with one,

484
00:28:02,130 --> 00:28:07,565
and then we want to work out how this gradient changes as we go along.

485
00:28:07,565 --> 00:28:14,515
Um, so what we're doing here is when we're working out things for one node,

486
00:28:14,515 --> 00:28:18,815
that a node is going to have passed in towards it upstream gradient,

487
00:28:18,815 --> 00:28:20,465
which is its error signal.

488
00:28:20,465 --> 00:28:26,045
So, that's the partial of our final, f- final result,

489
00:28:26,045 --> 00:28:29,320
which was our loss, um, by um,

490
00:28:29,320 --> 00:28:32,970
the va- variable was the output of these computation nodes.

491
00:28:32,970 --> 00:28:35,315
So, that's the partial of S I H, here.

492
00:28:35,315 --> 00:28:39,340
And then, we did some operation here.

493
00:28:39,340 --> 00:28:42,800
Here's the non-linearity, but it might be something else.

494
00:28:42,800 --> 00:28:47,100
And so what we want to then work out is a downstream gradient,

495
00:28:47,100 --> 00:28:49,700
which is the partial of S by Z,

496
00:28:49,700 --> 00:28:51,825
which was the input to this function.

497
00:28:51,825 --> 00:28:53,320
And well then the question is,

498
00:28:53,320 --> 00:28:54,845
how do we do that?

499
00:28:54,845 --> 00:28:56,895
And the answer to that is,

500
00:28:56,895 --> 00:28:59,045
we use the chain rule, of course, right?

501
00:28:59,045 --> 00:29:02,760
So, at, we have a concept of a local gradients.

502
00:29:02,760 --> 00:29:06,425
So, here's H as the output,

503
00:29:06,425 --> 00:29:08,505
um, Z is the input.

504
00:29:08,505 --> 00:29:10,175
So, this function here,

505
00:29:10,175 --> 00:29:11,980
this is our non-linearity, right?

506
00:29:11,980 --> 00:29:14,825
So, this is whatever we're using as our non-linearity,

507
00:29:14,825 --> 00:29:19,095
like a logistic or T and H. We calculate H in terms of Z,

508
00:29:19,095 --> 00:29:21,690
and we can work out the partial of H by Z.

509
00:29:21,690 --> 00:29:23,440
So, that's our local gradient.

510
00:29:23,440 --> 00:29:28,370
And so then, if we have both the upstream gradient and the local gradient.

511
00:29:28,370 --> 00:29:32,825
We can then work out the downstream gradient because we know the

512
00:29:32,825 --> 00:29:38,880
partial of S by Z is going to be DSDH times, um, DHDZ.

513
00:29:38,880 --> 00:29:44,995
And so, then we'll be able to pass down the downstream gradient to the next node.

514
00:29:44,995 --> 00:29:47,275
Okay. So our basic rule,

515
00:29:47,275 --> 00:29:52,320
which is just the chain rule written in different terms

516
00:29:52,320 --> 00:29:58,010
is downstream gradient equals upstream gradient times local gradient.

517
00:29:58,010 --> 00:30:01,480
Um, easy as that,um, okay.

518
00:30:01,480 --> 00:30:03,435
So, this was um,

519
00:30:03,435 --> 00:30:09,510
the very simplest case where we have a node with one input and one output.

520
00:30:09,510 --> 00:30:11,230
So, that's a function um,

521
00:30:11,230 --> 00:30:13,040
like our logistic function.

522
00:30:13,040 --> 00:30:16,780
But, we also want to have things work out for general computation graphs.

523
00:30:16,780 --> 00:30:18,390
So, how are we going to do that?

524
00:30:18,390 --> 00:30:20,780
Well, the next case is,

525
00:30:20,780 --> 00:30:24,250
um, what about if we have multiple inputs?

526
00:30:24,250 --> 00:30:29,760
So, if we're calculating something like Z equals W times X.

527
00:30:29,760 --> 00:30:36,965
Um, where actually yes Z and X are themselves vectors and W um,

528
00:30:36,965 --> 00:30:42,305
is a matrix, but we're treating X as an input and W as an input,

529
00:30:42,305 --> 00:30:44,735
and Z as our output, right?

530
00:30:44,735 --> 00:30:47,405
We kind of group vectors and matrices together.

531
00:30:47,405 --> 00:30:51,350
Well, if you have multiple inputs,

532
00:30:51,350 --> 00:30:54,030
you then end up with multiple local gradients.

533
00:30:54,030 --> 00:30:55,640
So, you can work out um,

534
00:30:55,640 --> 00:30:57,860
the partial of Z with respect to X,

535
00:30:57,860 --> 00:31:01,645
or the partial of Z with respect to W. And so,

536
00:31:01,645 --> 00:31:05,405
you essentially you take the upstream gradient,

537
00:31:05,405 --> 00:31:09,155
you multiply it by each of the local gradients,

538
00:31:09,155 --> 00:31:12,340
and you pass it down the respective path,

539
00:31:12,340 --> 00:31:17,530
and we calculate these different downstream gradients to pass along.

540
00:31:17,530 --> 00:31:20,310
Is that making sense?

541
00:31:22,260 --> 00:31:25,930
Yeah. Okay. How chug.

542
00:31:25,930 --> 00:31:31,930
Okay. So, let's sort of look in an example of this and then we'll see one other case.

543
00:31:31,930 --> 00:31:34,420
So here's the little baby example.

544
00:31:34,420 --> 00:31:37,150
This isn't kind of really looking like a neural net,

545
00:31:37,150 --> 00:31:41,260
but we've got three inputs x, y, and z.

546
00:31:41,260 --> 00:31:45,895
And x and y get added together and y and z you get maxed.

547
00:31:45,895 --> 00:31:50,830
And then we take the results of those two operations and we multiply them together.

548
00:31:50,830 --> 00:31:57,340
So overall what we're calculating is x plus y times the max of y plus z.

549
00:31:57,340 --> 00:32:05,350
But, you know, we have here a general technique and we can apply it in any cases.

550
00:32:05,350 --> 00:32:09,895
Okay, so if we wanted to have this graph and we want to run it forward,

551
00:32:09,895 --> 00:32:13,270
well, we need to know the values of x, y, and z.

552
00:32:13,270 --> 00:32:19,180
So, for my example x equals one y equals two z equals zero.

553
00:32:19,180 --> 00:32:23,650
Um, so, we take the values of those variables and

554
00:32:23,650 --> 00:32:28,600
push them onto the calculations for the forward arrows.

555
00:32:28,600 --> 00:32:32,650
And then well the first thing we do is add and the result of that is three.

556
00:32:32,650 --> 00:32:34,300
And so we can put that onto the arrow.

557
00:32:34,300 --> 00:32:35,575
That's the output of add.

558
00:32:35,575 --> 00:32:39,730
Max it's two as the output of the value of add times is six.

559
00:32:39,730 --> 00:32:42,910
And so the forward pass we have evaluated the expression.

560
00:32:42,910 --> 00:32:44,710
Its value is six.

561
00:32:44,710 --> 00:32:48,700
That wasn't hard. Okay. So then the next step is we

562
00:32:48,700 --> 00:32:54,100
then want to run back-propagation to work out gradients.

563
00:32:54,100 --> 00:33:00,205
Um, and so we sort of want to know how to sort of,

564
00:33:00,205 --> 00:33:03,190
um work out these local gradients.

565
00:33:03,190 --> 00:33:10,570
So a is our right a is the result of sum.

566
00:33:10,570 --> 00:33:12,460
So here's a as the result of sum.

567
00:33:12,460 --> 00:33:15,040
So a equals x plus y.

568
00:33:15,040 --> 00:33:23,560
So if you're taking da dx that's just one and d a d y is also one that makes sense.

569
00:33:23,560 --> 00:33:28,030
Um, the max is slightly trickier because where

570
00:33:28,030 --> 00:33:33,610
there's some slopes and gradient for the max depends on which one's bigger.

571
00:33:33,610 --> 00:33:37,930
So, if y is bigger than z d- delta,

572
00:33:37,930 --> 00:33:40,299
the partial of b by z,

573
00:33:40,299 --> 00:33:49,030
plus partial b by y is one otherwise it's 0 and conversely for the partial of b by z.

574
00:33:49,030 --> 00:33:52,315
So that one's a little bit dependent.

575
00:33:52,315 --> 00:33:56,410
And then we do the multiplication, um,

576
00:33:56,410 --> 00:33:58,900
case at the end, um,

577
00:33:58,900 --> 00:34:04,495
and work out its partials with respect to a and b.

578
00:34:04,495 --> 00:34:09,520
And, um, since that's a and b which has the values two and three.

579
00:34:09,520 --> 00:34:14,725
If you're taking the partial of f by a it equals b which is two and vice versa.

580
00:34:14,725 --> 00:34:19,645
Okay. So that means we can work out the local gradients at each node.

581
00:34:19,645 --> 00:34:22,690
And so then we want to use those to

582
00:34:22,690 --> 00:34:26,590
calculate our gradients backwards and the back-propagation paths.

583
00:34:26,590 --> 00:34:28,165
So we start at the top.

584
00:34:28,165 --> 00:34:31,030
The partial of f with respect to F is one.

585
00:34:31,030 --> 00:34:37,375
Because if you move if you know by a tenth then you've moved the f by a tenth.

586
00:34:37,375 --> 00:34:39,535
So that's a cancels out as one.

587
00:34:39,535 --> 00:34:42,460
Okay. So then we want to pass backwards.

588
00:34:42,460 --> 00:34:47,230
So, the first thing that we have is this sort of multiply node.

589
00:34:47,230 --> 00:34:53,095
And so we worked- we know its local gradients that partial of f by a is two,

590
00:34:53,095 --> 00:34:57,085
and the partial of f by b is three.

591
00:34:57,085 --> 00:34:59,350
And so we get those values.

592
00:34:59,350 --> 00:35:03,280
So formally we're taking the local gradients

593
00:35:03,280 --> 00:35:07,795
multiplying them by the upstream gradients and getting our three and two.

594
00:35:07,795 --> 00:35:13,045
And notice the fact that so effectively what happens is the values on the two arcs swaps.

595
00:35:13,045 --> 00:35:15,460
Um, and then we sort of continue back.

596
00:35:15,460 --> 00:35:17,500
Okay. There's a max node.

597
00:35:17,500 --> 00:35:23,650
So our upstream gradient is now three and then we want to multiply by the local gradient.

598
00:35:23,650 --> 00:35:29,920
And since the max of these two as two has a slope of one on this side.

599
00:35:29,920 --> 00:35:31,300
So you get three,

600
00:35:31,300 --> 00:35:34,855
there's no gradient on this side and we get zero.

601
00:35:34,855 --> 00:35:37,720
And then we do the similar calculation on

602
00:35:37,720 --> 00:35:41,295
the other side where we have local gradients of one.

603
00:35:41,295 --> 00:35:47,744
And so both of them come out of two And then the one other thing to do is we notice,

604
00:35:47,744 --> 00:35:48,975
well, wait a minute.

605
00:35:48,975 --> 00:35:52,295
There are two arcs that started from the y

606
00:35:52,295 --> 00:35:56,290
both of which we've backed complicated some gradient on.

607
00:35:56,290 --> 00:35:58,870
And so what do we do about that.

608
00:35:58,870 --> 00:36:02,125
Um, what we do about that is we sum.

609
00:36:02,125 --> 00:36:07,740
So, the partial of f by x is to the partial of f by z is 0 that the

610
00:36:07,740 --> 00:36:13,450
partial of f by y is the sum of the two and five, right?

611
00:36:13,450 --> 00:36:15,670
And so this isn't complete voodoo.

612
00:36:15,670 --> 00:36:21,730
This is something that should make sense in terms of what gradients are, right?

613
00:36:21,730 --> 00:36:23,845
So, that what we're saying,

614
00:36:23,845 --> 00:36:25,615
is what we're calculating,

615
00:36:25,615 --> 00:36:27,860
is if you wiggle x a little bit

616
00:36:27,860 --> 00:36:32,175
how big an effect does that have on the outcome of the whole thing?

617
00:36:32,175 --> 00:36:34,650
And so, you know, we should be able to work this out.

618
00:36:34,650 --> 00:36:40,185
So, our x started offers one but let's suppose we wiggle it up a bit

619
00:36:40,185 --> 00:36:47,685
and make it 1,1 well according to this output should change by about 0,2,

620
00:36:47,685 --> 00:36:49,485
it should be magnified by two.

621
00:36:49,485 --> 00:36:51,330
And we should be able to work that out, right?

622
00:36:51,330 --> 00:36:55,510
So it's then 1,1 plus two,

623
00:36:55,510 --> 00:36:58,495
so that's then 3,1.

624
00:36:58,495 --> 00:37:03,610
And then we've got the two here that multiplies by it and it's 6,2.

625
00:37:03,610 --> 00:37:05,890
And lo and behold it went up by 0,2, right?

626
00:37:05,890 --> 00:37:07,510
So that seems correct.

627
00:37:07,510 --> 00:37:09,940
And if we try and do the same for,

628
00:37:09,940 --> 00:37:11,935
well, let's do the z. It's easy.

629
00:37:11,935 --> 00:37:16,510
So if we wiggle the z which had a value of zero by 0,1.

630
00:37:16,510 --> 00:37:18,400
This is 0,1.

631
00:37:18,400 --> 00:37:21,370
When we max if this is still two and

632
00:37:21,370 --> 00:37:24,655
so a calculated value doesn't change, it's still six.

633
00:37:24,655 --> 00:37:26,830
So the gradient here is zero.

634
00:37:26,830 --> 00:37:28,690
Wiggling this does nothing.

635
00:37:28,690 --> 00:37:32,230
And then the final one is y.

636
00:37:32,230 --> 00:37:35,350
So, it's starting off value as two.

637
00:37:35,350 --> 00:37:38,545
So, if we wiggle it a little and make it 2,1,

638
00:37:38,545 --> 00:37:44,350
our claim is that the results are change by about 0,5.

639
00:37:44,350 --> 00:37:46,735
It should be multiplied by five times.

640
00:37:46,735 --> 00:37:54,430
So, if we make this 2,1 we then have 2,1 plus one and b 3,1.

641
00:37:54,430 --> 00:37:59,245
When we get the max here would also be 2,1.

642
00:37:59,245 --> 00:38:03,100
And so we'd have 2,1 times 3,1.

643
00:38:03,100 --> 00:38:06,820
And that's too hard arithmetic for me to do in my head.

644
00:38:06,820 --> 00:38:16,360
But if we take 2,1 times 3,1 it comes out to 6,51.

645
00:38:16,360 --> 00:38:19,210
So, basically it's gone up by half.

646
00:38:19,210 --> 00:38:22,390
We don't expect the answers to be exact of course, right?

647
00:38:22,390 --> 00:38:24,730
Because you know that's not the way calculus works, right?

648
00:38:24,730 --> 00:38:29,545
[NOISE]. Where that it's showing that we're getting the gradients right.

649
00:38:29,545 --> 00:38:32,620
Okay. So this actually works.

650
00:38:32,620 --> 00:38:35,800
So, what are the techniques that we need to know?

651
00:38:35,800 --> 00:38:39,310
Um, so we've sort of already seen them all.

652
00:38:39,310 --> 00:38:44,050
So, you know, we discussed when there are multiple incoming arcs,

653
00:38:44,050 --> 00:38:47,905
how he saw workout the different local derivatives.

654
00:38:47,905 --> 00:38:52,000
The main other case that we need to know is if, um,

655
00:38:52,000 --> 00:38:54,805
in the function computation there's a branch

656
00:38:54,805 --> 00:38:59,050
outward the resultant something is used in multiple places.

657
00:38:59,050 --> 00:39:01,060
And so this was like the case here.

658
00:39:01,060 --> 00:39:03,490
I mean, here this was an initial variable,

659
00:39:03,490 --> 00:39:06,400
but you know, it could have been computed by something further back.

660
00:39:06,400 --> 00:39:10,090
So, if this thing is used in multiple places and

661
00:39:10,090 --> 00:39:14,020
you have the computation going out in different ways.

662
00:39:14,020 --> 00:39:17,680
It's just this simple rule that when you do backpropagation

663
00:39:17,680 --> 00:39:23,320
backwards you sum the gradients that you get from the different output branches.

664
00:39:23,320 --> 00:39:28,270
Okay. So, if a equals X plus Y and while that's the one we showed you

665
00:39:28,270 --> 00:39:34,555
before that were doing this some operation to work out the total partial of f by y.

666
00:39:34,555 --> 00:39:40,570
Okay. And if you sort of think about it just a little bit more,

667
00:39:40,570 --> 00:39:43,480
there's sort of these obvious patterns,

668
00:39:43,480 --> 00:39:46,855
um, which we saw in this very simple example.

669
00:39:46,855 --> 00:39:54,310
So, if you've got a plus that really the upstream gradient is going to

670
00:39:54,310 --> 00:39:56,920
be sort of heading down every one of

671
00:39:56,920 --> 00:40:02,080
these grant branches when you have multiple branches are things being summed.

672
00:40:02,080 --> 00:40:03,950
Now, in this case,

673
00:40:03,950 --> 00:40:10,080
it just as copied unchanged but that's because our computation was x plus y.

674
00:40:10,080 --> 00:40:11,700
You know, it could be more complicated,

675
00:40:11,700 --> 00:40:14,970
but we're passing it down down each of those branches.

676
00:40:14,970 --> 00:40:19,310
So plus distributes upstream gradient.

677
00:40:19,310 --> 00:40:23,950
When you have a max that's kind of like a routing operation,

678
00:40:23,950 --> 00:40:29,419
because max is going to be sending the gradient to in the direction that's the max,

679
00:40:29,419 --> 00:40:33,140
and other things are going to get no gradient being passed down to them.

680
00:40:33,140 --> 00:40:36,280
Um, and then when you have, um,

681
00:40:36,280 --> 00:40:39,130
a multiplication this has this kind of

682
00:40:39,130 --> 00:40:42,610
fun effect that what you do is switch the gradient, right?

683
00:40:42,610 --> 00:40:46,355
And so this reflects the fact that when you have u times

684
00:40:46,355 --> 00:40:50,865
v regardless of whether u and v are vectors or just,

685
00:40:50,865 --> 00:40:55,120
um, scalars that the derivative of the result with respect to

686
00:40:55,120 --> 00:41:00,050
u is v and the derivative of those spot- result with respect to v is u.

687
00:41:00,050 --> 00:41:01,550
And so, the, um,

688
00:41:01,550 --> 00:41:03,715
gradient signal is the flip,

689
00:41:03,715 --> 00:41:07,890
um, of the tw- two numbers on the different sides.

690
00:41:07,890 --> 00:41:14,070
Okay. Um, so this is sort of most of how we have

691
00:41:14,070 --> 00:41:19,730
these computation graphs and we can work out backpropagation backwards in them.

692
00:41:19,730 --> 00:41:23,765
There's sort of one more part of this to do,

693
00:41:23,765 --> 00:41:25,780
um, which is to say g,

694
00:41:25,780 --> 00:41:28,070
we want to do this eff- efficiently.

695
00:41:28,070 --> 00:41:31,830
So, there's a bad way to do this which is to say, "Oh well,

696
00:41:31,830 --> 00:41:37,535
we wanted to calculate the partial of this by b and so we can calculate that partial."

697
00:41:37,535 --> 00:41:41,045
Which was essentially what I was doing on last time slides.

698
00:41:41,045 --> 00:41:48,545
We say, "Um, partial of s by b equals the partial of s by h,

699
00:41:48,545 --> 00:41:51,040
times the partial of h by z,

700
00:41:51,040 --> 00:41:53,590
times the partial of z by b,

701
00:41:53,590 --> 00:41:55,120
and we have all of those partials.

702
00:41:55,120 --> 00:41:59,670
We work them all out and multiply them together and then someone says,

703
00:41:59,670 --> 00:42:02,590
um, what's the partial of s by w?

704
00:42:02,590 --> 00:42:05,105
And we say, huh, that's the chain rule again, I'll do it all again.

705
00:42:05,105 --> 00:42:08,449
It's the partial of s by,

706
00:42:08,449 --> 00:42:11,530
um, h times the partial of h by z,

707
00:42:11,530 --> 00:42:17,435
times the partial of and z by x,

708
00:42:17,435 --> 00:42:19,750
no, no, right, ah, lost it.

709
00:42:19,750 --> 00:42:23,385
But you do big long list of them and you calculate all again.

710
00:42:23,385 --> 00:42:25,275
That's not what we want to do.

711
00:42:25,275 --> 00:42:26,885
Instead we want to say, "Oh,

712
00:42:26,885 --> 00:42:29,010
look there's this shared stuff.

713
00:42:29,010 --> 00:42:31,985
There's this error signal coming from above."

714
00:42:31,985 --> 00:42:37,285
And we can work out the error signal the upstream gradient for this node.

715
00:42:37,285 --> 00:42:41,000
We can use it to calculate the upstream gradient for this node.

716
00:42:41,000 --> 00:42:45,860
We can use this to calculate the upstream gradient for this node and then,

717
00:42:45,860 --> 00:42:49,360
using the local gradients of which there are two calculated

718
00:42:49,360 --> 00:42:53,880
this node we can then calculate this one and that one.

719
00:42:53,880 --> 00:42:59,885
Um, and then, from here having knowing this upstream gradient,

720
00:42:59,885 --> 00:43:05,035
we can use the local gradients at this node to compute this one and that one.

721
00:43:05,035 --> 00:43:10,380
And so, we're sort of doing this efficient computer science like computation,

722
00:43:10,380 --> 00:43:14,300
um, where we don't do any repeated work. That makes sense?

723
00:43:14,300 --> 00:43:18,880
Yeah. Okay. Um, and so if that is,

724
00:43:18,880 --> 00:43:20,945
um, the whole of backprop.

725
00:43:20,945 --> 00:43:26,220
So, um, here's sort of a slightly sketchy um graph

726
00:43:26,220 --> 00:43:29,350
which is sort of just re-capitulating this thing.

727
00:43:29,350 --> 00:43:37,175
So, if you have any computation that you want to perform, um, well,

728
00:43:37,175 --> 00:43:42,800
the hope is that you can sort your nodes into

729
00:43:42,800 --> 00:43:48,215
what's called a topological sort which means that things that are arguments,

730
00:43:48,215 --> 00:43:50,970
variables that are arguments are sorted before

731
00:43:50,970 --> 00:43:54,485
variables that are results that depend on that argument.

732
00:43:54,485 --> 00:43:57,980
You know, providing you have something there's an a cyclic graph,

733
00:43:57,980 --> 00:43:59,465
you'll be able to do that.

734
00:43:59,465 --> 00:44:02,435
If you have a cyclic graph, you're in trouble.

735
00:44:02,435 --> 00:44:05,045
Um, well, I'd be there actually techniques people

736
00:44:05,045 --> 00:44:07,890
use to roll out those graphs but I'm not gonna go into that now.

737
00:44:07,890 --> 00:44:12,160
So, we've sorted the nodes which is kind of loosely represented here from

738
00:44:12,160 --> 00:44:16,615
bottom to top in a topological sort area, sort.

739
00:44:16,615 --> 00:44:21,660
Okay. So then, for the forward prop we sort of go through the nodes in

740
00:44:21,660 --> 00:44:25,445
the topological sort order and we

741
00:44:25,445 --> 00:44:30,640
if it's a variable we just set its value to what it's favorite val- variable value is.

742
00:44:30,640 --> 00:44:34,355
If it's computed from other variables their values must have been

743
00:44:34,355 --> 00:44:38,330
set already because there earlier in the topological sort, um,

744
00:44:38,330 --> 00:44:43,815
and then we compute the value of those nodes according to their predecessors,

745
00:44:43,815 --> 00:44:47,380
and we pass it up and work out the final output,

746
00:44:47,380 --> 00:44:51,845
the loss function of our neural network and that is our forward pass.

747
00:44:51,845 --> 00:44:55,050
Okay. So then, after that we do our backward pass and so for

748
00:44:55,050 --> 00:45:00,305
the backward pass we initialize the output gradient with one.

749
00:45:00,305 --> 00:45:01,930
The top thing is always one,

750
00:45:01,930 --> 00:45:04,310
the partial of z with respect to z.

751
00:45:04,310 --> 00:45:09,590
And then, we now sort of go through the nodes in reverse topological sort.

752
00:45:09,590 --> 00:45:14,645
And so therefore, each of them will all ready- anything that's,

753
00:45:14,645 --> 00:45:18,025
ah, anything that's, uh, language is complex.

754
00:45:18,025 --> 00:45:19,235
Anything that's above that.

755
00:45:19,235 --> 00:45:22,680
Anything that we calculated based on it in terms of, ah,

756
00:45:22,680 --> 00:45:28,085
forward pass will already have had calculated it's, um,

757
00:45:28,085 --> 00:45:32,050
it's gradient as a product of upstream gradient

758
00:45:32,050 --> 00:45:35,855
times local gradient and then we can use that,

759
00:45:35,855 --> 00:45:38,575
um, to compute the next thing down.

760
00:45:38,575 --> 00:45:43,299
Um, and so basically the ov- the overall role

761
00:45:43,299 --> 00:45:47,945
is for any node you work out its set of successors,

762
00:45:47,945 --> 00:45:49,770
the things that are above it that it,

763
00:45:49,770 --> 00:45:52,690
that depend on it and then you say, "Okay,

764
00:45:52,690 --> 00:45:59,080
the partial of z with respect to x is simply the sum over the set of

765
00:45:59,080 --> 00:46:03,040
successors of the local gradient that you

766
00:46:03,040 --> 00:46:08,105
calculated the node times the upstream gradient of that node."

767
00:46:08,105 --> 00:46:12,565
Um, and in the examples that I gave before there was never,

768
00:46:12,565 --> 00:46:14,950
never multiple upstream gradients.

769
00:46:14,950 --> 00:46:18,460
But if you imagine a, a general big graph there could actually be

770
00:46:18,460 --> 00:46:23,885
so different upstream gradients that are being used in- for the various successors.

771
00:46:23,885 --> 00:46:31,480
So, we apply that backwards and then we've worked out in backpropagation, um,

772
00:46:31,480 --> 00:46:33,710
the gradient of every,

773
00:46:33,710 --> 00:46:39,110
the gradient of the final result z with respect to every node in our graph.

774
00:46:39,110 --> 00:46:42,410
Um, and the thing to notice about this is,

775
00:46:42,410 --> 00:46:45,665
if you're doing it right and efficiently,

776
00:46:45,665 --> 00:46:50,550
the bigger o order of complexity of doing backpropagation is exactly the

777
00:46:50,550 --> 00:46:55,390
same as doing forward propagation i.e expression evaluation.

778
00:46:55,390 --> 00:46:59,620
So, it's not some super expensive complex procedure

779
00:46:59,620 --> 00:47:02,505
that you can imagine doing and scaling up.

780
00:47:02,505 --> 00:47:07,440
Um, you're actually in exactly the same complexity order.

781
00:47:07,440 --> 00:47:11,950
Okay. Um, so as [inaudible] entered it here this procedure,

782
00:47:11,950 --> 00:47:15,635
you could just think of something that you're running on

783
00:47:15,635 --> 00:47:21,875
an arbitrary graph and calculating this forward pass and the backwards pass.

784
00:47:21,875 --> 00:47:24,990
I mean, almost without exception that the kind of

785
00:47:24,990 --> 00:47:28,660
neural nets that we actually use have a regular layer

786
00:47:28,660 --> 00:47:31,715
like structure and that's then precisely why it makes

787
00:47:31,715 --> 00:47:35,935
to- sense to work out these gradients in terms of,

788
00:47:35,935 --> 00:47:40,595
um, vectors matrices and Jacobian's as the kind we were before.

789
00:47:40,595 --> 00:47:47,120
Okay. Um, so since we have this sort of really nice algorithm now, um,

790
00:47:47,120 --> 00:47:49,160
this sort of means that, um,

791
00:47:49,160 --> 00:47:55,205
we can do this just computationally and so we don't have to think or know how to do math.

792
00:47:55,205 --> 00:47:59,150
Um, and we can just have our computers do all of this with this.

793
00:47:59,150 --> 00:48:03,020
Um, so that using this graph structure, um,

794
00:48:03,020 --> 00:48:09,470
we can just automatically work out how to apply, um, backprop.

795
00:48:09,470 --> 00:48:12,485
And there are sort of two cases of this, right?

796
00:48:12,485 --> 00:48:16,455
So, if what was calculated at each node,

797
00:48:16,455 --> 00:48:20,165
um, is given as a symbolic expression,

798
00:48:20,165 --> 00:48:24,190
we could actually have our computer work out for

799
00:48:24,190 --> 00:48:28,530
us what the derivative of that symbolic expression is.

800
00:48:28,530 --> 00:48:30,760
So, it could actually calculate, um,

801
00:48:30,760 --> 00:48:36,605
the gradient of that node and that's referred to as often as automatic differentiation.

802
00:48:36,605 --> 00:48:39,810
So, this is kind of like Mathematica Wolfram Alpha.

803
00:48:39,810 --> 00:48:41,950
You know how you can do your math homework on it?

804
00:48:41,950 --> 00:48:43,235
You just type in your expression,

805
00:48:43,235 --> 00:48:45,755
say what's a derivative and it gives it back to you right?

806
00:48:45,755 --> 00:48:51,625
Um, it's working doing symbolic computation and working out the derivative for you.

807
00:48:51,625 --> 00:48:54,660
Um, so that- so that method could be used to

808
00:48:54,660 --> 00:48:57,970
work out the local gradients and then we can use

809
00:48:57,970 --> 00:49:00,925
the graph structure and now rule

810
00:49:00,925 --> 00:49:04,844
upstream gradient times local gradient gives downstream gradient,

811
00:49:04,844 --> 00:49:06,500
i.e the chain rule, um,

812
00:49:06,500 --> 00:49:09,070
to then propagate it through the graph and do

813
00:49:09,070 --> 00:49:13,340
the whole backward pass completely automatically.

814
00:49:13,340 --> 00:49:17,515
And so that sounds, um, great.

815
00:49:17,515 --> 00:49:20,380
Um, slight disappointment, um,

816
00:49:20,380 --> 00:49:23,530
current deep learning frameworks don't quite give you that.

817
00:49:23,530 --> 00:49:27,070
Um, there was actually a famous framework that attempted to give you that.

818
00:49:27,070 --> 00:49:32,020
So the Theano Framework that was developed at the University of Montreal, um,

819
00:49:32,020 --> 00:49:34,825
those they've now abandoned in the modern era

820
00:49:34,825 --> 00:49:38,065
of large technology corporation, deep learning frameworks.

821
00:49:38,065 --> 00:49:40,060
Theano did precisely that.

822
00:49:40,060 --> 00:49:43,720
It did the full thing of automatic differentiation, um,

823
00:49:43,720 --> 00:49:47,545
for reasons that we could either think of good or bad,

824
00:49:47,545 --> 00:49:50,260
current deep learning frameworks like TensorFlow or

825
00:49:50,260 --> 00:49:53,500
PyTorch actually do a little bit less than that.

826
00:49:53,500 --> 00:49:55,600
So what they do is, say,

827
00:49:55,600 --> 00:49:59,890
well for an indiv- for the computations at an individual node,

828
00:49:59,890 --> 00:50:03,145
you have to do the calculus for yourself.

829
00:50:03,145 --> 00:50:05,155
Um, for this individual node,

830
00:50:05,155 --> 00:50:08,860
you have to write the forward propagation, say, you know,

831
00:50:08,860 --> 00:50:13,870
return X plus Y and you have to write the backward propagation,

832
00:50:13,870 --> 00:50:16,300
saying the local gradients, uh,

833
00:50:16,300 --> 00:50:20,290
one and one to the two inputs X and Y, um,

834
00:50:20,290 --> 00:50:23,380
but providing you or someone else has

835
00:50:23,380 --> 00:50:28,105
written out the forward and backward local step at this node,

836
00:50:28,105 --> 00:50:31,060
then TensorFlow or PyTorch does all the rest

837
00:50:31,060 --> 00:50:34,030
of it for you and runs the backpropagation algorithm.

838
00:50:34,030 --> 00:50:37,420
[NOISE] Um, and then, you know, effectively,

839
00:50:37,420 --> 00:50:42,444
that sort of saves you having to have a big symbolic computation engine,

840
00:50:42,444 --> 00:50:45,970
because somewhat, the person coding

841
00:50:45,970 --> 00:50:49,030
the node computations is writing

842
00:50:49,030 --> 00:50:52,420
a bit of code as you might normally imagine doing it whether in,

843
00:50:52,420 --> 00:50:54,355
you know, C or Pascal,

844
00:50:54,355 --> 00:50:57,294
of saying returning X plus Y,

845
00:50:57,294 --> 00:51:00,325
and, you know, local Gradient return one.

846
00:51:00,325 --> 00:51:05,680
Right? And- and you don't actually have to have a whole symbolic computation engine.

847
00:51:05,680 --> 00:51:09,580
Okay. So that means the overall picture looks like this.

848
00:51:09,580 --> 00:51:12,040
Right? So um, schematically,

849
00:51:12,040 --> 00:51:14,905
we have a computation graph, um,

850
00:51:14,905 --> 00:51:19,480
and to calculate the forward computation, um,

851
00:51:19,480 --> 00:51:22,810
we, um, so- sort of put inputs into

852
00:51:22,810 --> 00:51:26,575
our computation graph where there's sort of X and Y variables,

853
00:51:26,575 --> 00:51:32,005
and then we run through the nodes in topologically sorted order,

854
00:51:32,005 --> 00:51:36,910
and for each node we calculate its forward and

855
00:51:36,910 --> 00:51:40,000
necessarily the things that depends on and have already been

856
00:51:40,000 --> 00:51:43,465
computed and we just do expression evaluation forward.

857
00:51:43,465 --> 00:51:46,345
And then we return, um,

858
00:51:46,345 --> 00:51:48,010
the final gate in the graph,

859
00:51:48,010 --> 00:51:51,100
which is our loss function, or objective function.

860
00:51:51,100 --> 00:51:54,430
But then, also we have the backward pass,

861
00:51:54,430 --> 00:51:55,750
and for the backward pass,

862
00:51:55,750 --> 00:52:00,520
we go in the nodes in reversed topological, um, resorted order,

863
00:52:00,520 --> 00:52:02,394
and for each of those nodes,

864
00:52:02,394 --> 00:52:04,990
we've return their backward value,

865
00:52:04,990 --> 00:52:06,580
and for their top node,

866
00:52:06,580 --> 00:52:08,560
we return backward value of one,

867
00:52:08,560 --> 00:52:11,200
and that will then give us our gradients.

868
00:52:11,200 --> 00:52:14,200
And so that means, um,

869
00:52:14,200 --> 00:52:19,195
for any node, any piece of computation that we perform,

870
00:52:19,195 --> 00:52:23,170
we need to write a little bit of code that um

871
00:52:23,170 --> 00:52:27,550
says what it's doing on the forward pass and what it's doing on the backward pass.

872
00:52:27,550 --> 00:52:30,745
So on the forward pass, um,

873
00:52:30,745 --> 00:52:32,440
this is our multiplication,

874
00:52:32,440 --> 00:52:35,935
so we're just saying return X times Y.

875
00:52:35,935 --> 00:52:37,030
So that's pretty easy.

876
00:52:37,030 --> 00:52:38,500
That's what you're used to doing.

877
00:52:38,500 --> 00:52:42,280
But while we also need to do the backward passes,

878
00:52:42,280 --> 00:52:45,430
local gradients of return what is the

879
00:52:45,430 --> 00:52:50,395
partial of L with respect to Z and with respect to X.

880
00:52:50,395 --> 00:52:51,730
And well, to do that,

881
00:52:51,730 --> 00:52:54,085
we have to do a little bit more work.

882
00:52:54,085 --> 00:52:56,425
So we have to do a little bit more work,

883
00:52:56,425 --> 00:52:58,555
first of all, in the forward pass.

884
00:52:58,555 --> 00:53:00,655
So, in the forward pass,

885
00:53:00,655 --> 00:53:04,870
we have to remember to sort of stuff away in some variables

886
00:53:04,870 --> 00:53:07,090
what values we computed in the for-

887
00:53:07,090 --> 00:53:10,420
what- what values were given to us in the forward pass,

888
00:53:10,420 --> 00:53:13,480
or else we won't be able to calculate the backward pass.

889
00:53:13,480 --> 00:53:17,620
So we store away the values of X and Y,

890
00:53:17,620 --> 00:53:19,030
um, and so then,

891
00:53:19,030 --> 00:53:21,250
when we're doing the backward pass,

892
00:53:21,250 --> 00:53:24,550
we are passed into us the upstream Gradient,

893
00:53:24,550 --> 00:53:29,845
the error signal, and now we just do calculate, um,

894
00:53:29,845 --> 00:53:35,064
upstream Gradient times local Gradient- upstream Gradient times local Gradient,

895
00:53:35,064 --> 00:53:37,510
and we return backwards,

896
00:53:37,510 --> 00:53:40,900
um, those um downstream Gradients.

897
00:53:40,900 --> 00:53:45,865
And so providing we do that for all the nodes of our graph,

898
00:53:45,865 --> 00:53:48,365
um, we then have something that, um,

899
00:53:48,365 --> 00:53:51,940
the system can learn for us as a deep learning system.

900
00:53:51,940 --> 00:53:54,580
And so what that means in practice,

901
00:53:54,580 --> 00:53:56,545
um, is that, you know,

902
00:53:56,545 --> 00:54:01,180
any of these deep learning frameworks come with a whole box of tools that says,

903
00:54:01,180 --> 00:54:04,090
um, here is a fully connected forward layer,

904
00:54:04,090 --> 00:54:05,770
here is a sigmoid unit,

905
00:54:05,770 --> 00:54:08,560
here is other more complicated things we'll do later,

906
00:54:08,560 --> 00:54:10,795
like convolutions and recurrent layers.

907
00:54:10,795 --> 00:54:13,570
And to the extent that you are using one of those,

908
00:54:13,570 --> 00:54:15,955
somebody else has done this work for you.

909
00:54:15,955 --> 00:54:19,795
Right? That they've um defined, um,

910
00:54:19,795 --> 00:54:25,855
nodes or a layer of nodes that have forward and backward already written for- for them.

911
00:54:25,855 --> 00:54:28,980
And to the extent that that's true, um,

912
00:54:28,980 --> 00:54:32,850
that means that making neural nets is heaps of fun. It's just like lego.

913
00:54:32,850 --> 00:54:35,340
Right? You just stick these layers together and say,

914
00:54:35,340 --> 00:54:37,005
"God, I have to learn on some data and train it."

915
00:54:37,005 --> 00:54:40,560
You know, it's so easy that my high school student is building these things.

916
00:54:40,560 --> 00:54:43,020
Right? Um, you don't have to understand much really,

917
00:54:43,020 --> 00:54:44,460
um, but, you know,

918
00:54:44,460 --> 00:54:47,715
to the extent that you actually want to do some original research and think,

919
00:54:47,715 --> 00:54:50,635
"I've got this really cool idea of how to do things differently.

920
00:54:50,635 --> 00:54:53,920
I'm going to define my own kind of different computation."

921
00:54:53,920 --> 00:54:57,580
Well, then you have to do this and define your class,

922
00:54:57,580 --> 00:54:59,050
and as well as, sort of saying,

923
00:54:59,050 --> 00:55:00,760
how to compute the forward value,

924
00:55:00,760 --> 00:55:02,710
you will have to pull out your copy of

925
00:55:02,710 --> 00:55:05,665
Wolfram Alpha and work out what the derivatives are,

926
00:55:05,665 --> 00:55:08,005
um, and put that into the backward pass.

927
00:55:08,005 --> 00:55:09,955
Um, yeah.

928
00:55:09,955 --> 00:55:13,600
Okay. So here's just one little more note on that.

929
00:55:13,600 --> 00:55:17,890
Um, you know, in the early days of deep learning,

930
00:55:17,890 --> 00:55:20,995
say prior to 2014,

931
00:55:20,995 --> 00:55:24,610
what we always used to state to everybody very sternly is,

932
00:55:24,610 --> 00:55:26,590
"You should check all your Gradients,

933
00:55:26,590 --> 00:55:28,660
by doing numeric Gradient checks.

934
00:55:28,660 --> 00:55:30,459
It's really really important."

935
00:55:30,459 --> 00:55:34,420
Um, and so what that meant was, well, you know,

936
00:55:34,420 --> 00:55:38,470
if you want to know whether you have coded your backward pass right,

937
00:55:38,470 --> 00:55:40,735
an easy way to check, um,

938
00:55:40,735 --> 00:55:43,015
whether you've coded it right,

939
00:55:43,015 --> 00:55:46,450
is to do this numeric Gradient

940
00:55:46,450 --> 00:55:50,785
where you're sort of estimating the slope by wiggling it a bit,

941
00:55:50,785 --> 00:55:52,915
and wiggling the input a bit,

942
00:55:52,915 --> 00:55:54,640
and seeing what effect it has.

943
00:55:54,640 --> 00:55:59,080
So I'm working out the value of the function the F of X plus H,

944
00:55:59,080 --> 00:56:01,840
for H very small like E to the minus four,

945
00:56:01,840 --> 00:56:04,150
and then F of X minus H, um,

946
00:56:04,150 --> 00:56:05,590
and then dividing by 2H,

947
00:56:05,590 --> 00:56:07,945
and I'm saying well, what is the slope at this point,

948
00:56:07,945 --> 00:56:11,920
and I'm getting a numeric estimate of the Gradient with respect,

949
00:56:11,920 --> 00:56:15,265
um, to my variable X here.

950
00:56:15,265 --> 00:56:18,310
Um, so this is what you will have seen in

951
00:56:18,310 --> 00:56:22,929
high school when you did the sort of first um estimates of Gradients,

952
00:56:22,929 --> 00:56:26,710
where you sort of worked out F of X plus H divided by H

953
00:56:26,710 --> 00:56:30,970
and you're doing rise over run and got a point estimate of the Gradient.

954
00:56:30,970 --> 00:56:32,770
Um, exactly the same thing,

955
00:56:32,770 --> 00:56:34,225
except for the fact,

956
00:56:34,225 --> 00:56:38,410
in this case, rather than doing it one sided like that,

957
00:56:38,410 --> 00:56:40,045
we are doing it two-sided.

958
00:56:40,045 --> 00:56:42,385
It turns out that if you actually wanna do this,

959
00:56:42,385 --> 00:56:47,035
two-sided is asymptotically hugely [NOISE] better,

960
00:56:47,035 --> 00:56:48,610
and so you're always better off doing

961
00:56:48,610 --> 00:56:52,900
two-sided Gradient checks rather than one-sided Gradient checks.

962
00:56:52,900 --> 00:56:56,920
Um, so since you saw that- since it's hard to implement this wrong,

963
00:56:56,920 --> 00:56:59,470
this is a good way to check that your Gradients are

964
00:56:59,470 --> 00:57:02,395
correct if you've defined them yourselves.

965
00:57:02,395 --> 00:57:06,625
Um, as a technique to use it [NOISE] for anything,

966
00:57:06,625 --> 00:57:08,950
it's completely, completely hopeless,

967
00:57:08,950 --> 00:57:12,040
because we're thinking of doing this over

968
00:57:12,040 --> 00:57:15,520
our deep learning model for a fully connected layer.

969
00:57:15,520 --> 00:57:17,080
What this means [NOISE] is that,

970
00:57:17,080 --> 00:57:22,555
if you've got this sort of like a W matrix of N by M and you want to, um,

971
00:57:22,555 --> 00:57:27,474
calculate um your partial derivatives to check if they're correct,

972
00:57:27,474 --> 00:57:31,360
it means that you have to do this for every element of the matrix.

973
00:57:31,360 --> 00:57:34,090
So you have to calculate the eventual loss,

974
00:57:34,090 --> 00:57:37,390
first jiggling W11, then jiggling W12,

975
00:57:37,390 --> 00:57:40,675
then jiggling one- W13, 14 et cetera.

976
00:57:40,675 --> 00:57:42,880
So you have- in the complex network,

977
00:57:42,880 --> 00:57:46,030
you'll end up literally doing millions of function evaluations

978
00:57:46,030 --> 00:57:49,565
to check the Gradients at one point in time.

979
00:57:49,565 --> 00:57:51,520
So, you know, it's,

980
00:57:51,520 --> 00:57:54,010
it's not like what I advertised for

981
00:57:54,010 --> 00:57:57,220
backprop when I said it's just as efficient as calculating,

982
00:57:57,220 --> 00:57:59,875
um, the forward value.

983
00:57:59,875 --> 00:58:01,840
Doing this is forward

984
00:58:01,840 --> 00:58:06,190
value computation time multiplied by number of parameters in our model,

985
00:58:06,190 --> 00:58:08,680
which is often huge for deep learning networks.

986
00:58:08,680 --> 00:58:10,450
So this is something that you only want to

987
00:58:10,450 --> 00:58:14,170
have inside- if statements that you could turn off.

988
00:58:14,170 --> 00:58:18,805
So you could just sort of run it to check that your code isn't bre- um, debuggy.

989
00:58:18,805 --> 00:58:21,640
Um, you know, in honesty,

990
00:58:21,640 --> 00:58:24,220
this is just much less needed now because, you know,

991
00:58:24,220 --> 00:58:28,120
by and large you can plug together your components and layers and PyTorch,

992
00:58:28,120 --> 00:58:32,665
um, and other people wrote the code right and it will work.

993
00:58:32,665 --> 00:58:35,515
Um, so you probably don't need to do this all the time.

994
00:58:35,515 --> 00:58:38,050
But it is still a useful thing to look at and to know

995
00:58:38,050 --> 00:58:42,190
about if things um, are going wrong.

996
00:58:42,190 --> 00:58:46,840
Yeah. Okay, so we- we've now mastered the core technology of neural nets.

997
00:58:46,840 --> 00:58:51,070
We saw now well, basically everything we need to know about neural nets,

998
00:58:51,070 --> 00:58:54,280
and I sort of just, um, summarized it there.

999
00:58:54,280 --> 00:58:59,140
Um, just to sort of emphasize um once more.

1000
00:58:59,140 --> 00:59:03,760
Um, you know, I think some people think,

1001
00:59:03,760 --> 00:59:07,840
why do we even lear- need to learn all this stuff about gradients?'

1002
00:59:07,840 --> 00:59:09,790
And there's a sense in which it's [inaudible] really,

1003
00:59:09,790 --> 00:59:14,770
because these modern deep learning frameworks will compute all of the gradients for you.

1004
00:59:14,770 --> 00:59:17,080
You know, we make you suffer on homework two,

1005
00:59:17,080 --> 00:59:18,640
but in homework three,

1006
00:59:18,640 --> 00:59:21,625
you can have your gradients computed for you.

1007
00:59:21,625 --> 00:59:24,655
But, you know, I- so you know it's sort of just, like, well,

1008
00:59:24,655 --> 00:59:27,940
why should you take a c- a class on compilers, right?

1009
00:59:27,940 --> 00:59:33,415
That there's actually something useful in understanding what goes on under the hood,

1010
00:59:33,415 --> 00:59:35,080
even though most of the time,

1011
00:59:35,080 --> 00:59:38,815
we're just perfectly happy to let the C compiler do its thing,

1012
00:59:38,815 --> 00:59:44,080
without being experts on X86 assembler every day of the wa- week.

1013
00:59:44,080 --> 00:59:46,525
But, you know, there is more to it than that.

1014
00:59:46,525 --> 00:59:49,840
Um, you know, because even though backpropagation is great,

1015
00:59:49,840 --> 00:59:51,850
once you're building complex models,

1016
00:59:51,850 --> 00:59:56,320
backpropagation doesn't always work as you would expect it to.

1017
00:59:56,320 --> 00:59:58,180
Perfectly is maybe the wrong word,

1018
00:59:58,180 --> 01:00:00,565
because you know mathematically it's perfect.

1019
01:00:00,565 --> 01:00:03,595
Um, but it might not be achieving what you're wanting it to.

1020
01:00:03,595 --> 01:00:06,850
And well, if you want to sort of then debug an improved models,

1021
01:00:06,850 --> 01:00:09,775
it's kind of crucial to understand what's going on.

1022
01:00:09,775 --> 01:00:12,880
So, there's a nice medium piece by Andre Karpathy,

1023
01:00:12,880 --> 01:00:17,890
of yes you should understand backprop um that's on the syllabus page, um,

1024
01:00:17,890 --> 01:00:21,520
that talks about this and indeed um, um,

1025
01:00:21,520 --> 01:00:26,410
week after next, Abby is actually going to lecture about recurrent neural networks,

1026
01:00:26,410 --> 01:00:28,300
and you know one of the places, um,

1027
01:00:28,300 --> 01:00:30,790
where you can easily fail um,

1028
01:00:30,790 --> 01:00:33,580
and doing backpropagation turns up there,

1029
01:00:33,580 --> 01:00:35,485
um, is a good example.

1030
01:00:35,485 --> 01:00:43,250
Okay. So anyone have any questions about backpropagation and computation graphs?

1031
01:00:45,660 --> 01:00:51,205
Okay. If not the remainder of the time is, um,

1032
01:00:51,205 --> 01:00:55,165
the grab bag of things that you really should know about,

1033
01:00:55,165 --> 01:00:57,310
if you're going to be doing deep learning.

1034
01:00:57,310 --> 01:01:00,445
And so, yeah, this is just itsy-bitsy and,

1035
01:01:00,445 --> 01:01:01,920
but let me say them.

1036
01:01:01,920 --> 01:01:04,335
Um, so up until now,

1037
01:01:04,335 --> 01:01:07,080
when we've had um loss functions,

1038
01:01:07,080 --> 01:01:10,560
and we've been maximizing the likelihood of our data,

1039
01:01:10,560 --> 01:01:11,760
and stuff like that,

1040
01:01:11,760 --> 01:01:17,235
we've sort of just had this part here which is the likelihood of our data,

1041
01:01:17,235 --> 01:01:19,500
and we've worked to maximize it.

1042
01:01:19,500 --> 01:01:27,445
Um, however, um, in practice that works badly usually,

1043
01:01:27,445 --> 01:01:31,510
and we need to do something else which is regularize our models.

1044
01:01:31,510 --> 01:01:34,645
And if you've done the Machine Learning class,

1045
01:01:34,645 --> 01:01:38,065
or something like that you will have seen regularization.

1046
01:01:38,065 --> 01:01:42,445
And there are various techniques to do regularization, but, um,

1047
01:01:42,445 --> 01:01:43,930
compared to anything else,

1048
01:01:43,930 --> 01:01:46,975
regularization is even more important,

1049
01:01:46,975 --> 01:01:48,820
um, for deep learning models, right?

1050
01:01:48,820 --> 01:01:54,610
So, um, the general idea is if you have a lot of parameters in your model,

1051
01:01:54,610 --> 01:02:00,850
those parameters can just essentially memorize what's in the data that you trained at.

1052
01:02:00,850 --> 01:02:04,030
And so they're very good at predicting the answers.

1053
01:02:04,030 --> 01:02:09,205
The model becomes very good at predicting the answers to the data you trained it on,

1054
01:02:09,205 --> 01:02:15,040
but the model may become poor at working in the real world, and different examples.

1055
01:02:15,040 --> 01:02:18,250
And somehow we want to stop that.

1056
01:02:18,250 --> 01:02:22,000
And this problem is especially bad for deep learning models,

1057
01:02:22,000 --> 01:02:24,760
because typically deep learning models have vast,

1058
01:02:24,760 --> 01:02:26,485
vast numbers of parameters.

1059
01:02:26,485 --> 01:02:29,800
So in the good old days when statisticians ruled the show,

1060
01:02:29,800 --> 01:02:34,270
they told people that it was completely ridiculous to

1061
01:02:34,270 --> 01:02:38,650
have a number of parameters that approached your number of training examples.

1062
01:02:38,650 --> 01:02:41,260
You know, you should never have more parameters in your model,

1063
01:02:41,260 --> 01:02:44,710
than one-tenth of the number of your training examples.

1064
01:02:44,710 --> 01:02:47,545
So it's the kind of um rules of thumb you are told,

1065
01:02:47,545 --> 01:02:51,865
so that you had lots of examples with which to estimate every parameter.

1066
01:02:51,865 --> 01:02:54,970
Um, that's just not true with deep learning models,

1067
01:02:54,970 --> 01:02:57,010
is just really common that we trained

1068
01:02:57,010 --> 01:03:00,550
deep learning models that have 10 times as many parameters,

1069
01:03:00,550 --> 01:03:02,980
as we have training examples.

1070
01:03:02,980 --> 01:03:05,485
Um, but miraculously it works.

1071
01:03:05,485 --> 01:03:06,955
In fact it works brilliantly.

1072
01:03:06,955 --> 01:03:10,120
Those highly over parameterized models,

1073
01:03:10,120 --> 01:03:14,935
and this one of the big secret sources of why deep learning has been so brilliant,

1074
01:03:14,935 --> 01:03:18,085
but it only works if we regularize the model.

1075
01:03:18,085 --> 01:03:22,630
So, if you train a model without sufficient regularization,

1076
01:03:22,630 --> 01:03:29,050
what you find is that you're training it and working out your loss on the training data,

1077
01:03:29,050 --> 01:03:30,910
and the model keeps on getting better,

1078
01:03:30,910 --> 01:03:32,515
and better, and better, and better.

1079
01:03:32,515 --> 01:03:38,170
Um, necessarily, alg- algorithm has to improve loss on the training data.

1080
01:03:38,170 --> 01:03:39,805
So the worst thing that could happen,

1081
01:03:39,805 --> 01:03:43,375
is that the graph could become absolutely fa- flat.

1082
01:03:43,375 --> 01:03:47,199
What you'll find is with most models that we train,

1083
01:03:47,199 --> 01:03:51,565
they have so many parameters that this will just keep on going down,

1084
01:03:51,565 --> 01:03:56,124
until the loss is sort of approaching the numerical precision of zero,

1085
01:03:56,124 --> 01:03:57,940
if you leave it training for long enough.

1086
01:03:57,940 --> 01:04:01,450
It just learns the correct answer for every example,

1087
01:04:01,450 --> 01:04:04,405
beca- because effectively can memorize the examples.

1088
01:04:04,405 --> 01:04:06,085
Okay, but if you then say,

1089
01:04:06,085 --> 01:04:09,640
''Let me test out this model on some different data.''

1090
01:04:09,640 --> 01:04:11,890
What you find is this red curve,

1091
01:04:11,890 --> 01:04:15,610
that up until a certain point, um,

1092
01:04:15,610 --> 01:04:20,110
that you are also building a model that's better at predicting on different data,

1093
01:04:20,110 --> 01:04:23,845
but after some point this curve starts to curve up again.

1094
01:04:23,845 --> 01:04:25,930
And ignore that bit where it seems to curve down again,

1095
01:04:25,930 --> 01:04:27,340
that was a mistake in the drawing.

1096
01:04:27,340 --> 01:04:31,075
Um, and so this is then referred to as over-fitting,

1097
01:04:31,075 --> 01:04:35,290
that the- from here on the training model is

1098
01:04:35,290 --> 01:04:39,535
just learning to memorize whatever was in the training data,

1099
01:04:39,535 --> 01:04:44,590
but not in a way that later generalized to other examples.

1100
01:04:44,590 --> 01:04:46,765
And so this is not what we want.

1101
01:04:46,765 --> 01:04:50,575
We want to try and avoid over-fitting as much as possible,

1102
01:04:50,575 --> 01:04:54,160
and there are various regularization techniques that we use for that.

1103
01:04:54,160 --> 01:05:01,060
And simple starting one is this one here where we penalize the log-likelihood by saying,

1104
01:05:01,060 --> 01:05:07,389
''You're going to be penalized to the extent that you move parameters away from zero.''

1105
01:05:07,389 --> 01:05:11,500
So the default state of nature is all parameters are zeros,

1106
01:05:11,500 --> 01:05:13,675
so they're ignored on computations.

1107
01:05:13,675 --> 01:05:16,345
You can have parameters that have big values,

1108
01:05:16,345 --> 01:05:18,370
but you'll pee penalized a bit four,

1109
01:05:18,370 --> 01:05:21,490
and this is referred to as L-2 regularization.

1110
01:05:21,490 --> 01:05:23,980
And, you know, that's sort of a starting point of

1111
01:05:23,980 --> 01:05:26,530
something sensible you could do with regularization,

1112
01:05:26,530 --> 01:05:28,600
but there's more to say later.

1113
01:05:28,600 --> 01:05:32,320
And we'll talk in this sort of lecture before we discuss

1114
01:05:32,320 --> 01:05:37,480
final projects of other clever regularization techniques at neural networks.

1115
01:05:37,480 --> 01:05:40,840
Okay. Um, grab bag number two,

1116
01:05:40,840 --> 01:05:44,290
vectorization is the term that you have here,

1117
01:05:44,290 --> 01:05:46,330
um, but it's not only vectors.

1118
01:05:46,330 --> 01:05:48,820
This is also matrixization,

1119
01:05:48,820 --> 01:05:52,870
and higher dimensional matrices what are called tensors,

1120
01:05:52,870 --> 01:05:55,135
in this field tensorization.

1121
01:05:55,135 --> 01:05:58,690
Um, getting deep learning systems to run fast and

1122
01:05:58,690 --> 01:06:05,215
efficiently is only possible if we vectorize things.

1123
01:06:05,215 --> 01:06:07,210
Um, and what does that mean?

1124
01:06:07,210 --> 01:06:09,685
What that means is, you know,

1125
01:06:09,685 --> 01:06:13,300
the straightforward way to write a lot of code um,

1126
01:06:13,300 --> 01:06:15,730
that you saw in your first CS class,

1127
01:06:15,730 --> 01:06:22,120
is you say for I in range in um calculate random randi-1.

1128
01:06:22,120 --> 01:06:25,990
Um, but when we want to be clever,

1129
01:06:25,990 --> 01:06:32,305
um, people, um, that are doing things fast,

1130
01:06:32,305 --> 01:06:38,620
um, we say rather than work out this W dot one word vector at a time,

1131
01:06:38,620 --> 01:06:40,285
and do it in a four loop,

1132
01:06:40,285 --> 01:06:44,950
we could instead put all of our word vectors into one matrix,

1133
01:06:44,950 --> 01:06:52,675
and then do simply one matrix-matrix multiply of W by our word vector matrix.

1134
01:06:52,675 --> 01:06:58,735
And even if you run your code on your laptop on a CPU,

1135
01:06:58,735 --> 01:07:02,560
you will find out that if you do it the vectorized way,

1136
01:07:02,560 --> 01:07:04,900
things will become hugely faster.

1137
01:07:04,900 --> 01:07:05,950
So in this example,

1138
01:07:05,950 --> 01:07:08,484
it became over an order of magnitude faster,

1139
01:07:08,484 --> 01:07:11,785
when doing it with a vector- vectorized rather than,

1140
01:07:11,785 --> 01:07:13,735
um, with a full loop.

1141
01:07:13,735 --> 01:07:19,000
Um, and those gains are only compounded when we run code on a GPU,

1142
01:07:19,000 --> 01:07:22,600
that you'll get no gains and speed of tall on a GPU,

1143
01:07:22,600 --> 01:07:24,190
unless your code is vectorized.

1144
01:07:24,190 --> 01:07:25,705
But if it is vectorized,

1145
01:07:25,705 --> 01:07:27,580
then you can hope to have results, of oh,

1146
01:07:27,580 --> 01:07:29,650
yeah, this runs 40 times faster,

1147
01:07:29,650 --> 01:07:31,735
than it did on the CPU.

1148
01:07:31,735 --> 01:07:39,415
Okay, um, yeah, so always try to use vectors and matrices not for loops.

1149
01:07:39,415 --> 01:07:42,550
Um, of course it's useful when developing stuff to time your code,

1150
01:07:42,550 --> 01:07:44,065
and find out what's slow.

1151
01:07:44,065 --> 01:07:45,955
Um, okay.

1152
01:07:45,955 --> 01:07:47,515
Point three.

1153
01:07:47,515 --> 01:07:53,845
Um, okay, so we discussed this idea, um, last time,

1154
01:07:53,845 --> 01:07:59,575
and the time before that after- after having the sort of affine layer,

1155
01:07:59,575 --> 01:08:01,270
where we took, you know,

1156
01:08:01,270 --> 01:08:03,880
go from X to WX, plus B.

1157
01:08:03,880 --> 01:08:05,500
That's referred to as an affine layer,

1158
01:08:05,500 --> 01:08:06,880
so we're doing this, um,

1159
01:08:06,880 --> 01:08:09,925
multiplying a vector by a matrice- matrix,

1160
01:08:09,925 --> 01:08:12,505
and adding um biases.

1161
01:08:12,505 --> 01:08:15,925
We necessarily to have power and a deep network, um,

1162
01:08:15,925 --> 01:08:19,915
have to have some form of non-linearity.

1163
01:08:19,915 --> 01:08:22,495
And so, I just wanted to go through a bit of background

1164
01:08:22,495 --> 01:08:25,644
on non-linearity is in what people use,

1165
01:08:25,644 --> 01:08:27,085
and what to use.

1166
01:08:27,085 --> 01:08:33,340
So, if you're sort of starting from the idea of what we know is logistic regression, um,

1167
01:08:33,340 --> 01:08:36,549
what's commonly referred to as the sigmoid curve,

1168
01:08:36,549 --> 01:08:39,670
or maybe more precisely is the logistic,

1169
01:08:39,670 --> 01:08:42,955
um, function is this picture here.

1170
01:08:42,955 --> 01:08:46,060
So something that's squashes any real

1171
01:08:46,060 --> 01:08:49,660
number positive or negative into the range zero to one.

1172
01:08:49,660 --> 01:08:51,730
It gives you a probability output.

1173
01:08:51,730 --> 01:08:55,600
Um, these- this use of this, um,

1174
01:08:55,600 --> 01:09:00,400
logistic function was really really common in early neural nets.

1175
01:09:00,400 --> 01:09:02,785
If you go back to '80s, '90s neural nets,

1176
01:09:02,785 --> 01:09:07,135
there were, um, sigmoid functions absolutely everywhere.

1177
01:09:07,135 --> 01:09:10,150
Um, in more recent times,

1178
01:09:10,150 --> 01:09:13,150
90 percent of the time nobody uses

1179
01:09:13,150 --> 01:09:16,435
this and they've been found to sort of actually work quite poorly.

1180
01:09:16,435 --> 01:09:19,870
The only place these are used is when you

1181
01:09:19,870 --> 01:09:24,730
actually want a value between zero and one is your output.

1182
01:09:24,730 --> 01:09:28,270
So we'll talk later about how you have gating in networks,

1183
01:09:28,270 --> 01:09:32,800
and so gating as a place where you want to have a probability between two things.

1184
01:09:32,800 --> 01:09:34,795
And then you will use one of those,

1185
01:09:34,795 --> 01:09:37,240
but you use some absolutely nowhere else.

1186
01:09:37,240 --> 01:09:40,240
Um, here is the tanh curve.

1187
01:09:40,240 --> 01:09:42,880
Um, so the formula for tanh, um,

1188
01:09:42,880 --> 01:09:46,300
looks like a scary thing with thoughts of exponentials in it,

1189
01:09:46,300 --> 01:09:51,145
and it doesn't really look much like a logistic curve whatsoever.

1190
01:09:51,145 --> 01:09:56,740
Um, but if you um dig up your math textbook you can convince yourself that

1191
01:09:56,740 --> 01:09:59,920
a tanh curve is actually exactly the same as

1192
01:09:59,920 --> 01:10:04,090
the logistic curve apart from you multiply it by two,

1193
01:10:04,090 --> 01:10:06,505
so it has a range of two rather than one,

1194
01:10:06,505 --> 01:10:08,095
and you shift it down line.

1195
01:10:08,095 --> 01:10:10,735
So, this is sort of just a re-scaled logistic.

1196
01:10:10,735 --> 01:10:13,690
There's now symmetric between one and minus one,

1197
01:10:13,690 --> 01:10:16,900
and the fact that some metric in the output actually helps

1198
01:10:16,900 --> 01:10:20,545
a lot for putting into neural networks. Um.

1199
01:10:20,545 --> 01:10:25,070
So, tanh's, are still reasonably widely used

1200
01:10:25,070 --> 01:10:29,270
in quite a number of places um in um your networks.

1201
01:10:29,270 --> 01:10:32,755
So, tanh should be a friend of yours and you should know about that.

1202
01:10:32,755 --> 01:10:36,545
But you know, one of the bad things about using

1203
01:10:36,545 --> 01:10:41,320
um transcendental functions like the sigmoid or tanh is,

1204
01:10:41,320 --> 01:10:48,300
you know, they involve this expensive math operations um that slow you down.

1205
01:10:48,300 --> 01:10:51,050
Like, it's sort of a nuisance to be kind

1206
01:10:51,050 --> 01:10:53,830
of computing exponentials and tanh's in your computer,

1207
01:10:53,830 --> 01:10:55,070
things are kind of slow.

1208
01:10:55,070 --> 01:10:58,870
So people started um playing around with ways

1209
01:10:58,870 --> 01:11:02,940
to make things faster and so someone came up with this idea like,

1210
01:11:02,940 --> 01:11:05,360
maybe we could come up with a hard tanh,

1211
01:11:05,360 --> 01:11:08,560
um where it's just sort of flat out here

1212
01:11:08,560 --> 01:11:12,000
and then it has a linear slope and then it's flat at the top.

1213
01:11:12,000 --> 01:11:16,035
You know, it sort of looks like a tanh but we just squared it off.

1214
01:11:16,035 --> 01:11:19,580
Um, and while this is really cheap to compute right, you say,

1215
01:11:19,580 --> 01:11:21,965
x less than minus one,

1216
01:11:21,965 --> 01:11:26,630
return minus one, return plus one or just return the number.

1217
01:11:26,630 --> 01:11:29,135
No complex transcendentals.

1218
01:11:29,135 --> 01:11:30,700
The funny thing is,

1219
01:11:30,700 --> 01:11:33,475
it turns out that this actually works pretty well.

1220
01:11:33,475 --> 01:11:36,580
You might be scared and you might justifiably be

1221
01:11:36,580 --> 01:11:40,340
scared because if you start thinking about gradients,

1222
01:11:40,340 --> 01:11:41,645
once you're over here,

1223
01:11:41,645 --> 01:11:43,190
there's no gradient, right?

1224
01:11:43,190 --> 01:11:46,495
It's completely flat at zero.

1225
01:11:46,495 --> 01:11:51,035
So, things go dead as soon as they're at one of the ends.

1226
01:11:51,035 --> 01:11:54,350
So, it's sort of important to stay in this middle section at least for

1227
01:11:54,350 --> 01:11:58,120
a while and then its just got a slope of one, right?

1228
01:11:58,120 --> 01:11:59,800
It's a constant slope of one.

1229
01:11:59,800 --> 01:12:03,920
But this is enough of a linearity that actually it

1230
01:12:03,920 --> 01:12:08,770
works well in neural networks and you can train neural networks.

1231
01:12:08,770 --> 01:12:13,995
So, that's sent the whole field in the opposite direction and people thought,

1232
01:12:13,995 --> 01:12:15,880
oh, if that works,

1233
01:12:15,880 --> 01:12:18,855
maybe we can make things even simpler.

1234
01:12:18,855 --> 01:12:24,100
And that led to the now famous what's referred to [inaudible] as ReLU.

1235
01:12:24,100 --> 01:12:27,090
So there is a mistake in my editing there,

1236
01:12:27,090 --> 01:12:28,500
delete off hard tanh.

1237
01:12:28,500 --> 01:12:30,830
That was in slides by mistake.

1238
01:12:30,830 --> 01:12:32,370
[LAUGHTER] The ReLU unit,

1239
01:12:32,370 --> 01:12:36,795
everyone calls it ReLU which stands for rectified linear unit.

1240
01:12:36,795 --> 01:12:42,250
So, the Re-, the ReLU is essentially the simplest non-linearity you can have.

1241
01:12:42,250 --> 01:12:45,950
So the ReLU is zero,

1242
01:12:45,950 --> 01:12:52,015
slope zero as soon as you're in the negative regime and it's just a line slope one,

1243
01:12:52,015 --> 01:12:53,800
when you're in the positive regime.

1244
01:12:53,800 --> 01:12:56,600
I mean, when I first saw this,

1245
01:12:56,600 --> 01:12:59,835
I mean, it's sort of blew my mind it could possibly work.

1246
01:12:59,835 --> 01:13:01,770
Because it sort of, I guess,

1247
01:13:01,770 --> 01:13:07,220
I was brought up on these sort of tanh's and sigmoids and the sorts of these arguments

1248
01:13:07,220 --> 01:13:13,250
about the slope and you get these gradients and you can move around with the gradient.

1249
01:13:13,250 --> 01:13:17,240
And how is it meant to work if half of this function just says

1250
01:13:17,240 --> 01:13:21,720
output zero and no gradient and the other half is just this straight line.

1251
01:13:21,720 --> 01:13:25,365
And in particular, when you're in the positive regime,

1252
01:13:25,365 --> 01:13:27,905
this is just an identity function.

1253
01:13:27,905 --> 01:13:35,140
And, you know, I sort of argued before that if you just compose linear transforms,

1254
01:13:35,140 --> 01:13:40,560
you don't get any power but provided when this is the right-hand part of the regime.

1255
01:13:40,560 --> 01:13:42,190
Since this is an identity function,

1256
01:13:42,190 --> 01:13:43,400
that's exactly what we're doing.

1257
01:13:43,400 --> 01:13:45,770
We're just composing linear transforms.

1258
01:13:45,770 --> 01:13:48,280
So you- you sort of believe it just can't possibly

1259
01:13:48,280 --> 01:13:51,755
work but it turns out that this works brilliantly.

1260
01:13:51,755 --> 01:13:54,860
And this is now by far

1261
01:13:54,860 --> 01:13:59,640
the default choice when people are building feed for deep networks.

1262
01:13:59,640 --> 01:14:05,190
That people use ReLU non-linearities and they are very fast,

1263
01:14:05,190 --> 01:14:09,260
they train very quickly and they perform very well.

1264
01:14:09,260 --> 01:14:10,845
And so, effectively, you know,

1265
01:14:10,845 --> 01:14:13,630
it is, it is simply just each u-,

1266
01:14:13,630 --> 01:14:15,350
depending on the inputs,

1267
01:14:15,350 --> 01:14:20,495
each unit is just either dead or it's passing things on as an identity function.

1268
01:14:20,495 --> 01:14:22,590
But that's enough of lini-,

1269
01:14:22,590 --> 01:14:24,460
non-linearity that you can do

1270
01:14:24,460 --> 01:14:28,400
arbitrary function approximation still with a deep learning network.

1271
01:14:28,400 --> 01:14:32,255
And people now make precisely the opposite argument which is,

1272
01:14:32,255 --> 01:14:41,775
because this unit just has a slope of one over it's non-zero range, that means,

1273
01:14:41,775 --> 01:14:45,280
the gradient is past spec very efficiently to

1274
01:14:45,280 --> 01:14:50,860
the inputs and therefore the models train very efficiently whereas,

1275
01:14:50,860 --> 01:14:53,655
when you are with these kind of curves,

1276
01:14:53,655 --> 01:14:58,850
when you're over here, there's very little slope so your models might train very slowly.

1277
01:14:58,850 --> 01:15:01,760
Okay. So, you know,

1278
01:15:01,760 --> 01:15:05,325
for feed-forward network, try this before you try anything else.

1279
01:15:05,325 --> 01:15:08,855
But there's sort of then been a sub literature that says,

1280
01:15:08,855 --> 01:15:12,620
well, maybe that's too simple and we could do a bit better.

1281
01:15:12,620 --> 01:15:15,610
And so that led to the leaky ReLU which said,

1282
01:15:15,610 --> 01:15:19,775
"Maybe we should put a tiny bit of slope over here so it's not completely dead."

1283
01:15:19,775 --> 01:15:22,085
So you can make it something like one,

1284
01:15:22,085 --> 01:15:25,405
one 100th as the slope of this part.

1285
01:15:25,405 --> 01:15:26,690
And then people had, well,

1286
01:15:26,690 --> 01:15:27,880
let's build off that,

1287
01:15:27,880 --> 01:15:31,360
maybe we could actually put another parameter into

1288
01:15:31,360 --> 01:15:34,960
our neural network and we could have a parametric ReLU.

1289
01:15:34,960 --> 01:15:38,980
So, there's some slope over here but we're also going to

1290
01:15:38,980 --> 01:15:45,280
backpropagate into our non-linearity which has this extra alpha parameter,

1291
01:15:45,280 --> 01:15:47,645
which is how ma- much slope it has.

1292
01:15:47,645 --> 01:15:50,835
And so, variously people have used these,

1293
01:15:50,835 --> 01:15:55,150
you can sort of find 10 papers on archive where people say,

1294
01:15:55,150 --> 01:15:57,950
you can get better results from using one or other of these.

1295
01:15:57,950 --> 01:16:00,770
You can also find papers where people said it made

1296
01:16:00,770 --> 01:16:03,955
no difference for them versus just using a ReLU.

1297
01:16:03,955 --> 01:16:05,334
So, I think basically,

1298
01:16:05,334 --> 01:16:08,925
you can start off with a ReLU and work from there.

1299
01:16:08,925 --> 01:16:13,495
Yes. So, parameter initialization,

1300
01:16:13,495 --> 01:16:18,225
it's when, so, when we have these matrices and parameters in our model,

1301
01:16:18,225 --> 01:16:20,910
it's vital, vital, vital,

1302
01:16:20,910 --> 01:16:27,900
that you have to initialize those parameter weights with small random values.

1303
01:16:27,900 --> 01:16:30,140
This was precisely the lesson that

1304
01:16:30,140 --> 01:16:33,520
some people hadn't discovered when it came to final project time.

1305
01:16:33,520 --> 01:16:36,255
So I'll emphasize it is vital, vital.

1306
01:16:36,255 --> 01:16:40,025
So, if you just start off with the weights being zero,

1307
01:16:40,025 --> 01:16:42,699
you kind of have these complete symmetries,

1308
01:16:42,699 --> 01:16:45,320
right, that everything will be calculated the same,

1309
01:16:45,320 --> 01:16:49,350
everything will move the same and you're not actually training

1310
01:16:49,350 --> 01:16:54,600
this complex network with a lot of units that are specializing to learn different things.

1311
01:16:54,600 --> 01:16:57,550
So, somehow, you have to break the symmetry and we

1312
01:16:57,550 --> 01:17:00,510
do that by giving small random weights.

1313
01:17:00,510 --> 01:17:02,980
So, you know, there's sort of some fine points.

1314
01:17:02,980 --> 01:17:04,660
When you have biases,

1315
01:17:04,660 --> 01:17:06,400
you may as well just start them at Zero,

1316
01:17:06,400 --> 01:17:11,640
as neutral and see how the system learn the bias that you want et cetera.

1317
01:17:11,640 --> 01:17:18,965
But in general, the weights you want to initialize to small random values.

1318
01:17:18,965 --> 01:17:24,905
You'll find in PyTorch or other deep learning practi- packages,

1319
01:17:24,905 --> 01:17:30,540
a common initialization that's used and often recommended is this Xavier Initialization.

1320
01:17:30,540 --> 01:17:34,110
And so, the trick of this is that,

1321
01:17:34,110 --> 01:17:37,350
for a lot of models and a lot of places,

1322
01:17:37,350 --> 01:17:40,720
think of some of these things like these ones and these,

1323
01:17:40,720 --> 01:17:46,205
you'd like the values in the network to sort of stay small,

1324
01:17:46,205 --> 01:17:48,880
in this sort of middle range here.

1325
01:17:48,880 --> 01:17:53,145
And well, if you kind of have a matrix with big values in it

1326
01:17:53,145 --> 01:17:57,470
and you multiply a vector by this matrix,

1327
01:17:57,470 --> 01:17:58,910
you know, things might get bigger.

1328
01:17:58,910 --> 01:18:00,550
And then if you put in through another layer,

1329
01:18:00,550 --> 01:18:03,180
it'll get bigger again and then sort of everything

1330
01:18:03,180 --> 01:18:05,980
will be too big and you will have problems.

1331
01:18:05,980 --> 01:18:10,285
So, really, Xavier Initialization is seeking to avoid that by saying,

1332
01:18:10,285 --> 01:18:14,590
how many inputs are there to this node?

1333
01:18:14,590 --> 01:18:16,405
How many outputs are there?

1334
01:18:16,405 --> 01:18:20,920
We want to sort of temp it down the initialization based on the inputs

1335
01:18:20,920 --> 01:18:26,395
and the outputs because effectively we'll be using this number that many times.

1336
01:18:26,395 --> 01:18:30,185
It's a good thing to use, you can use that.

1337
01:18:30,185 --> 01:18:34,955
Optimizers. Up till now,

1338
01:18:34,955 --> 01:18:37,770
we saw, just talked about plain SGD.

1339
01:18:37,770 --> 01:18:43,785
You know, normally plain SGD actually works just fine.

1340
01:18:43,785 --> 01:18:46,500
But often if you want to use just plain SGD,

1341
01:18:46,500 --> 01:18:49,860
you have to spend time tuning the learning rate,

1342
01:18:49,860 --> 01:18:54,440
that alpha that we multiplied the gradient by.

1343
01:18:54,440 --> 01:18:58,535
For complex nets and situations or to avoid worry,

1344
01:18:58,535 --> 01:19:03,710
there's sort of now this big family and more sophisticated adaptive optimizers.

1345
01:19:03,710 --> 01:19:10,055
And so, effectively they're scaling the parameter adjustment by accumulated gradients,

1346
01:19:10,055 --> 01:19:14,370
which have the effect that they learn per parameter learning rates.

1347
01:19:14,370 --> 01:19:18,120
So that they can see which parameters would be useful to move

1348
01:19:18,120 --> 01:19:22,580
more and which one is less depending on the sensitivity of those parameters.

1349
01:19:22,580 --> 01:19:24,040
So, where things are flat,

1350
01:19:24,040 --> 01:19:25,960
you can be trying to move quickly.

1351
01:19:25,960 --> 01:19:27,450
Where things are bouncing around a lot,

1352
01:19:27,450 --> 01:19:30,550
you are going to be trying to move just a little so as not to overshoot.

1353
01:19:30,550 --> 01:19:32,850
And so, there's a whole family of these; Adagrad,

1354
01:19:32,850 --> 01:19:35,200
RMSprop, Adam, there are actually other ones.

1355
01:19:35,200 --> 01:19:37,675
There's Adam Max and whole lot of them.

1356
01:19:37,675 --> 01:19:43,395
I mean, Adam is one fairly reliable one that many people use and that's not bad.

1357
01:19:43,395 --> 01:19:45,685
And then one more slide and I'm done.

1358
01:19:45,685 --> 01:19:47,555
Yes, so learning rates.

1359
01:19:47,555 --> 01:19:51,420
So, normally you have to choose a learning rate.

1360
01:19:51,420 --> 01:19:54,515
So, one choice is just have a constant learning rate.

1361
01:19:54,515 --> 01:19:59,500
You pick a number, may be 10 to the minus three and say that's my learning rate.

1362
01:19:59,500 --> 01:20:04,165
You want your learning rate to be order of magnitude, right.

1363
01:20:04,165 --> 01:20:07,670
If your learning rate is too big,

1364
01:20:07,670 --> 01:20:12,990
your model might diverge or not converge because it just sort of leaps you around by

1365
01:20:12,990 --> 01:20:19,915
huge cram movements and you completely miss the good parts of your function space.

1366
01:20:19,915 --> 01:20:22,925
If your model, if your learning rate is too small,

1367
01:20:22,925 --> 01:20:28,600
your model may not train by the assignment deadline and then you'll be unhappy.

1368
01:20:28,600 --> 01:20:30,675
So, you saw that, you know,

1369
01:20:30,675 --> 01:20:35,730
commonly people sort of try powers of 10 and sees how it looks, right.

1370
01:20:35,730 --> 01:20:39,445
They might try, you know, 0,01, 0,001,

1371
01:20:39,445 --> 01:20:45,680
0,0001 and see, look at how the loss is declining and see what seems to work.

1372
01:20:45,680 --> 01:20:46,690
In general, you want to use

1373
01:20:46,690 --> 01:20:50,960
the fastest learning rate that isn't making things become unstable.

1374
01:20:50,960 --> 01:20:58,000
Commonly, you could get better results by decreasing the learning rate as you train.

1375
01:20:58,000 --> 01:21:00,660
So, sometimes people just do that by hand.

1376
01:21:00,660 --> 01:21:03,190
So, we use the term epoch for a full pass

1377
01:21:03,190 --> 01:21:05,895
through your training data and people might say,

1378
01:21:05,895 --> 01:21:08,520
half the learning rate after every three epochs

1379
01:21:08,520 --> 01:21:11,215
as you train and that can work pretty well.

1380
01:21:11,215 --> 01:21:16,385
You can use formulas to get per epoch tra- learning rates.

1381
01:21:16,385 --> 01:21:18,550
There are even fancier methods.

1382
01:21:18,550 --> 01:21:22,075
You can look up cyclic learning rates online if you want,

1383
01:21:22,075 --> 01:21:24,350
which sort of actually makes the learning rates

1384
01:21:24,350 --> 01:21:26,770
sometimes bigger and then sometimes smaller,

1385
01:21:26,770 --> 01:21:29,460
and people have found that that can be useful for getting you out

1386
01:21:29,460 --> 01:21:32,440
of bad regions in interesting ways.

1387
01:21:32,440 --> 01:21:35,820
The one other thing to know is,

1388
01:21:35,820 --> 01:21:38,775
if you're using one of the fancier optimizers,

1389
01:21:38,775 --> 01:21:43,160
they still ask you for a learning rate but that learning rate is

1390
01:21:43,160 --> 01:21:49,790
the initial learning rate which typically the optimizer will shrink as you train.

1391
01:21:49,790 --> 01:21:53,690
So, commonly if you're using something like Adam,

1392
01:21:53,690 --> 01:21:58,740
you might be starting off by saying the learning rate is 0,1,

1393
01:21:58,740 --> 01:22:03,945
so of a bigger number and it will be shrinking it later as the training goes along.

1394
01:22:03,945 --> 01:22:08,480
Okay, all done. See you next week.

