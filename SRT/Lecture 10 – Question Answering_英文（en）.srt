1
00:00:05,480 --> 00:00:11,115
Okay. Hi, everyone. Um, so let's get started again today.

2
00:00:11,115 --> 00:00:14,610
So today's lecture what I'm going to do,

3
00:00:14,610 --> 00:00:16,710
is be talking about, um,

4
00:00:16,710 --> 00:00:19,060
question answering over text.

5
00:00:19,060 --> 00:00:22,025
Um, this is another of the big successes

6
00:00:22,025 --> 00:00:25,655
in using deep learning inside natural language processing,

7
00:00:25,655 --> 00:00:30,140
and it's also a technology that has some really obvious commercial uses.

8
00:00:30,140 --> 00:00:32,660
So it's an, it's an area that has attracted

9
00:00:32,660 --> 00:00:36,265
a lot of attention in the last couple of years.

10
00:00:36,265 --> 00:00:38,790
So this is the overall plan.

11
00:00:38,790 --> 00:00:43,970
Um, just a couple of reminders and things at the beginning about final project stuff,

12
00:00:43,970 --> 00:00:48,875
and then we'll, basically all of it is talking about question-answering starting with, um,

13
00:00:48,875 --> 00:00:53,000
motivation history, um, talking about the SQuAD data,

14
00:00:53,000 --> 00:00:56,390
uh, a particular simple model, our Stanford Attentive Reader.

15
00:00:56,390 --> 00:00:58,940
Then talking about some other more complex,

16
00:00:58,940 --> 00:01:02,460
um, stuff into the most modern stuff.

17
00:01:02,460 --> 00:01:05,815
Um, yeah, so in a census, um,

18
00:01:05,815 --> 00:01:09,365
lecture serves a double purpose because if you're going to do the,

19
00:01:09,365 --> 00:01:11,390
the default final project, well,

20
00:01:11,390 --> 00:01:13,410
it's about textual question-answering,

21
00:01:13,410 --> 00:01:17,855
and this is your chance to learn something about the area of textual question-answering,

22
00:01:17,855 --> 00:01:21,410
and the kinds of models you might want to be thinking about and building.

23
00:01:21,410 --> 00:01:24,890
Um but the content of this lecture pretty much is in

24
00:01:24,890 --> 00:01:28,915
no way specifically tied to the default final project,

25
00:01:28,915 --> 00:01:32,720
apart from by subject matter that really it's telling you about

26
00:01:32,720 --> 00:01:37,580
how people use neural nets to build question-answering systems.

27
00:01:37,580 --> 00:01:41,205
Okay. So first just quickly on the reminders,

28
00:01:41,205 --> 00:01:43,050
um, mid-quarter survey.

29
00:01:43,050 --> 00:01:45,149
I mean, a huge number of people,

30
00:01:45,149 --> 00:01:47,330
um, have actually filled this in already.

31
00:01:47,330 --> 00:01:51,140
Uh, we already had over 60 percent, um, um,

32
00:01:51,140 --> 00:01:54,170
filling-it-in rate by which by the standards of people

33
00:01:54,170 --> 00:01:57,245
who do surveys they come as a huge success already.

34
00:01:57,245 --> 00:01:59,510
But if you're not in that percent, um,

35
00:01:59,510 --> 00:02:03,480
we'd still love to have your feedback and now's the perfect time to do it.

36
00:02:03,480 --> 00:02:05,515
Um, yeah.

37
00:02:05,515 --> 00:02:09,455
I just wanted to sort of have a note on custom final projects.

38
00:02:09,455 --> 00:02:11,390
Um, so in general, um,

39
00:02:11,390 --> 00:02:14,645
it's great to get feedback on custom final projects.

40
00:02:14,645 --> 00:02:16,910
There's a formal mechanism for that which is

41
00:02:16,910 --> 00:02:19,625
the project proposal that I mentioned last time.

42
00:02:19,625 --> 00:02:22,330
It's also great to chat to people,

43
00:02:22,330 --> 00:02:25,930
um, informally about, um, final projects.

44
00:02:25,930 --> 00:02:28,685
And so I'm one of those people and I have

45
00:02:28,685 --> 00:02:31,610
been talking to lots of people about final projects,

46
00:02:31,610 --> 00:02:33,455
and, uh, very happy to do so.

47
00:02:33,455 --> 00:02:36,500
But there's sort of a problem that there's only one of me.

48
00:02:36,500 --> 00:02:38,630
Um, so I do also, um,

49
00:02:38,630 --> 00:02:42,080
encourage you to realize that among the various TAs that

50
00:02:42,080 --> 00:02:46,070
really lots of them have had experience of different deep learning projects,

51
00:02:46,070 --> 00:02:48,620
and in particular on the office hours page,

52
00:02:48,620 --> 00:02:53,420
there's a table that's like this but you can read it if you look at it on your own laptop,

53
00:02:53,420 --> 00:02:57,125
which talks about the experience of different TA's.

54
00:02:57,125 --> 00:02:59,930
And many of them have experience in different areas,

55
00:02:59,930 --> 00:03:04,930
and many of them are also good people to talk to about final projects.

56
00:03:04,930 --> 00:03:10,860
Okay. Um, so for the default final project, the textual question-answering.

57
00:03:10,860 --> 00:03:15,194
So um, draft materials for that app today,

58
00:03:15,194 --> 00:03:17,340
um, right now on the website actually.

59
00:03:17,340 --> 00:03:20,720
Um, we're calling them draft because we think that there are still

60
00:03:20,720 --> 00:03:24,230
probably a few things that are gonna get changed over the next week,

61
00:03:24,230 --> 00:03:29,840
so um, don't regard as completely final in terms of the code that,

62
00:03:29,840 --> 00:03:32,120
you know, it's sort of 90 percent final.

63
00:03:32,120 --> 00:03:35,405
So in terms of deciding whether you're going to do, um,

64
00:03:35,405 --> 00:03:38,495
a custom final project or a default final project,

65
00:03:38,495 --> 00:03:41,465
and working out what you're putting into your project proposal.

66
00:03:41,465 --> 00:03:42,755
Um, it should be, you know,

67
00:03:42,755 --> 00:03:44,270
well more than, um,

68
00:03:44,270 --> 00:03:46,470
what you need for this year.

69
00:03:46,470 --> 00:03:48,670
Okay. The one other, um,

70
00:03:48,670 --> 00:03:52,040
final bit I just wanted to say that I didn't get to

71
00:03:52,040 --> 00:03:55,520
last time is so for the final projects,

72
00:03:55,520 --> 00:03:58,055
regardless of which kind you're doing,

73
00:03:58,055 --> 00:04:00,750
um, well, part of it is, um,

74
00:04:00,750 --> 00:04:02,545
doing some experiments, of

75
00:04:02,545 --> 00:04:04,700
doing stuff with data and code,

76
00:04:04,700 --> 00:04:06,880
and getting some numbers and things like that.

77
00:04:06,880 --> 00:04:08,480
But I do really, um,

78
00:04:08,480 --> 00:04:11,630
encourage people to also remember that an important part of

79
00:04:11,630 --> 00:04:15,515
the final project is writing a final project report.

80
00:04:15,515 --> 00:04:20,900
And this is no different to any research project of the kinds that,

81
00:04:20,900 --> 00:04:25,590
um, students do for conferences or journals and things like that, right?

82
00:04:25,590 --> 00:04:30,020
You spend months commonly working over your code and experiments.

83
00:04:30,020 --> 00:04:31,970
But in most cases,

84
00:04:31,970 --> 00:04:36,365
the main evaluation of your work is from people reading,

85
00:04:36,365 --> 00:04:39,200
a written paper output version of things.

86
00:04:39,200 --> 00:04:41,420
So it's really important that,

87
00:04:41,420 --> 00:04:44,480
that paper version sort of reflects the work

88
00:04:44,480 --> 00:04:47,840
that you did and the interesting ideas that you came up with,

89
00:04:47,840 --> 00:04:50,720
and explains them well and present your experiments,

90
00:04:50,720 --> 00:04:52,100
and all of those things.

91
00:04:52,100 --> 00:04:56,670
And so we encourage you to sort of do a good job at writing up your projects.

92
00:04:56,670 --> 00:04:59,680
Um, here is just sort of a vague outline of, you know,

93
00:04:59,680 --> 00:05:03,320
what a typical project write-up is likely to look like.

94
00:05:03,320 --> 00:05:06,620
Now, there isn't really one size completely fits all

95
00:05:06,620 --> 00:05:09,950
because depending on what you've done different things might be appropriate.

96
00:05:09,950 --> 00:05:11,990
But, you know, typically the first page,

97
00:05:11,990 --> 00:05:15,905
you'll have an abstract for the paper and the introduction to the paper.

98
00:05:15,905 --> 00:05:19,220
You'll spend some time talking about related prior work.

99
00:05:19,220 --> 00:05:23,615
Um, you'll talk about what kind of models you built for a while.

100
00:05:23,615 --> 00:05:28,565
Um, there's probably some discussion of what data you are using for your projects.

101
00:05:28,565 --> 00:05:34,920
Um, experiments commonly with some tables and figures about the things that you're doing.

102
00:05:34,920 --> 00:05:39,740
Um, more tables and figures talking about the results as to how well your systems work.

103
00:05:39,740 --> 00:05:43,010
Um, it's great to have some error analysis to see

104
00:05:43,010 --> 00:05:46,290
what kind of things that you got right and wrong,

105
00:05:46,290 --> 00:05:48,500
and then maybe at the end there's sort of

106
00:05:48,500 --> 00:05:51,965
plans for the future, conclusions, or something like that.

107
00:05:51,965 --> 00:05:59,475
Okay. Um, that's sort of it for my extra administrative reminders.

108
00:05:59,475 --> 00:06:03,470
Um, are there any questions on final projects that people are dying to know?

109
00:06:03,470 --> 00:06:09,800
[NOISE] Okay. Good luck.

110
00:06:09,800 --> 00:06:10,925
I just meant to say good luck.

111
00:06:10,925 --> 00:06:13,470
Yeah. Good luck with your final projects. [LAUGHTER] Okay.

112
00:06:13,470 --> 00:06:15,375
So now moving into,

113
00:06:15,375 --> 00:06:18,550
um, yeah, the question answering.

114
00:06:18,550 --> 00:06:23,165
Okay. So, I mean- so question answering is

115
00:06:23,165 --> 00:06:28,610
a very direct application for something that human beings,

116
00:06:28,610 --> 00:06:30,095
um, want to do.

117
00:06:30,095 --> 00:06:33,620
Um, well, maybe human beings don't in general want to know this.

118
00:06:33,620 --> 00:06:37,355
Um, here's my query of "Who was Australia's third prime minister?".

119
00:06:37,355 --> 00:06:39,500
Um, maybe, yeah, that's not really the kind of

120
00:06:39,500 --> 00:06:41,645
thing you're gonna put into your queries but,

121
00:06:41,645 --> 00:06:43,145
you know, maybe you query,

122
00:06:43,145 --> 00:06:45,110
"Who was the lead singer of Big Thief?"

123
00:06:45,110 --> 00:06:46,745
or something like that. I don't know.

124
00:06:46,745 --> 00:06:48,050
Um, you're, uh, but you know,

125
00:06:48,050 --> 00:06:51,770
lots- a large percentage of stuff [NOISE] on the web

126
00:06:51,770 --> 00:06:56,090
is that people actually are asking for answers to questions.

127
00:06:56,090 --> 00:06:59,120
And so, if I put in this query into Google,

128
00:06:59,120 --> 00:07:00,530
it actually just works.

129
00:07:00,530 --> 00:07:03,920
It tells me the answer is John Christian Watson.

130
00:07:03,920 --> 00:07:08,915
And, um, so that's sort of question answering working in the real world.

131
00:07:08,915 --> 00:07:11,540
Um, if you try different kinds of questions in Google,

132
00:07:11,540 --> 00:07:14,585
you'll find that some of them work and lots of them don't work.

133
00:07:14,585 --> 00:07:15,770
And when they don't work,

134
00:07:15,770 --> 00:07:20,090
you're just sort of getting whatever kind of information retrieval, web search results.

135
00:07:20,090 --> 00:07:23,315
Um, there is one fine point that I just wanted,

136
00:07:23,315 --> 00:07:25,130
um, to mention down here.

137
00:07:25,130 --> 00:07:28,790
So another thing that Google has is the Google Knowledge Graph,

138
00:07:28,790 --> 00:07:32,225
which is a structured graph representation of knowledge.

139
00:07:32,225 --> 00:07:35,405
And some kinds of questions,

140
00:07:35,405 --> 00:07:39,080
um, being answered from that structured knowledge representation.

141
00:07:39,080 --> 00:07:40,430
And so, I mean,

142
00:07:40,430 --> 00:07:43,025
quite a lot of the time for things like movies,

143
00:07:43,025 --> 00:07:44,870
it's coming from that structured graph.

144
00:07:44,870 --> 00:07:47,690
If you're sort of saying, "Who's the director of a movie?"

145
00:07:47,690 --> 00:07:48,890
or something like that.

146
00:07:48,890 --> 00:07:51,050
But this answer isn't coming from that.

147
00:07:51,050 --> 00:07:53,000
This answer is a genuine,

148
00:07:53,000 --> 00:07:55,400
the kind of stuff we're gonna talk about today.

149
00:07:55,400 --> 00:07:59,360
It's textual question answering from a web page where

150
00:07:59,360 --> 00:08:01,580
Google's question and answering system has

151
00:08:01,580 --> 00:08:04,505
extracted the answer and is sticking it up there.

152
00:08:04,505 --> 00:08:06,365
Um, if you're, um,

153
00:08:06,365 --> 00:08:09,485
wanting to explore these things, um,

154
00:08:09,485 --> 00:08:14,735
if you get one of these boxes sort of down here where I've cut it off,

155
00:08:14,735 --> 00:08:16,340
there's a little bit of gray that says,

156
00:08:16,340 --> 00:08:17,990
"How did I get this result?".

157
00:08:17,990 --> 00:08:19,415
And if you click on that,

158
00:08:19,415 --> 00:08:23,300
it actually tells you what source it's getting it from and you can see if it's doing it

159
00:08:23,300 --> 00:08:28,130
from the textual question answering system or from something like the Knowledge Graph.

160
00:08:28,130 --> 00:08:31,040
Okay. Um, so the- in general,

161
00:08:31,040 --> 00:08:35,600
the motivation for question answering is that these days there's

162
00:08:35,600 --> 00:08:40,355
just these sort of massive collections of full text documents,

163
00:08:40,355 --> 00:08:42,110
i.e., there's the web.

164
00:08:42,110 --> 00:08:46,580
Um, so that there are sort of billions of documents of information.

165
00:08:46,580 --> 00:08:49,730
And traditionally, when people first started

166
00:08:49,730 --> 00:08:53,330
thinking about search information retrieval as a field,

167
00:08:53,330 --> 00:08:59,020
you know, nothing of that kind of quantity and size existed, right?

168
00:08:59,020 --> 00:09:02,320
That when people first started building search systems,

169
00:09:02,320 --> 00:09:05,200
it was sort of unthinkable to index

170
00:09:05,200 --> 00:09:09,340
whole documents because no one had hard disks big enough in those days, right?

171
00:09:09,340 --> 00:09:15,335
That really- they were indexing titles or titles and abstracts or something like that.

172
00:09:15,335 --> 00:09:19,925
And so, it seemed perfectly adequate in those days to say, "Okay.

173
00:09:19,925 --> 00:09:22,760
We're just gonna send you- give you your results."

174
00:09:22,760 --> 00:09:24,680
as to "Here's a list of documents."

175
00:09:24,680 --> 00:09:27,440
because the documents are only a hundred words long.

176
00:09:27,440 --> 00:09:31,010
But that's clearly not the case now when we have the sort of, you know,

177
00:09:31,010 --> 00:09:36,275
ten minute read, Medium posts um, which might have the answer to a question.

178
00:09:36,275 --> 00:09:39,080
And so, there's this need to sort of say, "Well,

179
00:09:39,080 --> 00:09:43,205
can we just have systems that will give us answers to questions?".

180
00:09:43,205 --> 00:09:49,730
And a lot of the recent changes in technology have hugely underlined that need.

181
00:09:49,730 --> 00:09:54,950
So, returning documents works okay if you're sitting at your laptop,

182
00:09:54,950 --> 00:09:59,150
but it works really terribly if you're on your phone and it works even more

183
00:09:59,150 --> 00:10:04,040
terribly if you're trying to work with speech on a digital assistant device,

184
00:10:04,040 --> 00:10:06,110
something like an Alexa system.

185
00:10:06,110 --> 00:10:08,840
And so, we really want to actually be able to produce

186
00:10:08,840 --> 00:10:12,260
systems that can give the answers to people's questions.

187
00:10:12,260 --> 00:10:16,865
And so typically, doing that is factored into two parts.

188
00:10:16,865 --> 00:10:21,500
That the first part of that is we still do information retrieval.

189
00:10:21,500 --> 00:10:26,270
We use stand- normally quite standard information retrieval techniques to

190
00:10:26,270 --> 00:10:32,150
find documents that quite likely to con- maintain- contain an answer.

191
00:10:32,150 --> 00:10:36,200
And the reason that this is normally done by quite traditional techniques is because

192
00:10:36,200 --> 00:10:41,390
the traditional techniques are extremely scalable over billions of documents,

193
00:10:41,390 --> 00:10:43,790
whereas current neural systems actually

194
00:10:43,790 --> 00:10:46,225
aren't really scalable over billions of documents.

195
00:10:46,225 --> 00:10:50,380
But that's an area in sort of which research is ongoing.

196
00:10:50,380 --> 00:10:53,920
But then once we have sort of some candidate likely documents,

197
00:10:53,920 --> 00:10:55,645
we want to find, uh,

198
00:10:55,645 --> 00:10:57,369
do they contain an answer,

199
00:10:57,369 --> 00:10:59,305
and if so, what is the answer?

200
00:10:59,305 --> 00:11:00,520
And so at that point,

201
00:11:00,520 --> 00:11:03,275
we have a document or a paragraph,

202
00:11:03,275 --> 00:11:07,445
and we're saying, "Can we answer this question from there?"

203
00:11:07,445 --> 00:11:11,345
And then that problem is often referred to as the Reading Comprehension problem.

204
00:11:11,345 --> 00:11:14,705
And so that's really what I'm gonna focus on today.

205
00:11:14,705 --> 00:11:19,535
Um, Reading Comprehension isn't a new problem.

206
00:11:19,535 --> 00:11:26,345
I mean it- you can trace it back into the early days of artificial intelligence and NLP.

207
00:11:26,345 --> 00:11:28,295
So, back in the 70's,

208
00:11:28,295 --> 00:11:31,520
a lot of NLP work was trying to do Reading Comprehension.

209
00:11:31,520 --> 00:11:35,420
I mean one of the famous strands of that, um, was, um,

210
00:11:35,420 --> 00:11:38,435
Sir Roger Shank was a famous,

211
00:11:38,435 --> 00:11:41,030
um, early NLP person.

212
00:11:41,030 --> 00:11:42,650
Though not a terribly nice man.

213
00:11:42,650 --> 00:11:43,985
I don't think, actually.

214
00:11:43,985 --> 00:11:48,440
Um, but the Yale School of AI was a very well-known,

215
00:11:48,440 --> 00:11:51,830
um, NLP approach and really,

216
00:11:51,830 --> 00:11:55,385
it was very focused on Reading Comprehension.

217
00:11:55,385 --> 00:11:58,205
Um, but it's sort of,

218
00:11:58,205 --> 00:12:01,070
you know, I think it was sort of the time, it was too early in any way.

219
00:12:01,070 --> 00:12:03,725
It sort of died out. Nothing much came out of that.

220
00:12:03,725 --> 00:12:07,670
Um, but then in- right just before the turn of the mil- millennium,

221
00:12:07,670 --> 00:12:11,150
Lynette Hirschman revived this idea and said, "Well,

222
00:12:11,150 --> 00:12:14,000
maybe a good challenge would be to find the kind of

223
00:12:14,000 --> 00:12:18,155
Reading Comprehension questions that elementary school kids do,

224
00:12:18,155 --> 00:12:19,700
and let's see if we could get,

225
00:12:19,700 --> 00:12:21,500
um, computers to do that.

226
00:12:21,500 --> 00:12:24,530
And some people tried that with fairly simple methods,

227
00:12:24,530 --> 00:12:26,690
which only work mediocrely.

228
00:12:26,690 --> 00:12:29,180
Then sort of somewhat after that, um,

229
00:12:29,180 --> 00:12:31,460
Chris Burges who was a guy who was at

230
00:12:31,460 --> 00:12:34,610
Microsoft Research and he wasn't really an NLP person at all.

231
00:12:34,610 --> 00:12:36,335
He was a machine learning person,

232
00:12:36,335 --> 00:12:39,065
but he got it into his head, um,

233
00:12:39,065 --> 00:12:43,820
that while really a big problem that should be being worked on is

234
00:12:43,820 --> 00:12:49,115
Machine Comprehension and he suggested that you sort of could codify it like this.

235
00:12:49,115 --> 00:12:52,715
And this is a particular clean codification

236
00:12:52,715 --> 00:12:55,340
that has lived on and we'll look at more today.

237
00:12:55,340 --> 00:12:58,880
All right. So, a machine comprehends a passage of text.

238
00:12:58,880 --> 00:13:01,640
If there's any question regarding that text that can be

239
00:13:01,640 --> 00:13:04,490
answered correctly by a majority of native speakers,

240
00:13:04,490 --> 00:13:06,890
that machine can provide a string,

241
00:13:06,890 --> 00:13:09,470
which those speakers would agree both answers

242
00:13:09,470 --> 00:13:13,565
that question and does not contain information irrelevant to that question.

243
00:13:13,565 --> 00:13:17,750
Um, and he sort of proposed this as sort of a challenge problem for

244
00:13:17,750 --> 00:13:21,980
artificial intelligence and set about collecting a corpus,

245
00:13:21,980 --> 00:13:27,410
the MCTest corpus, which was meant to be a simple Reading Comprehension challenge.

246
00:13:27,410 --> 00:13:29,855
Um, so they collected, um,

247
00:13:29,855 --> 00:13:32,840
stories, um, which, um,

248
00:13:32,840 --> 00:13:35,510
were meant to be kids' stories, you know.

249
00:13:35,510 --> 00:13:37,790
"Alyssa got to the beach after a long trip.

250
00:13:37,790 --> 00:13:40,010
She's from Charlotte. She traveled from Atlanta.

251
00:13:40,010 --> 00:13:41,570
She's now in Miami".

252
00:13:41,570 --> 00:13:43,505
Sort of pretty easy stuff.

253
00:13:43,505 --> 00:13:45,185
And then there were questions.

254
00:13:45,185 --> 00:13:47,795
"Why did Alyssa go to Miami?"

255
00:13:47,795 --> 00:13:49,895
Um, and then the answer is,

256
00:13:49,895 --> 00:13:51,320
"To visit some friends".

257
00:13:51,320 --> 00:13:55,130
And so you've got there this string that is coming from the passage.

258
00:13:55,130 --> 00:13:57,515
That's the answer to the question.

259
00:13:57,515 --> 00:14:00,950
Um, so the MCTest is a corpus of

260
00:14:00,950 --> 00:14:07,160
about 600 such stories and that challenge existed, and a few people worked on it.

261
00:14:07,160 --> 00:14:11,240
But that never really went very far either for the next couple of years.

262
00:14:11,240 --> 00:14:15,350
But what really changed things was that in 2015,

263
00:14:15,350 --> 00:14:18,515
and then with more stuff in 2016,

264
00:14:18,515 --> 00:14:23,000
um, deep learning people got interested in this idea of,

265
00:14:23,000 --> 00:14:27,620
"Could we perhaps build neural question answering systems?"

266
00:14:27,620 --> 00:14:30,965
And it seemed like if you wanted to do that, um,

267
00:14:30,965 --> 00:14:33,980
something like MCTest could only be a test set

268
00:14:33,980 --> 00:14:38,240
and the ways to make progress would be to do what had been done

269
00:14:38,240 --> 00:14:45,680
in other domains and to actually build just- hand build a large training set of passages,

270
00:14:45,680 --> 00:14:50,870
questions, and answers in such a way that would be able to train neural networks using

271
00:14:50,870 --> 00:14:53,600
the kind of supervised learning techniques that we've

272
00:14:53,600 --> 00:14:56,540
concentrated on so far in this class.

273
00:14:56,540 --> 00:15:00,445
And indeed, the kind of supervised neural network learning techniques,

274
00:15:00,445 --> 00:15:02,990
which is [NOISE] actually the successful stuff that

275
00:15:02,990 --> 00:15:06,500
powers nearly all the applications of deep learning,

276
00:15:06,500 --> 00:15:07,955
not only in NLP,

277
00:15:07,955 --> 00:15:10,200
but also in other fields like vision.

278
00:15:10,200 --> 00:15:15,680
Um, and so the first subs- the first such dataset was built by

279
00:15:15,680 --> 00:15:20,990
people at DeepMind over CNN and Daily Mail news stories.

280
00:15:20,990 --> 00:15:23,300
Um, but then the next year, um,

281
00:15:23,300 --> 00:15:26,270
Pranav Rajpurkar is a Stanford PhD student

282
00:15:26,270 --> 00:15:29,270
working with Percy Liang and a couple of other students, um,

283
00:15:29,270 --> 00:15:31,055
produced the SQuAD dataset,

284
00:15:31,055 --> 00:15:34,580
which was actually a much better designed dataset and proved to be

285
00:15:34,580 --> 00:15:38,120
sort of much more successful at driving this forward.

286
00:15:38,120 --> 00:15:39,830
And then following along from that,

287
00:15:39,830 --> 00:15:42,770
other people started to produce lots of other,

288
00:15:42,770 --> 00:15:45,590
um, question answering datasets which, you know,

289
00:15:45,590 --> 00:15:48,215
many of them have interesting advantages

290
00:15:48,215 --> 00:15:51,320
and disadvantages of their own including MS MARCO,

291
00:15:51,320 --> 00:15:53,810
TriviaQA, RACE, blah, blah, blah, lots of them.

292
00:15:53,810 --> 00:15:55,760
Um, but for today's class,

293
00:15:55,760 --> 00:15:58,100
I'm gonna concentrate on SQuAD,

294
00:15:58,100 --> 00:16:03,890
because SQuAD is actually the one that has been by far the most widely used.

295
00:16:03,890 --> 00:16:10,145
And because it - it was just a well-constructed clean dataset,

296
00:16:10,145 --> 00:16:13,670
that it sort of just proved a profitable one for people to work with.

297
00:16:13,670 --> 00:16:17,260
[NOISE]

298
00:16:17,260 --> 00:16:20,230
Okay. Um, so, that was reading comprehension.

299
00:16:20,230 --> 00:16:23,050
I'll also just quickly tell you the, um,

300
00:16:23,050 --> 00:16:26,485
the history of open domain question answering.

301
00:16:26,485 --> 00:16:29,080
So, the difference here for the- the field of

302
00:16:29,080 --> 00:16:33,305
Open-domain Question Answering that we're saying, okay,

303
00:16:33,305 --> 00:16:37,350
there's an encyclopedia or there's a web crawl,

304
00:16:37,350 --> 00:16:39,405
I'm just going to ask a question,

305
00:16:39,405 --> 00:16:40,560
can you answer it?

306
00:16:40,560 --> 00:16:43,555
So, it's this bigger task of question answering.

307
00:16:43,555 --> 00:16:46,570
And, you know, that was something that again was thought about,

308
00:16:46,570 --> 00:16:49,000
um, very early on.

309
00:16:49,000 --> 00:16:51,460
So, there's this kind of early, um,

310
00:16:51,460 --> 00:16:56,170
CACM paper by Simmons who sort of explores how you could

311
00:16:56,170 --> 00:17:00,940
do answering questions as textual question-answering, um, and yet, you know,

312
00:17:00,940 --> 00:17:03,010
he has the idea that what's going to

313
00:17:03,010 --> 00:17:05,755
happen is you're gonna dependency parse the question,

314
00:17:05,755 --> 00:17:08,470
and dependency parse sentences of the text,

315
00:17:08,470 --> 00:17:11,830
and then sort of do tree matching over the dependency parses,

316
00:17:11,830 --> 00:17:13,660
um, to get out the answers.

317
00:17:13,660 --> 00:17:15,865
And, you know, that's in some sense

318
00:17:15,865 --> 00:17:22,120
actually prefigured work that people actually were then attempting to do 35 years later.

319
00:17:22,120 --> 00:17:25,570
Um, getting a bit more modern, um, Julian Kupiec,

320
00:17:25,570 --> 00:17:28,000
she was working at Xerox PARC at the time,

321
00:17:28,000 --> 00:17:31,240
um, came up with this system called MURAX,

322
00:17:31,240 --> 00:17:35,890
and so at this stage in the 90s there started to be the first, um,

323
00:17:35,890 --> 00:17:38,770
digitally available encyclopedias available,

324
00:17:38,770 --> 00:17:41,275
so he was using the Grolier's Encyclopedia,

325
00:17:41,275 --> 00:17:44,560
and so he said about trying to build a system that could answer

326
00:17:44,560 --> 00:17:47,980
questions over that encyclopedia using,

327
00:17:47,980 --> 00:17:50,590
in general, fairly sort of shallow, um,

328
00:17:50,590 --> 00:17:55,435
linguistic processing methods, i.e, regular expressions.

329
00:17:55,435 --> 00:17:58,210
Um, for, after [LAUGHTER] having, um,

330
00:17:58,210 --> 00:18:01,555
done information retrieval search over that.

331
00:18:01,555 --> 00:18:05,515
But that started to evoke more interest from other people,

332
00:18:05,515 --> 00:18:13,135
and so in 1999 the US National Institutes of Standards and Technology, um,

333
00:18:13,135 --> 00:18:17,170
instituted a TREC question-answering track where the idea was,

334
00:18:17,170 --> 00:18:21,145
there was a large collection of News-wire documents,

335
00:18:21,145 --> 00:18:25,090
and you could be asked to provide the question of them,

336
00:18:25,090 --> 00:18:28,390
and lots of people started to build question answering systems.

337
00:18:28,390 --> 00:18:30,850
Indeed, if in some sense that was

338
00:18:30,850 --> 00:18:35,560
this competition which was where people at IBM started,

339
00:18:35,560 --> 00:18:38,320
um, working on textual question-answering,

340
00:18:38,320 --> 00:18:42,010
and then, um, sort of a decade later, um,

341
00:18:42,010 --> 00:18:47,305
IBM rejigged things into the sexier format of,

342
00:18:47,305 --> 00:18:52,975
um, let's build a Jeopardy contestant rather than let's answer questions from the news,

343
00:18:52,975 --> 00:18:56,620
and that then led to their DeepQA system in 2011.

344
00:18:56,620 --> 00:18:59,155
Which I presume quite a few of you saw,

345
00:18:59,155 --> 00:19:02,545
these people saw Jeopardy IBM?

346
00:19:02,545 --> 00:19:04,120
Yeah, some of you.

347
00:19:04,120 --> 00:19:07,195
Okay. So, that they were able to successfully, um,

348
00:19:07,195 --> 00:19:13,180
build a question answering system that could compete at Jeopardy, um, and win.

349
00:19:13,180 --> 00:19:17,710
Um, and, you know, like a lot of these demonstrations of

350
00:19:17,710 --> 00:19:23,950
technological success there are things you can quibble about the way it was set up,

351
00:19:23,950 --> 00:19:27,250
um, that really the kind of computer just had

352
00:19:27,250 --> 00:19:32,260
a speed advantage versus the human beings that had to buzz in to answer the question.

353
00:19:32,260 --> 00:19:34,945
But, you know, nevertheless, fundamentally,

354
00:19:34,945 --> 00:19:37,540
the textual question-answering had to work,

355
00:19:37,540 --> 00:19:42,895
that this was a system that was answering questions mainly based on textual passages,

356
00:19:42,895 --> 00:19:47,079
and it had to be able to find the answers to those questions correctly,

357
00:19:47,079 --> 00:19:48,790
for the system to work.

358
00:19:48,790 --> 00:19:52,090
Um, so then, more recently again, um,

359
00:19:52,090 --> 00:19:55,990
and really the first piece of work that did this with a neural system was,

360
00:19:55,990 --> 00:19:58,000
um, work that was, um,

361
00:19:58,000 --> 00:19:59,650
done by a Stanford PhD student,

362
00:19:59,650 --> 00:20:00,925
that I'll get to later,

363
00:20:00,925 --> 00:20:02,350
was then the idea of well,

364
00:20:02,350 --> 00:20:06,940
could we replace traditional complex question answering systems

365
00:20:06,940 --> 00:20:09,954
by using a neural reading comprehension system,

366
00:20:09,954 --> 00:20:12,280
and that's proved to be very successful.

367
00:20:12,280 --> 00:20:15,970
So, to, to explain that a little bit more, um,

368
00:20:15,970 --> 00:20:20,410
if you look at the kind of systems that were built for TREC question-answering,

369
00:20:20,410 --> 00:20:24,640
um, they were very complex multi-part systems.

370
00:20:24,640 --> 00:20:27,565
And really, if you then look at something like,

371
00:20:27,565 --> 00:20:31,510
IBM's Deep QA system it was sort of like this

372
00:20:31,510 --> 00:20:35,950
times 10 because it both had very complex systems like this,

373
00:20:35,950 --> 00:20:40,465
but it ensembled together sort of six different components in every place,

374
00:20:40,465 --> 00:20:41,860
and then did sort of,

375
00:20:41,860 --> 00:20:45,220
um, classify a combination on top of them.

376
00:20:45,220 --> 00:20:46,660
But so far, the current-.

377
00:20:46,660 --> 00:20:51,850
This is sort of around a sort of a 2003 question answering system,

378
00:20:51,850 --> 00:20:55,120
and so the kind of things that went through is,

379
00:20:55,120 --> 00:20:56,980
so when there was a question,

380
00:20:56,980 --> 00:20:59,470
it parsed the question with a parser

381
00:20:59,470 --> 00:21:02,380
kind of like the ones we saw with our dependency parsers.

382
00:21:02,380 --> 00:21:03,875
It did some sort of

383
00:21:03,875 --> 00:21:09,435
handwritten semantic normalization rules to try and get them into a better semantic form.

384
00:21:09,435 --> 00:21:13,140
It then had a question type classifier which tried to

385
00:21:13,140 --> 00:21:16,890
work out what kind of semantic type is this question looking for,

386
00:21:16,890 --> 00:21:18,780
is it looking for a person name,

387
00:21:18,780 --> 00:21:19,890
or a country name,

388
00:21:19,890 --> 00:21:22,860
or a temperature, or something like that.

389
00:21:22,860 --> 00:21:27,825
Um, it would, um, then, um,

390
00:21:27,825 --> 00:21:32,280
have an information retrieval system out of the document collection,

391
00:21:32,280 --> 00:21:37,565
um, which would find paragraphs that were likely to contain the answers.

392
00:21:37,565 --> 00:21:40,510
Um, and then it would have a method of ranking

393
00:21:40,510 --> 00:21:45,175
those paragraph choices to see which ones are likely to have the answers.

394
00:21:45,175 --> 00:21:47,740
Um, it would then,

395
00:21:47,740 --> 00:21:50,365
um, over there somewhere, um,

396
00:21:50,365 --> 00:21:56,320
run Named Entity Recognition on those passages to find entities that were in them.

397
00:21:56,320 --> 00:21:59,515
These systems depended strongly on the use of

398
00:21:59,515 --> 00:22:02,350
fine matching entities because then it could look for

399
00:22:02,350 --> 00:22:05,755
an entity which corresponded to the question type.

400
00:22:05,755 --> 00:22:09,970
Um, then once it had candidate entities,

401
00:22:09,970 --> 00:22:11,980
it had to actually try and determine whether

402
00:22:11,980 --> 00:22:14,980
these entities did or didn't answer the question.

403
00:22:14,980 --> 00:22:18,745
So, these people, this is the system from LCC by,

404
00:22:18,745 --> 00:22:21,100
um, Sanda Harabagiu and Dan Moldovan.

405
00:22:21,100 --> 00:22:23,605
They actually had some quite interesting stuff here,

406
00:22:23,605 --> 00:22:28,900
where they had a kind of a loose theorem prover that would try and prove that, um,

407
00:22:28,900 --> 00:22:31,510
the semantic form of a piece of text,

408
00:22:31,510 --> 00:22:34,120
um, gave an answer to what the question was.

409
00:22:34,120 --> 00:22:38,410
So, you know, that was kind of cool stuff with an Axiomatic Knowledge Base,

410
00:22:38,410 --> 00:22:41,275
um, and eventually out would come an answer.

411
00:22:41,275 --> 00:22:44,305
Um, so, you know, something that is,

412
00:22:44,305 --> 00:22:46,300
I do just want to emphasize, you know,

413
00:22:46,300 --> 00:22:50,050
sometimes with these deep learning courses you get these days,

414
00:22:50,050 --> 00:22:55,330
the impression you have is that absolutely nothing worked before 2014,

415
00:22:55,330 --> 00:22:57,445
uh, when we got back to deep learning,

416
00:22:57,445 --> 00:22:59,440
and that's not actually true.

417
00:22:59,440 --> 00:23:01,570
So, these kind of factoid question on,

418
00:23:01,570 --> 00:23:03,970
these kind of question answering systems within

419
00:23:03,970 --> 00:23:07,135
a certain domain actually really worked rather well.

420
00:23:07,135 --> 00:23:10,690
Um, so, I started saying the word Factoid Question Answering,

421
00:23:10,690 --> 00:23:13,120
and so let me explain that because that's the secret.

422
00:23:13,120 --> 00:23:14,860
So, people, at least in NLP,

423
00:23:14,860 --> 00:23:17,965
use the term "Factoid Question Answering" to mean

424
00:23:17,965 --> 00:23:21,790
the case that your answer is a named entity.

425
00:23:21,790 --> 00:23:23,890
So, it's sort of something like, you know,

426
00:23:23,890 --> 00:23:26,215
what year was Elvis Presley born,

427
00:23:26,215 --> 00:23:32,050
or what is the name of Beyonce's husband, or, um,

428
00:23:32,050 --> 00:23:35,320
you know, which state,

429
00:23:35,320 --> 00:23:38,740
um, has the most pork or something, I don't know.

430
00:23:38,740 --> 00:23:40,240
Right, anything that's got,

431
00:23:40,240 --> 00:23:45,205
anything that's sort of the answer is sort of some clear semantic type entity,

432
00:23:45,205 --> 00:23:46,735
and that's your answer.

433
00:23:46,735 --> 00:23:50,935
I mean, so, within the space of those kind of questions,

434
00:23:50,935 --> 00:23:55,195
which actually is a significant part of the questions you get in web search, right?

435
00:23:55,195 --> 00:23:58,630
Lots of web search is just, you know,

436
00:23:58,630 --> 00:24:01,120
who was the star of this movie,

437
00:24:01,120 --> 00:24:03,355
or what year was somebody born, right?

438
00:24:03,355 --> 00:24:05,785
There's zillions of those all the time.

439
00:24:05,785 --> 00:24:08,710
These systems actually really did work quite well

440
00:24:08,710 --> 00:24:12,070
that they could get about 70 percent of those questions right,

441
00:24:12,070 --> 00:24:14,110
um, which wasn't bad at all, um,

442
00:24:14,110 --> 00:24:16,270
though that they really sort of didn't really

443
00:24:16,270 --> 00:24:19,380
extend it out to other kinds of stuff beyond that.

444
00:24:19,380 --> 00:24:22,400
But whatever virtues they had, um,

445
00:24:22,400 --> 00:24:28,280
they were extremely complex systems that people spent years put togeth- putting together,

446
00:24:28,280 --> 00:24:32,885
which had many components with a huge amount of hand-built stuff.

447
00:24:32,885 --> 00:24:39,035
And most of the stuff was sort of built quite separately and tied together,

448
00:24:39,035 --> 00:24:41,120
and you just sort of hope that it worked,

449
00:24:41,120 --> 00:24:44,045
um, well, when put together in composite.

450
00:24:44,045 --> 00:24:47,690
And so we can contrast that to what we then see later,

451
00:24:47,690 --> 00:24:51,275
um, for neural network-style systems.

452
00:24:51,275 --> 00:24:57,350
Okay. Um, so let me now say some more stuff about, um,

453
00:24:57,350 --> 00:25:02,870
the Stanford Question Answering Dataset or SQuAD that I just mentioned a little bit ago,

454
00:25:02,870 --> 00:25:07,055
and as this is the data for the default final project as well.

455
00:25:07,055 --> 00:25:10,040
Um, so what SQuAD has is,

456
00:25:10,040 --> 00:25:13,490
questions in SQuAD have a passage,

457
00:25:13,490 --> 00:25:16,070
which is a paragraph from Wikipedia.

458
00:25:16,070 --> 00:25:18,425
And then there is a question,

459
00:25:18,425 --> 00:25:21,755
here it's, "Which team won Super Bowl 50?"

460
00:25:21,755 --> 00:25:27,270
And the goal of the system is to come up with the answer to this question.

461
00:25:27,270 --> 00:25:30,430
Um, human reading comprehension.

462
00:25:30,430 --> 00:25:32,350
What is the answer to the question?

463
00:25:32,350 --> 00:25:36,640
[NOISE]

464
00:25:36,640 --> 00:25:37,510
Broncos.

465
00:25:37,510 --> 00:25:39,130
Broncos. [LAUGHTER] Okay.

466
00:25:39,130 --> 00:25:42,730
Yeah. Um, so that's the answer to the question.

467
00:25:42,730 --> 00:25:47,060
Um, and so by construction for SQuAD,

468
00:25:47,060 --> 00:25:53,570
the answer to a question is always a sub-sequence of words from the passage which is,

469
00:25:53,570 --> 00:25:56,345
normally, it ends up being referred to as a span,

470
00:25:56,345 --> 00:25:58,580
a sub-sequence of words from the passage.

471
00:25:58,580 --> 00:26:01,670
So that's the only kind of questions you can have.

472
00:26:01,670 --> 00:26:04,639
You can't have questions that are counting questions,

473
00:26:04,639 --> 00:26:07,130
or yes, no questions, or anything like that.

474
00:26:07,130 --> 00:26:10,475
You can just pick out a sub-sequence.

475
00:26:10,475 --> 00:26:12,260
Um, okay.

476
00:26:12,260 --> 00:26:18,650
But, um, so they created in the first version about 100,000 examples.

477
00:26:18,650 --> 00:26:22,040
So there are a bunch of questions about each passage.

478
00:26:22,040 --> 00:26:24,200
So it's sort of something like, um,

479
00:26:24,200 --> 00:26:28,580
I think it's maybe sort of about five questions per passage,

480
00:26:28,580 --> 00:26:32,315
and there are 20,000 different bits that Wikipedia uses, used.

481
00:26:32,315 --> 00:26:34,910
Um, and this sort of must be a span form,

482
00:26:34,910 --> 00:26:39,260
as often referred to as extractive question answering.

483
00:26:39,260 --> 00:26:43,520
Okay. Um, here's just one more example

484
00:26:43,520 --> 00:26:47,540
that can give you some more sense of some of the things that are there,

485
00:26:47,540 --> 00:26:50,345
and it illustrates a couple of other factors.

486
00:26:50,345 --> 00:26:52,760
Um, so, you know,

487
00:26:52,760 --> 00:26:56,360
even this one, I guess the previous one wasn't, um,

488
00:26:56,360 --> 00:26:59,600
completely obvious what your answers should be because

489
00:26:59,600 --> 00:27:02,900
maybe you could say the answer should just have been Broncos,

490
00:27:02,900 --> 00:27:05,720
or you could have said it was Denver Broncos.

491
00:27:05,720 --> 00:27:07,340
Um, and in general,

492
00:27:07,340 --> 00:27:09,785
even if you're answering with a span,

493
00:27:09,785 --> 00:27:13,445
there's gonna be variation as to how long a span you choose.

494
00:27:13,445 --> 00:27:16,040
Um, so what they did, um,

495
00:27:16,040 --> 00:27:18,680
and so this was done with, on Mechanical Turk,

496
00:27:18,680 --> 00:27:21,170
gathering the data, or building questions,

497
00:27:21,170 --> 00:27:25,790
and getting answers, is that they got answers from three different people.

498
00:27:25,790 --> 00:27:26,900
So here's this question,

499
00:27:26,900 --> 00:27:29,810
"Along with non-governmental and non-state schools,

500
00:27:29,810 --> 00:27:32,029
what is another name for private schools?"

501
00:27:32,029 --> 00:27:35,585
And three human beings were asked the answer based on this passage.

502
00:27:35,585 --> 00:27:37,009
And one said independent,

503
00:27:37,009 --> 00:27:39,485
and two said independent schools.

504
00:27:39,485 --> 00:27:42,950
Um, this one, all three people gave the same answer.

505
00:27:42,950 --> 00:27:45,515
This one, again, you get two different answers,

506
00:27:45,515 --> 00:27:48,020
so that they sample three answers.

507
00:27:48,020 --> 00:27:52,670
And basically, then, you can be correct if you're going with any of the answers.

508
00:27:52,670 --> 00:27:59,330
And so that sort of at least gives you a bit of robustness to variation in human answers.

509
00:27:59,330 --> 00:28:04,460
Okay. And that starts me into the topic of evaluation.

510
00:28:04,460 --> 00:28:05,855
Um, yeah.

511
00:28:05,855 --> 00:28:08,450
And these slides here are entitled

512
00:28:08,450 --> 00:28:12,140
SQuAD version 1,1 because that means in five minutes time,

513
00:28:12,140 --> 00:28:14,600
I'm gonna tell you about SQuAD version 2,

514
00:28:14,600 --> 00:28:16,640
which adds a bit more stuff into it,

515
00:28:16,640 --> 00:28:19,535
but we'll just get 1,1 straight first.

516
00:28:19,535 --> 00:28:22,895
All right. So there are three answers that col- were collected.

517
00:28:22,895 --> 00:28:25,280
And so for evaluation metrics,

518
00:28:25,280 --> 00:28:28,145
they suggested two evaluation metrics.

519
00:28:28,145 --> 00:28:31,340
The first one is exact match.

520
00:28:31,340 --> 00:28:34,250
So you're going to return a span.

521
00:28:34,250 --> 00:28:37,970
If the span is one of these three,

522
00:28:37,970 --> 00:28:39,515
you get one point,

523
00:28:39,515 --> 00:28:40,820
and if the scan,

524
00:28:40,820 --> 00:28:42,980
span is not one of these three,

525
00:28:42,980 --> 00:28:45,185
you get zero for that question.

526
00:28:45,185 --> 00:28:48,560
And then your accuracy is just the percent correct,

527
00:28:48,560 --> 00:28:50,345
so that's extremely simple.

528
00:28:50,345 --> 00:28:52,910
But the second metric, and actually,

529
00:28:52,910 --> 00:28:55,984
the one that was favored as the primary metric,

530
00:28:55,984 --> 00:28:58,235
was an F1 metric.

531
00:28:58,235 --> 00:29:01,504
So what you do for this F1 metric

532
00:29:01,504 --> 00:29:05,105
is you're matching at the word level for the different answers.

533
00:29:05,105 --> 00:29:06,935
So you've treat each,

534
00:29:06,935 --> 00:29:12,275
you treat the system span and each gold answer as a bag of words,

535
00:29:12,275 --> 00:29:14,930
and then you work out a precision, which is,

536
00:29:14,930 --> 00:29:22,780
um, the percent of words in the system's answer that are actually in a span,

537
00:29:22,780 --> 00:29:25,765
i- in a gold span, the recall,

538
00:29:25,765 --> 00:29:31,615
which is the percent of words in a gold span that are in the system's span.

539
00:29:31,615 --> 00:29:34,720
And then you calculate the harmonic mean of those two numbers

540
00:29:34,720 --> 00:29:37,760
and the harmonic mean is sort of a very conservative average.

541
00:29:37,760 --> 00:29:40,460
So it's close to the mean of those two numbers,

542
00:29:40,460 --> 00:29:42,800
and that gives you a score.

543
00:29:42,800 --> 00:29:47,375
And what you then do is, for each question,

544
00:29:47,375 --> 00:29:50,090
you'd return, you say its score is

545
00:29:50,090 --> 00:29:55,355
the maximum F1 over the three different answers that were collected from human beings.

546
00:29:55,355 --> 00:29:58,850
And then for the whole, um, dataset,

547
00:29:58,850 --> 00:30:05,195
you then average those F1 scores across questions and that's then your final F1 result.

548
00:30:05,195 --> 00:30:08,345
So that's a more complicated thing to say.

549
00:30:08,345 --> 00:30:12,080
Um, and we provide there sort of a val code,

550
00:30:12,080 --> 00:30:13,970
um, for you that does that.

551
00:30:13,970 --> 00:30:18,230
Um, but it sort of seems that F1 is actually

552
00:30:18,230 --> 00:30:24,199
a more reliable and better measure because if you use exact match,

553
00:30:24,199 --> 00:30:25,850
you know, even though there's of,

554
00:30:25,850 --> 00:30:29,525
a bit of robustness that comes on three people's answers,

555
00:30:29,525 --> 00:30:31,940
three is not a very large sample,

556
00:30:31,940 --> 00:30:34,310
so there's sort of a bit of guessing as to whether you get

557
00:30:34,310 --> 00:30:37,760
exactly the same span some human being got,

558
00:30:37,760 --> 00:30:41,180
whereas you're sort of going to get a reasonable score

559
00:30:41,180 --> 00:30:44,330
in the F1 even if your boundaries are off by a little.

560
00:30:44,330 --> 00:30:47,345
So the F1 metric sort of, um,

561
00:30:47,345 --> 00:30:52,760
is more reliable and avoids various kinds of artifacts as to how big

562
00:30:52,760 --> 00:30:58,295
or small an answer human beings tend to choose in some circumstances.

563
00:30:58,295 --> 00:31:00,650
Um, and so that's sort of being used as

564
00:31:00,650 --> 00:31:04,955
the primary metric that people score people on in the leader boards.

565
00:31:04,955 --> 00:31:07,970
Um, final detail, both metrics, um,

566
00:31:07,970 --> 00:31:13,235
ignore punctuation and the English articles a, an, the.

567
00:31:13,235 --> 00:31:17,390
Okay. Um, so how did things work out?

568
00:31:17,390 --> 00:31:21,170
Um, so for SQuAD version 1,1, um.

569
00:31:21,170 --> 00:31:23,090
A long time ago,

570
00:31:23,090 --> 00:31:25,250
at the end of 2016,

571
00:31:25,250 --> 00:31:27,905
um, this is how the leaderboard looked.

572
00:31:27,905 --> 00:31:30,680
Um, this is the bottom of the leaderboard at this point in

573
00:31:30,680 --> 00:31:34,145
time because that allows me to show you a couple of things.

574
00:31:34,145 --> 00:31:36,890
So down at the bottom of the leaderboard, um,

575
00:31:36,890 --> 00:31:40,520
so they tested how well human beings did, um,

576
00:31:40,520 --> 00:31:42,830
at answering these questions because you know,

577
00:31:42,830 --> 00:31:45,875
human beings aren't perfect at answering questions either.

578
00:31:45,875 --> 00:31:49,145
Um, and so the human performance that they measured,

579
00:31:49,145 --> 00:31:52,895
um, had an F1 score of 91,2.

580
00:31:52,895 --> 00:31:56,285
And I'll come back to that again in a minute.

581
00:31:56,285 --> 00:31:59,015
Um, and so when they built the dataset,

582
00:31:59,015 --> 00:32:04,790
they built a logistic regression baseline which was sort of a conventional NLP system.

583
00:32:04,790 --> 00:32:09,320
So, they dependency parsed the question and sentences of the answer.

584
00:32:09,320 --> 00:32:12,200
They looked for dependency.

585
00:32:12,200 --> 00:32:14,780
So dependency link matches,

586
00:32:14,780 --> 00:32:18,350
so a word at both ends with the dependency relation in

587
00:32:18,350 --> 00:32:23,615
between and count and matches of those and sort of pointing to a likely answer.

588
00:32:23,615 --> 00:32:29,795
Um, so as sort of a fairly competently built traditional NLP system of it's

589
00:32:29,795 --> 00:32:32,150
not as complex as but it's sort of in

590
00:32:32,150 --> 00:32:36,110
the same vein of that early question answering system I mentioned.

591
00:32:36,110 --> 00:32:39,410
And it got an F1 of about 51.

592
00:32:39,410 --> 00:32:41,225
So not hopeless, um,

593
00:32:41,225 --> 00:32:43,985
but not that great compared to human beings.

594
00:32:43,985 --> 00:32:46,520
And so, very shortly after that, um,

595
00:32:46,520 --> 00:32:48,635
people then started building

596
00:32:48,635 --> 00:32:53,750
neural network systems to try and do better at this task on this dataset.

597
00:32:53,750 --> 00:32:58,040
And so, one of the first people to do this quite successfully,

598
00:32:58,040 --> 00:33:01,580
um, were these people from Singapore Management University,

599
00:33:01,580 --> 00:33:05,150
maybe not the first place you would have thought of but, um,

600
00:33:05,150 --> 00:33:08,870
they were really sort of the first people who showed that, yes,

601
00:33:08,870 --> 00:33:12,320
you could build an end-to-end trained neural network

602
00:33:12,320 --> 00:33:15,320
for this task and do rather better.

603
00:33:15,320 --> 00:33:18,935
And so, they got up to 67 F1.

604
00:33:18,935 --> 00:33:22,100
Um, and well, then they had a second system.

605
00:33:22,100 --> 00:33:24,995
They got 70 and then things started,

606
00:33:24,995 --> 00:33:28,145
um, to, um, go on.

607
00:33:28,145 --> 00:33:29,675
So that even by,

608
00:33:29,675 --> 00:33:32,570
um, the end of 2016,

609
00:33:32,570 --> 00:33:38,180
um, there started to be systems that really worked rather well on this task.

610
00:33:38,180 --> 00:33:40,985
Um, so here, this time was the,

611
00:33:40,985 --> 00:33:42,815
um, top of the leaderboard.

612
00:33:42,815 --> 00:33:46,455
So I'll talk later about this BiDAF system from, uh,

613
00:33:46,455 --> 00:33:48,380
the AI to,

614
00:33:48,380 --> 00:33:51,800
Allen Institute for Artificial Intelligence and the University of Washington.

615
00:33:51,800 --> 00:33:53,810
So, it was getting to 77 as

616
00:33:53,810 --> 00:33:57,770
a single system that like in just about all machine learning,

617
00:33:57,770 --> 00:34:00,260
people pretty soon noticed that if you made

618
00:34:00,260 --> 00:34:03,440
an ensemble of identically structured systems,

619
00:34:03,440 --> 00:34:06,830
you could push the number higher and so if you ensemble those,

620
00:34:06,830 --> 00:34:11,090
you could then get another sort of whatever it is about four points

621
00:34:11,090 --> 00:34:15,800
and get up to 81, um, F1.

622
00:34:15,800 --> 00:34:22,445
And so this was sort of around the situation when in the, uh, 2017, um,

623
00:34:22,445 --> 00:34:30,440
224N class, we first used SQuAD version one as jus- as a default final project.

624
00:34:30,440 --> 00:34:32,240
And at that point, you know,

625
00:34:32,240 --> 00:34:36,470
actually the best students got almost to the top of this leaderboard.

626
00:34:36,470 --> 00:34:38,180
So our best, um,

627
00:34:38,180 --> 00:34:44,239
CS224N Final Project in winter 2017 made it into,

628
00:34:44,239 --> 00:34:47,690
um, the equivalent of fourth place on this leaderboard,

629
00:34:47,690 --> 00:34:51,080
um, with 77,5 as their score.

630
00:34:51,080 --> 00:34:52,790
So that was really rather cool.

631
00:34:52,790 --> 00:34:56,105
Um, but that's a couple of years ago and since then,

632
00:34:56,105 --> 00:34:58,100
people have started building, um,

633
00:34:58,100 --> 00:35:02,780
bigger and bigger and more and more complex, um, systems.

634
00:35:02,780 --> 00:35:06,140
And, um, so essentially,

635
00:35:06,140 --> 00:35:10,790
you could sort of say that SQuAD version one is basically solved.

636
00:35:10,790 --> 00:35:13,970
So the very best systems are now getting

637
00:35:13,970 --> 00:35:18,470
F1 scores that are in the low 90s and in particular,

638
00:35:18,470 --> 00:35:22,910
you can see that the best couple of, um,

639
00:35:22,910 --> 00:35:25,895
systems have higher F1s and

640
00:35:25,895 --> 00:35:31,250
well higher exact matches than what was measured for human beings.

641
00:35:31,250 --> 00:35:34,145
Uh, but like a lot of the claims of

642
00:35:34,145 --> 00:35:37,310
deep learning being better and performing from human being,

643
00:35:37,310 --> 00:35:41,000
than human beings, there's sort of some asterisks you can put after that.

644
00:35:41,000 --> 00:35:43,520
I mean, in particular for this dataset,

645
00:35:43,520 --> 00:35:48,125
the way they measured human performance was a little bit

646
00:35:48,125 --> 00:35:53,870
unfair because they only actually collected three human beings' answers.

647
00:35:53,870 --> 00:35:58,340
So, to judge, um, the human performance,

648
00:35:58,340 --> 00:36:05,780
the hu- those hu- each of those humans was being scored versus only two other humans.

649
00:36:05,780 --> 00:36:08,780
And so, that means you only had two chances to match instead of three.

650
00:36:08,780 --> 00:36:13,820
So, there's actually sort of a systematic underscoring of the human performance.

651
00:36:13,820 --> 00:36:17,745
But whatever, systems got very good at doing this.

652
00:36:17,745 --> 00:36:20,960
Um, so the next step, um,

653
00:36:20,960 --> 00:36:22,520
was then to introduce, uh,

654
00:36:22,520 --> 00:36:25,445
the SQuAD vers- version 2 task.

655
00:36:25,445 --> 00:36:29,990
And so many people felt that a defect of SQuAD version

656
00:36:29,990 --> 00:36:34,985
1 was that in all cases, questions had answers.

657
00:36:34,985 --> 00:36:40,445
So, that you just had to find the answer in the paragraph,

658
00:36:40,445 --> 00:36:44,120
um, and so that's sort of turned into a kind of a ranking task.

659
00:36:44,120 --> 00:36:48,355
You just had to work out what seems the most likely answer.

660
00:36:48,355 --> 00:36:50,500
I'll return that without really having

661
00:36:50,500 --> 00:36:53,910
any idea whether it was an answer to the question or not.

662
00:36:53,910 --> 00:36:56,525
And so, for SQuAD version two,

663
00:36:56,525 --> 00:36:58,790
for the dev and test sets,

664
00:36:58,790 --> 00:37:01,760
half of the questions have answers and half of

665
00:37:01,760 --> 00:37:04,955
the questions just don't have an answer in the passage,

666
00:37:04,955 --> 00:37:08,015
um, it's slightly different distribution, the training data.

667
00:37:08,015 --> 00:37:12,785
Um, and the way it works for scoring is the sort of, like,

668
00:37:12,785 --> 00:37:18,920
the no answer kind of counts as like one word as a sort of a special token.

669
00:37:18,920 --> 00:37:23,690
So, if it's, if it should be a no answer and you say no answer,

670
00:37:23,690 --> 00:37:28,580
you get a score of one on the either exact match or the F-measure.

671
00:37:28,580 --> 00:37:30,560
And if you don't do that,

672
00:37:30,560 --> 00:37:32,210
you get a score of zero.

673
00:37:32,210 --> 00:37:38,690
Um, and so, the simplest way of approaching SQuAD 2,0 would be to say, well,

674
00:37:38,690 --> 00:37:42,274
rather than just always returning the best match in my system,

675
00:37:42,274 --> 00:37:47,075
I'll use some kind of threshold and only if the score is above a threshold,

676
00:37:47,075 --> 00:37:48,785
our counters and answer.

677
00:37:48,785 --> 00:37:51,050
You could do more sophisticated things.

678
00:37:51,050 --> 00:37:54,080
So another area that we've worked on quite a bit at Stanford is

679
00:37:54,080 --> 00:37:58,520
this natural language inference task that I'll talk about later in the course.

680
00:37:58,520 --> 00:38:02,840
Um, but that's really about saying whether one piece of,

681
00:38:02,840 --> 00:38:05,630
um, text is the conclusion of another,

682
00:38:05,630 --> 00:38:06,890
um, piece of text.

683
00:38:06,890 --> 00:38:10,670
And so that's sort of a way that you can try and see whether, uh,

684
00:38:10,670 --> 00:38:17,120
a piece of text actually gives you a justification and answer to what the question was.

685
00:38:17,120 --> 00:38:21,530
But at any rate, this trying to decide whether

686
00:38:21,530 --> 00:38:27,005
you've actually got an answer or not is a quite difficult problem in many cases.

687
00:38:27,005 --> 00:38:31,880
So here's an example from SQuAD, um, 2,0.

688
00:38:31,880 --> 00:38:35,120
So Genghis Khan united the Mongol and Turkic tribes of

689
00:38:35,120 --> 00:38:38,855
the steppes and became Great Khan in 1206.

690
00:38:38,855 --> 00:38:42,290
He and his successors expanded the Mongol Empire across Asia,

691
00:38:42,290 --> 00:38:43,940
blah, blah, blah, blah.

692
00:38:43,940 --> 00:38:45,635
And the question is,

693
00:38:45,635 --> 00:38:48,260
when did Genghis Khan kill Great Khan?

694
00:38:48,260 --> 00:38:50,480
And the answer to that is,

695
00:38:50,480 --> 00:38:53,525
you know, uh, there isn't an answer because actually,

696
00:38:53,525 --> 00:38:59,150
Genghis Khan was a person named Great Khan and he didn't kill a Great Khan.

697
00:38:59,150 --> 00:39:01,835
It's just not a question with an answer.

698
00:39:01,835 --> 00:39:07,985
Um, but it's precisely what happens with systems is, you know,

699
00:39:07,985 --> 00:39:11,645
even though these systems get high scores in terms of points,

700
00:39:11,645 --> 00:39:15,980
they don't actually understand human language that well.

701
00:39:15,980 --> 00:39:17,615
So they look at something that says,

702
00:39:17,615 --> 00:39:20,855
when did Genghis Khan kill Great Khan?

703
00:39:20,855 --> 00:39:23,930
Well, this is something that's looking for a date and there are

704
00:39:23,930 --> 00:39:27,740
some obvious dates in this passage there's 1206, 1234,

705
00:39:27,740 --> 00:39:31,835
1251 and well, there's kill,

706
00:39:31,835 --> 00:39:36,560
and kill looks a little bit similar to destroyed.

707
00:39:36,560 --> 00:39:38,645
I can see the word destroyed.

708
00:39:38,645 --> 00:39:41,345
So that probably kind of matches.

709
00:39:41,345 --> 00:39:43,400
And then we're talking about, um,

710
00:39:43,400 --> 00:39:45,560
Genghis Khan and there,

711
00:39:45,560 --> 00:39:48,395
I can see Genghis and Khan in this passage.

712
00:39:48,395 --> 00:39:50,960
And so it sort of puts that together and says

713
00:39:50,960 --> 00:39:55,175
1234 is the answer when that isn't the answer at all.

714
00:39:55,175 --> 00:39:59,870
And that's actually kind of pretty typical of the behavior of these systems.

715
00:39:59,870 --> 00:40:03,560
And so that, on the one hand, they work great.

716
00:40:03,560 --> 00:40:06,155
On the other hand, they don't actually understand that much,

717
00:40:06,155 --> 00:40:10,025
and effectively asking whether there's,

718
00:40:10,025 --> 00:40:14,930
this question is actually answered in the passage is a way of

719
00:40:14,930 --> 00:40:17,360
revealing the extent to which these models

720
00:40:17,360 --> 00:40:20,945
do or don't understand what's actually going on.

721
00:40:20,945 --> 00:40:23,915
Okay. So, at the time, um,

722
00:40:23,915 --> 00:40:27,095
they built SQuAD version 2,0.

723
00:40:27,095 --> 00:40:28,835
They took some of, um,

724
00:40:28,835 --> 00:40:32,090
the existing SQuAD version one's systems,

725
00:40:32,090 --> 00:40:36,725
and, um, modified them in a very simple way.

726
00:40:36,725 --> 00:40:39,275
I put in a threshold, um,

727
00:40:39,275 --> 00:40:43,175
score as to how good the final match was deemed to be,

728
00:40:43,175 --> 00:40:47,645
and said, Well, how well do you do on SQuAD 2,0?

729
00:40:47,645 --> 00:40:50,825
And the kind of systems that we saw doing well before,

730
00:40:50,825 --> 00:40:52,370
now didn't do that well,

731
00:40:52,370 --> 00:40:58,820
so something like the BiDAF system that we mentioned before was now scoring about 62 F1,

732
00:40:58,820 --> 00:41:01,370
so that that was sort of hugely lowering

733
00:41:01,370 --> 00:41:05,210
its performance and reflecting the limits of understanding.

734
00:41:05,210 --> 00:41:09,650
Um, but it turned out actually that this problem didn't prove to

735
00:41:09,650 --> 00:41:14,240
be q- quite as difficult as the dataset authors,

736
00:41:14,240 --> 00:41:16,820
um, maybe thought either.

737
00:41:16,820 --> 00:41:19,775
Um, because it turns out that um,

738
00:41:19,775 --> 00:41:23,375
here we are now in February 2019,

739
00:41:23,375 --> 00:41:26,285
and if you look at the top of the leaderboard,

740
00:41:26,285 --> 00:41:29,465
we're kind of getting close again to the point

741
00:41:29,465 --> 00:41:32,780
where the best systems are almost as good as human beings.

742
00:41:32,780 --> 00:41:39,080
So, um, the current top rate system there you can see is getting 87,6 F1,

743
00:41:39,080 --> 00:41:43,220
which is less than two points behind where the human beings are.

744
00:41:43,220 --> 00:41:47,510
Um, the SQuAD version 2 they also co- corrected the,

745
00:41:47,510 --> 00:41:49,400
um, scoring of human beings,

746
00:41:49,400 --> 00:41:52,805
so it's more of a fair evaluation this time, um,

747
00:41:52,805 --> 00:41:54,920
so there's still a bit of a gap but, you know,

748
00:41:54,920 --> 00:41:58,010
the systems are actually doing, um, really well.

749
00:41:58,010 --> 00:42:01,040
And the interesting thing there is,

750
00:42:01,040 --> 00:42:04,625
you know, on the one hand these systems are impressively good.

751
00:42:04,625 --> 00:42:06,890
Um, you can go on the SQuAD website and look

752
00:42:06,890 --> 00:42:09,275
at the output of several of the good systems,

753
00:42:09,275 --> 00:42:12,335
and you can see that there are just a ton of things that they get right.

754
00:42:12,335 --> 00:42:14,330
They're absolutely not bad systems.

755
00:42:14,330 --> 00:42:18,980
You have to be a good system to be getting five out of six of the questions right.

756
00:42:18,980 --> 00:42:21,860
Um, but, you know, on the other hand they still

757
00:42:21,860 --> 00:42:25,130
make quite elementary Natural Language Understanding Errors.

758
00:42:25,130 --> 00:42:28,295
And so here's an example of one of those.

759
00:42:28,295 --> 00:42:29,720
Okay, so this one,

760
00:42:29,720 --> 00:42:32,540
the Yuan dynasty is considered both a successor to

761
00:42:32,540 --> 00:42:36,155
the Mongol Empire and an imperial Chinese dynasty.

762
00:42:36,155 --> 00:42:38,840
It was the khanate ruled by the successors of

763
00:42:38,840 --> 00:42:42,665
Mongke Khan after the division of the Mongol Empire.

764
00:42:42,665 --> 00:42:46,730
In official Chinese histories the Yuan dynasty bore the Mandate of Heaven,

765
00:42:46,730 --> 00:42:50,480
following the Song dynasty and preceding the Ming dynasty.

766
00:42:50,480 --> 00:42:52,655
Okay. And then the question is,

767
00:42:52,655 --> 00:42:55,760
what dynasty came before the Yuan?

768
00:42:55,760 --> 00:42:58,490
And that's a pretty easy question,

769
00:42:58,490 --> 00:42:59,990
I'd hope, for a human being.

770
00:42:59,990 --> 00:43:02,790
Everyone can answer that question?

771
00:43:02,830 --> 00:43:08,480
Okay, um, yeah, so it says in official Chinese histories Yuan Dynast- uh,

772
00:43:08,480 --> 00:43:09,920
sorry the next sentence.

773
00:43:09,920 --> 00:43:12,560
Um, yeah followed- right the Yuan Dynasty following

774
00:43:12,560 --> 00:43:15,245
the Song dynasty and preceding the Ming dynasty.

775
00:43:15,245 --> 00:43:17,555
But, you know actually um,

776
00:43:17,555 --> 00:43:20,960
this sort of the leading um,

777
00:43:20,960 --> 00:43:25,310
Google BERT model says that it was the Ming dynasty that came before

778
00:43:25,310 --> 00:43:29,450
the Yuan Dynasty which you know is sort of elementarily

779
00:43:29,450 --> 00:43:33,320
wrong that reveals some of the same kind of it's

780
00:43:33,320 --> 00:43:38,240
not really understanding everything but it's doing a sort of a matching problem still.

781
00:43:38,240 --> 00:43:45,620
Okay. So, this SQuAD dataset has been useful and good.

782
00:43:45,620 --> 00:43:48,860
It still has some major limitations and I just thought I'd

783
00:43:48,860 --> 00:43:52,370
mentioned what a few of those are so you're aware of some of the issues.

784
00:43:52,370 --> 00:43:54,950
So one of them I've already mentioned, right,

785
00:43:54,950 --> 00:44:00,740
that you're in this space where all answers are a span from the passage.

786
00:44:00,740 --> 00:44:03,890
And that just limits the kind of questions you can

787
00:44:03,890 --> 00:44:07,025
ask and the kind of difficult situations there can be.

788
00:44:07,025 --> 00:44:10,370
So, there can't be yes-no questions counting

789
00:44:10,370 --> 00:44:15,785
questions or even any of the sort of more difficult implicit questions.

790
00:44:15,785 --> 00:44:21,185
So, if you think back to when you were in middle school and did reading comprehension,

791
00:44:21,185 --> 00:44:23,825
I mean, it wasn't typically um,

792
00:44:23,825 --> 00:44:27,440
the case um, that you're being asked

793
00:44:27,440 --> 00:44:31,400
questions that were just stated explicitly in the text of,

794
00:44:31,400 --> 00:44:34,880
you know, Sue is visiting her mother in Miami.

795
00:44:34,880 --> 00:44:36,335
And the question was,

796
00:44:36,335 --> 00:44:38,315
who was visiting in Miami?

797
00:44:38,315 --> 00:44:43,730
That wasn't the kind of questions you were asked you were normally asked questions um,

798
00:44:43,730 --> 00:44:46,310
like um, you know,

799
00:44:46,310 --> 00:44:52,505
um, Sue is going to a job interview this morning,

800
00:44:52,505 --> 00:44:56,360
um, it's a really important job interview for her future.

801
00:44:56,360 --> 00:44:59,435
At breakfast she um,

802
00:44:59,435 --> 00:45:03,395
starts buttering both sides of her piece of toast um,

803
00:45:03,395 --> 00:45:06,410
and you are asked a question like, um,

804
00:45:06,410 --> 00:45:11,320
why um, is Sue buttering both sides of her piece of toast?

805
00:45:11,320 --> 00:45:13,420
And you're meant to be able to answer,

806
00:45:13,420 --> 00:45:17,680
"She's distracted by her important job interview coming up later in the day."

807
00:45:17,680 --> 00:45:20,995
Which isn't the- something that you can answer um,

808
00:45:20,995 --> 00:45:23,505
by just picking out a sub span.

809
00:45:23,505 --> 00:45:31,055
Um, a second problem which is sort of actually a bigger problem is um,

810
00:45:31,055 --> 00:45:35,645
the way SQuAD was constructed for ease

811
00:45:35,645 --> 00:45:41,970
and not to be too expensive and various other reasons was um,

812
00:45:41,970 --> 00:45:46,235
paragraphs of Wikipedia were selected and then,

813
00:45:46,235 --> 00:45:48,680
Mechanical Turkers were hired to say,

814
00:45:48,680 --> 00:45:51,215
"Come up with some questions um,

815
00:45:51,215 --> 00:45:56,210
that can be answered by this this passage in version 1,1."

816
00:45:56,210 --> 00:45:59,315
And then in version two they were said- told,

817
00:45:59,315 --> 00:46:03,170
"Also come up with some questions that

818
00:46:03,170 --> 00:46:07,385
look like they're related to this passage but aren't actually answered in the passage."

819
00:46:07,385 --> 00:46:10,070
But, in all cases people were coming up with

820
00:46:10,070 --> 00:46:14,870
the questions staring at the passage and if you do that,

821
00:46:14,870 --> 00:46:18,260
it means that your questions are strongly

822
00:46:18,260 --> 00:46:21,905
overlapping with the passage both in terms of the,

823
00:46:21,905 --> 00:46:26,630
the words that are used and even the syntactic structures that are

824
00:46:26,630 --> 00:46:31,520
used for your questions tending to match the syntactic structures of the passage.

825
00:46:31,520 --> 00:46:37,085
And so that makes question answering um, naturally easy.

826
00:46:37,085 --> 00:46:39,125
What happens in the real world,

827
00:46:39,125 --> 00:46:42,260
is this human beings think up questions and

828
00:46:42,260 --> 00:46:46,010
type something into a search engine and the way

829
00:46:46,010 --> 00:46:49,355
that they type it in is completely distinct

830
00:46:49,355 --> 00:46:53,075
from the way something might be worded on a website.

831
00:46:53,075 --> 00:46:56,600
So that they might be saying something like,

832
00:46:56,600 --> 00:47:02,720
you know, "In what year did the price of hard disks drop below a dollar a megabyte?"

833
00:47:02,720 --> 00:47:07,220
Um, and the webpage will say something like

834
00:47:07,220 --> 00:47:12,050
the cost of hard disks has being dropping for many years um,

835
00:47:12,050 --> 00:47:18,470
in I know whenever it was 2004 prices eventually crossed um,

836
00:47:18,470 --> 00:47:20,870
the dollar megabyte barrier or something like that.

837
00:47:20,870 --> 00:47:24,785
But there's a quite different discussion of the ideas.

838
00:47:24,785 --> 00:47:28,220
And that kinda matching is much harder and that's one of

839
00:47:28,220 --> 00:47:32,270
the things that people have done other datasets have tried to do differently.

840
00:47:32,270 --> 00:47:35,960
Um, another limitation is that these questions and

841
00:47:35,960 --> 00:47:40,355
answers are very much, find the sentence that's addressing the fact,

842
00:47:40,355 --> 00:47:42,545
match your question to the sentence,

843
00:47:42,545 --> 00:47:45,080
return the right thing,

844
00:47:45,080 --> 00:47:49,400
that there's nothing sort of more difficult than involves multi sentence,

845
00:47:49,400 --> 00:47:53,210
combine facts together styles of inferencing,

846
00:47:53,210 --> 00:47:57,050
that the limits of cross sentence stuff there is pretty much limited to

847
00:47:57,050 --> 00:48:01,295
resolving co-reference which is something we'll talk about later in the class,

848
00:48:01,295 --> 00:48:04,310
that means that you see a he or she or an it,

849
00:48:04,310 --> 00:48:09,125
and you can work out who that refers to earlier in the, this course.

850
00:48:09,125 --> 00:48:12,590
Um, nevertheless, despite all those disadvantages,

851
00:48:12,590 --> 00:48:15,230
it sort of proved that SQuAD was, you know,

852
00:48:15,230 --> 00:48:20,180
well-targeted in terms of its level of difficulty, well-structured,

853
00:48:20,180 --> 00:48:22,910
clean dataset, and it's just been

854
00:48:22,910 --> 00:48:27,140
sort of everybody's favorite for a question answering dataset.

855
00:48:27,140 --> 00:48:30,080
It also seems to have proved that actually for

856
00:48:30,080 --> 00:48:33,530
people who work in industry and want to build a question answering system,

857
00:48:33,530 --> 00:48:36,005
starting off by training a model in SQuAD,

858
00:48:36,005 --> 00:48:39,230
actually turns out to work pretty well it turns out.

859
00:48:39,230 --> 00:48:41,420
I mean, it's not everything you want to do.

860
00:48:41,420 --> 00:48:46,250
You definitely wanna have relevant in domain data and be using that as well,

861
00:48:46,250 --> 00:48:50,450
but you know, it turns out that it seems to actually be a quite useful starting point.

862
00:48:50,450 --> 00:48:55,865
Okay. So, what I wanted to show you now was a- is a concrete,

863
00:48:55,865 --> 00:49:00,710
simple, neural question answering system.

864
00:49:00,710 --> 00:49:08,300
Um, and this is the model that was built by here and I guess she was

865
00:49:08,300 --> 00:49:15,860
sort of an Abby predecessor since she was the preceding head TA for CS 224N.

866
00:49:15,860 --> 00:49:18,650
Um, so this system,

867
00:49:18,650 --> 00:49:21,830
um, Stanford Attentive Reader it kind of gets called now.

868
00:49:21,830 --> 00:49:24,575
I mean, this is sort of essentially

869
00:49:24,575 --> 00:49:29,990
the simplest neural question answering system that works pretty well.

870
00:49:29,990 --> 00:49:32,780
So, it's not a bad thing to have in mind as

871
00:49:32,780 --> 00:49:36,320
a baseline and it's not the current state of the art by any means.

872
00:49:36,320 --> 00:49:40,790
But you know, if you're sort of wondering what's the simplest thing that I can build

873
00:49:40,790 --> 00:49:45,215
that basically works as a question answering system decently,

874
00:49:45,215 --> 00:49:47,315
this is basically it.

875
00:49:47,315 --> 00:49:50,390
Um, okay. So how does this work?

876
00:49:50,390 --> 00:49:52,595
So the way it works is like this.

877
00:49:52,595 --> 00:49:53,930
So, first of all,

878
00:49:53,930 --> 00:49:58,205
we have a question which team won Super Bowl 50?

879
00:49:58,205 --> 00:50:04,175
And what we're gonna wanna do is build a representation of a question as a vector.

880
00:50:04,175 --> 00:50:06,920
And the way we can do that is like this,

881
00:50:06,920 --> 00:50:09,035
for each word in the question,

882
00:50:09,035 --> 00:50:10,835
we look up a word embedding.

883
00:50:10,835 --> 00:50:15,440
So, in particular it used GloVe- GloVe 300 dimensional word embeddings.

884
00:50:15,440 --> 00:50:19,235
Um, we then run an LSTM

885
00:50:19,235 --> 00:50:23,330
forward through the question and then kind of like Abby talked about,

886
00:50:23,330 --> 00:50:25,295
we actually make it a bi-LSTM.

887
00:50:25,295 --> 00:50:29,030
So, we run a second LSTM backwards through the question.

888
00:50:29,030 --> 00:50:34,880
And so then, we grab the end state of both LSTMs

889
00:50:34,880 --> 00:50:40,760
and we simply concatenate them together into a vector of dimension 2D if,

890
00:50:40,760 --> 00:50:43,730
if our hidden states of the LSTM are dimension

891
00:50:43,730 --> 00:50:48,425
d and we say that is the representation of the question.

892
00:50:48,425 --> 00:50:51,245
Okay. So, once we have that,

893
00:50:51,245 --> 00:50:54,230
we then start looking at the passage.

894
00:50:54,230 --> 00:50:57,635
And so, for the start of dealing with the passage,

895
00:50:57,635 --> 00:50:59,180
we do the same thing.

896
00:50:59,180 --> 00:51:03,110
We, um, look up a word vector for every word in

897
00:51:03,110 --> 00:51:07,340
the passage and we run a bidirectional LSTM,

898
00:51:07,340 --> 00:51:12,200
now being represented a bit more compactly um, across the passage.

899
00:51:12,200 --> 00:51:15,710
But then we have to do a little bit more work because we actually

900
00:51:15,710 --> 00:51:19,040
have to find the answer in the passage.

901
00:51:19,040 --> 00:51:21,680
And so what we're gonna do is use

902
00:51:21,680 --> 00:51:28,175
the question representation to sort of work out where the answer is using attention.

903
00:51:28,175 --> 00:51:31,805
So this is a different use of attention to machine translation.

904
00:51:31,805 --> 00:51:35,105
That kind of attention equations are still exactly the same.

905
00:51:35,105 --> 00:51:39,170
But we've now got this sort of one question vector that we gonna be trying to

906
00:51:39,170 --> 00:51:43,385
match against to return the answer.

907
00:51:43,385 --> 00:51:47,150
So, what we do is we, um,

908
00:51:47,150 --> 00:51:51,125
work out an attention score between

909
00:51:51,125 --> 00:51:57,575
each word's bi-LSTM representation and the question.

910
00:51:57,575 --> 00:52:02,930
And so the way that's being done is we're using this bi-linear attention,

911
00:52:02,930 --> 00:52:07,370
um, that um, Abby briefly discussed and we'll see more of today.

912
00:52:07,370 --> 00:52:09,140
We've got the question vector,

913
00:52:09,140 --> 00:52:12,530
the vector for a particular position in the passage

914
00:52:12,530 --> 00:52:15,770
to the two concatenated LSTM hidden states.

915
00:52:15,770 --> 00:52:17,930
So they're the same dimensionality.

916
00:52:17,930 --> 00:52:21,020
We have this intervening learn W matrix.

917
00:52:21,020 --> 00:52:23,360
So, we work out that quantity,

918
00:52:23,360 --> 00:52:25,115
um, for each position,

919
00:52:25,115 --> 00:52:27,890
and then we put that through a softmax which will give us

920
00:52:27,890 --> 00:52:32,180
probabilities over the different words in the passage.

921
00:52:32,180 --> 00:52:34,220
Um, and those give us,

922
00:52:34,220 --> 00:52:36,665
um, our attention weights.

923
00:52:36,665 --> 00:52:39,350
And so at that point we have attention weights,

924
00:52:39,350 --> 00:52:42,140
um, for different positions, um,

925
00:52:42,140 --> 00:52:45,410
in the passage and we just declare that,

926
00:52:45,410 --> 00:52:47,030
um, that is where,

927
00:52:47,030 --> 00:52:49,610
um, the answer starts.

928
00:52:49,610 --> 00:52:53,270
Um, and then to get the end of the answer,

929
00:52:53,270 --> 00:53:01,310
we simply do exactly the same thing again apart from we train a different W matrix here,

930
00:53:01,310 --> 00:53:02,840
and we have that,

931
00:53:02,840 --> 00:53:04,940
um, predict the end token.

932
00:53:04,940 --> 00:53:07,490
And there's something a little bit subtle here.

933
00:53:07,490 --> 00:53:10,610
Um, because, you know, really we're asking it to sort

934
00:53:10,610 --> 00:53:13,685
of predict the starts and the ends of the answer,

935
00:53:13,685 --> 00:53:15,830
and you might think, but wait a minute.

936
00:53:15,830 --> 00:53:19,595
Surely, we need to look at the middle of the answer as well because maybe the,

937
00:53:19,595 --> 00:53:23,405
the most indicative words are actually going to be in the middle of the answer.

938
00:53:23,405 --> 00:53:27,710
Um, but, you know, really really what we're,

939
00:53:27,710 --> 00:53:32,960
we're sort of implicitly telling the model of well,

940
00:53:32,960 --> 00:53:37,055
when you're training, if there's stuff in the middle that's useful,

941
00:53:37,055 --> 00:53:42,440
it's the bi-LSTM's job to push it to the extremes of the span,

942
00:53:42,440 --> 00:53:47,075
so that this simple bi-linear attention

943
00:53:47,075 --> 00:53:51,950
will be able to get a big score at the start of the span.

944
00:53:51,950 --> 00:53:55,040
And you might also think there's something

945
00:53:55,040 --> 00:53:58,370
funny that this equation and that equation are exactly the same.

946
00:53:58,370 --> 00:54:02,270
So, how come one of them is meant to know it's picking up beginning, um,

947
00:54:02,270 --> 00:54:04,400
and the other at the end?

948
00:54:04,400 --> 00:54:07,475
And again, you know, we're not doing anything to impose that.

949
00:54:07,475 --> 00:54:09,890
We're just saying, neural network.

950
00:54:09,890 --> 00:54:11,915
It is your job to learn.

951
00:54:11,915 --> 00:54:16,115
Um, you have to learn a matrix here and a different one over there,

952
00:54:16,115 --> 00:54:20,240
so that one of them will pick out parts of the representation that

953
00:54:20,240 --> 00:54:25,175
indicate starts of answer spans and the other one ends of answer spans.

954
00:54:25,175 --> 00:54:28,160
And so, that will then again pressure

955
00:54:28,160 --> 00:54:31,550
the neural network to sort of self organize itself in

956
00:54:31,550 --> 00:54:34,100
such a way that there'll be some parts of

957
00:54:34,100 --> 00:54:38,270
this hidden representation that will be good at learning starts of spans.

958
00:54:38,270 --> 00:54:40,010
You know, maybe there'll be carried backwards by

959
00:54:40,010 --> 00:54:43,520
the backwards LSTM and and some parts of it will be good at

960
00:54:43,520 --> 00:54:45,980
learning where the spans end and then

961
00:54:45,980 --> 00:54:50,610
the W matrix will be able to pick out those parts of the representation.

962
00:54:50,610 --> 00:54:54,130
Um, but yeah, uh,

963
00:54:54,130 --> 00:54:58,360
that's the system. Um, yeah.

964
00:54:58,360 --> 00:55:00,640
So, um, so this is

965
00:55:00,640 --> 00:55:05,980
the basic Stanford Attentive Reader model and it's just no more complex than that.

966
00:55:05,980 --> 00:55:08,770
Um, and the interesting thing is, you know,

967
00:55:08,770 --> 00:55:14,245
that very simple model actually works nicely well.

968
00:55:14,245 --> 00:55:16,360
Um, so this is going back in time.

969
00:55:16,360 --> 00:55:23,230
Again, this was the February 2017 SQuAD version 1 leaderboard.

970
00:55:23,230 --> 00:55:28,690
Um, but at that time, that provide- like,

971
00:55:28,690 --> 00:55:32,680
it always in neural networks quite a bit of your success

972
00:55:32,680 --> 00:55:39,280
is training your hyperparameters and optimizing your model really well.

973
00:55:39,280 --> 00:55:41,260
And some time, you know,

974
00:55:41,260 --> 00:55:47,020
it's been repeatedly proven in neural network land that often you can get

975
00:55:47,020 --> 00:55:50,170
much better scores than you would think from

976
00:55:50,170 --> 00:55:53,845
very simple models if you optimize them really well.

977
00:55:53,845 --> 00:55:57,280
So there have been multiple cycles in sort of

978
00:55:57,280 --> 00:55:59,830
deep learning research where there

979
00:55:59,830 --> 00:56:02,950
was a paper that did something and then the next person says,

980
00:56:02,950 --> 00:56:04,960
"Here's a more- more- more complex model that

981
00:56:04,960 --> 00:56:07,540
works better," and then someone else published a paper saying,

982
00:56:07,540 --> 00:56:09,640
"Here's an even more complex than that model that works

983
00:56:09,640 --> 00:56:12,490
better," and then someone points out, "No.

984
00:56:12,490 --> 00:56:17,140
If you go back to the first model and just really train its hyperparameters well,

985
00:56:17,140 --> 00:56:19,375
you can beat both of those two models."

986
00:56:19,375 --> 00:56:21,880
And that was effectively the case about what

987
00:56:21,880 --> 00:56:24,610
was happening with the Stanford Attentive Reader.

988
00:56:24,610 --> 00:56:29,245
That, you know, back in- back in February 2017,

989
00:56:29,245 --> 00:56:32,920
if you just train this model really well,

990
00:56:32,920 --> 00:56:37,990
it could actually outperform most of the early SQuAD systems.

991
00:56:37,990 --> 00:56:39,235
I mean, in particular,

992
00:56:39,235 --> 00:56:41,875
it could outperform, um, the BiDAF,

993
00:56:41,875 --> 00:56:46,390
the version of BiDAF that was around in early 2017 and,

994
00:56:46,390 --> 00:56:49,315
you know, various of these other systems from other people.

995
00:56:49,315 --> 00:56:51,340
But it was actually, at that time,

996
00:56:51,340 --> 00:56:55,405
it was pretty close to the best system that anyone had built.

997
00:56:55,405 --> 00:56:57,970
Um, as I've already pointed out to you,

998
00:56:57,970 --> 00:57:00,280
um, the numbers have gone up a lot since then.

999
00:57:00,280 --> 00:57:02,500
So I'm not claiming that, um,

1000
00:57:02,500 --> 00:57:08,785
this system is still as good as the best systems that you can build. But there you go.

1001
00:57:08,785 --> 00:57:13,000
Um, so that's the simple system that already works pretty well,

1002
00:57:13,000 --> 00:57:15,070
but of course you want this system to work better.

1003
00:57:15,070 --> 00:57:19,690
Um, and so Danqi did quite a bit of work on that.

1004
00:57:19,690 --> 00:57:23,305
And so here I'll just mention a few things for, um,

1005
00:57:23,305 --> 00:57:26,125
Stanford Attentive Reader++ as to

1006
00:57:26,125 --> 00:57:29,635
what kind of things can you do to make the model better.

1007
00:57:29,635 --> 00:57:34,705
And so here's a sort of a picture of, um,

1008
00:57:34,705 --> 00:57:37,960
the sort of the improved system and we'll go through

1009
00:57:37,960 --> 00:57:41,290
some of the differences and what makes it better.

1010
00:57:41,290 --> 00:57:45,190
Um, there's something I didn't have before that I should just mention, right?

1011
00:57:45,190 --> 00:57:50,215
Sort of this whole model, all the parameters of this model are just trained end to end,

1012
00:57:50,215 --> 00:57:53,980
where your training objective is simply, um,

1013
00:57:53,980 --> 00:57:56,380
working out how accurately you're predicting

1014
00:57:56,380 --> 00:57:59,050
the start position and how accurately you're predicting

1015
00:57:59,050 --> 00:58:02,680
the end position so that the attention gives

1016
00:58:02,680 --> 00:58:06,505
you a probability distribution over start positions and end positions.

1017
00:58:06,505 --> 00:58:09,820
So you're just being asked what probability estimate

1018
00:58:09,820 --> 00:58:13,330
are you giving to the true start position and the true end position.

1019
00:58:13,330 --> 00:58:15,250
And to the extent that though,

1020
00:58:15,250 --> 00:58:17,289
you know, those aren't one,

1021
00:58:17,289 --> 00:58:22,375
you've then got loss that is then being sort of summed in terms of log probability.

1022
00:58:22,375 --> 00:58:25,570
Okay. So how is this model, um,

1023
00:58:25,570 --> 00:58:28,855
more complex now than what I showed before?

1024
00:58:28,855 --> 00:58:31,945
Essentially in two main ways.

1025
00:58:31,945 --> 00:58:36,370
So the first one is looking at the question,

1026
00:58:36,370 --> 00:58:40,075
we still run the BiLSTM as before.

1027
00:58:40,075 --> 00:58:44,530
Um, but now what we're going to do is it's a little bit crude

1028
00:58:44,530 --> 00:58:48,850
just to take the end states of the LSTM and concatenate them together.

1029
00:58:48,850 --> 00:58:54,280
It turns out that you can do better by making use of all states in an LSTM.

1030
00:58:54,280 --> 00:58:57,880
And this is true for most tasks where you

1031
00:58:57,880 --> 00:59:01,975
want some kind of sentence representation from a sequence model.

1032
00:59:01,975 --> 00:59:04,585
It turns out you can generally gain by using

1033
00:59:04,585 --> 00:59:07,510
all of them rather than just the endpoints or that.

1034
00:59:07,510 --> 00:59:12,685
Um, so but this is just an interesting general thing to know again because, you know,

1035
00:59:12,685 --> 00:59:18,415
this is actually another variant of how that- how you can use attention.

1036
00:59:18,415 --> 00:59:25,525
There are, you know, a lot of sort of the last two years of neural NLP can be summed

1037
00:59:25,525 --> 00:59:29,230
up as people have found a lot of clever ways to use

1038
00:59:29,230 --> 00:59:33,220
attention and that's been pairing just about all the advances.

1039
00:59:33,220 --> 00:59:41,890
Um, so what we wanna do is we want to have attention over the positions in this LSTM.

1040
00:59:41,890 --> 00:59:46,255
But, you know, this- we're processing the query first.

1041
00:59:46,255 --> 00:59:51,355
So it sort of seems like we've got nothing to calculate attention with respect to.

1042
00:59:51,355 --> 00:59:55,150
So what we do is we just invent something.

1043
00:59:55,150 --> 00:59:56,860
So we just sort of invent.

1044
00:59:56,860 --> 01:00:01,660
Here is a vector and it's sometimes called a sentinel or some word like that,

1045
01:00:01,660 --> 01:00:03,850
but, you know, we just in our PyTorch say,

1046
01:00:03,850 --> 01:00:05,185
"Here is a vector.

1047
01:00:05,185 --> 01:00:07,690
Um, we're going to calculate, um,

1048
01:00:07,690 --> 01:00:09,460
we initialize it randomly,

1049
01:00:09,460 --> 01:00:13,495
and we're gonna calculate attention with respect to that vector,

1050
01:00:13,495 --> 01:00:20,950
and we're going to use those attention scores, um, to, um,

1051
01:00:20,950 --> 01:00:24,250
work out where to pay attention, um,

1052
01:00:24,250 --> 01:00:30,625
in this BiLSTM, and then we just sort of train that vector so it gets values.

1053
01:00:30,625 --> 01:00:34,270
And so then we end up with a weighted sum of the time

1054
01:00:34,270 --> 01:00:39,430
steps of that LSTM that uh, then form the question representation.

1055
01:00:39,430 --> 01:00:42,370
Um, second change, uh,

1056
01:00:42,370 --> 01:00:45,400
the pictures only show a shallow BiLSTM but, you know,

1057
01:00:45,400 --> 01:00:48,940
it turns out you can do better if you have a deep BiLSTM and say

1058
01:00:48,940 --> 01:00:53,005
use a three-layer deep BiLSTM rather than a single layer.

1059
01:00:53,005 --> 01:00:56,200
Okay. Then the other changes in

1060
01:00:56,200 --> 01:01:02,350
the passage representations and this part arguably gets a little bit more hacky,

1061
01:01:02,350 --> 01:01:06,520
um, but there are things that you can do that make the numbers go up, I guess.

1062
01:01:06,520 --> 01:01:07,810
Um, okay.

1063
01:01:07,810 --> 01:01:13,840
So- so firstly for the representation of words rather than only using

1064
01:01:13,840 --> 01:01:18,070
the GloVe representation that the input vectors are

1065
01:01:18,070 --> 01:01:24,055
expanded so that- so a named entity recognizer and a part of speech tagger is run.

1066
01:01:24,055 --> 01:01:28,615
And since those are sort of small sets of values,

1067
01:01:28,615 --> 01:01:33,910
that the output of those is just one-hot encoded and concatenated onto

1068
01:01:33,910 --> 01:01:36,490
the word vector, so it represents if it's

1069
01:01:36,490 --> 01:01:40,195
a location or a person name and whether it's a noun or a verb.

1070
01:01:40,195 --> 01:01:44,080
Um, word frequency proves to be a bit useful.

1071
01:01:44,080 --> 01:01:52,165
So there's your concatenating on sort of a representation of the word frequency as,

1072
01:01:52,165 --> 01:01:57,370
um, just sort of a float of the unigram probability.

1073
01:01:57,370 --> 01:02:05,335
Um, and then this part is kind of key to getting some further advances which is, well,

1074
01:02:05,335 --> 01:02:11,140
it turns out that we can do a better job by doing some sort

1075
01:02:11,140 --> 01:02:16,945
of better understanding of the matching between the question and the passage.

1076
01:02:16,945 --> 01:02:20,170
And, um, this feature seems like it's

1077
01:02:20,170 --> 01:02:23,815
very simple but turns out to actually give you quite a lot of value.

1078
01:02:23,815 --> 01:02:28,420
So you're simply saying for each word in the question,

1079
01:02:28,420 --> 01:02:32,215
uh, so for each word- well,  I said that wrong.

1080
01:02:32,215 --> 01:02:35,920
For each word in the passage,

1081
01:02:35,920 --> 01:02:39,040
you were just saying, "Does this word appear in the question?"

1082
01:02:39,040 --> 01:02:42,160
And if so you're setting a one bit into

1083
01:02:42,160 --> 01:02:46,105
the input and that's done in three different ways: exact match,

1084
01:02:46,105 --> 01:02:48,580
uncased match, and lemma match.

1085
01:02:48,580 --> 01:02:51,655
So that means something like drive and driving, um,

1086
01:02:51,655 --> 01:02:53,590
will match, and just that sort of

1087
01:02:53,590 --> 01:02:56,755
indicator of here's where in the passage that's in the question.

1088
01:02:56,755 --> 01:02:59,230
In theory, the system should be able to work that out

1089
01:02:59,230 --> 01:03:03,115
anyway that explicitly indicate and it gives quite a bit of value.

1090
01:03:03,115 --> 01:03:09,310
And then this last one does a sort of a softer version of that where it's using word

1091
01:03:09,310 --> 01:03:12,550
embedding similarities to sort of calculate

1092
01:03:12,550 --> 01:03:16,210
a kind of similarity between questions and answers,

1093
01:03:16,210 --> 01:03:19,345
and that's a slightly complex equation that you can look up.

1094
01:03:19,345 --> 01:03:26,035
But effectively, um, that you're getting the embedding of words and the question answers.

1095
01:03:26,035 --> 01:03:30,085
Each of those, you're running through a single hidden layer,

1096
01:03:30,085 --> 01:03:31,585
neural network, you know,

1097
01:03:31,585 --> 01:03:35,245
dot producting it, and then putting all that through a Softmax,

1098
01:03:35,245 --> 01:03:40,820
and that kind of gives you a sort of word similarity score and that helps as well.

1099
01:03:41,040 --> 01:03:46,510
Okay. So here's the kind of just overall picture this gives you.

1100
01:03:46,510 --> 01:03:49,435
So if you remember, um, um,

1101
01:03:49,435 --> 01:03:52,540
there was the sort of the classical NLP

1102
01:03:52,540 --> 01:03:55,825
with logistic regression baseline, there's around 51.

1103
01:03:55,825 --> 01:03:58,810
So for sort of a fairly simple model,

1104
01:03:58,810 --> 01:04:00,970
like the Stanford Attentive Reader,

1105
01:04:00,970 --> 01:04:03,760
it gives you an enormous boost in performance, right?

1106
01:04:03,760 --> 01:04:07,765
That's giving you close to 30 percent performance gain.

1107
01:04:07,765 --> 01:04:10,180
And then, you know, from there,

1108
01:04:10,180 --> 01:04:13,420
people have kept on pushing up neural systems.

1109
01:04:13,420 --> 01:04:17,410
But, you know, so this gives you kind of in some sense three quarters of

1110
01:04:17,410 --> 01:04:22,525
the value over the traditional NLP system and in the much more,

1111
01:04:22,525 --> 01:04:26,080
um, complex, um, neural systems that come after it.

1112
01:04:26,080 --> 01:04:27,145
Um, yeah.

1113
01:04:27,145 --> 01:04:28,555
In terms of error reduction,

1114
01:04:28,555 --> 01:04:31,780
they're huge but it's sort of more like they're giving you the sort of,

1115
01:04:31,780 --> 01:04:34,880
um, 12 percent after that.

1116
01:04:35,310 --> 01:04:43,030
Why did these systems work such a ton better um, than traditional systems?

1117
01:04:43,030 --> 01:04:46,750
And so we actually did some error analysis of this and, you know,

1118
01:04:46,750 --> 01:04:52,180
it turns out that most of their gains is because they can just do

1119
01:04:52,180 --> 01:04:56,890
better semantic matching of word similarities

1120
01:04:56,890 --> 01:05:02,080
or rephrasings that are semantically related but don't use the same words.

1121
01:05:02,080 --> 01:05:10,675
So, to- to the extent that the question is where was Christopher Manning born?

1122
01:05:10,675 --> 01:05:15,595
And the sentence says Christopher Manning was born in Australia,

1123
01:05:15,595 --> 01:05:18,790
a traditional NLP system would get that right too.

1124
01:05:18,790 --> 01:05:21,565
But that to the extent that you being able to get it right,

1125
01:05:21,565 --> 01:05:23,980
depends on being able to match,

1126
01:05:23,980 --> 01:05:29,575
sort of looser semantic matches so that we understand the sort of um,

1127
01:05:29,575 --> 01:05:33,610
you know, the place of birth has to be matching was born or something.

1128
01:05:33,610 --> 01:05:37,750
That's where the neural systems actually do work much much better.

1129
01:05:37,750 --> 01:05:44,950
Okay. So, that's not the end of the story on question-answering systems.

1130
01:05:44,950 --> 01:05:48,400
And I wanted to say just a little bit about um,

1131
01:05:48,400 --> 01:05:51,670
more complex systems to give you some idea um,

1132
01:05:51,670 --> 01:05:53,725
of what goes on after that.

1133
01:05:53,725 --> 01:05:56,260
Um, but before I go further into that,

1134
01:05:56,260 --> 01:05:59,980
are there any questions on uh,

1135
01:05:59,980 --> 01:06:03,130
up until now, Stanford Attentive Reader?

1136
01:06:03,130 --> 01:06:09,760
[NOISE] Yeah.

1137
01:06:09,760 --> 01:06:12,925
I have a question about attention in general.

1138
01:06:12,925 --> 01:06:18,550
Every example we've seen has been just linear mapping with a weight matrix.

1139
01:06:18,550 --> 01:06:23,695
Has anybody tried to convert that to a deep neural network and see what happens?

1140
01:06:23,695 --> 01:06:26,335
Um, so yes they have.

1141
01:06:26,335 --> 01:06:30,040
Well, at least a shallow neural network.

1142
01:06:30,040 --> 01:06:33,010
Um, I'll actually show an example of that in just a minute.

1143
01:06:33,010 --> 01:06:35,800
So maybe I will um, save it till then.

1144
01:06:35,800 --> 01:06:38,305
But yeah absolutely, um,

1145
01:06:38,305 --> 01:06:43,520
yeah people have done that and that can be a good thing to um, play with.

1146
01:06:45,030 --> 01:06:52,060
Anything else? Okay. Um, okay.

1147
01:06:52,060 --> 01:06:57,970
So, this is a picture of the BiDAF system,

1148
01:06:57,970 --> 01:07:00,730
so this is the one from AI2 UDub.

1149
01:07:00,730 --> 01:07:03,490
And the BiDAF system is very well known.

1150
01:07:03,490 --> 01:07:06,880
Um, it's another sort of classic version of

1151
01:07:06,880 --> 01:07:11,140
question-answering system that lots of people have used and built off.

1152
01:07:11,140 --> 01:07:14,260
Um, and, you know,

1153
01:07:14,260 --> 01:07:20,260
some of it isn't completely different to what we saw before but it has various additions.

1154
01:07:20,260 --> 01:07:23,980
So, there are word embeddings just like we had before,

1155
01:07:23,980 --> 01:07:28,225
there's a biLSTM running just like what we had before,

1156
01:07:28,225 --> 01:07:31,435
and that's being done for both the um,

1157
01:07:31,435 --> 01:07:33,865
passage and the question.

1158
01:07:33,865 --> 01:07:37,210
Um, but there are some different things that are happening as well.

1159
01:07:37,210 --> 01:07:40,510
So one of them is rather than just having word embeddings,

1160
01:07:40,510 --> 01:07:45,085
it also processes the questions and passages at the character level.

1161
01:07:45,085 --> 01:07:48,730
And that's something that we're going to talk about coming up ahead in the class.

1162
01:07:48,730 --> 01:07:54,204
There's been a lot of work at doing character level processing in recent neural NLP,

1163
01:07:54,204 --> 01:07:56,365
but I don't want to talk about that now.

1164
01:07:56,365 --> 01:08:00,460
Um, the main technical innovation of the BiDAF model

1165
01:08:00,460 --> 01:08:06,175
is this attention flow layout because that's in its name bidirectional attention flow.

1166
01:08:06,175 --> 01:08:10,300
And so, there was a model of attention flow where you have attention

1167
01:08:10,300 --> 01:08:14,740
flowing in both directions between the query and the passage.

1168
01:08:14,740 --> 01:08:18,985
And that was their main innovation and it was quite useful in their model.

1169
01:08:18,985 --> 01:08:20,575
Um, but beyond that,

1170
01:08:20,575 --> 01:08:23,500
there's you know, sort of more stuff to this model.

1171
01:08:23,500 --> 01:08:27,324
So after the attention flow layer there's again

1172
01:08:27,324 --> 01:08:31,675
multiple layers of bidirectional LSTMs running.

1173
01:08:31,675 --> 01:08:35,770
And then on top of that their output layer is more

1174
01:08:35,770 --> 01:08:41,230
complex than the sort of simple attention version that I showed previously.

1175
01:08:41,230 --> 01:08:45,145
So let's just look at that in a bit more detail.

1176
01:08:45,145 --> 01:08:47,935
Um so, for the attention flow layer.

1177
01:08:47,935 --> 01:08:53,905
So, the motivation here was in the Stanford Attentive Reader,

1178
01:08:53,905 --> 01:08:57,460
we used attention to map from

1179
01:08:57,460 --> 01:09:03,175
the representation of the question onto the words of the passage.

1180
01:09:03,175 --> 01:09:09,325
But, you know so as questions are whole mapping onto the words of the passage.

1181
01:09:09,325 --> 01:09:11,950
Where their idea was well,

1182
01:09:11,950 --> 01:09:18,760
presumably you could do better by mapping in both directions at the word level.

1183
01:09:18,760 --> 01:09:23,890
So you should be sort of finding passage words that you can map onto question words,

1184
01:09:23,890 --> 01:09:26,605
and question words that you can map onto passage words.

1185
01:09:26,605 --> 01:09:29,965
And if you do that in both directions with attention flowing,

1186
01:09:29,965 --> 01:09:34,315
and then run another round of sequence models on top of that,

1187
01:09:34,315 --> 01:09:38,530
that you'll just be able to do much better matching between the two of them.

1188
01:09:38,530 --> 01:09:42,940
And so the way they do that is, um,

1189
01:09:42,940 --> 01:09:46,600
that they- they've got the bottom- so at

1190
01:09:46,600 --> 01:09:50,800
the bottom layers they've sort of run these two LSTMs.

1191
01:09:50,800 --> 01:09:57,480
So they have representations in the LSTM for each word and um,

1192
01:09:57,480 --> 01:10:00,480
word and passage position.

1193
01:10:00,480 --> 01:10:04,440
And at this point I have to put it in a slight apology because I just

1194
01:10:04,440 --> 01:10:08,760
stole the equations and so the letters that are used change.

1195
01:10:08,760 --> 01:10:12,845
Sorry. But, so these are the um,

1196
01:10:12,845 --> 01:10:18,505
question individual words and these are the passage individual words.

1197
01:10:18,505 --> 01:10:23,485
And so, what they're then wanting to do is to say for each passage word,

1198
01:10:23,485 --> 01:10:28,105
and each question word, I want to work out a similarity score.

1199
01:10:28,105 --> 01:10:34,570
And the way they work out that similarity score is they build a big concatenated vector.

1200
01:10:34,570 --> 01:10:40,359
So there's the LSTM representation of the passage word, the question word,

1201
01:10:40,359 --> 01:10:45,070
and then they throw in a third thing where they do a Hadamard product,

1202
01:10:45,070 --> 01:10:49,855
so an element-wise product of the question word and the context word.

1203
01:10:49,855 --> 01:10:53,590
Um, you know, for a neural net purist, throwing in

1204
01:10:53,590 --> 01:10:57,580
these kind of Hadamard products is a little bit of a cheat because

1205
01:10:57,580 --> 01:11:01,180
you kind of would hope that a neural net might just learn that

1206
01:11:01,180 --> 01:11:05,635
this relation between the passage and the question was useful to look at.

1207
01:11:05,635 --> 01:11:08,380
But you can find a lot of models that put in

1208
01:11:08,380 --> 01:11:11,920
these kind of Hadamard product because it's sort of

1209
01:11:11,920 --> 01:11:18,415
a very easy way of sort of having a model that knows that matching is a good idea.

1210
01:11:18,415 --> 01:11:24,790
Because essentially this is sort of looking for each question and passage word pair.

1211
01:11:24,790 --> 01:11:28,810
You know, do the vectors look similar in various dimensions?

1212
01:11:28,810 --> 01:11:32,965
You can sort of access very well from looking at that Hadamard product.

1213
01:11:32,965 --> 01:11:35,815
So that- so you take that big vector,

1214
01:11:35,815 --> 01:11:40,765
and you then dot-product that with a learned weight matrix,

1215
01:11:40,765 --> 01:11:43,389
and that gives you a similarity score

1216
01:11:43,389 --> 01:11:47,050
between each position in the question and the context.

1217
01:11:47,050 --> 01:11:50,395
And so then what you're gonna do is use that to

1218
01:11:50,395 --> 01:11:55,325
define attentions that go in both directions. Um-

1219
01:11:55,325 --> 01:11:58,989
So for the, um, context,

1220
01:11:58,989 --> 01:12:02,415
the question attention, this one's completely straightforward.

1221
01:12:02,415 --> 01:12:08,550
So, you put these similarity scores through a soft-max.

1222
01:12:08,550 --> 01:12:13,515
So for each of the i positions in the passage or sort of,

1223
01:12:13,515 --> 01:12:17,300
having a softmax which is giving you a probability distribution,

1224
01:12:17,300 --> 01:12:20,375
over question words and then you're coming up with

1225
01:12:20,375 --> 01:12:26,750
a new representation of the i-th position which is then the attention weighted,

1226
01:12:26,750 --> 01:12:31,350
um, version, the attention weighted average of those question words.

1227
01:12:31,350 --> 01:12:32,760
Um, so you're sort of,

1228
01:12:32,760 --> 01:12:38,775
having attention weighted view of the question mapped onto each position in the passage.

1229
01:12:38,775 --> 01:12:43,860
Um, you then want to do something in the reverse direction.

1230
01:12:43,860 --> 01:12:49,815
Um, but the one in the reverse direction is done subtly differently.

1231
01:12:49,815 --> 01:12:53,325
So you're again starting off, um,

1232
01:12:53,325 --> 01:13:00,690
with the- the same similarity scores but this time they're sort of wanting to, sort of,

1233
01:13:00,690 --> 01:13:04,875
really assign which position,

1234
01:13:04,875 --> 01:13:12,120
in which position in the question is the one that's, sort of,

1235
01:13:12,120 --> 01:13:16,980
aligning the most so that they're finding a max and so that they're finding

1236
01:13:16,980 --> 01:13:22,545
which is the most aligned one and so then for each of,

1237
01:13:22,545 --> 01:13:24,930
for each of the i's,

1238
01:13:24,930 --> 01:13:27,885
they're finding the most aligned question word.

1239
01:13:27,885 --> 01:13:33,670
And so then they're doing a softmax over these m scores and then those are being

1240
01:13:33,670 --> 01:13:39,900
used to form a new representation of the passage by,

1241
01:13:39,900 --> 01:13:43,110
sort of, summing over these attention weights.

1242
01:13:43,110 --> 01:13:47,310
Okay. So you build these things up and this then

1243
01:13:47,310 --> 01:13:51,330
gives you a new representation where you have,

1244
01:13:51,330 --> 01:13:57,090
um, your original representations of the passage words.

1245
01:13:57,090 --> 01:14:00,120
You'd have a new representation that you've built from

1246
01:14:00,120 --> 01:14:02,585
this bidirectional attention flow and you

1247
01:14:02,585 --> 01:14:05,310
look at these sort of Hadamard products of them and

1248
01:14:05,310 --> 01:14:10,110
that then gives you kind of the output of the BiDAF layer and that output of

1249
01:14:10,110 --> 01:14:12,990
the BiDAF layer is then what's sort of being fed as

1250
01:14:12,990 --> 01:14:18,160
the input into these nick- next sequence of LSTM layers.

1251
01:14:18,350 --> 01:14:22,240
Okay. Um, and so yeah,

1252
01:14:22,240 --> 01:14:24,335
um, so then that's the modeling layer.

1253
01:14:24,335 --> 01:14:29,085
You have another two BiLSTM layers and so the way they do the,

1254
01:14:29,085 --> 01:14:32,400
um, suspense selection is a bit more complex as well.

1255
01:14:32,400 --> 01:14:35,620
Um, so that they're then, um,

1256
01:14:35,620 --> 01:14:40,020
sort of taking the output of the modeling layer and putting it through a sort of

1257
01:14:40,020 --> 01:14:45,915
a dense feed-forward neural network layer and then softmaxing over that,

1258
01:14:45,915 --> 01:14:49,020
um, and that's then getting a distribution of

1259
01:14:49,020 --> 01:14:53,430
a start and you're running yet another LSTM kind of a distribution finish.

1260
01:14:53,430 --> 01:14:58,020
Um, yeah. So, that gives you some idea of a more complex model.

1261
01:14:58,020 --> 01:15:01,730
Um, you know, in some sense,

1262
01:15:01,730 --> 01:15:05,895
um, the summary if you go further forward than here is that, sort of,

1263
01:15:05,895 --> 01:15:08,835
most of the work in the last couple of years,

1264
01:15:08,835 --> 01:15:14,220
people have been producing progressively more complex architectures with

1265
01:15:14,220 --> 01:15:19,710
lots of variants of attention and effectively that has been giving good gains.

1266
01:15:19,710 --> 01:15:23,010
Um, I think I'll skip since time is running,

1267
01:15:23,010 --> 01:15:25,230
out, showing you that one.

1268
01:15:25,230 --> 01:15:28,980
But, um, let me just mention this FusionNet model

1269
01:15:28,980 --> 01:15:32,500
which was done by people at Microsoft because this relates to the answer,

1270
01:15:32,500 --> 01:15:35,145
the attention question, right?

1271
01:15:35,145 --> 01:15:40,740
So p- so people have definitely used different versions of attention, right?

1272
01:15:40,740 --> 01:15:44,880
So that in some of the stuff that we've shown we tend to emphasize

1273
01:15:44,880 --> 01:15:49,335
this bi-linear attention where you've got two vectors mediated by a matrix.

1274
01:15:49,335 --> 01:15:51,825
And I guess traditionally at Stanford NLP,

1275
01:15:51,825 --> 01:15:53,460
we've liked this, um,

1276
01:15:53,460 --> 01:15:56,460
version of attention since it seems to very directly learn

1277
01:15:56,460 --> 01:16:00,690
a similarity but other people have used a little neural net.

1278
01:16:00,690 --> 01:16:03,000
So this is, sort of, a shallow neural net to

1279
01:16:03,000 --> 01:16:05,340
work out attention scores and there's, sort of,

1280
01:16:05,340 --> 01:16:07,740
no reason why you couldn't say, maybe it would be even better if I

1281
01:16:07,740 --> 01:16:10,710
make that a deep neural net and add another layer.

1282
01:16:10,710 --> 01:16:12,465
Um, and some of, you know,

1283
01:16:12,465 --> 01:16:14,920
to be perfectly honest, um,

1284
01:16:14,920 --> 01:16:18,425
some of the results that have been done by people including Google

1285
01:16:18,425 --> 01:16:22,520
argue that actually that NLP version of attention is better.

1286
01:16:22,520 --> 01:16:25,700
Um, so there's something to explore in that direction.

1287
01:16:25,700 --> 01:16:31,635
But actually, um, the people in FusionNet didn't head that direction because they said,

1288
01:16:31,635 --> 01:16:34,710
"Look, we want to use tons and tons of attention.

1289
01:16:34,710 --> 01:16:37,740
So we want an attention computation that's pretty

1290
01:16:37,740 --> 01:16:41,160
efficient and so it's bad news if you have to

1291
01:16:41,160 --> 01:16:44,115
be evaluating a little dense neural net at

1292
01:16:44,115 --> 01:16:47,880
every position every time that you do attention."

1293
01:16:47,880 --> 01:16:51,630
So this bi-linear form is fairly appealing

1294
01:16:51,630 --> 01:16:55,665
but they then did some playing with it so rather than having a W matrix

1295
01:16:55,665 --> 01:16:59,700
you can reduce the rank and complexity of

1296
01:16:59,700 --> 01:17:06,135
your W matrix by dividing it into the product of two lower rank matrices.

1297
01:17:06,135 --> 01:17:08,985
So you can have a U and a V matrix.

1298
01:17:08,985 --> 01:17:12,689
And if you make these rectangular matrices that are kind of skinny,

1299
01:17:12,689 --> 01:17:16,455
you can then have a sort of a lower rank factorization and,

1300
01:17:16,455 --> 01:17:18,420
that seems a good idea.

1301
01:17:18,420 --> 01:17:19,680
And then they thought well,

1302
01:17:19,680 --> 01:17:23,265
maybe really you want your attention distribution to be symmetric.

1303
01:17:23,265 --> 01:17:26,460
So we can actually put in the middle here,

1304
01:17:26,460 --> 01:17:29,100
we can have the U and the V, so to speak,

1305
01:17:29,100 --> 01:17:32,160
be the same and just have a diagonal matrix in

1306
01:17:32,160 --> 01:17:35,565
the middle and that might be a useful way to think of it.

1307
01:17:35,565 --> 01:17:39,555
And that all makes sense from linear algebra terms but then they thought,

1308
01:17:39,555 --> 01:17:43,055
"Oh, non-linearity is really good in deep learning.

1309
01:17:43,055 --> 01:17:44,640
So why don't we, sort of,

1310
01:17:44,640 --> 01:17:48,790
stick the left and right half through a ReLU and maybe that will help.

1311
01:17:48,790 --> 01:17:52,380
[LAUGHTER] Which doesn't so much make sense in your linear algebra terms, um,

1312
01:17:52,380 --> 01:17:56,850
but that's actually what they ended up using as their, um, attention forms.

1313
01:17:56,850 --> 01:18:00,150
There are lots of things you can play with when doing your final project.

1314
01:18:00,150 --> 01:18:02,085
Um, yeah.

1315
01:18:02,085 --> 01:18:04,740
And, but, you know, their argument is still, you know,

1316
01:18:04,740 --> 01:18:07,920
that doing attention this way is actually much much

1317
01:18:07,920 --> 01:18:11,070
cheaper and so they can use a lot of attention.

1318
01:18:11,070 --> 01:18:16,640
And so they build this very complex tons of attention model, um,

1319
01:18:16,640 --> 01:18:19,155
which I'm not going to try and explain, um,

1320
01:18:19,155 --> 01:18:21,555
all of now, um,

1321
01:18:21,555 --> 01:18:24,750
but I will show you this picture.

1322
01:18:24,750 --> 01:18:28,295
Um, so a point that they make is that a lot of

1323
01:18:28,295 --> 01:18:32,340
the different models that people have explored in different years you,

1324
01:18:32,340 --> 01:18:33,915
that, you know, they're sort of,

1325
01:18:33,915 --> 01:18:36,305
doing different kinds of attention.

1326
01:18:36,305 --> 01:18:39,180
That you could be doing attention right,

1327
01:18:39,180 --> 01:18:42,240
lining up with the original LSTM,

1328
01:18:42,240 --> 01:18:46,340
you could run both sides through some stuff and do attention,

1329
01:18:46,340 --> 01:18:49,740
you can do self attention inside your layer that there are a lot of

1330
01:18:49,740 --> 01:18:53,300
different attentions that different models have explored.

1331
01:18:53,300 --> 01:18:55,710
And essentially what they are wanting to say is,

1332
01:18:55,710 --> 01:18:59,980
let's do all of those and let's make it deep and do it all

1333
01:18:59,980 --> 01:19:04,210
five times and the numbers will go up. And to some extent the answer is,

1334
01:19:04,210 --> 01:19:09,405
yeah they do and the model ends up scoring very well.

1335
01:19:09,405 --> 01:19:15,585
Okay, um, so the one last thing I just wanted to mention but not explain is,

1336
01:19:15,585 --> 01:19:18,450
I mean in the last year there's then been

1337
01:19:18,450 --> 01:19:22,955
a further revolution in how well people can do these tasks.

1338
01:19:22,955 --> 01:19:29,795
And so people have developed algorithms which produce contextual word representation.

1339
01:19:29,795 --> 01:19:32,790
So that means that rather than a traditional word vector,

1340
01:19:32,790 --> 01:19:36,660
you have a representation for each word in a particular context.

1341
01:19:36,660 --> 01:19:41,700
So here's the word frog in this particular context and the way people build

1342
01:19:41,700 --> 01:19:44,490
those representations is using something

1343
01:19:44,490 --> 01:19:47,580
like a language modeling tasks like Abby talked about,

1344
01:19:47,580 --> 01:19:50,730
of saying putting probabilities of words in

1345
01:19:50,730 --> 01:19:54,795
context to learn a context-specific word representation.

1346
01:19:54,795 --> 01:19:57,870
And ELMo was the first well-known such model.

1347
01:19:57,870 --> 01:20:00,410
And then people from Google came up with BERT,

1348
01:20:00,410 --> 01:20:01,830
which worked even better.

1349
01:20:01,830 --> 01:20:06,490
Um, and so BERT is really in some sense is

1350
01:20:06,490 --> 01:20:11,235
super complex attention Architecture doing a language modeling like objective.

1351
01:20:11,235 --> 01:20:13,680
We're going to talk about these later, um,

1352
01:20:13,680 --> 01:20:16,580
I'm not going to talk about them now, um,

1353
01:20:16,580 --> 01:20:22,260
but if you look at the current SQuAD 2,0 Leaderboard,

1354
01:20:22,260 --> 01:20:24,090
um, you will quickly,

1355
01:20:24,090 --> 01:20:28,485
um - sorry that's- oh I put the wrong slide and that was the bottom of the leaderboard.

1356
01:20:28,485 --> 01:20:30,270
Oops, slipped at the last minute.

1357
01:20:30,270 --> 01:20:34,785
If you go back to my slide which had the top of the leaderboard, um,

1358
01:20:34,785 --> 01:20:38,805
you will have noticed that the top of the leaderboard,

1359
01:20:38,805 --> 01:20:42,825
every single one of the top systems uses BERT.

1360
01:20:42,825 --> 01:20:45,240
So that's something that you may want to

1361
01:20:45,240 --> 01:20:47,820
consider but you may want to consider how you could

1362
01:20:47,820 --> 01:20:52,800
use it as a sub-module which you could add other stuff too as many of these systems do.

1363
01:20:52,800 --> 01:20:56,140
Okay. Done for today.

