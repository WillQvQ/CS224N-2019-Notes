1
00:00:05,660 --> 00:00:08,145
Okay hi everyone.

2
00:00:08,145 --> 00:00:11,110
Let's get started again.

3
00:00:11,210 --> 00:00:17,010
Um. Okay. So, first of all for a couple of announcements.

4
00:00:17,010 --> 00:00:20,040
Um, first of all thanks to everyone, um,

5
00:00:20,040 --> 00:00:23,955
who filled in our mid-quarter survey we've actually gotten,

6
00:00:23,955 --> 00:00:27,320
um, great participation in that.

7
00:00:27,320 --> 00:00:29,855
Here are my two little Pac-Man figures.

8
00:00:29,855 --> 00:00:31,640
So, the Pac-Man figures thinks,

9
00:00:31,640 --> 00:00:34,580
means that almost everyone thinks the lectures are at

10
00:00:34,580 --> 00:00:38,390
the right pace and those that don't are pretty much evenly divided.

11
00:00:38,390 --> 00:00:42,680
Um, if we go for how challenging was Assignment three,

12
00:00:42,680 --> 00:00:46,190
slightly more people thought it was too easy than too hard.

13
00:00:46,190 --> 00:00:48,680
So, I guess we're setting about rectifying that with

14
00:00:48,680 --> 00:00:53,460
assignments four and five, um, [NOISE].

15
00:00:53,460 --> 00:00:56,030
So, though there are a whole bunch of other questions and we've

16
00:00:56,030 --> 00:00:59,180
been trying to absorb all the feedback.

17
00:00:59,180 --> 00:01:03,980
I mean one of the questions was what people wanted most from the remaining lectures.

18
00:01:03,980 --> 00:01:09,260
I guess the good news here is really we're very good at predicting, um,

19
00:01:09,260 --> 00:01:11,780
what people wanted, that or else everybody

20
00:01:11,780 --> 00:01:14,660
just looked ahead in the syllabus and wrote down what it said was

21
00:01:14,660 --> 00:01:19,160
ahead in the syllabus but I guess the most popular four answers to

22
00:01:19,160 --> 00:01:23,810
topics that they wanted in the remaining lectures were Transformers and BERT,

23
00:01:23,810 --> 00:01:25,860
both of which are gonna be covered this week.

24
00:01:25,860 --> 00:01:29,780
Uh, question-answering which we talked about last week, um,

25
00:01:29,780 --> 00:01:33,230
and then text generation and summarization

26
00:01:33,230 --> 00:01:38,275
and you guys get Abby back next week to talk about that.

27
00:01:38,275 --> 00:01:42,380
Um, there are also a lot of people also answered this question

28
00:01:42,380 --> 00:01:46,400
a different way as to what kind of style of stuff,

29
00:01:46,400 --> 00:01:50,960
um, some people emphasized new research and the latest updates from the field.

30
00:01:50,960 --> 00:01:53,300
I guess we'll get some of that today as well,

31
00:01:53,300 --> 00:01:55,235
some people are more interested in

32
00:01:55,235 --> 00:02:00,000
successful applications in industry or trying to do a bit of that,

33
00:02:00,000 --> 00:02:02,840
um, cool new neural architectures.

34
00:02:02,840 --> 00:02:05,660
Um, the bottom answer wasn't the most popular one,

35
00:02:05,660 --> 00:02:08,390
I'll admit but at least a few people, um,

36
00:02:08,390 --> 00:02:11,810
wish that we were teaching more linguistic stuff.

37
00:02:11,810 --> 00:02:14,300
Um, I mean that is something that I actually feel

38
00:02:14,300 --> 00:02:18,580
a bit awkward about the way things were merged with CS224N,

39
00:02:18,580 --> 00:02:20,100
with this deep learning,

40
00:02:20,100 --> 00:02:22,370
I mean the truth of the matter is that sort of seems

41
00:02:22,370 --> 00:02:24,850
like in the early part of the course,

42
00:02:24,850 --> 00:02:27,210
there's so much to cover with,

43
00:02:27,210 --> 00:02:29,660
um, neural networks, backpropagation,

44
00:02:29,660 --> 00:02:34,250
different, um, neural net architectures and so on that the reality is that we

45
00:02:34,250 --> 00:02:39,890
teach rather less linguistic stuff than we used to in the class.

46
00:02:39,890 --> 00:02:43,160
I mean, for the last four weeks of the class we really do try and

47
00:02:43,160 --> 00:02:46,730
cover some more linguistic stuff topics.

48
00:02:46,730 --> 00:02:49,255
Um, so look forward to that.

49
00:02:49,255 --> 00:02:51,255
Um, announcements.

50
00:02:51,255 --> 00:02:54,365
Okay. So we've made a couple of deadline changes.

51
00:02:54,365 --> 00:02:57,410
Um, firstly, a number of people have

52
00:02:57,410 --> 00:03:00,920
mentioned that they think assignment five is a bit tough.

53
00:03:00,920 --> 00:03:04,160
And so, we're giving people one extra day,

54
00:03:04,160 --> 00:03:06,105
um, to do assignment five.

55
00:03:06,105 --> 00:03:09,830
Um, I'm realizing in one sense that one extra day is not a ton

56
00:03:09,830 --> 00:03:13,760
but you know there's sort of this complex balance here because on the other hand,

57
00:03:13,760 --> 00:03:18,695
we don't really want to undermine time that people have available for final projects.

58
00:03:18,695 --> 00:03:23,010
And if you're one of the people who hasn't yet started assignment five,

59
00:03:23,010 --> 00:03:26,075
um, we do really encourage you to get underway on it.

60
00:03:26,075 --> 00:03:29,955
Um, yeah, in the reverse direction

61
00:03:29,955 --> 00:03:34,160
we decided that the project milestone was really too late.

62
00:03:34,160 --> 00:03:38,060
If we are going to be able to give you feedback on it that you could usefully make use

63
00:03:38,060 --> 00:03:42,080
of, so we're moving the project milestone date two days earlier.

64
00:03:42,080 --> 00:03:45,935
And so, we've also gotten everyone's project proposals and our

65
00:03:45,935 --> 00:03:50,270
planned hope is to get them back to everybody on Friday.

66
00:03:50,270 --> 00:03:52,310
Yes, so, a lot of things moving.

67
00:03:52,310 --> 00:03:56,975
Um, and finally on other announcements I guess, um, on

68
00:03:56,975 --> 00:04:01,640
this Thursday is our first invited speaker, um, and so,

69
00:04:01,640 --> 00:04:05,165
if you're in person student you're meant to be here,

70
00:04:05,165 --> 00:04:08,930
um, and if you're not able to be here,

71
00:04:08,930 --> 00:04:12,350
you should know about our reaction paragraph policy and

72
00:04:12,350 --> 00:04:16,460
I actually stuck up on the Piazza pinned posts about, um,

73
00:04:16,460 --> 00:04:21,375
reaction pieces and attendance, an example of a reaction piece, um,

74
00:04:21,375 --> 00:04:27,320
from a past class to make it a little bit more concrete what's expected there.

75
00:04:27,320 --> 00:04:31,850
But, you know, the idea is what we're hoping for something that isn't a ton of work.

76
00:04:31,850 --> 00:04:35,855
You can just write 100, 150 words, a few sentences,

77
00:04:35,855 --> 00:04:40,040
but wanting you to pick out a specific thing that was

78
00:04:40,040 --> 00:04:42,410
interesting and write a couple of sentences

79
00:04:42,410 --> 00:04:45,140
about what it was and what your thoughts are about it.

80
00:04:45,140 --> 00:04:50,270
I, not just some very generic statement of this was a lecture about transformers.

81
00:04:50,270 --> 00:04:52,805
He talked about transformers and it was interesting,

82
00:04:52,805 --> 00:04:58,635
that is not what we want for the reaction piece. Um, okay.

83
00:04:58,635 --> 00:05:01,140
So, here's the plan for today.

84
00:05:01,140 --> 00:05:04,955
So, for today's, what I want to talk about is,

85
00:05:04,955 --> 00:05:09,785
um, the exciting recent work about contextual word representations.

86
00:05:09,785 --> 00:05:15,620
I mean I, I was thinking of what I was gonna say I was wanting to say, oh, this is

87
00:05:15,620 --> 00:05:18,770
the most exciting thing in deep learning for NLP in

88
00:05:18,770 --> 00:05:22,040
the last five years then something's just completely wrong,

89
00:05:22,040 --> 00:05:27,080
because really this is the most exciting thing in deep learning that happened in 2018.

90
00:05:27,080 --> 00:05:29,960
I mean, I guess things move very quickly, um,

91
00:05:29,960 --> 00:05:33,530
in deep learning at the moment and it's sort of I don't think it's

92
00:05:33,530 --> 00:05:38,130
really fair to say that you know it's got 5 years of life.

93
00:05:38,130 --> 00:05:40,490
But there's a very exciting thing that happened last year,

94
00:05:40,490 --> 00:05:42,655
and we'll talk about that.

95
00:05:42,655 --> 00:05:45,720
Okay. So, we'll talk about early stuff,

96
00:05:45,720 --> 00:05:47,455
the ELMo, ULMfit,

97
00:05:47,455 --> 00:05:50,630
transformer architectures briefly and then go on to

98
00:05:50,630 --> 00:05:55,320
talk about the BERT model that's being quite prominent lately.

99
00:05:55,670 --> 00:05:58,215
So, let's just recap,

100
00:05:58,215 --> 00:06:02,605
let's just go backwards a bit first to think about, um,

101
00:06:02,605 --> 00:06:08,075
where we've been and where we are now and why we might want something more.

102
00:06:08,075 --> 00:06:09,695
So, up until now,

103
00:06:09,695 --> 00:06:11,060
we've sort of just had,

104
00:06:11,060 --> 00:06:16,255
one representation for words which is what we learned at the beginning of class,

105
00:06:16,255 --> 00:06:22,085
there was a word, you trained a word vector for it and that's what you used in your model.

106
00:06:22,085 --> 00:06:24,774
Um, and you could do that, with algorithms like Word2vec,

107
00:06:24,774 --> 00:06:28,075
GloVe, or fastText that I mentioned last week.

108
00:06:28,075 --> 00:06:34,460
Um, so some on this sort of progression of ideas in deep learning,

109
00:06:34,460 --> 00:06:39,050
when deep learning for NLP or the general

110
00:06:39,050 --> 00:06:42,065
just the resurgence of neural networks for NLP

111
00:06:42,065 --> 00:06:45,620
came about sort of at the beginning of this decade.

112
00:06:45,620 --> 00:06:50,640
Um, these pre-trained word vectors.

113
00:06:50,640 --> 00:06:54,790
So, pre-trained unsupervised over a large amount of text.

114
00:06:54,790 --> 00:06:58,270
They were completely seen as the secret sauce,

115
00:06:58,270 --> 00:07:00,805
and they were the thing that transformed

116
00:07:00,805 --> 00:07:04,795
neural networks from NLP to something that didn't really work,

117
00:07:04,795 --> 00:07:06,650
to something that worked great.

118
00:07:06,650 --> 00:07:09,910
Um, so, this is actually an old slide of mine.

119
00:07:09,910 --> 00:07:12,670
So, this is a slide I guess I first made for

120
00:07:12,670 --> 00:07:18,490
2012 ACL tutorial and then sort of used in lectures.

121
00:07:18,490 --> 00:07:22,995
Sort of in 2013, 2014. Um-.

122
00:07:22,995 --> 00:07:26,460
And so this was sort of the picture in those years.

123
00:07:26,460 --> 00:07:28,000
So this was looking at two tasks,

124
00:07:28,000 --> 00:07:33,115
part of speech tagging and named entity recognition which I'll use quite a bit today.

125
00:07:33,115 --> 00:07:38,275
And, you know, the top line was showing a state of the art which was

126
00:07:38,275 --> 00:07:42,780
a traditional categorical feature based classifier of the kind

127
00:07:42,780 --> 00:07:47,445
that dominated NLP in the 2000s decade, in their performance.

128
00:07:47,445 --> 00:07:53,215
And what then the next line showed is that if you took the same data set

129
00:07:53,215 --> 00:07:59,515
and you trained a supervised neural network on it and said how good is your performance?

130
00:07:59,515 --> 00:08:01,845
Um, the story was, it wasn't great.

131
00:08:01,845 --> 00:08:06,720
Um, part-of-speech tagging has very high numbers always for various reasons.

132
00:08:06,720 --> 00:08:11,395
So perhaps the more indicative one to look at is these named entity recognition numbers.

133
00:08:11,395 --> 00:08:14,530
So, you know, this was sort of neural net sucked, right?

134
00:08:14,530 --> 00:08:18,310
The reason why last decade everybody used, um,

135
00:08:18,310 --> 00:08:20,790
categorical feature based, you know,

136
00:08:20,790 --> 00:08:23,340
CRF, SVM kind of classifiers.

137
00:08:23,340 --> 00:08:26,995
Well, if you look, it worked eight percent better than a neural network.

138
00:08:26,995 --> 00:08:28,330
Why wouldn't anybody?

139
00:08:28,330 --> 00:08:32,845
But then what had happened was people had come up with this idea that we could

140
00:08:32,845 --> 00:08:37,515
do unsupervised pre-training of word representations,

141
00:08:37,515 --> 00:08:40,770
um, to come up with word vectors for words.

142
00:08:40,770 --> 00:08:42,060
And, you know, in those days,

143
00:08:42,060 --> 00:08:45,620
this was very hard to do the alg- both because of

144
00:08:45,620 --> 00:08:49,500
the kind of algorithms and the kind of machines that were available, right?

145
00:08:49,500 --> 00:08:51,990
So Collobert and Weston, 2011,

146
00:08:51,990 --> 00:08:56,980
spent seven weeks training their unsupervised word representations.

147
00:08:56,980 --> 00:08:58,105
And at the end of the day,

148
00:08:58,105 --> 00:09:01,795
there are only 100 dimensional, um, word representations.

149
00:09:01,795 --> 00:09:03,865
But this was the miracle breakthrough, right?

150
00:09:03,865 --> 00:09:08,965
You've put in this miracle breakthrough of unsupervised word representations.

151
00:09:08,965 --> 00:09:12,215
And now, the neural net is getting to 88,87.

152
00:09:12,215 --> 00:09:15,380
So it's almost as good as the feature-based classifier,

153
00:09:15,380 --> 00:09:17,510
and then like any good engineers,

154
00:09:17,510 --> 00:09:19,500
they did some hacking with some extra features,

155
00:09:19,500 --> 00:09:21,175
because they had some stuff like that.

156
00:09:21,175 --> 00:09:26,720
And they got a system that was then slightly better than the feature based system.

157
00:09:26,720 --> 00:09:29,520
Okay. So that was sort of our picture that,

158
00:09:29,520 --> 00:09:33,180
um, having these pre-trained,

159
00:09:33,180 --> 00:09:36,560
unsuper- and unsupervised manner of word representations,

160
00:09:36,560 --> 00:09:39,000
that was sort of the big breakthrough and

161
00:09:39,000 --> 00:09:42,280
the secret sauce that gave all the oomph that made,

162
00:09:42,280 --> 00:09:44,765
um, neural networks competitive.

163
00:09:44,765 --> 00:09:46,200
Um, but, you know,

164
00:09:46,200 --> 00:09:51,565
it's a sort of a funny thing happened which was after people had sort of had

165
00:09:51,565 --> 00:09:54,430
some of these initial breakthroughs which were

166
00:09:54,430 --> 00:09:57,510
all about unsupervised methods for pre-training,

167
00:09:57,510 --> 00:09:59,030
it was the same in vision.

168
00:09:59,030 --> 00:10:00,625
This was the era in vision,

169
00:10:00,625 --> 00:10:03,380
where you were building restricted Boltzmann machines and doing

170
00:10:03,380 --> 00:10:07,410
complicated unsupervised pre-training techniques on them as well.

171
00:10:07,410 --> 00:10:13,425
Some- somehow, after people had kind of discovered that and started to get good on it,

172
00:10:13,425 --> 00:10:16,260
people sort of started to discover, well,

173
00:10:16,260 --> 00:10:20,280
actually we have some new technologies for non-linearities,

174
00:10:20,280 --> 00:10:22,665
regularization, and things like that.

175
00:10:22,665 --> 00:10:25,830
And if we keep using those same technologies,

176
00:10:25,830 --> 00:10:29,770
we can just go back to good old supervised learning.

177
00:10:29,770 --> 00:10:34,510
And shockingly, it works way better now inside neural networks.

178
00:10:34,510 --> 00:10:37,440
And so if you sort of go ahead to what I will call,

179
00:10:37,440 --> 00:10:43,615
sort of 2014 to 2018 picture,

180
00:10:43,615 --> 00:10:46,445
the, the picture is actually very different.

181
00:10:46,445 --> 00:10:48,270
So the picture is, so this,

182
00:10:48,270 --> 00:10:51,295
the results I'm actually gonna show you this is from the Chen and Manning,

183
00:10:51,295 --> 00:10:54,550
um, neural dependency parser that we talked about weeks ago.

184
00:10:54,550 --> 00:10:56,695
The picture there was, um,

185
00:10:56,695 --> 00:10:59,065
and you could- despite the fact that

186
00:10:59,065 --> 00:11:02,995
this dependency parser is being trained on a pretty small corpus,

187
00:11:02,995 --> 00:11:05,640
a million words of supervised data,

188
00:11:05,640 --> 00:11:09,250
you can just initialize it with random word vectors,

189
00:11:09,250 --> 00:11:11,830
um, and train a dependency parser.

190
00:11:11,830 --> 00:11:13,740
And to a first approximation,

191
00:11:13,740 --> 00:11:15,060
it just works fine.

192
00:11:15,060 --> 00:11:17,935
You get, get sort of a 90 percent accuracy,

193
00:11:17,935 --> 00:11:20,425
E- um, English dependency parser.

194
00:11:20,425 --> 00:11:23,740
Now, it is the case that instead,

195
00:11:23,740 --> 00:11:27,475
you could use pre-trained word embeddings and you do a bit better.

196
00:11:27,475 --> 00:11:29,230
You do about one percent better.

197
00:11:29,230 --> 00:11:31,550
And so this was sort of the,

198
00:11:31,550 --> 00:11:35,725
the new world order which was yeah, um,

199
00:11:35,725 --> 00:11:40,860
these pre-trained unsupervised word embeddings are useful because you can

200
00:11:40,860 --> 00:11:46,840
train them from a lot more data and they can know about a much larger vocabulary.

201
00:11:46,840 --> 00:11:48,025
That means they are useful.

202
00:11:48,025 --> 00:11:51,805
They help with rare words and things like that and they give you a percent,

203
00:11:51,805 --> 00:11:55,160
but they're definitely no longer the sort of night and day,

204
00:11:55,160 --> 00:11:59,910
uh, thing to make neural networks work that we used to believe.

205
00:11:59,910 --> 00:12:04,465
I'm, I'm just gonna deviate here to,

206
00:12:04,465 --> 00:12:08,455
from the main narrative to just sort of say, um,

207
00:12:08,455 --> 00:12:13,490
one more tip for dealing with unknown words with word vectors,

208
00:12:13,490 --> 00:12:16,290
um, just in case it's useful for some people,

209
00:12:16,290 --> 00:12:19,350
building question answering systems, right?

210
00:12:19,350 --> 00:12:24,450
So, um, so for sort of word vectors on unknown words, you know,

211
00:12:24,450 --> 00:12:29,695
the commonest thing historically is you've got your supervised training data,

212
00:12:29,695 --> 00:12:32,790
you define a vocab which might be words that occur

213
00:12:32,790 --> 00:12:36,255
five times or more in your supervised training data.

214
00:12:36,255 --> 00:12:39,040
And you treat everything else as an UNK.

215
00:12:39,040 --> 00:12:42,085
And so you also train one vector per UNK.

216
00:12:42,085 --> 00:12:46,140
Um, but that has some problems which you have no way to

217
00:12:46,140 --> 00:12:51,250
distinguish different UNK words either for identity or meaning.

218
00:12:51,250 --> 00:12:54,745
And that tends to be problematic for question answering systems.

219
00:12:54,745 --> 00:12:58,140
And so one way to fix that is what we talked about last week,

220
00:12:58,140 --> 00:13:00,630
you just say, "Oh, words are made out of characters.

221
00:13:00,630 --> 00:13:05,655
I can use character representations to learn word vectors for other words."

222
00:13:05,655 --> 00:13:06,960
And you can certainly do that.

223
00:13:06,960 --> 00:13:08,230
You might wanna try that.

224
00:13:08,230 --> 00:13:10,210
That adds some complexity.

225
00:13:10,210 --> 00:13:14,380
Um, but especially for things like question answering systems,

226
00:13:14,380 --> 00:13:16,380
there are a couple of other things that you can do

227
00:13:16,380 --> 00:13:18,720
that work considerably better and they've been

228
00:13:18,720 --> 00:13:23,365
explored in this paper by Dhingra et al., um, from 2017.

229
00:13:23,365 --> 00:13:26,695
Um, the first one is to say, well, um,

230
00:13:26,695 --> 00:13:34,255
when you at test-time encounter new words, probably your unsupervised word,

231
00:13:34,255 --> 00:13:39,335
pre-trained word embeddings have a much bigger vocabulary than your actual system does.

232
00:13:39,335 --> 00:13:42,090
So anytime you come across a word that isn't in

233
00:13:42,090 --> 00:13:44,955
your vocab but is in the pre-trained word embeddings,

234
00:13:44,955 --> 00:13:48,985
just use, get the word vector of that word and start using it.

235
00:13:48,985 --> 00:13:51,745
That'll be a much more useful thing to use.

236
00:13:51,745 --> 00:13:53,850
And then there's a second possible tip that if you

237
00:13:53,850 --> 00:13:56,305
see something that's still an unknown word,

238
00:13:56,305 --> 00:13:57,925
rather than treating it as UNK,

239
00:13:57,925 --> 00:14:00,030
you just assign it on the spot,

240
00:14:00,030 --> 00:14:01,750
a random word vector.

241
00:14:01,750 --> 00:14:06,810
And so this has the effect that each word does get a unique identity.

242
00:14:06,810 --> 00:14:09,360
Which means if you see the same word in the question,

243
00:14:09,360 --> 00:14:11,065
and a potential answer,

244
00:14:11,065 --> 00:14:14,940
they will match together beautifully in an accurate way which you're

245
00:14:14,940 --> 00:14:19,895
not getting with just UNK matching and those can be kind of useful ideas to try.

246
00:14:19,895 --> 00:14:25,275
Okay, end digression. Okay, so up until now,

247
00:14:25,275 --> 00:14:28,225
we just sort of had this representation of words,

248
00:14:28,225 --> 00:14:31,600
we ran Word2vec and we got a word vector,

249
00:14:31,600 --> 00:14:33,725
um, for each word.

250
00:14:33,725 --> 00:14:37,570
Um, so, um, that, that was useful.

251
00:14:37,570 --> 00:14:39,105
It's worked pretty well.

252
00:14:39,105 --> 00:14:41,545
Um, but it had, um,

253
00:14:41,545 --> 00:14:46,860
some big problems. So what were the big problems of doing that?

254
00:14:48,530 --> 00:14:50,785
The problems when we,

255
00:14:50,785 --> 00:14:53,475
of having a word vector in each word, yes.

256
00:14:53,475 --> 00:14:56,825
A lot of words have like one spelling, but a whole bunch of meanings.

257
00:14:56,825 --> 00:15:00,550
Right, so, a word can have- So, typically,

258
00:15:00,550 --> 00:15:05,620
you have one string of letters which has a whole bunch of meanings.

259
00:15:05,620 --> 00:15:09,220
So, words have a ton of senses.

260
00:15:09,220 --> 00:15:11,350
Um, and yeah, so that's

261
00:15:11,350 --> 00:15:13,405
the biggest and most obvious problem that we're

262
00:15:13,405 --> 00:15:15,550
collapsing together all the meanings of words.

263
00:15:15,550 --> 00:15:18,130
So, we talked about a bit where

264
00:15:18,130 --> 00:15:20,290
one solution to that was you could distinguish

265
00:15:20,290 --> 00:15:23,680
word senses and to have different word vectors for them.

266
00:15:23,680 --> 00:15:27,700
Um, and I then said something about also you could think of

267
00:15:27,700 --> 00:15:31,750
this word vector as a sort of a mixture of them and maybe your model could separate it.

268
00:15:31,750 --> 00:15:35,065
But it seems like we might want to take that more seriously.

269
00:15:35,065 --> 00:15:37,420
And one way, um,

270
00:15:37,420 --> 00:15:43,345
that we could take that more seriously is we could start to say, well,

271
00:15:43,345 --> 00:15:50,065
really, you know, traditional lists of word senses are themselves a crude approximation.

272
00:15:50,065 --> 00:15:57,925
What we actually want to know is the sense of the word inside a particular context of use.

273
00:15:57,925 --> 00:16:00,400
And sort of what I mean by that is, you know,

274
00:16:00,400 --> 00:16:04,570
we distinguish different senses of a word, right?

275
00:16:04,570 --> 00:16:08,380
Say for the word star there's the astronomical sense and

276
00:16:08,380 --> 00:16:12,415
there's the Hollywood sense and they're clearly different.

277
00:16:12,415 --> 00:16:16,270
But you know, if we then go to this what I'm calling the Hollywood sense,

278
00:16:16,270 --> 00:16:18,370
I could then say, well, wait a minute.

279
00:16:18,370 --> 00:16:21,520
There are movie stars and there are rock stars,

280
00:16:21,520 --> 00:16:24,070
and there, uh, are R&amp;B stars,

281
00:16:24,070 --> 00:16:25,825
and there are country stars.

282
00:16:25,825 --> 00:16:29,275
Now, all of those different senses, um,

283
00:16:29,275 --> 00:16:33,025
in certain contexts, though, one or other of them would be evoked.

284
00:16:33,025 --> 00:16:34,210
And so, you know,

285
00:16:34,210 --> 00:16:36,879
it's very hard if you're trying to actually enumerate

286
00:16:36,879 --> 00:16:40,825
senses of a word as to which ones count as different or the same.

287
00:16:40,825 --> 00:16:44,785
So, it's really you sort of wanna know what a word means in a context.

288
00:16:44,785 --> 00:16:50,515
There's a second limitation of these word vectors which is,

289
00:16:50,515 --> 00:16:53,710
we haven't really talked about and is less obvious,

290
00:16:53,710 --> 00:16:56,770
but it's also something that we might want to fix, and at least one of

291
00:16:56,770 --> 00:17:00,070
the models we discussed today takes some aim at that,

292
00:17:00,070 --> 00:17:04,045
and that is, we just sort of have one vector for a word.

293
00:17:04,045 --> 00:17:07,465
But there are sort of different dimensions of a word.

294
00:17:07,465 --> 00:17:10,390
So, words can have different meanings,

295
00:17:10,390 --> 00:17:14,605
some sort of real semantics or words can have

296
00:17:14,605 --> 00:17:19,765
different syntactic behavior like different parts of speech or grammatical behavior.

297
00:17:19,765 --> 00:17:23,065
So, in some sense, arrive and arrival,

298
00:17:23,065 --> 00:17:25,675
their semantics are almost the same,

299
00:17:25,675 --> 00:17:28,990
but they're different parts of speech.

300
00:17:28,990 --> 00:17:32,410
One is a, um, a verb and one is a noun,

301
00:17:32,410 --> 00:17:35,380
so they can kind of appear in quite different places.

302
00:17:35,380 --> 00:17:39,130
And you know, you'd wanna do different things with them in a dependency parser.

303
00:17:39,130 --> 00:17:41,290
And there are even other dimensions.

304
00:17:41,290 --> 00:17:47,200
So, words also have register and connotation differences.

305
00:17:47,200 --> 00:17:52,270
So, you can probably think of lots of different words for a bathroom,

306
00:17:52,270 --> 00:17:56,170
and a lot of those words all means semantically the same,

307
00:17:56,170 --> 00:17:58,330
but have rather different registers and

308
00:17:58,330 --> 00:18:01,225
connotations as to when they're appropriate to use.

309
00:18:01,225 --> 00:18:04,900
And so, we might want to distinguish words on that basis as well.

310
00:18:04,900 --> 00:18:08,350
And so these are the kinds of soluti- things we want to

311
00:18:08,350 --> 00:18:11,845
solve with our new contextual word embeddings.

312
00:18:11,845 --> 00:18:16,240
Um, so I've said up until now, you know,

313
00:18:16,240 --> 00:18:21,670
oh, we just had these word vectors that we use,

314
00:18:21,670 --> 00:18:24,205
words just had one vector.

315
00:18:24,205 --> 00:18:29,170
Um, but if you actually think about it, maybe that's wrong.

316
00:18:29,170 --> 00:18:34,270
I mean, maybe we never had a problem, or at any rate, we solved it six classes ago.

317
00:18:34,270 --> 00:18:36,205
Because if you remember back, [NOISE] um,

318
00:18:36,205 --> 00:18:39,460
to when we started talking about neural language models,

319
00:18:39,460 --> 00:18:41,950
well, what did a neural language model do?

320
00:18:41,950 --> 00:18:45,100
At the bottom, you fed into it the word vectors.

321
00:18:45,100 --> 00:18:49,599
But then you ran across that one or more recurrent layers,

322
00:18:49,599 --> 00:18:51,565
something like a LSTM layer,

323
00:18:51,565 --> 00:18:57,430
and it was calculating these representations that sit above each word and,

324
00:18:57,430 --> 00:19:00,760
you know, the role of those hidden states is a bit ambivalent.

325
00:19:00,760 --> 00:19:02,260
They are used for prediction.

326
00:19:02,260 --> 00:19:06,670
And they are used for next hidden state and output states and so on.

327
00:19:06,670 --> 00:19:09,205
But in many ways you can think huh,

328
00:19:09,205 --> 00:19:16,045
these representations are actually representations of a word in context.

329
00:19:16,045 --> 00:19:18,895
And if you think about what happened with, uh,

330
00:19:18,895 --> 00:19:21,310
the question answering systems,

331
00:19:21,310 --> 00:19:23,620
that's exactly how they were used, right?

332
00:19:23,620 --> 00:19:26,200
We ran LSTM's backwards and forwards,

333
00:19:26,200 --> 00:19:29,320
over a question in the passage, and then we say,

334
00:19:29,320 --> 00:19:33,085
okay those are a good representation of a word's meaning and context.

335
00:19:33,085 --> 00:19:36,205
Let's start matching them with attention functions et cetera.

336
00:19:36,205 --> 00:19:41,470
So, it sort of seemed like we'd already invented a way to have,

337
00:19:41,470 --> 00:19:47,185
um, context-specific representations of words.

338
00:19:47,185 --> 00:19:50,035
And effectively, you know,

339
00:19:50,035 --> 00:19:55,450
the rest of the content of this lecture is sort of basically no more complex than that.

340
00:19:55,450 --> 00:20:02,245
Um, that it took a while but sort of people woke up and started to notice, huh,

341
00:20:02,245 --> 00:20:04,600
really when you're running any language model,

342
00:20:04,600 --> 00:20:08,365
you generate a context-specific representation of words.

343
00:20:08,365 --> 00:20:11,364
Maybe, if we just took those context-specific

344
00:20:11,364 --> 00:20:16,810
representation of words, they'd be useful for doing other things with them.

345
00:20:16,810 --> 00:20:18,595
And that's sort of, you know,

346
00:20:18,595 --> 00:20:19,780
there are a few more details,

347
00:20:19,780 --> 00:20:23,980
but that's really the summary of the entire of this lecture.

348
00:20:23,980 --> 00:20:34,165
Um, so one of the first things to do that was a paper that Matt Peters wrote in 2017,

349
00:20:34,165 --> 00:20:36,265
um, the year before last.

350
00:20:36,265 --> 00:20:41,080
Um, and this was sort of a predecessor to the sort of modern, um,

351
00:20:41,080 --> 00:20:46,720
versions of, um, these context-sensitive word embeddings.

352
00:20:46,720 --> 00:20:49,840
So, um, together with co-authors,

353
00:20:49,840 --> 00:20:53,185
he came up with a paper called TagLM,

354
00:20:53,185 --> 00:20:56,920
but it essentially already had all the main ideas.

355
00:20:56,920 --> 00:21:01,255
So, what, um, was wanted was okay.

356
00:21:01,255 --> 00:21:05,620
We want to do better at tasks such as named-entity recognition.

357
00:21:05,620 --> 00:21:10,945
And what we'd like to do is know about the meaning of a word in context.

358
00:21:10,945 --> 00:21:14,500
Um, but you know, standardly if we're doing named-entity recognition,

359
00:21:14,500 --> 00:21:18,175
we just train it on half a million words of supervised data.

360
00:21:18,175 --> 00:21:20,230
And that's not much of a source of

361
00:21:20,230 --> 00:21:23,950
information to be learning about the meaning of words and context.

362
00:21:23,950 --> 00:21:28,810
So, why don't we adopt the semi-supervised approach and so that's what we do.

363
00:21:28,810 --> 00:21:32,740
So, we start off with a ton of unlabeled data.

364
00:21:32,740 --> 00:21:35,545
Um, and from that unlabeled data,

365
00:21:35,545 --> 00:21:39,850
we can train a conventional word embedding model like Word2vec.

366
00:21:39,850 --> 00:21:43,810
But we can also at the same time train a neural language model.

367
00:21:43,810 --> 00:21:47,590
So, something like a bi-LSTM language model.

368
00:21:47,590 --> 00:21:55,190
Okay. So, then for step two when we're using our supervised data,

369
00:21:55,740 --> 00:21:58,900
um, actually, I guess that's step three.

370
00:21:58,900 --> 00:22:05,965
Okay. Um, so for then when we want to learn our supervised part-of-speech tagger at the top,

371
00:22:05,965 --> 00:22:09,190
what we're gonna do is say, well,

372
00:22:09,190 --> 00:22:13,420
for the input words New what York is located,

373
00:22:13,420 --> 00:22:18,340
we can not only use the word embedding which is context independent,

374
00:22:18,340 --> 00:22:24,505
but we can use our trained recurrent language model and also run it over this import,

375
00:22:24,505 --> 00:22:31,180
and then we'll generate hidden states in our bi-LSTM language model and we can also

376
00:22:31,180 --> 00:22:38,380
feed those in as features into ou- our sequence tagging model,

377
00:22:38,380 --> 00:22:41,335
and those features will let it work better.

378
00:22:41,335 --> 00:22:47,095
Here's a second picture that runs this through in much greater detail.

379
00:22:47,095 --> 00:22:52,885
So, so, we're assuming that we have trained, uh,

380
00:22:52,885 --> 00:22:56,875
bi-LSTM language model, um,

381
00:22:56,875 --> 00:22:59,755
on a lot of unsupervised data.

382
00:22:59,755 --> 00:23:06,370
Then what we wanna do is we want to do named entity recognition for New York is located.

383
00:23:06,370 --> 00:23:09,160
So, the first thing we do is say,

384
00:23:09,160 --> 00:23:16,150
let's just run New York is located through our separately trained neural language model.

385
00:23:16,150 --> 00:23:18,925
So, we run it through a forward language model.

386
00:23:18,925 --> 00:23:21,490
We run it through a backward language model.

387
00:23:21,490 --> 00:23:23,830
We get from that, um,

388
00:23:23,830 --> 00:23:26,515
a hidden state representation,

389
00:23:26,515 --> 00:23:28,750
um, for each word,

390
00:23:28,750 --> 00:23:31,644
we concatenate the forward and backward ones,

391
00:23:31,644 --> 00:23:35,529
and that's going to give a set, a concatenated language model embedding

392
00:23:35,529 --> 00:23:40,090
which we'll use as features in our named entity recognizer.

393
00:23:40,090 --> 00:23:43,870
So, then for the named entity recognizer itself that we're gonna

394
00:23:43,870 --> 00:23:48,700
train supervised while we have the same sentence,

395
00:23:48,700 --> 00:23:55,390
so we can both look up a Word2vec-style token embedding for it.

396
00:23:55,390 --> 00:24:01,315
We can use what we learned about with character level CNNs and RNNs and we can build

397
00:24:01,315 --> 00:24:04,450
a character level representation for it which we also

398
00:24:04,450 --> 00:24:07,795
concatenate to have two representations.

399
00:24:07,795 --> 00:24:15,685
So, we feed these representations into a bi-LSTM layer.

400
00:24:15,685 --> 00:24:19,945
But then when we get the output of the, this bi-LSTM layer,

401
00:24:19,945 --> 00:24:22,180
as well as this normal output,

402
00:24:22,180 --> 00:24:28,285
we can concatenate with each output what was- what we get from our,

403
00:24:28,285 --> 00:24:30,730
um, neural language model.

404
00:24:30,730 --> 00:24:33,370
So, each of these things becomes a pair of states.

405
00:24:33,370 --> 00:24:36,490
One that's spit up from the first bi-LSTM layer and

406
00:24:36,490 --> 00:24:39,760
then it's concatenated with something from the neural language model.

407
00:24:39,760 --> 00:24:46,450
And so that concatenated representation is then fed into a second layer of bi-LSTM.

408
00:24:46,450 --> 00:24:48,265
And then from the output of that,

409
00:24:48,265 --> 00:24:51,310
we do the usual kind of softmax classification

410
00:24:51,310 --> 00:24:54,790
where we're then giving tags like beginning of location,

411
00:24:54,790 --> 00:24:59,380
end of location, say New York is a location and then is, we'll get

412
00:24:59,380 --> 00:25:05,540
another tag to say it's not a location. Does that makes sense?

413
00:25:07,860 --> 00:25:14,305
Yeah so, um, so the central thing is

414
00:25:14,305 --> 00:25:20,455
sort of having seen that these sort of representations that we get from Bi-LSTMs are useful.

415
00:25:20,455 --> 00:25:24,580
We're just going to feed them into supervised models as we train them,

416
00:25:24,580 --> 00:25:28,600
and the idea is that will give us better features of words.

417
00:25:28,600 --> 00:25:32,305
Some kind of representation of their meaning and context,

418
00:25:32,305 --> 00:25:39,610
which will allow us to learn better named entity recognizers or what it- whatever it is.

419
00:25:39,610 --> 00:25:42,580
Maybe I should put this slide earlier,

420
00:25:42,580 --> 00:25:45,955
but this slide was meant to remind you what a named entity recognizer is.

421
00:25:45,955 --> 00:25:47,305
I hope you remember that,

422
00:25:47,305 --> 00:25:50,605
something where are we going to find and label

423
00:25:50,605 --> 00:25:54,850
entities for things like person, location, date, organization.

424
00:25:54,850 --> 00:25:57,625
So anyway, doing this worked.

425
00:25:57,625 --> 00:25:59,905
So, here's a little bit of a history.

426
00:25:59,905 --> 00:26:07,285
So the most famous Named Entity Recognition dataset is this CoNLL 2003 dataset,

427
00:26:07,285 --> 00:26:10,180
which actually exists in multiple languages.

428
00:26:10,180 --> 00:26:14,770
But whenever people say CoNLL 2003 and don't mention a language,

429
00:26:14,770 --> 00:26:17,665
they mean the English version of it.

430
00:26:17,665 --> 00:26:20,410
That's the way the world works.

431
00:26:20,410 --> 00:26:24,430
Um, okay so on this dataset- yeah.

432
00:26:24,430 --> 00:26:29,035
So, it's sort of been around for whatever, 15 years roughly now.

433
00:26:29,035 --> 00:26:32,920
So, in the- so it was originally a competition, right?

434
00:26:32,920 --> 00:26:36,235
So, this is in 2003 was the original bake-off.

435
00:26:36,235 --> 00:26:38,455
My group actually took place in that.

436
00:26:38,455 --> 00:26:42,055
Took part in it. I think we got third or fourth place or something,

437
00:26:42,055 --> 00:26:46,720
and our F1 score was 86.

438
00:26:46,720 --> 00:26:52,810
The people who won were from IBM Research Labs,

439
00:26:52,810 --> 00:26:55,870
and they got 88 almost 89.

440
00:26:55,870 --> 00:27:00,490
But a difference between these two things is our system was

441
00:27:00,490 --> 00:27:05,290
a single clean machine-learning model categorical,

442
00:27:05,290 --> 00:27:08,800
whereas the IBM one was not only an ensemble

443
00:27:08,800 --> 00:27:13,600
of four different machine learning models, plus gazetteers.

444
00:27:13,600 --> 00:27:16,090
It also fit in the output of

445
00:27:16,090 --> 00:27:22,450
two other old NER systems that IBM people were trained years ago on different data.

446
00:27:22,450 --> 00:27:25,030
So it was- I guess it worked for them but,

447
00:27:25,030 --> 00:27:27,100
it was a fairly complex system.

448
00:27:27,100 --> 00:27:29,170
Here's another system from Stanford.

449
00:27:29,170 --> 00:27:33,910
So this was our classic Stanford NER system that is widely used.

450
00:27:33,910 --> 00:27:39,475
So, this was then using a conditional random field model which generally dominated

451
00:27:39,475 --> 00:27:46,935
sort of the second half of the 2000s and the first half of the 2010s for doing NER,

452
00:27:46,935 --> 00:27:52,800
and it was sort of, you know, a bit but not usually better than the 2003 system.

453
00:27:52,800 --> 00:27:59,910
This system here was sort of the best ever built categorical CRF system.

454
00:27:59,910 --> 00:28:06,105
But rather than only using the training data to build the model as this system did,

455
00:28:06,105 --> 00:28:11,065
it threw in Wikipedia and other stuff to make it work better,

456
00:28:11,065 --> 00:28:14,725
and that got you to about 90,8 F1.

457
00:28:14,725 --> 00:28:23,770
So, essentially, once sort of BiLSTM style models started to be known and used in NLP.

458
00:28:23,770 --> 00:28:28,060
That was when people were able to train, build training

459
00:28:28,060 --> 00:28:33,175
just on the training data systems that worked a lot better.

460
00:28:33,175 --> 00:28:38,440
Because essentially you're going from the same data from this system to that system.

461
00:28:38,440 --> 00:28:41,530
So, you're getting about 4 percent gain on it,

462
00:28:41,530 --> 00:28:45,835
because it's not- wasn't making use of Wikipedia and things like that;

463
00:28:45,835 --> 00:28:51,805
and so this Ma and Hovy system is pretty well-known getting about 91,21.

464
00:28:51,805 --> 00:28:56,140
Okay, but if we then go to this TagLM system, um,

465
00:28:56,140 --> 00:29:00,610
that Matt Peters and Co have a system that

466
00:29:00,610 --> 00:29:05,590
was sort of similar to the Ma and Hovy system that is a little bit worse.

467
00:29:05,590 --> 00:29:12,670
But the point is that this BiLSTM uses sorry- using the neural language model,

468
00:29:12,670 --> 00:29:17,080
is just a useful oomph giver which sort of takes the results up.

469
00:29:17,080 --> 00:29:18,610
Yeah, not night and day but,

470
00:29:18,610 --> 00:29:24,160
slightly over a percent and then gives them the best NER system that was then available.

471
00:29:24,160 --> 00:29:25,990
So that sort of proved these sort of

472
00:29:25,990 --> 00:29:33,380
contextual word representations really had some power and started to be useful,

473
00:29:33,660 --> 00:29:38,620
and then there's a white space at the top because we'll get back to more of this later.

474
00:29:38,620 --> 00:29:43,240
Um, there's some details on their language model.

475
00:29:43,240 --> 00:29:46,330
Some of their details are that it's useful to have

476
00:29:46,330 --> 00:29:49,285
a bidirectional language model, not unidirectional.

477
00:29:49,285 --> 00:29:51,640
It's useful to have a big um,

478
00:29:51,640 --> 00:29:55,510
language model to get much in the way of gains,

479
00:29:55,510 --> 00:30:01,960
um and, you need to train this language model over much more data.

480
00:30:01,960 --> 00:30:07,070
It doesn't work if you're just sort of training it over your supervised training data.

481
00:30:08,160 --> 00:30:11,140
Another model that was around was CoVe,

482
00:30:11,140 --> 00:30:12,610
but I think I'll skip that.

483
00:30:12,610 --> 00:30:15,895
Okay. So, then the next year, um,

484
00:30:15,895 --> 00:30:18,865
Matt Peters and a different set of colleagues

485
00:30:18,865 --> 00:30:23,410
then came up with an improved system called ELMo,

486
00:30:23,410 --> 00:30:27,610
and effectively this was the breakthrough system.

487
00:30:27,610 --> 00:30:30,960
That this was sort of just the system that everybody

488
00:30:30,960 --> 00:30:35,880
noticed and said "Wow these contextual word vectors are great.

489
00:30:35,880 --> 00:30:37,680
Everyone should be using them,

490
00:30:37,680 --> 00:30:41,620
not traditional word vectors." Yes?

491
00:30:41,790 --> 00:30:51,490
I have a simple question, imagine re-training a system, what exactly

492
00:30:59,330 --> 00:31:02,910
what measure [inaudible]

493
00:31:02,910 --> 00:31:06,250
It's pre-trained because this piece over here;

494
00:31:06,250 --> 00:31:11,035
a big neural language model is trained first,

495
00:31:11,035 --> 00:31:13,270
and there's an important thing I forgot to say.

496
00:31:13,270 --> 00:31:15,280
So, thank you for the question.

497
00:31:15,280 --> 00:31:20,020
The main reason why it's- in some sense pre-trained,

498
00:31:20,020 --> 00:31:21,670
is this was trained first.

499
00:31:21,670 --> 00:31:26,245
But the main reason why people think of this as pre-training

500
00:31:26,245 --> 00:31:30,985
is after you've trained this, it is frozen.

501
00:31:30,985 --> 00:31:35,680
So, this is just something that you can run with parameters which will give

502
00:31:35,680 --> 00:31:40,840
you a vector which is your contextual word representation each position,

503
00:31:40,840 --> 00:31:43,960
and then that's just going to be used in this system.

504
00:31:43,960 --> 00:31:46,420
So, when you're training this system,

505
00:31:46,420 --> 00:31:48,580
there's no gradient flowing back into

506
00:31:48,580 --> 00:31:52,885
this neural language model that's changing and updating it; it's just fixed.

507
00:31:52,885 --> 00:31:56,260
And so that's sort of the sense when people are talking about pre-training.

508
00:31:56,260 --> 00:31:59,185
It's sort of normally a model that you trained

509
00:31:59,185 --> 00:32:02,680
somewhere else and that you're using to give features,

510
00:32:02,680 --> 00:32:06,280
but isn't part of the model that you are now training. Yeah?

511
00:32:06,280 --> 00:32:12,060
[inaudible]

512
00:32:12,060 --> 00:32:16,650
Well, I guess that's, I wouldn't quite call it reconstruction.

513
00:32:16,650 --> 00:32:20,190
Yeah, it's unsupervised in the sense that this is a language model,

514
00:32:20,190 --> 00:32:22,470
you're training it to predict the next word.

515
00:32:22,470 --> 00:32:28,335
So here are words one to k. What is the k plus oneth word during a cross entropy loss,

516
00:32:28,335 --> 00:32:30,150
and repeat over for each position.

517
00:32:30,150 --> 00:32:37,530
[NOISE] Yes, so I mean,

518
00:32:37,530 --> 00:32:45,240
having gone through TagLM in some detail, I mean,

519
00:32:45,240 --> 00:32:52,350
in some sense, the difference between TagLM and ELMo is kind of small,

520
00:32:52,350 --> 00:32:54,090
it's sort of in the details.

521
00:32:54,090 --> 00:32:56,385
So I mean, to a first approximation,

522
00:32:56,385 --> 00:32:58,890
they're doing exactly the same again,

523
00:32:58,890 --> 00:33:00,675
but a little bit better.

524
00:33:00,675 --> 00:33:06,360
Um, so, um, I sort of hope it made sense the last time,

525
00:33:06,360 --> 00:33:09,015
I mean, what are the things that are different?

526
00:33:09,015 --> 00:33:13,710
Um, they do the bidirectional language model a bit differently,

527
00:33:13,710 --> 00:33:16,800
and actually one of their concerns was to try and come up with

528
00:33:16,800 --> 00:33:21,435
a compact language model that would be easy for people to use,

529
00:33:21,435 --> 00:33:27,390
um, in other tasks even if they don't have the beefiest computer hardware in the world.

530
00:33:27,390 --> 00:33:29,940
And so they decided to dispense with having

531
00:33:29,940 --> 00:33:34,185
word representations altogether and just use, um,

532
00:33:34,185 --> 00:33:38,610
character CNNs to build word representations,

533
00:33:38,610 --> 00:33:42,045
because that lessens the number of parameters you have to store,

534
00:33:42,045 --> 00:33:45,510
the big matrices you have to, um, use.

535
00:33:45,510 --> 00:33:50,280
Um, they expanded the hidden dimension to 4,096,

536
00:33:50,280 --> 00:33:52,020
but then they project it down to

537
00:33:52,020 --> 00:33:57,450
512 dimensions with a sort of feed-forward projection layer,

538
00:33:57,450 --> 00:34:00,300
and that's a fairly common technique to again reduce

539
00:34:00,300 --> 00:34:03,360
the parameterization of the model so that you have a lot of

540
00:34:03,360 --> 00:34:06,060
parameters going in their current direction but you

541
00:34:06,060 --> 00:34:09,315
need much smaller matrices for including,

542
00:34:09,315 --> 00:34:11,400
um, the input at the next level.

543
00:34:11,400 --> 00:34:13,530
Um, between the layers,

544
00:34:13,530 --> 00:34:18,300
they now use a residual connection and they do a bit of parameter tying.

545
00:34:18,300 --> 00:34:21,615
So it's sort of all in the little details there.

546
00:34:21,615 --> 00:34:25,200
Um, but there's another interesting thing

547
00:34:25,200 --> 00:34:28,890
that they did which was an important innovation of ELMo,

548
00:34:28,890 --> 00:34:30,405
so we should get this bit.

549
00:34:30,405 --> 00:34:32,400
So in TagLM,

550
00:34:32,400 --> 00:34:36,930
what was fed from the pre-trained LM into

551
00:34:36,930 --> 00:34:43,695
the main model was just the top level of the neural language model stack,

552
00:34:43,695 --> 00:34:47,040
and that was completely standard de rigueur in those days,

553
00:34:47,040 --> 00:34:49,800
that you might have had three layers of

554
00:34:49,800 --> 00:34:53,790
neural language model that you regard at the top-level as your sort

555
00:34:53,790 --> 00:34:57,120
of one that's really captured the meaning of

556
00:34:57,120 --> 00:35:01,185
the sentence and the lower layers for processing that led up to it.

557
00:35:01,185 --> 00:35:05,295
Um, and they had the idea that maybe

558
00:35:05,295 --> 00:35:09,780
it would be useful to actually use all layers of the,

559
00:35:09,780 --> 00:35:12,960
biLSTM of the neural language models.

560
00:35:12,960 --> 00:35:16,935
So maybe not just the top layer but all layers would be kind of useful.

561
00:35:16,935 --> 00:35:20,760
So, um, there are these kind of complex equations,

562
00:35:20,760 --> 00:35:24,480
uh, but essentially the point of it over here is,

563
00:35:24,480 --> 00:35:27,360
we going- for a particular position,

564
00:35:27,360 --> 00:35:29,505
word seven in the language model,

565
00:35:29,505 --> 00:35:33,930
we're going to take the hidden state at each level of our,

566
00:35:33,930 --> 00:35:36,599
our neural language model stack,

567
00:35:36,599 --> 00:35:40,545
we're going to give- learn a weight for that level,

568
00:35:40,545 --> 00:35:42,540
we go in to sort of sum them,

569
00:35:42,540 --> 00:35:47,190
so this is sort of a weighted average of the hidden layers at each position,

570
00:35:47,190 --> 00:35:51,225
and that will be used as our basic representation.

571
00:35:51,225 --> 00:35:55,785
Um, and so, they found that that gave quite a bit

572
00:35:55,785 --> 00:36:00,480
of extra usefulness for- and different tasks could prefer different layers.

573
00:36:00,480 --> 00:36:03,045
There's one other bit here which is,

574
00:36:03,045 --> 00:36:08,625
they learn a global scaling factor Gamma for a particular task.

575
00:36:08,625 --> 00:36:13,665
And this allows them to control that for some tasks, the, um,

576
00:36:13,665 --> 00:36:16,080
contextual word embeddings might be really

577
00:36:16,080 --> 00:36:19,515
useful and for other tasks they might not be so useful,

578
00:36:19,515 --> 00:36:21,449
so you're just sort of learning a specific,

579
00:36:21,449 --> 00:36:25,095
um, usefulness for the entire task.

580
00:36:25,095 --> 00:36:30,285
Okay. So, um, that's the sort of new version of language model.

581
00:36:30,285 --> 00:36:33,390
But this, this is allowing this idea of well,

582
00:36:33,390 --> 00:36:36,750
maybe there's sort of more syntactic meanings

583
00:36:36,750 --> 00:36:39,855
of a word and more semantic meanings of a word,

584
00:36:39,855 --> 00:36:43,380
possibly those could be represented at different layers of

585
00:36:43,380 --> 00:36:45,510
your neural language model and then for

586
00:36:45,510 --> 00:36:48,330
different tasks you can differentially weight them.

587
00:36:48,330 --> 00:36:51,330
Um, so that's the basic model.

588
00:36:51,330 --> 00:36:56,850
So you run your biLSTM before to g et representations of each word.

589
00:36:56,850 --> 00:36:59,610
And then the generic ELMo recipe was,

590
00:36:59,610 --> 00:37:03,215
well, with that frozen language model,

591
00:37:03,215 --> 00:37:08,540
you want to feed it into some supervised model depending on what the task was,

592
00:37:08,540 --> 00:37:10,070
and they sort of say in the paper, well,

593
00:37:10,070 --> 00:37:12,500
how you do this maybe depends on the task.

594
00:37:12,500 --> 00:37:15,965
You might want to kind of concatenate it to the intermediate layer,

595
00:37:15,965 --> 00:37:17,660
just as the TagLM did,

596
00:37:17,660 --> 00:37:19,085
that might be fine.

597
00:37:19,085 --> 00:37:22,220
But you know it might also be useful to make use of

598
00:37:22,220 --> 00:37:25,700
these ELMo representations when producing outputs,

599
00:37:25,700 --> 00:37:28,910
so if you're doing something like a

600
00:37:28,910 --> 00:37:35,210
generation system or you might just sort of feed in the ELMo representation again,

601
00:37:35,210 --> 00:37:38,630
be- before you sort of do the softmax to find the output,

602
00:37:38,630 --> 00:37:41,580
they sort of left it flexible as to how it was used,

603
00:37:41,580 --> 00:37:42,960
but the general picture,

604
00:37:42,960 --> 00:37:45,960
you know, was kinda like we saw before.

605
00:37:45,960 --> 00:37:49,590
Indeed I'm reusing the same picture that you've calculated

606
00:37:49,590 --> 00:37:54,105
an ELMo representation for each position as a weighted average,

607
00:37:54,105 --> 00:37:57,360
and then you're sort of concatenating that to the hidden state of

608
00:37:57,360 --> 00:38:01,125
your supervised system and generating your output.

609
00:38:01,125 --> 00:38:04,890
And anyway, um, one way or another,

610
00:38:04,890 --> 00:38:07,920
um, they were able to do this, uh,

611
00:38:07,920 --> 00:38:11,925
and that with the little improvements that gave them about an extra

612
00:38:11,925 --> 00:38:16,770
0,3 percent in Named Entity Recognition.

613
00:38:16,770 --> 00:38:21,165
Um, now, that sort of sounds like not very much.

614
00:38:21,165 --> 00:38:26,055
And you might conclude from this why the excitement [LAUGHTER] and,

615
00:38:26,055 --> 00:38:28,695
you know, in some sense, um,

616
00:38:28,695 --> 00:38:33,720
that's right because sort of to the extent that there was an interesting idea here really

617
00:38:33,720 --> 00:38:39,060
that come up with it for the TagLM paper which gave a much better gain.

618
00:38:39,060 --> 00:38:45,254
But, you know, why everyone got really excited was that in the ELMo paper,

619
00:38:45,254 --> 00:38:48,030
they then showed this isn't something that you can

620
00:38:48,030 --> 00:38:50,910
do one-off to improve a Named Entity Recognizer,

621
00:38:50,910 --> 00:38:58,035
you can take these ELMo representations and use them for pretty much any NLP task,

622
00:38:58,035 --> 00:39:01,695
and they can be very useful and give good gains.

623
00:39:01,695 --> 00:39:08,340
And so, essentially why people got excited was because of the data that's in this table.

624
00:39:08,340 --> 00:39:11,250
So here we're taking a whole bunch of very different tasks,

625
00:39:11,250 --> 00:39:13,620
so there's SQuAD question-answering, uh,

626
00:39:13,620 --> 00:39:16,380
there's natural language inference,

627
00:39:16,380 --> 00:39:18,345
there's semantic role labeling,

628
00:39:18,345 --> 00:39:23,760
there's co-reference, the Named Entity Recognition, doing sentiment analysis,

629
00:39:23,760 --> 00:39:26,730
so a wide range of different NLP tasks,

630
00:39:26,730 --> 00:39:30,315
and they have a previous state of the art system.

631
00:39:30,315 --> 00:39:34,860
They produced their own baseline um, which is,

632
00:39:34,860 --> 00:39:40,080
you know, commonly sort of similar to the previous state of the art,

633
00:39:40,080 --> 00:39:43,620
but usually actually a bit worse than

634
00:39:43,620 --> 00:39:45,360
the current state of the art because it's

635
00:39:45,360 --> 00:39:48,315
whatever simpler cleaner system that they came up with,

636
00:39:48,315 --> 00:39:51,345
but then they could say in each case,

637
00:39:51,345 --> 00:39:55,260
oh, just take this system and add

638
00:39:55,260 --> 00:39:59,985
ELMo vectors into the hidden representations in the middle,

639
00:39:59,985 --> 00:40:02,040
and have those help you predict.

640
00:40:02,040 --> 00:40:04,710
And in general, in all cases,

641
00:40:04,710 --> 00:40:08,970
that's giving you about a three percent or so gain absolute

642
00:40:08,970 --> 00:40:13,470
which was then producing this huge performance increase,

643
00:40:13,470 --> 00:40:18,450
which in all cases was moving the performance well above the previous,

644
00:40:18,450 --> 00:40:20,040
um, state of the art system.

645
00:40:20,040 --> 00:40:24,000
So you know, this sort of then made it seem like magic pixie dust,

646
00:40:24,000 --> 00:40:28,050
because, you know, in the stakes of NLP conference land, you know,

647
00:40:28,050 --> 00:40:30,960
a lot of people use to try and to come up

648
00:40:30,960 --> 00:40:34,500
with a paper for the next year that's one percent better

649
00:40:34,500 --> 00:40:37,080
on one task and writing it up and that's

650
00:40:37,080 --> 00:40:41,715
their big breakthrough for the year to get their new paper out.

651
00:40:41,715 --> 00:40:44,355
And the idea that there's just well this set of

652
00:40:44,355 --> 00:40:48,045
this way of creating context sensitive, um,

653
00:40:48,045 --> 00:40:51,660
word representations and you just use them in any task,

654
00:40:51,660 --> 00:40:55,245
and they'll give you around three percent and take you past the state of the art,

655
00:40:55,245 --> 00:40:58,395
this seemed like it was really great stuff.

656
00:40:58,395 --> 00:41:01,800
And so people got very excited about this and that won

657
00:41:01,800 --> 00:41:06,390
the Best Paper Award at the NAACL 2018 conference.

658
00:41:06,390 --> 00:41:10,590
Ah, and then, a- as I sort of vaguely mentioned,

659
00:41:10,590 --> 00:41:14,370
um, so the model that they actually used wasn't a deep stack,

660
00:41:14,370 --> 00:41:17,520
there were actually only two layers of biLSTMs,

661
00:41:17,520 --> 00:41:22,620
but they do show this interesting result that the lower level better captures

662
00:41:22,620 --> 00:41:26,790
low-level syntax word properties

663
00:41:26,790 --> 00:41:30,389
and its most useful things like part-of-speech tagging,  syntactic

664
00:41:30,389 --> 00:41:33,210
dependencies, NER, where the top layer of

665
00:41:33,210 --> 00:41:35,310
their language model is better for

666
00:41:35,310 --> 00:41:38,940
higher level semantics that is more useful for things like sentiments,

667
00:41:38,940 --> 00:41:42,495
semantic role labeling and question answering.

668
00:41:42,495 --> 00:41:45,150
Um, so that seemed interesting,

669
00:41:45,150 --> 00:41:47,940
though it'll actually be interesting to see how that panned

670
00:41:47,940 --> 00:41:51,820
out more if you had sort of more layers to play with.

671
00:41:52,100 --> 00:41:55,875
Okay. ELMo, done.

672
00:41:55,875 --> 00:41:58,590
Um, so I'm moving right ahead.

673
00:41:58,590 --> 00:42:05,550
Um, here's something else that I just thought I should mention a little bit about,

674
00:42:05,550 --> 00:42:09,270
another piece of work that came out around the same time,

675
00:42:09,270 --> 00:42:12,450
a few months later maybe or maybe not,

676
00:42:12,450 --> 00:42:14,430
came out around the same time, uh,

677
00:42:14,430 --> 00:42:18,420
in, in 2018, was this work on

678
00:42:18,420 --> 00:42:23,025
Universal Language Model Fine-tuning for text classification,

679
00:42:23,025 --> 00:42:25,995
um, or ULMfit, by Howard and Ruder.

680
00:42:25,995 --> 00:42:31,335
And essentially this had the same general idea of saying, Well,

681
00:42:31,335 --> 00:42:39,370
what we want to do is transfer learning where we could learn a big language model, um.

682
00:42:40,560 --> 00:42:43,075
A big language model,

683
00:42:43,075 --> 00:42:48,220
and then for our target task which might be named entity recognition.

684
00:42:48,220 --> 00:42:50,200
But here's text classification,

685
00:42:50,200 --> 00:42:55,690
we can transfer this language model information and help us to do better with the task.

686
00:42:55,690 --> 00:42:58,690
And so, they proposed an architecture to do that.

687
00:42:58,690 --> 00:43:00,640
And so, their architecture was,

688
00:43:00,640 --> 00:43:07,960
you have a big unsupervised corpus from which you train a neural language model.

689
00:43:07,960 --> 00:43:12,775
They used the deeper neural language model with three hidden layers.

690
00:43:12,775 --> 00:43:14,920
Um, you then fine tune

691
00:43:14,920 --> 00:43:19,660
your neural language model on the actual domain that you're interested in working in.

692
00:43:19,660 --> 00:43:22,255
So, this was sort of an extra stage that they did.

693
00:43:22,255 --> 00:43:24,730
And then finally, um,

694
00:43:24,730 --> 00:43:28,960
you now introduce your classification objectives.

695
00:43:28,960 --> 00:43:31,930
So, what they're going to be doing is making text classifiers.

696
00:43:31,930 --> 00:43:33,535
So, we're now wanting to,

697
00:43:33,535 --> 00:43:39,280
take this model and turn it from a language model into a text classifier.

698
00:43:39,280 --> 00:43:42,340
Um, but there's something that they did differently, um,

699
00:43:42,340 --> 00:43:43,720
which is in some sense,

700
00:43:43,720 --> 00:43:46,840
foreshadows the later work in transformers.

701
00:43:46,840 --> 00:43:52,210
So, rather than just feeding features from this into a completely different network,

702
00:43:52,210 --> 00:43:58,710
they keep using the same network but they introduce a different objective at the top.

703
00:43:58,710 --> 00:44:01,710
So, one thing you could do with this network is use

704
00:44:01,710 --> 00:44:05,015
it to predict the next word as a language model.

705
00:44:05,015 --> 00:44:06,460
And so at this point,

706
00:44:06,460 --> 00:44:09,820
they freeze the parameters of that softmax at the top,

707
00:44:09,820 --> 00:44:11,455
that's why it's shown in black.

708
00:44:11,455 --> 00:44:14,935
Um, but instead, they could stick on

709
00:44:14,935 --> 00:44:19,825
a different prediction unit where it's predicting stuff for a particular task.

710
00:44:19,825 --> 00:44:21,610
So, it might be predicting

711
00:44:21,610 --> 00:44:26,680
positive or negative sentiment in a text classification task or something like that.

712
00:44:26,680 --> 00:44:27,760
So, in their model,

713
00:44:27,760 --> 00:44:31,915
they're sort of reusing the same network but sticking on the top of that,

714
00:44:31,915 --> 00:44:36,205
a different layer, to do the new classification task.

715
00:44:36,205 --> 00:44:39,700
Um, they were also interested in something small,

716
00:44:39,700 --> 00:44:43,615
the sort of one GPU model of research, um,

717
00:44:43,615 --> 00:44:47,620
the paper has a lot of detail, the sort of tricks

718
00:44:47,620 --> 00:44:52,150
and care and feeding of your neural models to maximize performance.

719
00:44:52,150 --> 00:44:56,245
If you're interested in that, you could sort of look up some of the details about that.

720
00:44:56,245 --> 00:45:00,250
Um, but what they were able to show again,

721
00:45:00,250 --> 00:45:03,820
was making use of this language model pre-training was

722
00:45:03,820 --> 00:45:07,495
a very effective way to improve performance,

723
00:45:07,495 --> 00:45:09,865
this time for text classification.

724
00:45:09,865 --> 00:45:12,520
So, these are text classification datasets,

725
00:45:12,520 --> 00:45:14,260
IMDb is for sentiment,

726
00:45:14,260 --> 00:45:18,970
um, TREC is for topical text classification, and again,

727
00:45:18,970 --> 00:45:22,780
there are preceding systems that other people have developed and they

728
00:45:22,780 --> 00:45:26,620
are showing that by making use of this language model pre-training,

729
00:45:26,620 --> 00:45:31,390
they're able to significantly improve on the state of the art of these error rates,

730
00:45:31,390 --> 00:45:33,590
so that low is good.

731
00:45:33,900 --> 00:45:39,715
They also showed another interesting result which is kind of,

732
00:45:39,715 --> 00:45:44,395
um, what you would expect or hope from doing this kind of transfer learning,

733
00:45:44,395 --> 00:45:46,330
that what they were able to show is,

734
00:45:46,330 --> 00:45:51,205
if you can train this neural language model on a big amount of data,

735
00:45:51,205 --> 00:45:54,430
that that means you will then be able to do well on

736
00:45:54,430 --> 00:45:59,110
your supervised task even when trained on pretty little data.

737
00:45:59,110 --> 00:46:01,780
Um, so, here this is error rate,

738
00:46:01,780 --> 00:46:03,355
so low is good.

739
00:46:03,355 --> 00:46:05,170
So, what the- and here's the number of

740
00:46:05,170 --> 00:46:08,815
training examples which has being done on a log scale.

741
00:46:08,815 --> 00:46:11,710
And so the blue line is if you're just training

742
00:46:11,710 --> 00:46:15,730
a text classifier from scratch on supervised data.

743
00:46:15,730 --> 00:46:19,765
So, you need a lot of data to start to do pretty well.

744
00:46:19,765 --> 00:46:24,715
Um, but if you're making use of this transfer learning, um,

745
00:46:24,715 --> 00:46:27,894
from a pre-trained language model,

746
00:46:27,894 --> 00:46:30,310
you can get to that you're sort of doing pretty

747
00:46:30,310 --> 00:46:33,700
well with way less, um, training examples.

748
00:46:33,700 --> 00:46:35,889
Essentially, an order of magnitude,

749
00:46:35,889 --> 00:46:39,655
less training examples will give you the same amount of performance.

750
00:46:39,655 --> 00:46:44,020
And the difference between these two lines corresponds to the extra,

751
00:46:44,020 --> 00:46:48,670
um, phase that they had in the middle of theirs, um, which is,

752
00:46:48,670 --> 00:46:53,920
whether you're doing this sort of extra fine tuning on your target domain,

753
00:46:53,920 --> 00:46:58,690
um, it's part of your process and they found that to be pretty helpful.

754
00:46:58,690 --> 00:47:05,215
Okay. So, that, um, is another precursor.

755
00:47:05,215 --> 00:47:11,545
Um, and so, one big part of what has happened since then,

756
00:47:11,545 --> 00:47:15,820
is effectively people said this is a good idea, uh,

757
00:47:15,820 --> 00:47:21,910
maybe it'll become a really really good idea if we just make things way bigger.

758
00:47:21,910 --> 00:47:24,250
Um, so, ULMfit, um,

759
00:47:24,250 --> 00:47:28,045
was something that you could train in one GPU day,

760
00:47:28,045 --> 00:47:31,870
sounds appealing for CS224N final projects,

761
00:47:31,870 --> 00:47:34,930
remember that, um, and but well,

762
00:47:34,930 --> 00:47:39,115
then the people at OpenAI decided, well,

763
00:47:39,115 --> 00:47:43,300
we could build a pretrain language model and train it on

764
00:47:43,300 --> 00:47:47,590
a much larger amount of data on a much larger amount of compute,

765
00:47:47,590 --> 00:47:54,130
and use about 242 GPU days and that will get a lot better, and it did.

766
00:47:54,130 --> 00:47:57,190
Um, and then the people at Google said,

767
00:47:57,190 --> 00:48:00,445
well we could train a model, um,

768
00:48:00,445 --> 00:48:04,660
in to 256 TPU days,

769
00:48:04,660 --> 00:48:07,645
which means maybe about double the amount of computation.

770
00:48:07,645 --> 00:48:09,565
It's hard to figure out exactly,

771
00:48:09,565 --> 00:48:12,175
and that might be able to do exciting things,

772
00:48:12,175 --> 00:48:14,950
and that was the BERT model, and it did.

773
00:48:14,950 --> 00:48:18,370
Um, and then if you're following along these things, um,

774
00:48:18,370 --> 00:48:20,110
just last week, um,

775
00:48:20,110 --> 00:48:22,270
the OpenAI people said,

776
00:48:22,270 --> 00:48:26,845
well we can go much bigger again and we can train a model, um,

777
00:48:26,845 --> 00:48:32,830
for approximately 2,000 TPU version three days.

778
00:48:32,830 --> 00:48:36,340
Um, and it will be able to,

779
00:48:36,340 --> 00:48:39,294
um, do much bigger again,

780
00:48:39,294 --> 00:48:41,080
a bit much better again,

781
00:48:41,080 --> 00:48:44,410
um, and so, this is this GP2,

782
00:48:44,410 --> 00:48:47,800
GPT-2 language model, um,

783
00:48:47,800 --> 00:48:50,680
which OpenAI released last week.

784
00:48:50,680 --> 00:48:56,740
Um, and they're, they're actually very impressive results, um,

785
00:48:56,740 --> 00:49:00,730
when they're showing that if you're sort of building a really,

786
00:49:00,730 --> 00:49:05,155
really huge language model over a very large amount of data.

787
00:49:05,155 --> 00:49:09,745
And then you say language model go off and generate some text,

788
00:49:09,745 --> 00:49:11,800
on this particular topic,

789
00:49:11,800 --> 00:49:15,100
that it can actually just do a great job of producing text.

790
00:49:15,100 --> 00:49:17,125
So, the way this was being do- done,

791
00:49:17,125 --> 00:49:19,930
was a humanist writing a couple of sentences;

792
00:49:19,930 --> 00:49:21,190
in a shocking finding,

793
00:49:21,190 --> 00:49:23,515
scientists discovered a herd of unicorns,

794
00:49:23,515 --> 00:49:27,700
living in remote previously unexplored valley in the Andes Mountains.

795
00:49:27,700 --> 00:49:29,905
Um, and so, we then,

796
00:49:29,905 --> 00:49:33,700
using our neural language model and chugging through that,

797
00:49:33,700 --> 00:49:35,680
so that gives us context,

798
00:49:35,680 --> 00:49:37,765
and then say generate more text,

799
00:49:37,765 --> 00:49:39,760
and it starts to generate the scientist

800
00:49:39,760 --> 00:49:42,160
named the population after their distinctive horn,

801
00:49:42,160 --> 00:49:44,320
Ovid's Unicorn, these four-horned,

802
00:49:44,320 --> 00:49:47,815
silver-white Uni four corns were previously unknown to science.

803
00:49:47,815 --> 00:49:50,080
Um, it produces remarkably,

804
00:49:50,080 --> 00:49:52,735
um, good text or at least in the,

805
00:49:52,735 --> 00:49:57,220
in the hand-picked examples [LAUGHTER] that they showed in the tech news,

806
00:49:57,220 --> 00:49:59,920
um, it produces extremely good text.

807
00:49:59,920 --> 00:50:04,960
Um, yeah so, I think one should be a little bit cautious about, um,

808
00:50:04,960 --> 00:50:07,930
that and sort of some of its random outputs actually

809
00:50:07,930 --> 00:50:10,900
aren't nearly as good but nevertheless you know,

810
00:50:10,900 --> 00:50:12,895
I think is is actually dramatic

811
00:50:12,895 --> 00:50:16,540
how good language models are becoming once you are training

812
00:50:16,540 --> 00:50:23,210
them on long contexts as we can do with modern models on vast amounts of data, um-.

813
00:50:23,280 --> 00:50:27,430
So then, um, the OpenAI people decided

814
00:50:27,430 --> 00:50:31,720
this language model was so good that they weren't gonna release it to the world, um,

815
00:50:31,720 --> 00:50:34,480
which then got transformed into headlines of,

816
00:50:34,480 --> 00:50:39,265
Elon Musk's OpenAI builds artificial intelligence so powerful,

817
00:50:39,265 --> 00:50:41,980
it must be kept locked up for the good of humanity.

818
00:50:41,980 --> 00:50:46,660
[LAUGHTER] Um, with the suitable pictures that always turn off at

819
00:50:46,660 --> 00:50:52,075
these moments down the bottom of the screen, um, and,

820
00:50:52,075 --> 00:50:57,520
um, yeah I guess that was the leading even Elon Musk to be wanting to clarify and say

821
00:50:57,520 --> 00:51:03,020
that it's not actually really that he's directing what's happening at OpenAI anymore.

822
00:51:03,020 --> 00:51:06,355
Um, anyway, moving right along.

823
00:51:06,355 --> 00:51:09,760
Um, so, part of the story here is

824
00:51:09,760 --> 00:51:14,635
just a scaling thing that these things have been getting bigger and bigger,

825
00:51:14,635 --> 00:51:18,760
um, but the other part of the story is that all three of

826
00:51:18,760 --> 00:51:23,785
these are then systems that use the transformer architecture.

827
00:51:23,785 --> 00:51:27,700
And transformer architectures have not only being very powerful,

828
00:51:27,700 --> 00:51:32,575
but technically had allowed scaling to much bigger sizes.

829
00:51:32,575 --> 00:51:35,575
So to understand some of the rest of these, um,

830
00:51:35,575 --> 00:51:39,055
we should learn more about transformers.

831
00:51:39,055 --> 00:51:42,610
And so, I'm sort of gonna do that, um,

832
00:51:42,610 --> 00:51:46,495
but I mean, um, in mix of orders,

833
00:51:46,495 --> 00:51:50,200
um, our invited speaker coming Thursday uh, is, um,

834
00:51:50,200 --> 00:51:52,420
one of the authors of the transformer paper,

835
00:51:52,420 --> 00:51:54,490
and he's gonna talk about transformers.

836
00:51:54,490 --> 00:51:57,430
So I think what I'm gonna do is, um,

837
00:51:57,430 --> 00:52:01,000
say a little bit about transformers quickly,

838
00:52:01,000 --> 00:52:04,090
but not really dwell on all the details, um,

839
00:52:04,090 --> 00:52:06,265
but hope that it's a bit of an introduction,

840
00:52:06,265 --> 00:52:10,360
and you can find out more on Thursday about the details and

841
00:52:10,360 --> 00:52:15,190
then talk some more about the BERT model before finishing.

842
00:52:15,190 --> 00:52:19,450
So the motivation for transformers is essentially

843
00:52:19,450 --> 00:52:23,245
we want things to go faster so we can build bigger models,

844
00:52:23,245 --> 00:52:26,125
and the problem as we mentioned for these, um,

845
00:52:26,125 --> 00:52:31,060
LSTM or in general any of the recurrent models is the fact that they're recurrent.

846
00:52:31,060 --> 00:52:36,190
You have to generate sort of one to n status time chugging through,

847
00:52:36,190 --> 00:52:41,275
and that means you just can't do the same kind of parallel computation, um,

848
00:52:41,275 --> 00:52:46,970
that GPUs love that you can do in things like convolutional neural networks.

849
00:52:46,970 --> 00:52:48,855
But, you know, on the other hand,

850
00:52:48,855 --> 00:52:51,210
we discovered that even though, um,

851
00:52:51,210 --> 00:52:56,005
these gated recurrent units like LSTMs and GRUs are great,

852
00:52:56,005 --> 00:53:00,070
that to get really great performance out of these recurrent models,

853
00:53:00,070 --> 00:53:05,680
we found that we wanted to- we had a problem within these long sequence lengths,

854
00:53:05,680 --> 00:53:09,010
and we can improve things by adding attention mechanisms.

855
00:53:09,010 --> 00:53:12,070
And so that led to the idea of- well,

856
00:53:12,070 --> 00:53:14,425
since attention works so great,

857
00:53:14,425 --> 00:53:17,440
maybe we can just use attention,

858
00:53:17,440 --> 00:53:22,195
and we can actually get rid of the recurrent part of the model [NOISE] altogether.

859
00:53:22,195 --> 00:53:27,625
And so that actually then leads to the idea of these transformer architectures,

860
00:53:27,625 --> 00:53:32,545
and the original paper on this is actually called attention is all you need,

861
00:53:32,545 --> 00:53:36,700
which reflects this idea of we're gonna keep the attention part,

862
00:53:36,700 --> 00:53:40,000
and we're getting- going to get rid of the, um,

863
00:53:40,000 --> 00:53:43,960
recurrent part, and we'll be able to build a great model.

864
00:53:43,960 --> 00:53:45,310
So in the initial work,

865
00:53:45,310 --> 00:53:48,790
what they're doing is machine translation kind of like

866
00:53:48,790 --> 00:53:52,720
the Neural Machine Translation with attention we described,

867
00:53:52,720 --> 00:53:56,185
but what they're wanting to do is build

868
00:53:56,185 --> 00:54:03,625
a complex encoder and a complex decoder that works non-recurrently,

869
00:54:03,625 --> 00:54:07,659
and, um, nevertheless is able to translate sentences

870
00:54:07,659 --> 00:54:13,075
well by making use of lots of attention distributions.

871
00:54:13,075 --> 00:54:18,070
And so, I wanted to say a little bit more quickly about that,

872
00:54:18,070 --> 00:54:20,965
and hopefully we'll get more of this on Thursday.

873
00:54:20,965 --> 00:54:24,685
Um, first as a- as a recommended resource,

874
00:54:24,685 --> 00:54:26,545
if you wanna look at, um,

875
00:54:26,545 --> 00:54:29,695
home and learn more about, um,

876
00:54:29,695 --> 00:54:34,000
the transformer architecture, there's this really great, um,

877
00:54:34,000 --> 00:54:39,100
bit of work by Sasha Rush called The Annotated Transformer that goes through

878
00:54:39,100 --> 00:54:45,025
the entire transformer paper accompanied by PyTorch code in a Jupyter Notebook,

879
00:54:45,025 --> 00:54:48,220
and so that can actually be a really useful thing,

880
00:54:48,220 --> 00:54:54,235
but I'll go through a little bit of the basics now of how we do things.

881
00:54:54,235 --> 00:54:57,460
So the basic idea, um,

882
00:54:57,460 --> 00:55:03,385
is that they're going to use attention everywhere to calculate things.

883
00:55:03,385 --> 00:55:07,540
And, um, we talked before about the different kinds of

884
00:55:07,540 --> 00:55:12,520
attention of the sort of multiplicative by linear attention and the little,

885
00:55:12,520 --> 00:55:15,490
um, feed-forward network additive attention.

886
00:55:15,490 --> 00:55:18,670
They kind of go for the simplest kind of attention,

887
00:55:18,670 --> 00:55:23,035
where the attention is just dot-products between two things.

888
00:55:23,035 --> 00:55:26,860
Um, but they sort of do the more comp- for various purposes,

889
00:55:26,860 --> 00:55:32,830
they do the more complicated version of dot-product between two things where they have,

890
00:55:32,830 --> 00:55:36,280
um, when the- the things that they're looking up are

891
00:55:36,280 --> 00:55:40,375
assumed to be key-value pairs, keys and values,

892
00:55:40,375 --> 00:55:46,765
and so you're calculating the similarity as a dot-product between a query and the key,

893
00:55:46,765 --> 00:55:48,415
and then based on that,

894
00:55:48,415 --> 00:55:52,060
you're going to be using the vector for the corresponding value.

895
00:55:52,060 --> 00:55:55,795
So our equation here for what we're calculating is where you are

896
00:55:55,795 --> 00:56:00,130
looking using the softmax over query, um,

897
00:56:00,130 --> 00:56:03,610
key similarities and using that to give

898
00:56:03,610 --> 00:56:08,680
the weightings as an attention based weighting over the corresponding values.

899
00:56:08,680 --> 00:56:12,220
Um, so that's the basic attention model.

900
00:56:12,220 --> 00:56:15,985
Um, so that add- saying it that way, um,

901
00:56:15,985 --> 00:56:18,100
adds a little bit of complexity,

902
00:56:18,100 --> 00:56:21,145
but sort of for the simplest part for their encoder.

903
00:56:21,145 --> 00:56:26,065
Actually, all of the query keys and values are exactly the same.

904
00:56:26,065 --> 00:56:28,225
They are the words, um,

905
00:56:28,225 --> 00:56:32,620
that they're using as their source language, um, things.

906
00:56:32,620 --> 00:56:38,000
So, it sort of adds some complexity that isn't really there.

907
00:56:38,340 --> 00:56:42,280
Um, okay. Um, I'll skip that.

908
00:56:42,280 --> 00:56:48,175
Um, so, there are a couple of other things that they do.

909
00:56:48,175 --> 00:56:52,165
One thing that they note is that, um,

910
00:56:52,165 --> 00:56:57,745
the- the values you get from, um, QTK, um,

911
00:56:57,745 --> 00:57:03,280
very, in variances the dimension gets large

912
00:57:03,280 --> 00:57:08,230
so that they sort of do some normalization by the size of the hidden state dimension,

913
00:57:08,230 --> 00:57:12,280
but I'll leave that out as well for details, right.

914
00:57:12,280 --> 00:57:13,945
So in the encoder, um,

915
00:57:13,945 --> 00:57:17,020
everything is just our word vectors,

916
00:57:17,020 --> 00:57:20,380
there are the queries, the keys, and the values.

917
00:57:20,380 --> 00:57:23,785
Um, and we're gonna use attention everywhere in the system.

918
00:57:23,785 --> 00:57:29,860
Oops. Okay. So the second new idea is, well,

919
00:57:29,860 --> 00:57:36,115
attention is great but maybe it's bad if you only have one attention distribution,

920
00:57:36,115 --> 00:57:39,190
because you're gonna only attend to things one way.

921
00:57:39,190 --> 00:57:42,415
Maybe for various users it would be great

922
00:57:42,415 --> 00:57:45,760
if you could attend from one position to various things.

923
00:57:45,760 --> 00:57:51,190
So, if you're thinking about syntax and what we did with dependency parsers.

924
00:57:51,190 --> 00:57:54,970
If you're a word, you might want to attend to your headword,

925
00:57:54,970 --> 00:57:59,155
but you might also wanna attend- attend to your dependent words.

926
00:57:59,155 --> 00:58:01,689
And if you happen to be a pronoun,

927
00:58:01,689 --> 00:58:06,010
you might want to attend to what the pronoun refers to you.

928
00:58:06,010 --> 00:58:07,855
You might want to have lots of attention.

929
00:58:07,855 --> 00:58:12,010
So they introduced this idea of multi-head attention.

930
00:58:12,010 --> 00:58:16,360
And so what you're doing with multi-head attention is you have,

931
00:58:16,360 --> 00:58:18,130
um, your hidden states,

932
00:58:18,130 --> 00:58:20,170
um, in your system,

933
00:58:20,170 --> 00:58:23,800
and you map them via projection layers, um,

934
00:58:23,800 --> 00:58:27,670
which are just multiplications by different W matrices as

935
00:58:27,670 --> 00:58:32,350
linear projections into sort of different lower dimensional spaces,

936
00:58:32,350 --> 00:58:37,030
and then you use each of those to calculate dot-product attention,

937
00:58:37,030 --> 00:58:40,270
and so you can attend to different things at the same time.

938
00:58:40,270 --> 00:58:42,670
And this multi-head attention was one of

939
00:58:42,670 --> 00:58:48,655
the very successful ideas of transformers that made them a more powerful architecture.

940
00:58:48,655 --> 00:58:54,715
Okay. Um, so, then for our complete transformer block,

941
00:58:54,715 --> 00:59:00,505
it's sort of then starting to build complex architectures like we sort of started seeing,

942
00:59:00,505 --> 00:59:02,200
um, the other week.

943
00:59:02,200 --> 00:59:05,320
Um, so- okay.

944
00:59:05,320 --> 00:59:06,969
Yeah. So, starting,

945
00:59:06,969 --> 00:59:10,060
um, from our word vectors,

946
00:59:10,060 --> 00:59:16,915
we're kind of going to do attention to multiple different things,

947
00:59:16,915 --> 00:59:19,900
um, and we're simultaneously gonna have

948
00:59:19,900 --> 00:59:23,530
a residual connection that short-circuits around them.

949
00:59:23,530 --> 00:59:28,045
Um, we're then going to sort of sum the two of these,

950
00:59:28,045 --> 00:59:33,115
and then they're going to do a normalization at that point.

951
00:59:33,115 --> 00:59:36,400
Um, I talked previously about batch normalization,

952
00:59:36,400 --> 00:59:38,020
they don't do batch normalization,

953
00:59:38,020 --> 00:59:41,200
they do another variant which is layer normalization,

954
00:59:41,200 --> 00:59:43,855
which is a different way of doing normalization,

955
00:59:43,855 --> 00:59:45,625
but I'll skip that for now.

956
00:59:45,625 --> 00:59:49,000
And then they sort of for one transformer block,

957
00:59:49,000 --> 00:59:52,045
you then go after the multi-head attention,

958
00:59:52,045 --> 00:59:56,755
you put things through a feed-forward layer which also has a residual connection,

959
00:59:56,755 --> 00:59:58,810
you sum the output of those,

960
00:59:58,810 --> 01:00:03,790
and you then again do another, um, layer normalization.

961
01:00:03,790 --> 01:00:08,965
So this is the basic transformer block that they're gonna use everywhere.

962
01:00:08,965 --> 01:00:11,320
And to make their complete architectures,

963
01:00:11,320 --> 01:00:13,210
they're then gonna sort of start stacking

964
01:00:13,210 --> 01:00:17,050
these transformer blocks to produce a very deep network.

965
01:00:17,050 --> 01:00:18,160
And in some sense,

966
01:00:18,160 --> 01:00:22,780
what has been found is that transformers performed very well.

967
01:00:22,780 --> 01:00:25,000
But, you know, there's no free lunch,

968
01:00:25,000 --> 01:00:26,440
um, you kind of can't.

969
01:00:26,440 --> 01:00:28,150
You're- now, no longer getting

970
01:00:28,150 --> 01:00:31,450
recurrent information actually being carried along a sequence.

971
01:00:31,450 --> 01:00:36,280
You've got a word at some position which can be casting attention,

972
01:00:36,280 --> 01:00:38,035
uh, on other words.

973
01:00:38,035 --> 01:00:41,560
So if you'd like to have information carried along in a chain,

974
01:00:41,560 --> 01:00:44,980
you've sort of first of all gotta walk the first step of the chain,

975
01:00:44,980 --> 01:00:46,690
and then you need to have another layer

976
01:00:46,690 --> 01:00:49,690
vertically which can walk the next step of the chain,

977
01:00:49,690 --> 01:00:53,800
and then you need to have another layer vertically that walks the next step of the chain.

978
01:00:53,800 --> 01:00:57,520
So, you're getting rid of the recurrence along the sequence,

979
01:00:57,520 --> 01:01:03,220
but you're substituting some depth to allow things to walk along multiple hops.

980
01:01:03,220 --> 01:01:07,885
But nevertheless, that's highly advantageous in GPU architectures

981
01:01:07,885 --> 01:01:13,300
because it allows you to use parallelization to calculate everything at each,

982
01:01:13,300 --> 01:01:16,400
um, depth at the same time. Um.

983
01:01:19,290 --> 01:01:22,900
Maybe I'll go light on explaining this as well.

984
01:01:22,900 --> 01:01:25,420
Um, so they use byte-pair encodings.

985
01:01:25,420 --> 01:01:27,490
But if you do nothing else,

986
01:01:27,490 --> 01:01:30,850
you just have words fed in this word vectors and you have

987
01:01:30,850 --> 01:01:34,765
no idea whether you're at the beginning of the sentence or at the end of the sentence.

988
01:01:34,765 --> 01:01:38,680
Though, they have a message of- method of doing positional encoding which gives

989
01:01:38,680 --> 01:01:42,865
you some ideas to pro- position your word has in the sentence.

990
01:01:42,865 --> 01:01:47,950
Okay. Um, so that's sort of the, um, encoder system.

991
01:01:47,950 --> 01:01:49,540
So from the words,

992
01:01:49,540 --> 01:01:51,550
they have an initial word embedding,

993
01:01:51,550 --> 01:01:54,085
you add in their positional encoding,

994
01:01:54,085 --> 01:01:58,105
you go into one of these transformer blocks,

995
01:01:58,105 --> 01:02:01,030
and you then repeat it n times.

996
01:02:01,030 --> 01:02:03,835
So you'll have a stack of these transformer blocks.

997
01:02:03,835 --> 01:02:06,775
So you're multiple times doing, um,

998
01:02:06,775 --> 01:02:11,590
multi-head attention to other parts of the sentence, calculating values,

999
01:02:11,590 --> 01:02:12,940
feeding forward a value,

1000
01:02:12,940 --> 01:02:14,860
putting it through a fully-connected layer,

1001
01:02:14,860 --> 01:02:19,735
and then you just sort of repeat, do attention to different places in the sentence.

1002
01:02:19,735 --> 01:02:21,310
Get all your information,

1003
01:02:21,310 --> 01:02:23,275
put it through a fully connected layer,

1004
01:02:23,275 --> 01:02:26,755
and go up, um, proceeding up deeply.

1005
01:02:26,755 --> 01:02:31,000
And and that sounds a little mysterious,

1006
01:02:31,000 --> 01:02:34,215
but it turns out to work just great.

1007
01:02:34,215 --> 01:02:36,600
And the way to think about,

1008
01:02:36,600 --> 01:02:39,900
I think is that at each stage,

1009
01:02:39,900 --> 01:02:44,760
you can look with your multi-headed attention and various other places in the sentence,

1010
01:02:44,760 --> 01:02:48,210
accumulate information, push it up to the next layer.

1011
01:02:48,210 --> 01:02:51,255
And if you do that sort of half a dozen times,

1012
01:02:51,255 --> 01:02:55,530
you can be starting to progressively push information along

1013
01:02:55,530 --> 01:03:01,455
the sequence in either direction to calculate values that are of interest.

1014
01:03:01,455 --> 01:03:08,605
Um, and the interesting thing is that these models turn out to work

1015
01:03:08,605 --> 01:03:15,970
really well at sort of learning to attend the interesting things in linguistic structure.

1016
01:03:15,970 --> 01:03:19,810
Um, so these are just sort of suggestive diagrams,

1017
01:03:19,810 --> 01:03:24,190
but this is looking at layer five of the transformer stack and

1018
01:03:24,190 --> 01:03:28,945
seeing what words are being attended to by different attention heads.

1019
01:03:28,945 --> 01:03:33,010
So these different colors correspond to different attention heads.

1020
01:03:33,010 --> 01:03:35,050
And so the sentence is,

1021
01:03:35,050 --> 01:03:39,010
um, it is, "In this spirit,

1022
01:03:39,010 --> 01:03:42,310
that a majority of American governments have passed new laws since

1023
01:03:42,310 --> 01:03:47,064
2009 making the registration or voting process more difficult."

1024
01:03:47,064 --> 01:03:53,275
And so what we see is sort of most of the attention heads,

1025
01:03:53,275 --> 01:03:58,840
uh, looking from making to making more difficult and that seems to be useful.

1026
01:03:58,840 --> 01:04:03,700
One of the attention heads seems to be looking at the word itself might be okay.

1027
01:04:03,700 --> 01:04:10,570
Um, then the other ones are sort of looking a bit at laws and at 2009.

1028
01:04:10,570 --> 01:04:14,530
So it's sort of picking out the arguments, um,

1029
01:04:14,530 --> 01:04:18,910
and modifiers and making in a syntax kind of like way.

1030
01:04:18,910 --> 01:04:21,880
Um, interestingly, for pronouns,

1031
01:04:21,880 --> 01:04:26,770
attention heads appear to learn to be able to look back to reference.

1032
01:04:26,770 --> 01:04:28,795
So the law will never be perfect,

1033
01:04:28,795 --> 01:04:35,185
but its application should be just that one attention head it for its,

1034
01:04:35,185 --> 01:04:39,055
is looking at what its is modifying in the application.

1035
01:04:39,055 --> 01:04:40,930
But another attention head,

1036
01:04:40,930 --> 01:04:45,640
the its is looking strongly at what its refers back to as the law.

1037
01:04:45,640 --> 01:04:47,740
So that seems kind of cool.

1038
01:04:47,740 --> 01:04:49,810
Um, yeah.

1039
01:04:49,810 --> 01:04:52,870
Um, okay.

1040
01:04:52,870 --> 01:04:56,035
And so then, for the rest of the model, um,

1041
01:04:56,035 --> 01:04:58,990
there's then some more complexity for how to use

1042
01:04:58,990 --> 01:05:05,020
the transformers decoder to give you a full neural machine translation system.

1043
01:05:05,020 --> 01:05:08,770
But I think maybe I will skip that and go

1044
01:05:08,770 --> 01:05:13,750
on and say a bit about BERT in my remaining minutes.

1045
01:05:13,750 --> 01:05:18,490
Okay. So, um, the latest and greatest contextual

1046
01:05:18,490 --> 01:05:23,590
word representations to help you flow your tasks have been these BERT vectors,

1047
01:05:23,590 --> 01:05:29,965
where BERT is Bidirectional Encoder Representations from Transformers.

1048
01:05:29,965 --> 01:05:35,095
And so essentially, it's using the encoder from a transformer network.

1049
01:05:35,095 --> 01:05:40,195
Uh, this deep multi-headed attention stack to calculate, um,

1050
01:05:40,195 --> 01:05:43,615
a representation of a sentence and saying,

1051
01:05:43,615 --> 01:05:49,750
"That's a great all-purpose representation of a sentence that you can use for tasks.

1052
01:05:49,750 --> 01:05:54,054
Be it named entity recognition or SQuAD question answering."

1053
01:05:54,054 --> 01:05:59,320
And so there's actually an interesting new idea that these people had.

1054
01:05:59,320 --> 01:06:04,990
And that well, their idea was well standard language models are

1055
01:06:04,990 --> 01:06:08,230
unidirectional and that's useful

1056
01:06:08,230 --> 01:06:11,755
because it gives you a probability distribution of a language model.

1057
01:06:11,755 --> 01:06:16,210
But it's bad because you'd like to be able to do

1058
01:06:16,210 --> 01:06:21,190
prediction from both sides to understand word meaning and context.

1059
01:06:21,190 --> 01:06:23,725
There's a second choice, um,

1060
01:06:23,725 --> 01:06:29,185
which is you can kind of do bidirectional models when you incorporate,

1061
01:06:29,185 --> 01:06:31,705
um, information in both ways.

1062
01:06:31,705 --> 01:06:35,050
But that sort of has problems as well,

1063
01:06:35,050 --> 01:06:37,480
because then you get crosstalk.

1064
01:06:37,480 --> 01:06:40,615
Um, and so if you run a BiLSTM,

1065
01:06:40,615 --> 01:06:43,090
and then you merge the representations by

1066
01:06:43,090 --> 01:06:46,765
concatenation and then feed them into the next layer.

1067
01:06:46,765 --> 01:06:48,655
When you're running the next layer,

1068
01:06:48,655 --> 01:06:51,430
the forward LSTM will have already gotten

1069
01:06:51,430 --> 01:06:54,385
information about the future from the first layer.

1070
01:06:54,385 --> 01:06:56,545
Um, so it sort of, um,

1071
01:06:56,545 --> 01:07:00,490
ends up with words that have already seen the future themselves.

1072
01:07:00,490 --> 01:07:03,685
So you have this sort of complex non-generative model.

1073
01:07:03,685 --> 01:07:08,005
Um, so somehow, they wanted to do things a bit differently,

1074
01:07:08,005 --> 01:07:13,600
so they can have bidirectional context without words being able to see themselves.

1075
01:07:13,600 --> 01:07:16,915
And the idea that they came up with is well,

1076
01:07:16,915 --> 01:07:21,430
we're gonna train things with a transformer encoder.

1077
01:07:21,430 --> 01:07:26,515
But what we're gonna do is mask out some of the words in the sentence,

1078
01:07:26,515 --> 01:07:30,160
like, maybe we'll mask here store and gallon.

1079
01:07:30,160 --> 01:07:34,180
And then, so our language mod- our language modelling like

1080
01:07:34,180 --> 01:07:36,130
objective will no longer be

1081
01:07:36,130 --> 01:07:40,090
a true language model that's sort of generating a probability of a sentence,

1082
01:07:40,090 --> 01:07:43,705
um, which is standardly done by working from left to right,

1083
01:07:43,705 --> 01:07:49,390
but it will instead be a Mad Libs style fill in the blank objective.

1084
01:07:49,390 --> 01:07:52,120
So you'll see this context,

1085
01:07:52,120 --> 01:07:53,800
which will be literally,

1086
01:07:53,800 --> 01:07:56,965
"The man went to the mask to buy a mask of milk."

1087
01:07:56,965 --> 01:08:00,790
And your, what's your training objective is to say,

1088
01:08:00,790 --> 01:08:03,430
try and predict what this word is,

1089
01:08:03,430 --> 01:08:08,035
which you can do with a cross entropy loss to the extent that you don't guess store.

1090
01:08:08,035 --> 01:08:12,880
And then, it will be trying to guess what this word is and you want to let guess gallon.

1091
01:08:12,880 --> 01:08:14,995
So you're training a model,

1092
01:08:14,995 --> 01:08:17,920
um, to fill in these blanks.

1093
01:08:17,920 --> 01:08:22,840
Um, and the rate at which they blank words is essentially one word in seven,

1094
01:08:22,840 --> 01:08:25,225
and they discuss how this is a trade-off.

1095
01:08:25,225 --> 01:08:28,540
Because if you blank too few words,

1096
01:08:28,540 --> 01:08:30,700
it gets very expensive to train.

1097
01:08:30,700 --> 01:08:32,590
And if you blank many words,

1098
01:08:32,590 --> 01:08:35,545
well you've blanked out most of the context of a word,

1099
01:08:35,545 --> 01:08:38,064
and that means it's not very useful for training,

1100
01:08:38,064 --> 01:08:42,325
and they found about sort of one in seven seemed to work pretty well for them.

1101
01:08:42,325 --> 01:08:46,585
But what they want to argue is, um,

1102
01:08:46,585 --> 01:08:51,220
that for the OpenAI's GPT,

1103
01:08:51,220 --> 01:08:53,470
which is also a transformer model.

1104
01:08:53,470 --> 01:08:56,845
It's a sort of a classic language model working from

1105
01:08:56,845 --> 01:09:00,700
left to right and so you only get left context.

1106
01:09:00,700 --> 01:09:03,805
Um, for the BERT language model,

1107
01:09:03,805 --> 01:09:07,285
sorry, the ELMo language model that's shown up at the top.

1108
01:09:07,285 --> 01:09:11,680
Um, well, they're running a left to right language model and they're running,

1109
01:09:11,680 --> 01:09:13,990
um, right to left language models.

1110
01:09:13,990 --> 01:09:16,030
So in some sense, um,

1111
01:09:16,030 --> 01:09:18,295
they have context from both sides.

1112
01:09:18,295 --> 01:09:22,690
But these two language models are trained completely independently

1113
01:09:22,690 --> 01:09:27,265
and then you're just sort of concatenating their representations, um, together.

1114
01:09:27,265 --> 01:09:32,170
So there's no sense in which we're actually kind of having a model that's jointly

1115
01:09:32,170 --> 01:09:37,930
using context from both sides at the time though that the pre-trained,

1116
01:09:37,930 --> 01:09:40,930
um, contextual word representations are built.

1117
01:09:40,930 --> 01:09:45,940
So their hope is using inside a transformer model

1118
01:09:45,940 --> 01:09:47,980
this trick of blanking out words,

1119
01:09:47,980 --> 01:09:53,290
and predicting it using the entire context will allow them to use two-sided context,

1120
01:09:53,290 --> 01:09:55,540
and be much more effective.

1121
01:09:55,540 --> 01:10:00,025
And that's what they seem to show, um.

1122
01:10:00,025 --> 01:10:03,835
There's one other complication and,

1123
01:10:03,835 --> 01:10:05,485
I mean, I'll show later.

1124
01:10:05,485 --> 01:10:09,835
Um, this last complication is a bit useful,

1125
01:10:09,835 --> 01:10:12,999
but it's sort of not really essential to their main idea,

1126
01:10:12,999 --> 01:10:14,845
was that they thought,

1127
01:10:14,845 --> 01:10:18,550
one of the, one of the goals in their head was clearly to be able to

1128
01:10:18,550 --> 01:10:22,660
have this be useful for things like question answering,

1129
01:10:22,660 --> 01:10:25,080
um, tasks, or, um,

1130
01:10:25,080 --> 01:10:26,770
natural language inference tasks,

1131
01:10:26,770 --> 01:10:30,640
and their relationships between, um, two sentences.

1132
01:10:30,640 --> 01:10:32,260
So, their idea was, well,

1133
01:10:32,260 --> 01:10:36,430
one good objective is this fill in the blank word objective which is,

1134
01:10:36,430 --> 01:10:39,085
sort of, like language modeling objective.

1135
01:10:39,085 --> 01:10:42,310
But they thought it would be useful to have a second objective

1136
01:10:42,310 --> 01:10:45,925
where you're predicting relationships between sentences.

1137
01:10:45,925 --> 01:10:51,415
So, they secondly have a loss function which is, um,

1138
01:10:51,415 --> 01:10:54,670
let's have two sentences where

1139
01:10:54,670 --> 01:10:58,359
the sentences might be two successive sentences in the text,

1140
01:10:58,359 --> 01:11:02,650
or a sentence followed by a random sentence from somewhere else.

1141
01:11:02,650 --> 01:11:06,475
And we want to train the system to predict when you've,

1142
01:11:06,475 --> 01:11:10,930
seeing an- a correct next sentence versus a random sentence.

1143
01:11:10,930 --> 01:11:16,330
And so you're also training a loss based on this next sentence prediction task.

1144
01:11:16,330 --> 01:11:19,660
And so it'll be something like: The man went to the store.

1145
01:11:19,660 --> 01:11:21,430
He bought a gallon of milk.

1146
01:11:21,430 --> 01:11:24,610
You're meant to predict true is the next sentence,

1147
01:11:24,610 --> 01:11:26,740
um: The man went to the store.

1148
01:11:26,740 --> 01:11:28,090
Penguins are flightless.

1149
01:11:28,090 --> 01:11:29,515
You're meant to say false.

1150
01:11:29,515 --> 01:11:31,285
This isn't the next sentence.

1151
01:11:31,285 --> 01:11:33,580
And so they're simultaneously also,

1152
01:11:33,580 --> 01:11:36,325
um, training with this representation.

1153
01:11:36,325 --> 01:11:40,345
So, what they end up looks, looks like this.

1154
01:11:40,345 --> 01:11:44,245
Um, so, they have,

1155
01:11:44,245 --> 01:11:45,490
um, for the input,

1156
01:11:45,490 --> 01:11:47,170
they'll have a pair of sentences.

1157
01:11:47,170 --> 01:11:48,700
My dog is cute.

1158
01:11:48,700 --> 01:11:50,095
Um, separator.

1159
01:11:50,095 --> 01:11:51,925
He likes playing.

1160
01:11:51,925 --> 01:11:57,955
Um, the words are represented as word pieces like we talked about last week.

1161
01:11:57,955 --> 01:12:01,570
Um, so there's a token embedding for each word piece.

1162
01:12:01,570 --> 01:12:05,350
Um, then there's a positional embedding for

1163
01:12:05,350 --> 01:12:09,535
each word piece which is gonna be summed with the token embedding.

1164
01:12:09,535 --> 01:12:14,470
And then finally, there's a segment embedding for each word piece which is simply

1165
01:12:14,470 --> 01:12:17,050
whether it comes from the first sentence or

1166
01:12:17,050 --> 01:12:19,915
the second sentence before or after the separator.

1167
01:12:19,915 --> 01:12:24,940
So, you're summing those three things together to get the token representations.

1168
01:12:24,940 --> 01:12:28,914
And then you're going to use those in a transformer model

1169
01:12:28,914 --> 01:12:33,835
where you will have losses to the extent that you can't predict the masked words.

1170
01:12:33,835 --> 01:12:38,410
And then your binary prediction function as to whether there's

1171
01:12:38,410 --> 01:12:43,525
a correct next sentence or not which is the training architecture.

1172
01:12:43,525 --> 01:12:47,485
Okay. So, it's a transformer as before,

1173
01:12:47,485 --> 01:12:50,740
it's trained on Wikipedia plus the BookCorpus.

1174
01:12:50,740 --> 01:12:52,720
And they built two models.

1175
01:12:52,720 --> 01:12:57,175
Um, the Base-BERT model was a twelve layer transformer.

1176
01:12:57,175 --> 01:13:02,470
And so this corresponded to what the previous transformer paper had used, right?

1177
01:13:02,470 --> 01:13:09,190
Those two layer transformer blocks repeated six times gave you 12 layers with 768 hidden,

1178
01:13:09,190 --> 01:13:14,665
um, dimension hidden states and 12 heads for the multi-head attention.

1179
01:13:14,665 --> 01:13:16,479
And then they went bigger,

1180
01:13:16,479 --> 01:13:18,610
um, and trained BERT-Large which is,

1181
01:13:18,610 --> 01:13:20,620
sort of, double the number of layers,

1182
01:13:20,620 --> 01:13:23,485
bigger hidden states, even more attention heads.

1183
01:13:23,485 --> 01:13:26,410
Um, and training these on,

1184
01:13:26,410 --> 01:13:29,185
um, pods of TPUs.

1185
01:13:29,185 --> 01:13:33,850
Um, so, first of all, you're training, um,

1186
01:13:33,850 --> 01:13:38,260
on this basis for masked words and,

1187
01:13:38,260 --> 01:13:40,375
um, next sentence or not.

1188
01:13:40,375 --> 01:13:45,940
Um, so then what they wanted to say was this pre-trained model,

1189
01:13:45,940 --> 01:13:51,685
um, evaluated on these losses and masked language model and next sentence prediction.

1190
01:13:51,685 --> 01:13:54,925
Um, we could then take this model,

1191
01:13:54,925 --> 01:13:59,050
fr- freeze most of its what weak. No, sorry, that's wrong.

1192
01:13:59,050 --> 01:14:01,270
We could take this model, um,

1193
01:14:01,270 --> 01:14:06,610
pre-trained and it would be incredibly useful for various different tasks.

1194
01:14:06,610 --> 01:14:08,800
We could use it for named entity recognition,

1195
01:14:08,800 --> 01:14:12,310
question answering, natural language inference et cetera.

1196
01:14:12,310 --> 01:14:14,890
And the way we're going to do it, is kind of,

1197
01:14:14,890 --> 01:14:18,550
doing the same thing as the ULMFit model did.

1198
01:14:18,550 --> 01:14:20,755
We're not just going to say here's our,

1199
01:14:20,755 --> 01:14:25,240
here's a contextual word representation like ELMo did.

1200
01:14:25,240 --> 01:14:29,560
Instead, what we're gonna say is just keep on using this,

1201
01:14:29,560 --> 01:14:32,230
keep on using this um,

1202
01:14:32,230 --> 01:14:36,880
transformer network that we trained as a, sort of,

1203
01:14:36,880 --> 01:14:42,535
language model, but fine tune it for a particular task.

1204
01:14:42,535 --> 01:14:45,190
So, you're now going to run this transformer

1205
01:14:45,190 --> 01:14:49,180
calculating representations for a particular task.

1206
01:14:49,180 --> 01:14:55,990
And what we're going to change is we're going to remove the very top-level prediction.

1207
01:14:55,990 --> 01:15:00,415
The bits that predict the mass language model and next sentence prediction.

1208
01:15:00,415 --> 01:15:02,770
And we're going to substitute on it,

1209
01:15:02,770 --> 01:15:08,080
on top, um, a final prediction layer that's appropriate for the task.

1210
01:15:08,080 --> 01:15:11,005
So, if our task is SQuAD question answering,

1211
01:15:11,005 --> 01:15:16,344
our final prediction layer will be predicting start of span and end of span,

1212
01:15:16,344 --> 01:15:20,740
kind of, like when we saw DrQA a couple of weeks ago.

1213
01:15:20,740 --> 01:15:23,979
If what we're doing is the NER task,

1214
01:15:23,979 --> 01:15:26,889
our final prediction layer will be predicting

1215
01:15:26,889 --> 01:15:33,895
the net- named entity recognition class of each token just like a standard NER system.

1216
01:15:33,895 --> 01:15:42,775
Okay, um, and so they built this system and tested it on a whole bunch of data sets.

1217
01:15:42,775 --> 01:15:45,610
Um, one of the main things they tested on was

1218
01:15:45,610 --> 01:15:48,625
this GLUE data set which has a whole bunch of tasks.

1219
01:15:48,625 --> 01:15:50,170
A lot of the tasks, they're,

1220
01:15:50,170 --> 01:15:53,530
uh, natural language inference tasks.

1221
01:15:53,530 --> 01:15:57,205
And I've kept saying that phrase all of this lecture but I haven't really defined it.

1222
01:15:57,205 --> 01:16:00,820
So, with a natural language inference you're given two sentences

1223
01:16:00,820 --> 01:16:05,935
like: Hills and mountains are especially sanctified in Jainism.

1224
01:16:05,935 --> 01:16:09,550
And then you can write a hypothesis on: Jainism hates nature.

1225
01:16:09,550 --> 01:16:11,530
And what you're meant to say is,

1226
01:16:11,530 --> 01:16:13,570
whether the hypothesis, um,

1227
01:16:13,570 --> 01:16:15,505
follows from the premise,

1228
01:16:15,505 --> 01:16:19,240
contradicts the premise, or has no relation to the premise.

1229
01:16:19,240 --> 01:16:21,265
So, that's a three-way classification.

1230
01:16:21,265 --> 01:16:23,845
And so here it contradicts the premise.

1231
01:16:23,845 --> 01:16:30,115
Um, there are various other tasks such as this linguistic acceptability task.

1232
01:16:30,115 --> 01:16:33,550
Um, but if we look at these, um, GLUE tasks.

1233
01:16:33,550 --> 01:16:37,735
Um, these are showing the Pre-OpenAI State Of The Art.

1234
01:16:37,735 --> 01:16:40,735
How well, um, ELMo works.

1235
01:16:40,735 --> 01:16:43,900
How well OpenAI GPT works,

1236
01:16:43,900 --> 01:16:48,415
and then how well do small and large BERT models work.

1237
01:16:48,415 --> 01:16:53,290
And effectively, what you're finding is,

1238
01:16:53,290 --> 01:16:57,370
um, that the OpenAI GPT was so,

1239
01:16:57,370 --> 01:16:58,495
you know, pretty good.

1240
01:16:58,495 --> 01:17:02,455
It showed actually good advances on most of these tasks.

1241
01:17:02,455 --> 01:17:05,890
For many, but not all of them that broke the previous state of the art,

1242
01:17:05,890 --> 01:17:08,995
showing the power of these contextual language models.

1243
01:17:08,995 --> 01:17:15,205
But the bidirectional form of BERT's prediction just seemed much better again.

1244
01:17:15,205 --> 01:17:19,180
So, going from this line to this line you're getting depending on

1245
01:17:19,180 --> 01:17:23,185
the task about two percent better performance.

1246
01:17:23,185 --> 01:17:27,010
And so the BERT people actually did their experiments carefully.

1247
01:17:27,010 --> 01:17:30,430
So, these models are pretty comparable in terms of size,

1248
01:17:30,430 --> 01:17:33,775
but the bidirectional context seems to really help.

1249
01:17:33,775 --> 01:17:35,470
And then what they found was,

1250
01:17:35,470 --> 01:17:37,570
well, by going to just a bigger model,

1251
01:17:37,570 --> 01:17:41,545
again, you could get another big lift in performance.

1252
01:17:41,545 --> 01:17:44,740
And so you're getting for many of the tasks about

1253
01:17:44,740 --> 01:17:48,145
another two percent lift in performance going into the bigger model.

1254
01:17:48,145 --> 01:17:51,010
So, this really produced super-strong results.

1255
01:17:51,010 --> 01:17:54,085
And in general, um, people have found,

1256
01:17:54,085 --> 01:17:57,400
um, that BERT continues to give super strong results.

1257
01:17:57,400 --> 01:18:01,480
So, if I return back to my ConLL NER task,

1258
01:18:01,480 --> 01:18:05,260
we had ELMo giving you 92,2,

1259
01:18:05,260 --> 01:18:06,640
um, and you, sort of,

1260
01:18:06,640 --> 01:18:08,050
continue to get gains.

1261
01:18:08,050 --> 01:18:13,900
So, BERT Base gets you to 92,4 and BERT Large takes you to 92,8.

1262
01:18:13,900 --> 01:18:17,650
Though in, um, truth in, truth in description,

1263
01:18:17,650 --> 01:18:23,125
there is now a system of beats BERT Large on NER which is actually a character-level,

1264
01:18:23,125 --> 01:18:25,990
um, transformer language model from Flair.

1265
01:18:25,990 --> 01:18:27,835
Um, but, you know,

1266
01:18:27,835 --> 01:18:30,790
this continued over to a lot of other things.

1267
01:18:30,790 --> 01:18:33,865
So, on SQuAD 1,1, um,

1268
01:18:33,865 --> 01:18:36,370
BERT immediately just outperformed

1269
01:18:36,370 --> 01:18:39,745
everything else that people have been working on for SQuAD for ages.

1270
01:18:39,745 --> 01:18:42,610
In particular, what was especially dramatic, um,

1271
01:18:42,610 --> 01:18:45,985
was the sing- a single BERT model, um,

1272
01:18:45,985 --> 01:18:50,770
beat everything else that had been done previously on SQuAD version 1,1,

1273
01:18:50,770 --> 01:18:53,575
even though they could also show that an

1274
01:18:53,575 --> 01:18:59,815
ensemble of BERT models could give further good, um, performance gains.

1275
01:18:59,815 --> 01:19:03,055
Um, and as I've mentioned before,

1276
01:19:03,055 --> 01:19:05,980
essentially if you look at the SQuAD 2,0, um,

1277
01:19:05,980 --> 01:19:08,935
leaderboard, all of the top ranked systems,

1278
01:19:08,935 --> 01:19:12,280
um, are using BERT one place or another.

1279
01:19:12,280 --> 01:19:14,590
Um, and so that,

1280
01:19:14,590 --> 01:19:16,060
sort of, led into this,

1281
01:19:16,060 --> 01:19:19,570
sort of, new world order, um, that, okay,

1282
01:19:19,570 --> 01:19:22,735
it seems like the state of NLP now is to,

1283
01:19:22,735 --> 01:19:25,240
if you want to have the best performance,

1284
01:19:25,240 --> 01:19:26,410
you want to be using

1285
01:19:26,410 --> 01:19:31,855
these deep pre-trained transformer stacks to get the best performance.

1286
01:19:31,855 --> 01:19:33,220
And so this is, sort of, making,

1287
01:19:33,220 --> 01:19:35,410
um, NLP more like vision.

1288
01:19:35,410 --> 01:19:38,560
Because really vision for five years has had

1289
01:19:38,560 --> 01:19:42,730
these deep pre-trained neural network stacks, um, like ResNets.

1290
01:19:42,730 --> 01:19:47,124
Where for most vision tasks what you do is you take a pre-trained ResNet,

1291
01:19:47,124 --> 01:19:49,870
and then you fine tune a layer at the top to

1292
01:19:49,870 --> 01:19:52,870
do some classification tasks you're interested in.

1293
01:19:52,870 --> 01:19:54,970
And this is, sort of, now, um,

1294
01:19:54,970 --> 01:19:57,520
starting to be what's happening in NLP as well.

1295
01:19:57,520 --> 01:20:00,280
That you can do the same thing by downloading

1296
01:20:00,280 --> 01:20:05,875
your pre-trained BERT and fine tuning it to do some particular performance task.

1297
01:20:05,875 --> 01:20:09,400
Okay, um, that's it for today and more on

1298
01:20:09,400 --> 01:20:18,330
transformers on Thursday [NOISE].

