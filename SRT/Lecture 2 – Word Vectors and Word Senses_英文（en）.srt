1
00:00:04,160 --> 00:00:07,125
Okay. Hello everyone.

2
00:00:07,125 --> 00:00:12,915
Um, welcome back to the second class of, um, CS224N.

3
00:00:12,915 --> 00:00:17,310
Okay, so right at the end of last time I was just showing you a little,

4
00:00:17,310 --> 00:00:18,975
um, from this, um,

5
00:00:18,975 --> 00:00:22,140
IPython Notebook of things that you could do with word vectors

6
00:00:22,140 --> 00:00:25,380
but I kind of ran out of time a little for a bit.

7
00:00:25,380 --> 00:00:28,050
So, I'll just spend a couple of more minutes first,

8
00:00:28,050 --> 00:00:29,850
um, showing the end of  this.

9
00:00:29,850 --> 00:00:33,360
I stuck this IPython Notebook up on the course page.

10
00:00:33,360 --> 00:00:37,595
So, under lecture one you can find a copy of it and you can download it.

11
00:00:37,595 --> 00:00:41,840
So, I both stuck up just an HTML version of it and a zip file.

12
00:00:41,840 --> 00:00:44,240
Like HTML file is only good to look at.

13
00:00:44,240 --> 00:00:45,470
You can't do anything with it.

14
00:00:45,470 --> 00:00:48,275
So, you wanna, if you wanna play with it by yourself, um,

15
00:00:48,275 --> 00:00:52,015
download the zip file and get the IPython Notebook out of that.

16
00:00:52,015 --> 00:00:53,390
Okay. So we were looking at

17
00:00:53,390 --> 00:00:57,965
these Glove word vectors which I'll talk about a bit more today and so there were

18
00:00:57,965 --> 00:01:04,930
these sort of basic results of similarity in this vector space work very nicely for,

19
00:01:04,930 --> 00:01:10,550
um, discovering similar words and then going on from that,

20
00:01:10,550 --> 00:01:15,830
there was this idea that we'll spend some more time on today which was, um,

21
00:01:15,830 --> 00:01:20,160
maybe this vector space is not only a similarity space where

22
00:01:20,160 --> 00:01:25,760
close together things have similar meaning but it actually captures meaning

23
00:01:25,760 --> 00:01:30,800
in a considerably deeper and more profound way which is to say that there are

24
00:01:30,800 --> 00:01:37,280
actually directions in the space that you can point which have a certain meaning.

25
00:01:37,280 --> 00:01:43,970
So, that if you are pointing in one direction it means this is more so the case,

26
00:01:43,970 --> 00:01:48,170
if you are pointing in a different direction and the meaning space it might be this is

27
00:01:48,170 --> 00:01:50,780
the capital of this country or

28
00:01:50,780 --> 00:01:54,205
all sorts of different meanings could be encoded in the space.

29
00:01:54,205 --> 00:01:56,655
And a way of testing that,

30
00:01:56,655 --> 00:01:59,680
is to use these analogy, um, problems.

31
00:01:59,680 --> 00:02:03,080
And I quickly showed this at the end but just to make sure if you're

32
00:02:03,080 --> 00:02:06,890
unguarded since it's sort of- it's sort of a clever thing right?

33
00:02:06,890 --> 00:02:14,600
So, the idea is that we're going to start with a pair of words like king and man.

34
00:02:14,600 --> 00:02:17,150
And so what we're gonna do is we're gonna say well,

35
00:02:17,150 --> 00:02:23,150
there's a vector for king in the space and there's a vector for man in

36
00:02:23,150 --> 00:02:26,840
the space and but what we're gonna do is we're going to

37
00:02:26,840 --> 00:02:32,000
subtract as in just good old vector subtraction that you hopefully learned in your,

38
00:02:32,000 --> 00:02:33,750
um, linear algebra class.

39
00:02:33,750 --> 00:02:36,020
We're gonna subtract the man vector from

40
00:02:36,020 --> 00:02:40,490
the king vector and the idea we have in our head then is if we do

41
00:02:40,490 --> 00:02:47,965
that what will happen is we'll be left with the meaning of kingship without the manness.

42
00:02:47,965 --> 00:02:54,270
Um, and so then there's also a direct vector for a woman.

43
00:02:54,270 --> 00:02:59,810
So, we can add the woman vector to that resulting vector and then we could say well,

44
00:02:59,810 --> 00:03:04,220
in the vector, we end up at some point in the vector space and then we're gonna say well,

45
00:03:04,220 --> 00:03:07,340
what's the closest word that you're gonna find the here

46
00:03:07,340 --> 00:03:11,960
and it's gonna print out the closest word and as we saw,

47
00:03:11,960 --> 00:03:14,765
um, last time, um,

48
00:03:14,765 --> 00:03:17,660
lo and behold if you do that,

49
00:03:17,660 --> 00:03:20,565
um, you get the answer.

50
00:03:20,565 --> 00:03:22,260
I'm saying you get,

51
00:03:22,260 --> 00:03:32,440
um, king, man, woman.

52
00:03:33,530 --> 00:03:36,420
No? All right. [LAUGHTER].

53
00:03:36,420 --> 00:03:39,195
You gotta reverse king and man.

54
00:03:39,195 --> 00:03:40,860
I have to reverse king and,

55
00:03:40,860 --> 00:03:42,900
ah, sure, sure, sure. I'm sorry.

56
00:03:42,900 --> 00:03:49,470
Oops. Yeah, okay, I kinda do it well like man, king.

57
00:03:49,470 --> 00:03:55,260
Ah, [LAUGHTER] Okay. Yeah, that's right.

58
00:03:55,260 --> 00:03:57,480
Sorry. Okay. Yeah, because it should be

59
00:03:57,480 --> 00:04:00,385
man is to king as woman is to something sorry yeah.

60
00:04:00,385 --> 00:04:03,680
I was getting [LAUGHTER] my order of components wrong.

61
00:04:03,680 --> 00:04:06,260
Okay. Um, and, you know,

62
00:04:06,260 --> 00:04:10,790
as I was sort of I guess I was showing some examples last time with

63
00:04:10,790 --> 00:04:18,620
nationality words but I mean this in a way that is sort of surprising to shocking,

64
00:04:18,620 --> 00:04:23,870
this actually works for all kinds of things that you can get meaning in this space.

65
00:04:23,870 --> 00:04:29,030
So, I can ask various kinds of analogies of sorts.

66
00:04:29,030 --> 00:04:33,380
So I can say Australia is to beer as France is to-.

67
00:04:33,380 --> 00:04:34,280
Wine.

68
00:04:34,280 --> 00:04:35,720
Wine. You might think wine.

69
00:04:35,720 --> 00:04:38,600
What it gives back as champagne which seems a pretty good answer.

70
00:04:38,600 --> 00:04:40,535
[LAUGHTER] Um, I'll go with that.

71
00:04:40,535 --> 00:04:44,490
Um, um, you can do more syntactic facts.

72
00:04:44,490 --> 00:04:51,000
So, I can say tall ta- tall is to tallest as long is to longest and it gets set.

73
00:04:51,000 --> 00:04:56,745
Um, if I say good is to fantastic as bad is to terrible.

74
00:04:56,745 --> 00:05:00,440
That it seems to get out that there's some kind of notion of

75
00:05:00,440 --> 00:05:04,880
make more extreme direction and get this direction out.

76
00:05:04,880 --> 00:05:06,275
I skipped over one.

77
00:05:06,275 --> 00:05:10,565
A bomber is to Clinton as Reagan is to.

78
00:05:10,565 --> 00:05:13,675
You may or may not like the answer it gives for this one

79
00:05:13,675 --> 00:05:17,615
as Obama is to- as Reagan is to Nixon.

80
00:05:17,615 --> 00:05:20,420
Um, now one thing you might notice at

81
00:05:20,420 --> 00:05:24,350
this point and this is something I actually want to come back to at the end.

82
00:05:24,350 --> 00:05:28,160
Um, well, there's this problem because Clinton's ambiguous, right?

83
00:05:28,160 --> 00:05:29,840
There's Bill or there's Hillary.

84
00:05:29,840 --> 00:05:32,925
Um, and, um, I forget,

85
00:05:32,925 --> 00:05:36,860
you know, so this data as I said is a few years old.

86
00:05:36,860 --> 00:05:39,320
So, this data was done in 2014.

87
00:05:39,320 --> 00:05:42,230
So, in sort of in- it definitely doesn't

88
00:05:42,230 --> 00:05:45,170
have Trump really in it as a politician, um, but, you know,

89
00:05:45,170 --> 00:05:50,420
it would have variously both Clintons but as sort of makes sense if probably um,

90
00:05:50,420 --> 00:05:53,565
for a sort of proof for 2014 data,

91
00:05:53,565 --> 00:05:55,575
um, that Bill Clinton dominated.

92
00:05:55,575 --> 00:05:57,510
So, I think what we're getting, um,

93
00:05:57,510 --> 00:06:03,330
out of this is that Clinton and Nixon are sort of similar of people in dangers,

94
00:06:03,330 --> 00:06:05,790
um, of being impeached.

95
00:06:05,790 --> 00:06:11,695
Um, and, uh, on both sides of the aisle had us thinking primarily of Bill Clinton.

96
00:06:11,695 --> 00:06:14,500
But, um, if this sort of brings up something that

97
00:06:14,500 --> 00:06:17,380
I'll come back to right at the end of, um,

98
00:06:17,380 --> 00:06:21,370
it sort of looks like we've got a sort of a problem here because we

99
00:06:21,370 --> 00:06:25,690
just have this string literally Clinton and that, um,

100
00:06:25,690 --> 00:06:35,275
string is any possible sense and meaning of the string Clinton and so minimally um,

101
00:06:35,275 --> 00:06:38,800
that we have Bill Clinton and Hillary Clinton that near.

102
00:06:38,800 --> 00:06:41,240
Maybe you have some friends that are called Clinton as well, right,

103
00:06:41,240 --> 00:06:43,920
and they're all mixed together in this Clinton.

104
00:06:43,920 --> 00:06:47,200
And so that seems kinda problematic and that's sort of been an issue

105
00:06:47,200 --> 00:06:50,905
that's been discussed some for these word vectors and I'll come back to that.

106
00:06:50,905 --> 00:06:53,780
Um, another thing you can do is you can give

107
00:06:53,780 --> 00:06:56,705
a set of words and say which is the odd one out.

108
00:06:56,705 --> 00:07:00,020
Maybe you used to do puzzles like that in middle school or something.

109
00:07:00,020 --> 00:07:02,630
Um, and so you can do that and it decides

110
00:07:02,630 --> 00:07:06,080
that cereal is the odd one out of that set. It seems okay.

111
00:07:06,080 --> 00:07:09,950
Um, and then one other thing I'll just show you is, so, um,

112
00:07:09,950 --> 00:07:13,250
it'll sort of be nice to look at these words that I've drawn

113
00:07:13,250 --> 00:07:16,220
them in some of the slide pictures.

114
00:07:16,220 --> 00:07:18,680
So, this is saying to put together a PCA or

115
00:07:18,680 --> 00:07:22,025
Principal Components Analysis, um, scatter plot.

116
00:07:22,025 --> 00:07:26,490
Um, so, I can do that and then I can say, "Um,

117
00:07:26,490 --> 00:07:33,480
give it a set of words and draw me these as a scatter plot" and um,

118
00:07:33,480 --> 00:07:36,375
hopefully if I can just about fit it in,

119
00:07:36,375 --> 00:07:38,475
um, here's my scatter plot.

120
00:07:38,475 --> 00:07:40,090
And it works pretty well, right?

121
00:07:40,090 --> 00:07:41,510
I've got the wine, champagne,

122
00:07:41,510 --> 00:07:43,910
beer up here then the coffee and tea.

123
00:07:43,910 --> 00:07:45,770
Um, here are the countries.

124
00:07:45,770 --> 00:07:49,055
Here is the schools, college institute, universities.

125
00:07:49,055 --> 00:07:51,380
Um, the animals are down here.

126
00:07:51,380 --> 00:07:55,410
Um, foodstuffs there.

127
00:07:55,410 --> 00:08:00,290
So, yeah, this sort of really does work with this two direction- dimensional display.

128
00:08:00,290 --> 00:08:02,810
It basically shows you similarity.

129
00:08:02,810 --> 00:08:06,355
Now, um, there are, you know,

130
00:08:06,355 --> 00:08:10,790
to some extent though you want to hold on to your wallet with these PCA displays.

131
00:08:10,790 --> 00:08:13,160
So, it's as I've discussed before since you're

132
00:08:13,160 --> 00:08:16,070
taking something that was 100-dimensional and we're just doing

133
00:08:16,070 --> 00:08:20,720
this 2D projection that is capturing some of the major geometry of

134
00:08:20,720 --> 00:08:25,430
the space but it just has to be losing a huge amount of the information.

135
00:08:25,430 --> 00:08:27,760
So, when things end up close together,

136
00:08:27,760 --> 00:08:30,890
they might be really close together in the original space or

137
00:08:30,890 --> 00:08:33,680
they might just have been words that lost in

138
00:08:33,680 --> 00:08:37,640
the 2D projection because they- there are other patterns that

139
00:08:37,640 --> 00:08:41,690
were more dominant and were chosen as the first two principal components.

140
00:08:41,690 --> 00:08:44,000
So, you sort of don't wanna over trust

141
00:08:44,000 --> 00:08:47,510
these things and something if you like Infoviz you might think

142
00:08:47,510 --> 00:08:50,270
about is how there are other ways that I might be able to

143
00:08:50,270 --> 00:08:53,690
represent the distances in a way that was more accurate.

144
00:08:53,690 --> 00:08:56,770
Um, but anyway this is very simple to do and I'm just getting

145
00:08:56,770 --> 00:09:00,650
a PCA to reduce the dimensionality of the matrix and then,

146
00:09:00,650 --> 00:09:04,585
um, transforming with it these word vectors and printing them.

147
00:09:04,585 --> 00:09:07,380
Um, it's mainly easy to do.

148
00:09:07,380 --> 00:09:09,755
The bit that wasn't easy for me to do, um,

149
00:09:09,755 --> 00:09:14,780
but if someone's got some clever Python um plotting tips I'd like one,

150
00:09:14,780 --> 00:09:17,480
if someone wants to send me a message after class.

151
00:09:17,480 --> 00:09:20,990
I would have thought there'd be some default way in which you could just

152
00:09:20,990 --> 00:09:25,310
label points in a scatter plot but I wasn't able to find one.

153
00:09:25,310 --> 00:09:27,230
So, what I did, um,

154
00:09:27,230 --> 00:09:29,360
was I'm just sort of plotting the texts and

155
00:09:29,360 --> 00:09:31,580
I'm offsetting it a little bit from the points.

156
00:09:31,580 --> 00:09:33,650
Um, now that works kinda crappily

157
00:09:33,650 --> 00:09:36,245
because they just collide with each other as you can see.

158
00:09:36,245 --> 00:09:40,970
Um, so, it'd be better if there was a better way to do point labeling in Python plots.

159
00:09:40,970 --> 00:09:44,915
So, if anyone knows the answer to that one you can send it to me.

160
00:09:44,915 --> 00:09:49,715
Um, okay. So, that's that. Ah.

161
00:09:49,715 --> 00:09:52,370
And if you haven't used IPython Notebooks

162
00:09:52,370 --> 00:09:55,670
before and don't want your computer to run really slowly,

163
00:09:55,670 --> 00:09:56,960
it's a good idea to halt

164
00:09:56,960 --> 00:10:00,095
your IPython Notebooks when you're not gonna be using them anymore,

165
00:10:00,095 --> 00:10:02,525
um, especially if they're computing something.

166
00:10:02,525 --> 00:10:04,250
Um, okay. [NOISE] Um.

167
00:10:04,250 --> 00:10:26,480
[NOISE]

168
00:10:26,480 --> 00:10:28,220
Okay. [NOISE] So now,

169
00:10:28,220 --> 00:10:32,180
[NOISE] um, lecture two and so for today,

170
00:10:32,180 --> 00:10:34,400
we're gonna keep on talking about things you can do with

171
00:10:34,400 --> 00:10:38,870
Word Vectors and say a little bit at the end about Word sensors.

172
00:10:38,870 --> 00:10:41,570
So, in more detail, [NOISE] um,

173
00:10:41,570 --> 00:10:45,530
I'm gonna say a bit more about, um, Word2Vec.

174
00:10:45,530 --> 00:10:50,300
I'm gonna have a sort of a very brief excursion on optimization, um,

175
00:10:50,300 --> 00:10:55,010
but then I sort of want to explain a bit more of the space of what

176
00:10:55,010 --> 00:11:00,515
people have done and can do with dense word representations.

177
00:11:00,515 --> 00:11:02,450
So I am gonna say something about

178
00:11:02,450 --> 00:11:07,205
count-based approaches to capturing meaning and how do they work.

179
00:11:07,205 --> 00:11:08,870
I'm gonna talk for a bit about a,

180
00:11:08,870 --> 00:11:12,755
a different model of Word Vectors which was the GloVe model that,

181
00:11:12,755 --> 00:11:16,055
um, as a post-doc of mine, um,

182
00:11:16,055 --> 00:11:18,305
Jeffrey Pennington and, uh,

183
00:11:18,305 --> 00:11:20,615
me worked on a couple of years ago,

184
00:11:20,615 --> 00:11:22,700
um, talk some about evaluation,

185
00:11:22,700 --> 00:11:25,070
really quite dominant theme on a lot of what

186
00:11:25,070 --> 00:11:27,785
we do on natural language processing is how do we,

187
00:11:27,785 --> 00:11:32,105
how do we evaluate things and how much do we trust our evaluations,

188
00:11:32,105 --> 00:11:35,120
um, and then say a little bit about, um, word sensors.

189
00:11:35,120 --> 00:11:38,540
I have a sort of a goal here which is that by the end of the class,

190
00:11:38,540 --> 00:11:41,900
um, you should actually sort of understand, um,

191
00:11:41,900 --> 00:11:43,910
enough of the lay of the land that you could

192
00:11:43,910 --> 00:11:47,090
read papers about word vectors such as the ones that

193
00:11:47,090 --> 00:11:49,310
are in the syllabus and actually understand

194
00:11:49,310 --> 00:11:52,430
them and where they're coming from and roughly how they work.

195
00:11:52,430 --> 00:11:54,410
And so, you know, if you really wanna minimize

196
00:11:54,410 --> 00:11:56,690
work for your c- this class, you could think, "I,

197
00:11:56,690 --> 00:11:59,780
I know everything I need to know after the first week and I'm gonna

198
00:11:59,780 --> 00:12:03,260
do a final project on word vectors and I'll be okay."

199
00:12:03,260 --> 00:12:05,150
Um, and you know, you could actually do that,

200
00:12:05,150 --> 00:12:06,650
I mentioned during the wo- um,

201
00:12:06,650 --> 00:12:11,585
class, um, a couple of recent pieces of work on word vectors.

202
00:12:11,585 --> 00:12:13,055
On the other hand, um,

203
00:12:13,055 --> 00:12:16,310
doing things with word vectors as a fairly mined out areas,

204
00:12:16,310 --> 00:12:18,200
so you're probably better off, um,

205
00:12:18,200 --> 00:12:21,260
also listening to some of the later parts of the class.

206
00:12:21,260 --> 00:12:24,650
Okay. So, remember we had this idea of Word2Vec,

207
00:12:24,650 --> 00:12:29,975
so it was an iterative updating algorithm that learned, um,

208
00:12:29,975 --> 00:12:32,450
these vector representations of words,

209
00:12:32,450 --> 00:12:36,290
then in some sense capture their meaning and the way it worked was we kinda

210
00:12:36,290 --> 00:12:40,580
moved position by position through a corpus and each point in time,

211
00:12:40,580 --> 00:12:45,170
we had a center word here into and it's trying to predict

212
00:12:45,170 --> 00:12:47,330
the words around that by having

213
00:12:47,330 --> 00:12:50,945
a probability distribution over words will occur around that,

214
00:12:50,945 --> 00:12:55,385
and that probability distribution is defined simply in terms

215
00:12:55,385 --> 00:13:00,275
of the.product of the word vectors via the Softmax function.

216
00:13:00,275 --> 00:13:03,620
And so, what we wanna do is change those vectors in

217
00:13:03,620 --> 00:13:06,920
a way that this gives good probability predictions,

218
00:13:06,920 --> 00:13:12,440
that gives as high probability as possible to words that you tend to see in the context.

219
00:13:12,440 --> 00:13:15,770
And so, just to drill that in a little bit more, you know,

220
00:13:15,770 --> 00:13:21,305
what we actually have is we have two matrices, right?

221
00:13:21,305 --> 00:13:23,525
We have for center words,

222
00:13:23,525 --> 00:13:26,930
we have a matrix where for each word in our vocabulary,

223
00:13:26,930 --> 00:13:29,525
we have a vector, um, and at this,

224
00:13:29,525 --> 00:13:32,450
this is probably as good a point as any to say that it

225
00:13:32,450 --> 00:13:36,065
turns out that all the major deep learning packages,

226
00:13:36,065 --> 00:13:40,115
TensorFlow, PyTorch, etc., for their word vectors,

227
00:13:40,115 --> 00:13:43,040
the word vectors are represented as rows.

228
00:13:43,040 --> 00:13:44,780
If you've done a bunch of math classes,

229
00:13:44,780 --> 00:13:46,790
that might not be what you would expect.

230
00:13:46,790 --> 00:13:48,830
You might have expected the other way around,

231
00:13:48,830 --> 00:13:50,540
but they all put them in rows.

232
00:13:50,540 --> 00:13:53,285
So we can have rows for our,

233
00:13:53,285 --> 00:13:56,915
um, so we have six words and a five dimensional vector each.

234
00:13:56,915 --> 00:14:00,035
Okay. And then, we have this outside, um,

235
00:14:00,035 --> 00:14:02,825
matrix where we also have a second, um,

236
00:14:02,825 --> 00:14:07,625
vector for each word which is this representation in context.

237
00:14:07,625 --> 00:14:11,345
Um, so when we have a particular center word here,

238
00:14:11,345 --> 00:14:12,950
word four, you know,

239
00:14:12,950 --> 00:14:14,930
when we're doing our computations,

240
00:14:14,930 --> 00:14:19,640
we're taking a.product between v_4 and each row of

241
00:14:19,640 --> 00:14:25,655
U and that's then giving us a vector of dot product scores.

242
00:14:25,655 --> 00:14:27,350
And so, then after that,

243
00:14:27,350 --> 00:14:31,190
we're running Softmaxes on each of those numbers doing it

244
00:14:31,190 --> 00:14:33,350
element-wise and that's been giving us

245
00:14:33,350 --> 00:14:37,080
a probability distribution over words in the context.

246
00:14:37,080 --> 00:14:41,650
Um, and the sort of things to notice there, um,

247
00:14:41,650 --> 00:14:43,360
which hopefully you noticed last time,

248
00:14:43,360 --> 00:14:45,020
but to make sure you noticed that,

249
00:14:45,020 --> 00:14:49,280
um, you know, we've just got one probability distribution, right?

250
00:14:49,280 --> 00:14:51,440
So in terms of what words we predict,

251
00:14:51,440 --> 00:14:55,640
we're predicting exactly the same probability distribution, every position.

252
00:14:55,640 --> 00:14:58,640
We've sort of saying the most likely word one to the left

253
00:14:58,640 --> 00:15:02,630
is whatever it is house or most likely word to the left is house,

254
00:15:02,630 --> 00:15:04,070
three to the left is house,

255
00:15:04,070 --> 00:15:06,470
the one to the right should be house too, right?

256
00:15:06,470 --> 00:15:09,320
So, it's sort of no sort of find us a prediction,

257
00:15:09,320 --> 00:15:11,345
it's just an overall kind of

258
00:15:11,345 --> 00:15:16,160
probability distribution of words that are likely to occur in my context.

259
00:15:16,160 --> 00:15:19,685
So, all we're asking for is a model that gives

260
00:15:19,685 --> 00:15:23,480
reasonably high probability estimates to all words that

261
00:15:23,480 --> 00:15:27,845
occur in the context of this word relatively often,

262
00:15:27,845 --> 00:15:29,975
is nothing more to it than that.

263
00:15:29,975 --> 00:15:32,450
And that's part of why it's sort of surprising when you've got

264
00:15:32,450 --> 00:15:36,380
such a simplistic thing that it seems like at the end of the day,

265
00:15:36,380 --> 00:15:38,915
it can end up capturing so much about

266
00:15:38,915 --> 00:15:42,770
the meanings of words and aspects of the meanings of words,

267
00:15:42,770 --> 00:15:47,345
like in the examples I've just showing you in the IPython Notebook.

268
00:15:47,345 --> 00:15:53,300
Um, and [NOISE] there's one other thing I was gonna say, oh yeah,

269
00:15:53,300 --> 00:15:59,254
one other thing I was gonna say was the other thing that might occur to you from this is,

270
00:15:59,254 --> 00:16:00,905
um, well, wait a minute,

271
00:16:00,905 --> 00:16:03,185
there was like that and-and,

272
00:16:03,185 --> 00:16:06,125
and-of that occur all the time.

273
00:16:06,125 --> 00:16:16,219
Um, so that means every word must have a high dot product with words like that and of and,

274
00:16:16,219 --> 00:16:18,605
um, they get their probabilities right.

275
00:16:18,605 --> 00:16:23,015
And the first answer to that is, "Yup, that's true."

276
00:16:23,015 --> 00:16:25,985
And it turns out that all word vectors, [NOISE] um,

277
00:16:25,985 --> 00:16:31,010
have a very strong prob- word probability component that reflects that.

278
00:16:31,010 --> 00:16:34,910
And I mean, one of the things that some workers discuss,

279
00:16:34,910 --> 00:16:36,980
so on the readings,

280
00:16:36,980 --> 00:16:41,150
there are two papers from Sanjeev Arora's group in Princeton and one of

281
00:16:41,150 --> 00:16:45,485
those papers sort of discusses, um, this probability,

282
00:16:45,485 --> 00:16:49,490
high frequency effect and your crude way of [NOISE] actually

283
00:16:49,490 --> 00:16:53,960
fixing this high frequency effect is that normally, um,

284
00:16:53,960 --> 00:16:55,685
the first, um,

285
00:16:55,685 --> 00:16:58,460
the first biggest component in

286
00:16:58,460 --> 00:17:02,390
your word vectors is actually a frequency effect and if you just lop it off,

287
00:17:02,390 --> 00:17:05,000
you can make your semantic similarities better.

288
00:17:05,000 --> 00:17:09,860
Um, but there are other things that we do to sort of deal with high frequencies.

289
00:17:09,860 --> 00:17:14,615
Okay, so we get these lovely spaces that I've shown some of.

290
00:17:14,615 --> 00:17:17,510
But I'll make one more remark. Um.

291
00:17:17,510 --> 00:17:21,500
Yeah, so did I say this last time? Oh, oh.

292
00:17:21,500 --> 00:17:24,155
Um, my remark anyway is that,

293
00:17:24,155 --> 00:17:28,580
um, we show all these two-dimensional pictures.

294
00:17:28,580 --> 00:17:33,455
They're exceedingly, exceedingly misleading because in these pic,

295
00:17:33,455 --> 00:17:36,410
two-dimensional pictures, you know,

296
00:17:36,410 --> 00:17:39,530
you have these effects that if, you know,

297
00:17:39,530 --> 00:17:42,470
Samsung is close to Nokia,

298
00:17:42,470 --> 00:17:47,690
it has to be over here and then it has to be far away from words that are over here.

299
00:17:47,690 --> 00:17:50,660
Um, whereas you might sort of also want to have the effect that

300
00:17:50,660 --> 00:17:54,619
Nokia is close to Finland for a different reason,

301
00:17:54,619 --> 00:17:57,245
um, and you can't do that in two-dimensional, um,

302
00:17:57,245 --> 00:18:00,665
vector spaces but, you know, one of the, um,

303
00:18:00,665 --> 00:18:05,120
most of the properties of high dimensional vector spaces are very unintuitive,

304
00:18:05,120 --> 00:18:09,800
and one of the ways that they're unintuitive is in a high dimensional vector space,

305
00:18:09,800 --> 00:18:15,275
a word can be close to lots of other words in different directions.

306
00:18:15,275 --> 00:18:18,545
Um, okay. So um,

307
00:18:18,545 --> 00:18:24,980
we sort of started to talk about how we went about learning these word vectors.

308
00:18:24,980 --> 00:18:31,580
I'm sort of going to take about a five minute detour into optimization.

309
00:18:31,580 --> 00:18:33,950
Now, this isn't really an optimization class,

310
00:18:33,950 --> 00:18:36,350
if you want to learn a lot about optimization.

311
00:18:36,350 --> 00:18:38,870
Well you can learn more about optimization if you do

312
00:18:38,870 --> 00:18:42,830
229 and if you do something like Stephen Boyd's optimization class,

313
00:18:42,830 --> 00:18:45,800
you can learn a lot of optimization but this is

314
00:18:45,800 --> 00:18:49,790
sort of really baby optimization but just to make sure everyone's on the same page,

315
00:18:49,790 --> 00:18:52,085
here are three slides.

316
00:18:52,085 --> 00:18:53,960
Right, so what we did at the end,

317
00:18:53,960 --> 00:18:55,490
what we did over there,

318
00:18:55,490 --> 00:18:59,090
where I apologized that my writing was too small,

319
00:18:59,090 --> 00:19:03,500
but that will give you the chance to when doing homework too and you have to

320
00:19:03,500 --> 00:19:08,495
write that out to work it out for yourselves and learn more in the process.

321
00:19:08,495 --> 00:19:11,780
Right, so what we had was a cost function that we wanted to

322
00:19:11,780 --> 00:19:15,440
minimize and so what we did was we did our bit of

323
00:19:15,440 --> 00:19:20,465
calculus to calculate the gradient of the cost function with respect

324
00:19:20,465 --> 00:19:26,570
to our word vectors which were our variables theta and then what we want to do is say,

325
00:19:26,570 --> 00:19:30,050
well if we take a small step in

326
00:19:30,050 --> 00:19:35,000
the direction of the negative of the gradient that will be taking us down,

327
00:19:35,000 --> 00:19:38,615
down hill in this space and we want to keep on

328
00:19:38,615 --> 00:19:42,410
doing that and sort of head to the minimum of our space.

329
00:19:42,410 --> 00:19:45,499
I mean, of course in our high multi-dimensional space,

330
00:19:45,499 --> 00:19:48,140
you know, it might not be a nice smooth curve like this.

331
00:19:48,140 --> 00:19:52,865
It might be a horrible and non-convex curve but that's just the idea.

332
00:19:52,865 --> 00:19:56,510
So, essentially we're saying we've got the old parameters,

333
00:19:56,510 --> 00:20:01,525
we work out the gradient of the objective function using those old parameters.

334
00:20:01,525 --> 00:20:05,830
We multiply that by a small alpha which is

335
00:20:05,830 --> 00:20:08,680
our step size or learning rate because we only want to move a

336
00:20:08,680 --> 00:20:11,860
little bit each time because if back here,

337
00:20:11,860 --> 00:20:15,250
if we sort of said downhill is this way and said,

338
00:20:15,250 --> 00:20:16,960
"Great let's go a long way that way."

339
00:20:16,960 --> 00:20:18,790
You could kind of completely overshoot,

340
00:20:18,790 --> 00:20:21,220
so we only want to go a little bit each time.

341
00:20:21,220 --> 00:20:24,890
So we normally have a small learning rate alpha and so we

342
00:20:24,890 --> 00:20:28,745
subtract a small multiple of the gradient and we,

343
00:20:28,745 --> 00:20:30,470
from the old parameters and we get

344
00:20:30,470 --> 00:20:34,580
our new parameters and that sort of effectively being worked out,

345
00:20:34,580 --> 00:20:37,190
component wise as is shown below,

346
00:20:37,190 --> 00:20:40,925
that we're just doing that to each of the partial derivatives and then,

347
00:20:40,925 --> 00:20:45,485
that our hope is that that will let us gradually walk down this surface.

348
00:20:45,485 --> 00:20:48,050
Now, if you actually did this,

349
00:20:48,050 --> 00:20:50,840
it would be unbelievably bad for the kind of

350
00:20:50,840 --> 00:20:54,080
systems that we build and there's a lot of work on

351
00:20:54,080 --> 00:20:57,890
clever optimization but the most basic thing

352
00:20:57,890 --> 00:21:02,015
which you definitely need to know is that well,

353
00:21:02,015 --> 00:21:04,895
our objective function here,

354
00:21:04,895 --> 00:21:09,230
J of theta was a function of our entire corpus, right?

355
00:21:09,230 --> 00:21:11,000
And to get this to work well,

356
00:21:11,000 --> 00:21:12,980
the first thing you want to do is,

357
00:21:12,980 --> 00:21:17,600
you know collect a few billion words of your favorite language and then say,

358
00:21:17,600 --> 00:21:21,245
"Go and build a Word2Vec model for me, " and so,

359
00:21:21,245 --> 00:21:23,945
if you have to evaluate

360
00:21:23,945 --> 00:21:30,110
a billion center words and maybe then to- for each of 10 billion context words,

361
00:21:30,110 --> 00:21:35,690
if you have a window size of five and you- so you have to do these sort of 10 billion um,

362
00:21:35,690 --> 00:21:40,775
Softmax calculations before you work out what your gradient is,

363
00:21:40,775 --> 00:21:45,350
that you're going to be having your computer compute for a quite a long time before

364
00:21:45,350 --> 00:21:50,045
you make one little step in the gradient and so things are going to go so, so slowly.

365
00:21:50,045 --> 00:21:53,705
So, no one does that in deep learning systems.

366
00:21:53,705 --> 00:21:56,270
Um, so what people- everyone does is use

367
00:21:56,270 --> 00:22:00,680
stochastic gradient descent and in stochastic gradient descent,

368
00:22:00,680 --> 00:22:05,325
we sample our window in the simplest case.

369
00:22:05,325 --> 00:22:08,170
We, just for this one window,

370
00:22:08,170 --> 00:22:13,900
work out an estimate of the gradient and we use it as a parameter update.

371
00:22:13,900 --> 00:22:16,060
So, this is sort of an amazingly,

372
00:22:16,060 --> 00:22:18,970
amazingly noisy estimate of

373
00:22:18,970 --> 00:22:23,600
the gradient but it sort of doesn't matter too much because as soon as we've done it,

374
00:22:23,600 --> 00:22:26,750
we're going to choose a different center word and do it again and again,

375
00:22:26,750 --> 00:22:30,830
so that gradually we sort of approach what we would have gotten if we'd sort

376
00:22:30,830 --> 00:22:34,999
of looked at all of the center words before we took any steps,

377
00:22:34,999 --> 00:22:37,280
but because we take steps as we go,

378
00:22:37,280 --> 00:22:43,655
we get to the minimum of the function orders and magnitude more quickly.

379
00:22:43,655 --> 00:22:48,995
So thi- this shows the simplest case where we're just sampling one window.

380
00:22:48,995 --> 00:22:51,980
In practice, that's not what we normally do.

381
00:22:51,980 --> 00:22:55,895
We normally sample as- a small bunch,

382
00:22:55,895 --> 00:22:59,210
you know, order of approximately 32 or 64.

383
00:22:59,210 --> 00:23:03,185
Um, so if we have a sample that's bigger,

384
00:23:03,185 --> 00:23:05,765
that's generally referred to as a mini-batch and we

385
00:23:05,765 --> 00:23:09,110
calculate a gradient estimate from the mini-batch.

386
00:23:09,110 --> 00:23:12,050
Um, so that has two advantages.

387
00:23:12,050 --> 00:23:16,550
One advantage is that you kind of get less noisy estimates of

388
00:23:16,550 --> 00:23:18,590
the gradient because you've kind of averaged

389
00:23:18,590 --> 00:23:21,830
over a bunch of examples rather than just using one,

390
00:23:21,830 --> 00:23:23,870
but the second advantage,

391
00:23:23,870 --> 00:23:25,955
which is the one why we really care,

392
00:23:25,955 --> 00:23:30,815
is if we want our computations to go fast when we're using a GPU,

393
00:23:30,815 --> 00:23:36,050
that you need to get parallelization of doing the same operation a whole bunch of

394
00:23:36,050 --> 00:23:38,960
times and then you gain a lot by using

395
00:23:38,960 --> 00:23:42,275
a mini-batch of 64 examples or something like that.

396
00:23:42,275 --> 00:23:44,510
Um, and you don't have to but you know,

397
00:23:44,510 --> 00:23:47,780
it turns out the details of the guts of the hardware that you know,

398
00:23:47,780 --> 00:23:50,600
it isn't- [inaudible] GPUs, you know, they have these,

399
00:23:50,600 --> 00:23:52,040
whatever they have inside them,

400
00:23:52,040 --> 00:23:53,495
there in powers of two.

401
00:23:53,495 --> 00:23:57,965
So, you get better speedups if you use batches like 32 or 64,

402
00:23:57,965 --> 00:24:01,070
rather than just deciding that 42 is still your favorite number from

403
00:24:01,070 --> 00:24:06,060
high school [LAUGHTER] and you're going to use that as the size of your mini-batch.

404
00:24:07,000 --> 00:24:10,610
Okay. um, yeah here's

405
00:24:10,610 --> 00:24:13,730
one other interesting thing which

406
00:24:13,730 --> 00:24:17,270
actually has some optimization details in it, it turns out.

407
00:24:17,270 --> 00:24:19,625
Um, if you think of these um,

408
00:24:19,625 --> 00:24:22,955
doing stochastic gradients with word vectors,

409
00:24:22,955 --> 00:24:24,500
that's actually very different to

410
00:24:24,500 --> 00:24:28,385
some other deep learning problems like vision deep learning problems.

411
00:24:28,385 --> 00:24:33,740
Because for either a single window or even a sort of a reasonably sized mini-batch,

412
00:24:33,740 --> 00:24:36,575
it will turn out that those mini-batch,

413
00:24:36,575 --> 00:24:39,260
mini-batch only has, you know,

414
00:24:39,260 --> 00:24:41,990
relatively speaking a handful of words in it, right?

415
00:24:41,990 --> 00:24:45,710
So, if you have mini-batch of size 32 and a window size of ten,

416
00:24:45,710 --> 00:24:50,000
you know, probably there are only about a 100,150 different words in it.

417
00:24:50,000 --> 00:24:53,030
Um, but yet we're building this model over

418
00:24:53,030 --> 00:24:56,135
a vocabulary of quarter of a million words or something like that.

419
00:24:56,135 --> 00:25:00,290
So, just about all of the elements in this vector are zero.

420
00:25:00,290 --> 00:25:03,440
Um, and so, um,

421
00:25:03,440 --> 00:25:06,935
we sort of really have this very sparse um,

422
00:25:06,935 --> 00:25:10,565
perimeter update and so, um,

423
00:25:10,565 --> 00:25:14,300
that sort of suggests that we actually probably um,

424
00:25:14,300 --> 00:25:17,780
want to sort of only update the word vectors that

425
00:25:17,780 --> 00:25:21,380
appear and then the question is whether you can achieve that, right?

426
00:25:21,380 --> 00:25:25,040
The dumb way to do it, is you just have this matrix that's normally,

427
00:25:25,040 --> 00:25:27,830
nearly all zeros and you say add

428
00:25:27,830 --> 00:25:33,230
those two matrices together and there you go and then the question is,

429
00:25:33,230 --> 00:25:38,030
can you actually have a sparse matrix update which only updates

430
00:25:38,030 --> 00:25:41,030
the certain rows of the matrix that contain

431
00:25:41,030 --> 00:25:44,645
the words that you've entered and do things much faster?

432
00:25:44,645 --> 00:25:47,240
And if you're doing something even cleverer like doing

433
00:25:47,240 --> 00:25:52,220
distributed computation over multiple computers and sharing your parameters,

434
00:25:52,220 --> 00:25:54,620
well then definitely you just sort of only want to update

435
00:25:54,620 --> 00:25:58,660
the word vectors that you've actually been getting a parameter estimate for.

436
00:25:58,660 --> 00:26:00,850
So, there's sort of some details there but I'm

437
00:26:00,850 --> 00:26:03,990
going to skip past them for more details, um.

438
00:26:03,990 --> 00:26:08,570
Right. So, a couple of people asked afterwards, yeah,

439
00:26:08,570 --> 00:26:14,525
why are there these two word vectors that sort of center and the outside one?

440
00:26:14,525 --> 00:26:16,670
And, I mean the answer to that is,

441
00:26:16,670 --> 00:26:19,520
it makes that math I showed you easy, right?

442
00:26:19,520 --> 00:26:21,875
So that if, um,

443
00:26:21,875 --> 00:26:25,445
if you do it as I showed you, well,

444
00:26:25,445 --> 00:26:28,130
you know, for working out, um,

445
00:26:28,130 --> 00:26:31,985
the partial derivatives for the center word.

446
00:26:31,985 --> 00:26:35,210
It's just as I showed you, it's easy.

447
00:26:35,210 --> 00:26:39,980
Um, but if you use only one set of word vectors,

448
00:26:39,980 --> 00:26:41,810
well then the same word,

449
00:26:41,810 --> 00:26:43,235
that's the center word,

450
00:26:43,235 --> 00:26:45,440
will be one of the choices for

451
00:26:45,440 --> 00:26:49,895
the context word when you're working out that Softmax for the context word.

452
00:26:49,895 --> 00:26:52,550
And then you'll get these terms that are then

453
00:26:52,550 --> 00:26:55,820
squared terms in terms of the two references,

454
00:26:55,820 --> 00:26:57,290
so that same word,

455
00:26:57,290 --> 00:26:59,930
and that makes your math more difficult.

456
00:26:59,930 --> 00:27:03,725
Um, so it's sort of just a practical thing,

457
00:27:03,725 --> 00:27:05,375
um, in the end.

458
00:27:05,375 --> 00:27:08,600
I mean it sort of doesn't make very much difference,

459
00:27:08,600 --> 00:27:12,005
because if you sort of think about it since you're going along through all the,

460
00:27:12,005 --> 00:27:13,625
um, positions, you know.

461
00:27:13,625 --> 00:27:17,000
What was a center word at one point is immediately afterwards

462
00:27:17,000 --> 00:27:20,510
a context word of what used to be a context word,

463
00:27:20,510 --> 00:27:21,890
which is now the center word.

464
00:27:21,890 --> 00:27:26,240
So, sort of doing the same computations because, you know,

465
00:27:26,240 --> 00:27:28,504
the dot product is symmetric actually,

466
00:27:28,504 --> 00:27:30,770
um, all over again.

467
00:27:30,770 --> 00:27:34,100
So, they get pretty similar vector representations.

468
00:27:34,100 --> 00:27:36,530
So, it seems like in general you can get the best results

469
00:27:36,530 --> 00:27:39,349
by averaging what comes out for your two vectors,

470
00:27:39,349 --> 00:27:41,990
and you end up with just one vector per word.

471
00:27:41,990 --> 00:27:45,740
Okay, more substantively, um,

472
00:27:45,740 --> 00:27:48,620
if you go to the word2vec paper,

473
00:27:48,620 --> 00:27:51,920
you'll discover that there's sort of more to word2vec that they

474
00:27:51,920 --> 00:27:55,565
define as sort of a family of word2vec models.

475
00:27:55,565 --> 00:27:59,045
And there are so two main parts of that family.

476
00:27:59,045 --> 00:28:03,455
Um, firstly, there's a choice between the Continuous Bag of Words model,

477
00:28:03,455 --> 00:28:05,345
and the skip-grams model.

478
00:28:05,345 --> 00:28:07,940
And what I presented with the skip-grams models.

479
00:28:07,940 --> 00:28:09,650
So, in the skip-grams model,

480
00:28:09,650 --> 00:28:12,170
you've got one center word and you're trying to

481
00:28:12,170 --> 00:28:15,515
predict all the words in context one at a time.

482
00:28:15,515 --> 00:28:19,225
For the Continuous Bag of Words model it's the opposite.

483
00:28:19,225 --> 00:28:23,980
You've got all of the outside words and you're trying to use all of them,

484
00:28:23,980 --> 00:28:29,715
though considered independently like a Naive Bayes model to predict the center word.

485
00:28:29,715 --> 00:28:34,325
Um, and then the second one is, um,

486
00:28:34,325 --> 00:28:37,685
the way I presented learning this was

487
00:28:37,685 --> 00:28:41,240
the method that's using the so called Naive Softmax.

488
00:28:41,240 --> 00:28:44,315
So, therefore when we are wanting to work things out,

489
00:28:44,315 --> 00:28:49,354
we were sort of saying okay we want probability estimates for the context words,

490
00:28:49,354 --> 00:28:50,990
and so we're just going to sum over

491
00:28:50,990 --> 00:28:55,550
the whole vocabulary and we'll come up with these probability estimates.

492
00:28:55,550 --> 00:28:59,540
Um, in practice, that turns out to be a sort of

493
00:28:59,540 --> 00:29:03,860
a bad idea because that would also make things mega slow.

494
00:29:03,860 --> 00:29:05,884
So, in homework two,

495
00:29:05,884 --> 00:29:08,000
coming up next week, um,

496
00:29:08,000 --> 00:29:11,585
you will get to implement a much more practical, um,

497
00:29:11,585 --> 00:29:15,530
way of doing this which they present in the word2vec papers, right?

498
00:29:15,530 --> 00:29:16,730
So, the problem is,

499
00:29:16,730 --> 00:29:21,035
if we're using this equation that we use to do the calculus,

500
00:29:21,035 --> 00:29:23,225
that down in this denominator here,

501
00:29:23,225 --> 00:29:26,420
we're doing the sum over the entire vocabulary.

502
00:29:26,420 --> 00:29:29,120
So, if you have a vocabulary of a quarter million words,

503
00:29:29,120 --> 00:29:31,610
we're sort of doing a quarter of a million dot products and

504
00:29:31,610 --> 00:29:36,170
exponentials and adding them all to work out that denominator.

505
00:29:36,170 --> 00:29:38,180
And that sort of seems uh,

506
00:29:38,180 --> 00:29:41,240
sort of a really bad idea if you want things to be fast.

507
00:29:41,240 --> 00:29:44,360
Um, so, um, Tomas Mikolov and

508
00:29:44,360 --> 00:29:48,830
colleagues came up with this idea of negative sampling would be near enough.

509
00:29:48,830 --> 00:29:51,035
And so the idea of negative sampling,

510
00:29:51,035 --> 00:29:55,745
is we're going to train binary logistic regressions instead.

511
00:29:55,745 --> 00:29:59,525
And so, we're going to train one binary logistic regression

512
00:29:59,525 --> 00:30:04,055
for the actual word observed what's in the numerator,

513
00:30:04,055 --> 00:30:09,140
and you want to give high probability to the word that was actually observed.

514
00:30:09,140 --> 00:30:11,825
And then, what we're going to do,

515
00:30:11,825 --> 00:30:15,680
is we're going to sort of randomly sample a bunch of other words,

516
00:30:15,680 --> 00:30:21,920
they're the negative samples and say they weren't the ones that were actually seen.

517
00:30:21,920 --> 00:30:26,795
So, you should be trying to give them as low a probability as possible.

518
00:30:26,795 --> 00:30:30,920
Okay, so, um, the sort of notation that

519
00:30:30,920 --> 00:30:35,195
they use in the paper is sort of slightly different to the one I've used.

520
00:30:35,195 --> 00:30:38,360
They actually do maximization not minimization,

521
00:30:38,360 --> 00:30:42,380
and that's the equation which I'll come back to.

522
00:30:42,380 --> 00:30:46,700
Um, though before we do that here's the sigmoid function.

523
00:30:46,700 --> 00:30:49,310
So, the sigmoid function is normally written like this,

524
00:30:49,310 --> 00:30:51,905
one over one plus E to the minus X.

525
00:30:51,905 --> 00:30:54,230
But, um, essentially,

526
00:30:54,230 --> 00:30:59,600
the sigmoid function is like a binary case of the Softmax function, right?

527
00:30:59,600 --> 00:31:03,020
That we have two possible outcomes, yes or no,

528
00:31:03,020 --> 00:31:07,610
and that you're sort of again got an input that is any real number,

529
00:31:07,610 --> 00:31:11,300
and it's mapping it onto a probability distribution between

530
00:31:11,300 --> 00:31:14,870
zero and one which represents these two binary outcomes.

531
00:31:14,870 --> 00:31:16,790
And to the extent that the number is positive,

532
00:31:16,790 --> 00:31:21,215
it kind of ceilings to one and negative goes down to zero.

533
00:31:21,215 --> 00:31:23,165
Okay, so with this time,

534
00:31:23,165 --> 00:31:26,239
we're going to take the dot for- for the good word,

535
00:31:26,239 --> 00:31:29,330
we're going to take the dot product of the two vectors,

536
00:31:29,330 --> 00:31:31,835
shove it through our sigmoid function

537
00:31:31,835 --> 00:31:34,280
and then we're going to want that probability estimate,

538
00:31:34,280 --> 00:31:36,710
um, to be as high as possible.

539
00:31:36,710 --> 00:31:38,810
So, if I show you this version,

540
00:31:38,810 --> 00:31:41,840
which is just written slightly differently, um,

541
00:31:41,840 --> 00:31:46,730
to look as much as possible like the notation that we used last time,

542
00:31:46,730 --> 00:31:50,675
here is our new objective function for using negative sampling.

543
00:31:50,675 --> 00:31:52,415
And we've got two terms,

544
00:31:52,415 --> 00:31:54,500
the first one, um,

545
00:31:54,500 --> 00:31:59,285
is the log of the sigmoid of the observed context word,

546
00:31:59,285 --> 00:32:02,810
the outside words, dot producted with the center word,

547
00:32:02,810 --> 00:32:05,180
and we're going to want that to be big.

548
00:32:05,180 --> 00:32:08,435
Um, and then on the other hand,

549
00:32:08,435 --> 00:32:12,785
um, we've got, um, the,

550
00:32:12,785 --> 00:32:16,850
um, randomly chosen K words,

551
00:32:16,850 --> 00:32:18,530
which are just other words,

552
00:32:18,530 --> 00:32:21,905
and we're going to work out dot products between them and the center word.

553
00:32:21,905 --> 00:32:25,100
And we're going to want those to be as small as possible.

554
00:32:25,100 --> 00:32:28,340
Um, note that extra minus sign in there which is causing

555
00:32:28,340 --> 00:32:31,550
the sign of the two things to be different, right?

556
00:32:31,550 --> 00:32:34,355
So, those are our negative samples.

557
00:32:34,355 --> 00:32:37,220
And for big K, it can be a reasonably modest number,

558
00:32:37,220 --> 00:32:38,600
you can just take kind of 10,

559
00:32:38,600 --> 00:32:42,665
15 negative samples and that works pretty fine.

560
00:32:42,665 --> 00:32:46,670
Um, I said we sort of sampled some words,

561
00:32:46,670 --> 00:32:48,755
um, to be the negative samples.

562
00:32:48,755 --> 00:32:54,260
They in particular propose a sampling distribution that helps them along

563
00:32:54,260 --> 00:33:00,260
a little in partly dealing with this pro- problem of very frequent words.

564
00:33:00,260 --> 00:33:03,980
Um, so the starting point of how you sample words is you

565
00:33:03,980 --> 00:33:07,460
use what we call the- the unigram distribution.

566
00:33:07,460 --> 00:33:11,390
So, that just means you take words on a large corpus and count up

567
00:33:11,390 --> 00:33:16,010
how often each one occurs just as a count of independent words,

568
00:33:16,010 --> 00:33:18,125
so there's the called unigram counts.

569
00:33:18,125 --> 00:33:20,480
And so you start off with unigram counts,

570
00:33:20,480 --> 00:33:23,585
but then you raise them to the three quarters power.

571
00:33:23,585 --> 00:33:25,730
And raising to the three quarters power,

572
00:33:25,730 --> 00:33:27,395
has the effect of, um,

573
00:33:27,395 --> 00:33:30,785
decreasing how often you sample very common words,

574
00:33:30,785 --> 00:33:35,195
and increasing how often you sample rarer words.

575
00:33:35,195 --> 00:33:39,395
Okay, um, and that's that.

576
00:33:39,395 --> 00:33:43,955
Okay, so that's everything about word2vec I am going to say.

577
00:33:43,955 --> 00:33:48,560
Anyone have any last thing.

578
00:33:48,560 --> 00:33:52,245
Yes. [NOISE]

579
00:33:52,245 --> 00:33:55,275
Oh, oh [NOISE]. This is a- sorry Z,

580
00:33:55,275 --> 00:34:00,660
that capital Z is often used as a normalization term and so this is saying,

581
00:34:00,660 --> 00:34:03,705
well if you want the probability distribution of words,

582
00:34:03,705 --> 00:34:06,960
is you work out this three quarters power of the count

583
00:34:06,960 --> 00:34:10,230
of the word for every word in the vocabulary and then these

584
00:34:10,230 --> 00:34:13,800
numbers you just sum them up over the vocabulary and it'll be sum

585
00:34:13,800 --> 00:34:17,670
total and we're dividing by that so we get a probability distribution.

586
00:34:17,670 --> 00:34:19,845
Good question because i hadn't explained that.

587
00:34:19,845 --> 00:34:23,610
Um, in this class, when you see the letter Z with no explanation,

588
00:34:23,610 --> 00:34:28,740
it normally means I am a normalization term to turn things into

589
00:34:28,740 --> 00:34:31,200
probabilities and you sort of iterate over

590
00:34:31,200 --> 00:34:34,800
the numerator term and summing them and divide through.

591
00:34:34,800 --> 00:34:40,530
Any other questions of things I haven't explained or otherwise? Yes.

592
00:34:40,530 --> 00:34:45,000
So the window [inaudible] that's a [inaudible]

593
00:34:45,000 --> 00:34:46,290
Yeah, yes.

594
00:34:46,290 --> 00:34:48,480
So, [NOISE] what size window do you use?

595
00:34:48,480 --> 00:34:52,515
I'll actually come back to that in a bit and show a little bit of data on that,

596
00:34:52,515 --> 00:34:54,480
but yeah, we haven't done anything about that.

597
00:34:54,480 --> 00:34:57,270
At the moment we're guessing a window size like five,

598
00:34:57,270 --> 00:34:58,995
which isn't a bad one um,

599
00:34:58,995 --> 00:35:04,305
but you know there isn't- there hasn't really been any science behind that, um,

600
00:35:04,305 --> 00:35:09,405
that people treat that as what's then called a hyperparameter which means that um,

601
00:35:09,405 --> 00:35:12,720
you try a few different numbers and see which one seems

602
00:35:12,720 --> 00:35:17,310
best and that's the one that you use in your future work. Yeah.

603
00:35:17,310 --> 00:35:19,485
Um, [inaudible] three quarters power

604
00:35:19,485 --> 00:35:24,180
chosen for any theoretical reason or just because it seems to work in practice?

605
00:35:24,180 --> 00:35:27,060
Um, no. Um, that,

606
00:35:27,060 --> 00:35:33,120
that was um, also chosen as a hyperparameter and improved performance.

607
00:35:33,120 --> 00:35:35,490
I mean, actually um, you know,

608
00:35:35,490 --> 00:35:38,580
for this Word2Vec paper, I mean,

609
00:35:38,580 --> 00:35:40,995
you know, it turns out that um,

610
00:35:40,995 --> 00:35:43,320
in the actual paper um,

611
00:35:43,320 --> 00:35:47,490
the model looks very- fairly clean but what

612
00:35:47,490 --> 00:35:51,449
people's discovered when they started digging through the code,

613
00:35:51,449 --> 00:35:56,325
which to- to their credit they did make available, reproducible research,

614
00:35:56,325 --> 00:35:59,175
that there are actually a whole bunch of tricks

615
00:35:59,175 --> 00:36:03,300
of different things like these hyperparameters of um,

616
00:36:03,300 --> 00:36:08,475
how you sample, and how you wait windows and various things to make the numbers better.

617
00:36:08,475 --> 00:36:11,190
So, you know, people play quite a few tricks to make

618
00:36:11,190 --> 00:36:15,205
the numbers go up which aren't particularly theoretical.

619
00:36:15,205 --> 00:36:17,530
Are we good?

620
00:36:17,530 --> 00:36:33,540
Yeah.

621
00:36:33,540 --> 00:36:39,230
[inaudible] [NOISE].

622
00:36:39,230 --> 00:36:42,325
Ah, sometimes.

623
00:36:42,325 --> 00:36:48,510
I so- I- you- so in general for a lot of these sampling things,

624
00:36:48,510 --> 00:36:53,145
it's a bad idea if you're going to be doing multiple passes if you just go bloom,

625
00:36:53,145 --> 00:36:55,530
bloom, bloom and then bloom, bloom, bloom again,

626
00:36:55,530 --> 00:36:56,670
that's a bad idea,

627
00:36:56,670 --> 00:36:59,580
but a common technique a lot of the packages use

628
00:36:59,580 --> 00:37:02,820
is that they do use this shuffling operation at the beginning.

629
00:37:02,820 --> 00:37:04,320
So for each epoch,

630
00:37:04,320 --> 00:37:08,880
they'll shuffle the data randomly and then they'll go through it in sequence and that has

631
00:37:08,880 --> 00:37:13,905
the benefits of faster computation from locality et cetera um,

632
00:37:13,905 --> 00:37:16,590
while ha- meaning that when you do it differently epoch,

633
00:37:16,590 --> 00:37:18,720
it will work out differently.

634
00:37:18,720 --> 00:37:28,650
Uh, yeah, yeah.

635
00:37:28,650 --> 00:37:28,950
[inaudible] [NOISE] [inaudible].

636
00:37:28,950 --> 00:37:34,185
That last question I think was talking about taking the mini-batches from the corpus and

637
00:37:34,185 --> 00:37:37,140
contrasting whether you actually say sample 20

638
00:37:37,140 --> 00:37:41,535
randomly from the whole corpus versus just sort of working from left to right.

639
00:37:41,535 --> 00:37:43,230
Yes, do you have a question?

640
00:37:43,230 --> 00:37:55,770
Um, yeah [inaudible] [NOISE].

641
00:37:55,770 --> 00:37:59,190
Yeah. So- so you could argue- you could argue

642
00:37:59,190 --> 00:38:02,895
whether or not this was written in the clearest way, but, right.

643
00:38:02,895 --> 00:38:07,110
So, we're making this dot product and then we're negating it

644
00:38:07,110 --> 00:38:11,510
which is then flipping which side of the space we're on, right?

645
00:38:11,510 --> 00:38:15,890
Because the sigmoid is symmetric around zero.

646
00:38:15,890 --> 00:38:19,215
So, if we've got some dot product um,

647
00:38:19,215 --> 00:38:20,790
and then we negate it,

648
00:38:20,790 --> 00:38:25,095
we're sort of working out a one minus probability and so

649
00:38:25,095 --> 00:38:30,225
that's the way in which we're actually for the first um,

650
00:38:30,225 --> 00:38:32,910
for the first time we're wanting the probability to be

651
00:38:32,910 --> 00:38:35,640
high and then for the negative samples,

652
00:38:35,640 --> 00:38:38,520
we're wanting their probability to be low.

653
00:38:38,520 --> 00:38:42,495
Okay, I'll maybe run ahead now.

654
00:38:42,495 --> 00:38:47,810
Um, so this was an algorithm which um,

655
00:38:47,810 --> 00:38:53,870
sort of you're going through this corpus position by position and you're sort of doing

656
00:38:53,870 --> 00:38:56,780
this prediction of words and then you're

657
00:38:56,780 --> 00:39:00,200
updating some parameters and you're learning something and you know,

658
00:39:00,200 --> 00:39:04,400
by job it seemed to work based on what we saw in the examples,

659
00:39:04,400 --> 00:39:07,155
but you know, you might have thought um,

660
00:39:07,155 --> 00:39:08,955
that that was kind of weird right?

661
00:39:08,955 --> 00:39:12,600
Look we have this whole big pile of data you know,

662
00:39:12,600 --> 00:39:16,560
sort of traditional, I'm thinking of statistics, right?

663
00:39:16,560 --> 00:39:18,180
So you have a big pile of data,

664
00:39:18,180 --> 00:39:22,530
you aggregate it and it sort of seems like there are obvious things you could do here.

665
00:39:22,530 --> 00:39:24,930
You could say, well there's a word like,

666
00:39:24,930 --> 00:39:27,030
whatever word we're using, banana.

667
00:39:27,030 --> 00:39:31,140
Let's just see what words occur in the context of the gut banana and count

668
00:39:31,140 --> 00:39:35,370
them all up and then we'll be able to use those to predict somehow and you know,

669
00:39:35,370 --> 00:39:38,940
those kinds of methods were traditionally

670
00:39:38,940 --> 00:39:43,350
used including even with distributed representation techniques.

671
00:39:43,350 --> 00:39:45,210
Um, so I want to say a bit about that,

672
00:39:45,210 --> 00:39:49,815
so you're fully educated and don't sound like one of those people who were

673
00:39:49,815 --> 00:39:55,440
aware of no work that happened before 2013 when your network's took off.

674
00:39:55,440 --> 00:39:59,310
Um, okay. So, what we could do is we can essentially

675
00:39:59,310 --> 00:40:03,300
do the same thing as sort of Word2Vec.

676
00:40:03,300 --> 00:40:07,185
We could say there's a five word window around

677
00:40:07,185 --> 00:40:11,160
each word instance that's often referred to as a word token, right?

678
00:40:11,160 --> 00:40:17,250
So at NLP, we often want to distinguish between a particular kind of type like banana

679
00:40:17,250 --> 00:40:20,400
or apple versus particular instances

680
00:40:20,400 --> 00:40:23,985
often in the text and that's referred to as sort of a type token distinction.

681
00:40:23,985 --> 00:40:26,160
So we could, um,

682
00:40:26,160 --> 00:40:29,670
look at each um token with a word,

683
00:40:29,670 --> 00:40:31,290
and the words five around that,

684
00:40:31,290 --> 00:40:34,680
and then we could so start counting up which words occur,

685
00:40:34,680 --> 00:40:40,845
occur with it and so we can then have a matrix of co-occurrence counts.

686
00:40:40,845 --> 00:40:42,930
Um, okay.

687
00:40:42,930 --> 00:40:44,415
So, we'll have again,

688
00:40:44,415 --> 00:40:46,080
and I'm going to give me an example of this.

689
00:40:46,080 --> 00:40:49,350
So, normally again you use the five to 10 but you know I can just

690
00:40:49,350 --> 00:40:53,055
use a window of one to keep my counts very simple and small.

691
00:40:53,055 --> 00:40:56,400
I ignore left or right just like Word2Vec did,

692
00:40:56,400 --> 00:40:59,475
and so if I have a teeny baby corpus like this,

693
00:40:59,475 --> 00:41:00,840
you know, what I could do,

694
00:41:00,840 --> 00:41:05,445
is just say here is the matrix of word co-occurrence accounts.

695
00:41:05,445 --> 00:41:08,295
So, within my window size of one,

696
00:41:08,295 --> 00:41:10,860
I occurs next to like twice,

697
00:41:10,860 --> 00:41:14,490
and that means that like occurs next I twice it's symmetric,

698
00:41:14,490 --> 00:41:18,720
and all my other accounts here are singletons, um.

699
00:41:18,720 --> 00:41:25,580
And so this gives me a big huge sparse matrix of word co-occurrence accounts.

700
00:41:25,580 --> 00:41:30,000
And so one thing that you could do is just use this matrix directly,

701
00:41:30,000 --> 00:41:32,870
because I haven't really got enough data here.

702
00:41:32,870 --> 00:41:35,180
But, you know, if you sort of,

703
00:41:35,180 --> 00:41:38,125
um, decided that, you know,

704
00:41:38,125 --> 00:41:40,915
the word like is like the word learning,

705
00:41:40,915 --> 00:41:42,760
what you'd do is you'd expect that

706
00:41:42,760 --> 00:41:46,880
these two vectors would end up kind of similar to each other.

707
00:41:46,880 --> 00:41:48,345
And [NOISE] they do.

708
00:41:48,345 --> 00:41:50,015
So, you can just measure, um,

709
00:41:50,015 --> 00:41:55,580
similarity of the vectors directly in terms of these co-occurrence counts.

710
00:41:55,580 --> 00:41:59,980
But, you know, it's a little bit unappealing doing things this way, right?

711
00:41:59,980 --> 00:42:03,160
If you have a quarter million word vocabulary that's where

712
00:42:03,160 --> 00:42:06,545
you are in this space where my math is bad,

713
00:42:06,545 --> 00:42:09,850
but it's in the trillions of the number of cells of this matrix,

714
00:42:09,850 --> 00:42:11,930
might require a lot of storage.

715
00:42:11,930 --> 00:42:15,490
Though if you're clever and notice that most of the cells were zero and could do

716
00:42:15,490 --> 00:42:20,040
some clever sparse matrix representation might take a little bit less.

717
00:42:20,040 --> 00:42:23,710
Um, your classification models might have sparsity issues cause, you know,

718
00:42:23,710 --> 00:42:27,460
a lot of those cells aren't present and so it might not be very robust.

719
00:42:27,460 --> 00:42:31,980
And so those are traditional answer to all of these things which is well,

720
00:42:31,980 --> 00:42:36,105
maybe we could have that big co-occurrence counts matrix

721
00:42:36,105 --> 00:42:40,930
and somehow reduce its dimensionality of just, um,

722
00:42:40,930 --> 00:42:45,545
find a corresponding low dimensional matrix which preserves,

723
00:42:45,545 --> 00:42:47,915
uh, most of the information, um,

724
00:42:47,915 --> 00:42:50,780
in the original matrix and, you know,

725
00:42:50,780 --> 00:42:56,140
maybe we'll reduce things to a dimensionality of somewhere around the size 25 to a 1,000,

726
00:42:56,140 --> 00:42:59,195
um as is done with Word2Vec.

727
00:42:59,195 --> 00:43:02,620
So, there's sort of a standard most common way of doing

728
00:43:02,620 --> 00:43:07,210
this dimensionality reduction and you don't really have to understand all the math,

729
00:43:07,210 --> 00:43:10,715
but you get to play with this and homework one which is, um,

730
00:43:10,715 --> 00:43:15,665
for any matrix you can do what's called the singular value decomposition, um,

731
00:43:15,665 --> 00:43:23,505
which is a way you can take an arbitrary matrix and decompose it into three matrices, um,

732
00:43:23,505 --> 00:43:27,250
where the center one is diagonal and has what- in it what are

733
00:43:27,250 --> 00:43:31,320
called singular vectors which are weightings of the different dimensions.

734
00:43:31,320 --> 00:43:34,895
So, they decrease in size as you go downwards.

735
00:43:34,895 --> 00:43:37,540
And then these two U and V are then

736
00:43:37,540 --> 00:43:41,945
orthogonal bases corresponding to the rows and columns.

737
00:43:41,945 --> 00:43:43,435
And so in particular,

738
00:43:43,435 --> 00:43:47,315
it's even simpler than the case where we just have these word-word vectors,

739
00:43:47,315 --> 00:43:51,095
because you have a square matrix and so they are effectively the same.

740
00:43:51,095 --> 00:43:53,490
But, you know, for the general case, um,

741
00:43:53,490 --> 00:43:57,089
although you get these sort of full orthogonal bases,

742
00:43:57,089 --> 00:44:00,065
you then have these bits sort of don't really

743
00:44:00,065 --> 00:44:03,855
matter cause they end up being used for nothing when you work out the product.

744
00:44:03,855 --> 00:44:09,180
Um, and then if you want to reduce the dimensionality, what you say is,

745
00:44:09,180 --> 00:44:14,675
throw away the smallest singular values which remember there are in decreasing

746
00:44:14,675 --> 00:44:17,740
size and that means you're then effectively

747
00:44:17,740 --> 00:44:22,040
throwing away rows and columns of these other matrices.

748
00:44:22,040 --> 00:44:23,140
And then it says,

749
00:44:23,140 --> 00:44:25,860
behold I've now reduced these things to

750
00:44:25,860 --> 00:44:28,360
a two-dimensional representation from

751
00:44:28,360 --> 00:44:32,440
the original three-dimensional representation and that's referred to as

752
00:44:32,440 --> 00:44:39,275
the reduced SVD and the classic result is in terms of least squares error in

753
00:44:39,275 --> 00:44:47,189
estimation that this- the product of these three things will give X k which is the best,

754
00:44:47,189 --> 00:44:52,210
um, k- rank k approximation to the original X in terms of,

755
00:44:52,210 --> 00:44:55,155
uh, X squared least squares criterion.

756
00:44:55,155 --> 00:44:58,975
So, we could do this and we could build word vectors.

757
00:44:58,975 --> 00:45:00,750
So, I can, um,

758
00:45:00,750 --> 00:45:02,445
make use of, um,

759
00:45:02,445 --> 00:45:07,175
NumPy's SVD function and I can throw into it,

760
00:45:07,175 --> 00:45:11,525
um, matrices and, um,

761
00:45:11,525 --> 00:45:14,040
I can make word vectors.

762
00:45:14,040 --> 00:45:16,175
And these ones look really bad, but hey,

763
00:45:16,175 --> 00:45:20,825
I give it a dataset of three centers [LAUGHTER] and it's not exactly a fair comparison.

764
00:45:20,825 --> 00:45:23,420
But- so this technique was in, um,

765
00:45:23,420 --> 00:45:28,925
popularized around, um, the turn- the turn of the millennium.

766
00:45:28,925 --> 00:45:30,990
It generally, um, went for

767
00:45:30,990 --> 00:45:34,900
some word applications under the name of latent semantic analysis or

768
00:45:34,900 --> 00:45:38,585
latent semantic indexing and the idea was that you could have

769
00:45:38,585 --> 00:45:41,490
these semantic directions that you are

770
00:45:41,490 --> 00:45:44,610
finding in this low dimensional space that had meaning.

771
00:45:44,610 --> 00:45:47,380
And people worked with it quite a bit for techniques

772
00:45:47,380 --> 00:45:50,440
like t- trying to do information retrieval

773
00:45:50,440 --> 00:45:56,945
using these LSA approximations and it sort of worked a bit.

774
00:45:56,945 --> 00:46:02,140
It kind of never really worked very well I think,

775
00:46:02,140 --> 00:46:06,565
um, and so it never sort of hugely caught on.

776
00:46:06,565 --> 00:46:11,770
Um, but it's- the methods kind of continued to be explored actually mainly in the sort of

777
00:46:11,770 --> 00:46:17,135
COG psych- COGS psych community where people were doing things with word meaning.

778
00:46:17,135 --> 00:46:20,140
And there's this sort of kind of interesting, um,

779
00:46:20,140 --> 00:46:24,955
the [NOISE] to the literature that there was this guy Doug Rohde, um,

780
00:46:24,955 --> 00:46:31,055
who, um, did a PhD at CMU, um, in 2005.

781
00:46:31,055 --> 00:46:34,225
And basically what he discovered was,

782
00:46:34,225 --> 00:46:37,715
look if rather than just using raw counts,

783
00:46:37,715 --> 00:46:42,175
I start doing quite a bit more in terms of,

784
00:46:42,175 --> 00:46:44,565
you know, fiddling with the counts,

785
00:46:44,565 --> 00:46:47,525
I can start to produce results that are much better.

786
00:46:47,525 --> 00:46:49,620
So, rather than using low counts,

787
00:46:49,620 --> 00:46:53,450
you have to do something to deal with those very high-frequency words.

788
00:46:53,450 --> 00:46:55,960
So, one idea is you could log scale them which

789
00:46:55,960 --> 00:46:58,505
is also commonly used in information retrieval.

790
00:46:58,505 --> 00:47:01,575
Another idea is you could just use something like,

791
00:47:01,575 --> 00:47:03,450
uh, a ceiling function,

792
00:47:03,450 --> 00:47:09,905
so you take the minimum of X,t for t set and that some number like around 100.

793
00:47:09,905 --> 00:47:15,430
Um, he had- he used the idea which was also another of the hacks that was put into

794
00:47:15,430 --> 00:47:21,215
the Word2Vec was rather than just treating the whole window the same that you should,

795
00:47:21,215 --> 00:47:24,045
um, count words that are closer more.

796
00:47:24,045 --> 00:47:29,745
So, in Word2Vec, they sample closer words more commonly than further away words.

797
00:47:29,745 --> 00:47:31,715
Um, in his system, you're sort of having to have

798
00:47:31,715 --> 00:47:34,925
a differential count for closer words et cetera.

799
00:47:34,925 --> 00:47:40,435
And then, um, compared to any of that rather than using counts at all,

800
00:47:40,435 --> 00:47:43,775
he then started using Pearson correlations which

801
00:47:43,775 --> 00:47:48,995
helped and set they're sometimes negative and he decided that it helped,

802
00:47:48,995 --> 00:47:52,310
um, if you then got rid of the negative values.

803
00:47:52,310 --> 00:47:54,030
So, in- in some sense,

804
00:47:54,030 --> 00:47:56,045
this sounds like a bag of hacks,

805
00:47:56,045 --> 00:47:58,459
um, but on the other hand,

806
00:47:58,459 --> 00:48:00,555
he was able to show that, you know,

807
00:48:00,555 --> 00:48:03,520
these transformed counts could actually then give

808
00:48:03,520 --> 00:48:07,150
you very useful word vectors as I'm about to show.

809
00:48:07,150 --> 00:48:13,120
And- well, we have to realize that actually in slightly different forms,

810
00:48:13,120 --> 00:48:17,165
several of these exact same counts are actually being used in Word2Vec as well.

811
00:48:17,165 --> 00:48:21,110
Do you hear that?

812
00:48:21,110 --> 00:48:26,885
Yeah. Were they [inaudible].

813
00:48:26,885 --> 00:48:31,265
Yeah. So, so that's an- I'm about to show exactly that.

814
00:48:31,265 --> 00:48:34,370
Um, that's actually a really interesting little,

815
00:48:34,370 --> 00:48:35,800
um, bit of the data.

816
00:48:35,800 --> 00:48:39,215
So, you know, what, um, yeah,

817
00:48:39,215 --> 00:48:42,005
so the, the thing- if you do that,

818
00:48:42,005 --> 00:48:44,915
you not only get word similarities pretty good.

819
00:48:44,915 --> 00:48:48,360
Let me show you this example which is cleaner.

820
00:48:48,360 --> 00:48:52,685
Um, so this- the precise idea of

821
00:48:52,685 --> 00:48:57,390
evaluating with analogies was not something that had really been developed.

822
00:48:57,390 --> 00:49:01,520
So, that was actually something that Marsh Mikolov, um, suggested.

823
00:49:01,520 --> 00:49:05,655
But actually, um, Doug Rohde made this, um,

824
00:49:05,655 --> 00:49:11,100
really interesting observation which was- he said, look,

825
00:49:11,100 --> 00:49:14,440
"Once I do these kind of transformations to

826
00:49:14,440 --> 00:49:17,925
improve the semantic representation of my word vectors,

827
00:49:17,925 --> 00:49:21,705
look this really interesting property emerges.

828
00:49:21,705 --> 00:49:27,240
Um, that what you find is that there is semantic vectors

829
00:49:27,240 --> 00:49:32,870
are which basically linear components in my carefully-constructed space.

830
00:49:32,870 --> 00:49:35,170
So, here we have the sort of, um,

831
00:49:35,170 --> 00:49:38,055
verb to the doer of the verb direction,

832
00:49:38,055 --> 00:49:40,740
drive, driver, um, clean,

833
00:49:40,740 --> 00:49:43,505
janitor, swim, swimmer, learn,

834
00:49:43,505 --> 00:49:45,645
teacher or teach, teacher,

835
00:49:45,645 --> 00:49:48,705
doctor, treat, priest, pray.

836
00:49:48,705 --> 00:49:51,000
I mean, you know, it's not exactly perfect,

837
00:49:51,000 --> 00:49:52,910
you know, there's a little bit of wiggle there, right?

838
00:49:52,910 --> 00:49:57,740
But, you know, roughly it's completely clear that there's sort of a direction

839
00:49:57,740 --> 00:50:03,320
in the space that corresponds to- from a verb to the doers of a verb.

840
00:50:03,320 --> 00:50:06,430
Um, and yeah, so he [inaudible] - he-

841
00:50:06,430 --> 00:50:10,450
no one had thought of this idea of doing the analogies and tests.

842
00:50:10,450 --> 00:50:14,895
But the thing in retrospect that's obvious is,

843
00:50:14,895 --> 00:50:20,195
if you can construct a vector space that has this linearity property,

844
00:50:20,195 --> 00:50:23,740
then you're definitely gonna do well in analogy.

845
00:50:23,740 --> 00:50:26,610
So, effectively he had invented a vector space that do

846
00:50:26,610 --> 00:50:29,855
well in analogies because this means that you've got

847
00:50:29,855 --> 00:50:33,845
this direction which is the doer and then you can immediately

848
00:50:33,845 --> 00:50:38,155
say that's the doer vector which you can get from subtracting clean from swimmer.

849
00:50:38,155 --> 00:50:40,745
And the- Right. So, it's clean from janitor.

850
00:50:40,745 --> 00:50:45,385
And then we can add it on to swim and we'll get somewhere close to swimmer.

851
00:50:45,385 --> 00:50:48,410
Um, so his space actually did do that.

852
00:50:48,410 --> 00:50:51,930
And so, um, this is- so the,

853
00:50:51,930 --> 00:50:53,705
the moral in some sense is,

854
00:50:53,705 --> 00:50:58,545
if you have- if you kind of do carefully control accounts and so on,

855
00:50:58,545 --> 00:51:04,715
that conventional methods can also give you good word vector spaces and- I mean,

856
00:51:04,715 --> 00:51:09,110
so that was actually the starting off point for our work on GloVe.

857
00:51:09,110 --> 00:51:11,010
Um, so that essentially,

858
00:51:11,010 --> 00:51:13,440
there had been these two schools of work.

859
00:51:13,440 --> 00:51:16,775
Um, there had been the school of work that had

860
00:51:16,775 --> 00:51:20,160
been explored more in COG psych than anywhere else,

861
00:51:20,160 --> 00:51:23,870
which had been based on counting and transforming counts.

862
00:51:23,870 --> 00:51:29,795
And, you know, it had some advantages or it seemed it had some advantages, right?

863
00:51:29,795 --> 00:51:33,670
That, um, you're making sort of efficient use of statistics as you're using

864
00:51:33,670 --> 00:51:38,045
the global statistics of the whole matrix directly to estimate things.

865
00:51:38,045 --> 00:51:41,310
Um, and at that poi- up until then,

866
00:51:41,310 --> 00:51:45,575
it had really only being used to capture word similarity, um,

867
00:51:45,575 --> 00:51:51,675
and a lot of it had suffered from disproportionate im- importance given to large counts.

868
00:51:51,675 --> 00:51:56,210
But Doug Rohde, he had sort of started to show how to solve both of these problems.

869
00:51:56,210 --> 00:51:57,700
And so on the other hand,

870
00:51:57,700 --> 00:51:59,760
there had been these neural network methods

871
00:51:59,760 --> 00:52:02,145
which are kind of direct prediction methods that

872
00:52:02,145 --> 00:52:07,110
we're defining that probability distribution and trying to predict the words that occur.

873
00:52:07,110 --> 00:52:09,970
And they had some advantages, right?

874
00:52:09,970 --> 00:52:15,030
The fact that your sampling means that you're not going to run out of memory hopefully.

875
00:52:15,030 --> 00:52:18,425
I know we've had some memory problems with homework one, but in principle,

876
00:52:18,425 --> 00:52:21,460
you're not as bad memory position and if you have to

877
00:52:21,460 --> 00:52:24,690
construct a huge matrix because you're going linearly,

878
00:52:24,690 --> 00:52:27,360
um, but, you know, since you're doing it sample by

879
00:52:27,360 --> 00:52:31,340
sample it's inefficient use of statistics, um.

880
00:52:31,340 --> 00:52:37,255
Okay. And so, but on the other hand Mikolov's work it performed perfectly.

881
00:52:37,255 --> 00:52:39,145
Not perfectly, but really well.

882
00:52:39,145 --> 00:52:42,880
Um, so this is sort of led into this work,

883
00:52:42,880 --> 00:52:45,340
um, that Jeffrey Pennington, um,

884
00:52:45,340 --> 00:52:50,155
Richard Socher [inaudible] can we sort of combine these ideas

885
00:52:50,155 --> 00:52:55,445
and sort of have some of the goodness of the neural net methods,

886
00:52:55,445 --> 00:53:00,535
um, while trying to do things with some kind of count matrix.

887
00:53:00,535 --> 00:53:02,650
And so in particular, um,

888
00:53:02,650 --> 00:53:04,480
we wanted to get the result in

889
00:53:04,480 --> 00:53:10,240
a slightly less hacky way that you want to have components of meaning

890
00:53:10,240 --> 00:53:13,540
being linear ope- linear operations in

891
00:53:13,540 --> 00:53:18,085
the vector space that they're just some effective or adding or something like this.

892
00:53:18,085 --> 00:53:22,540
And so the crucial observation of this model was that we could use

893
00:53:22,540 --> 00:53:27,685
ratios of co-occurrence probabilities to encode meaning components.

894
00:53:27,685 --> 00:53:29,320
And so the idea here is,

895
00:53:29,320 --> 00:53:32,020
if you have a word like ice and

896
00:53:32,020 --> 00:53:35,215
you say how often the thing's going to co-occur with that,

897
00:53:35,215 --> 00:53:38,965
well solid should co-occur a lot and gas shouldn't.

898
00:53:38,965 --> 00:53:45,545
But well water is also going to co-occur a lot and some random word won't occur much.

899
00:53:45,545 --> 00:53:49,525
If you have, oops.

900
00:53:49,525 --> 00:53:51,820
If you have steam,

901
00:53:51,820 --> 00:53:56,230
you get the opposite pattern with solid and gas, right?

902
00:53:56,230 --> 00:53:58,835
But so the thing to notice is,

903
00:53:58,835 --> 00:54:02,320
it's not enough to just have large by itself because large

904
00:54:02,320 --> 00:54:06,320
appears both here and here or small appears there and there,

905
00:54:06,320 --> 00:54:09,490
the thing that's interesting and sort of the difference between

906
00:54:09,490 --> 00:54:12,925
these components in there indicating a meaning component.

907
00:54:12,925 --> 00:54:20,275
And so we can get it that if we look at the ratio of co-occurrence probabilities.

908
00:54:20,275 --> 00:54:25,810
And so for the ratio of co-occurrence probabilities this is a dimension of

909
00:54:25,810 --> 00:54:33,200
meaning and where for other words and this sort of ratio cancels out to about one.

910
00:54:33,200 --> 00:54:36,970
And so in this slide I've moved so it's not how my

911
00:54:36,970 --> 00:54:40,840
small and large that these are actually actual counts from a corpus.

912
00:54:40,840 --> 00:54:43,660
So we roughly get dimension of meaning between

913
00:54:43,660 --> 00:54:46,240
solid and gas are the ones coming out

914
00:54:46,240 --> 00:54:49,540
as about one because they are not the dimension of meaning.

915
00:54:49,540 --> 00:54:53,960
And so, it seems like what we want is we want to have ratio of

916
00:54:53,960 --> 00:54:58,660
co-occurrence probabilities become linear and our space.

917
00:54:58,660 --> 00:55:00,515
And then we're in a good business.

918
00:55:00,515 --> 00:55:03,340
And so that's what we want to set about doing.

919
00:55:03,340 --> 00:55:05,410
Well, how can you do that?

920
00:55:05,410 --> 00:55:07,360
Well, the way you can do that,

921
00:55:07,360 --> 00:55:15,775
is by if you can make the dot products equal to the log of the co-occurrence probability,

922
00:55:15,775 --> 00:55:19,450
then immediately you get the fact that when you have

923
00:55:19,450 --> 00:55:26,605
a vector difference it turns into a ratio of the co-occurrence probabilities.

924
00:55:26,605 --> 00:55:30,640
And so, essentially the whole of the model is that we

925
00:55:30,640 --> 00:55:34,630
want to have dot products or logs of co-occurrence probabilities.

926
00:55:34,630 --> 00:55:36,940
And so, that's what we do.

927
00:55:36,940 --> 00:55:39,700
So, here is our objective function here

928
00:55:39,700 --> 00:55:43,325
and it's made to look a little bit more complicated.

929
00:55:43,325 --> 00:55:47,095
But essentially we've got this squared loss here

930
00:55:47,095 --> 00:55:51,820
and then we wanting to say the dot-product should be as similar

931
00:55:51,820 --> 00:55:53,980
as possible to the log of

932
00:55:53,980 --> 00:55:57,190
co-occurrence probability and so you'll they'll

933
00:55:57,190 --> 00:56:00,745
be lost to the extent that they're not the same,

934
00:56:00,745 --> 00:56:07,180
but we kind of complexify it a little by putting in bias terms for both of the two words.

935
00:56:07,180 --> 00:56:10,180
Because maybe the word is just overall common and likes to

936
00:56:10,180 --> 00:56:13,660
co-occur things or uncommon or does end.

937
00:56:13,660 --> 00:56:18,160
And then we do one more little trick because every [inaudible] does tricks to make the performance

938
00:56:18,160 --> 00:56:23,185
better is that we also use this f-function in front,

939
00:56:23,185 --> 00:56:25,510
so that we're sort of capping the effect that

940
00:56:25,510 --> 00:56:30,370
very common word pairs can have on the performance of the system.

941
00:56:30,370 --> 00:56:35,155
Okay. And so that gave us the GloVe model of word vectors.

942
00:56:35,155 --> 00:56:40,165
And theoretically, the interest of this was,

943
00:56:40,165 --> 00:56:43,240
you know, a lot of the preceding literature had been there had

944
00:56:43,240 --> 00:56:46,515
been these count methods and there had been these prediction methods.

945
00:56:46,515 --> 00:56:49,900
And the hope was that this could sort of unify the

946
00:56:49,900 --> 00:56:53,245
two by showing you how you could have a method that

947
00:56:53,245 --> 00:56:58,675
is estimated simply of a count matrix but it's done in the same kind of

948
00:56:58,675 --> 00:57:01,510
iterative loss based estimation method that's

949
00:57:01,510 --> 00:57:04,960
used for the neural methods to get good word vectors.

950
00:57:04,960 --> 00:57:07,390
And this also worked to give good word vectors.

951
00:57:07,390 --> 00:57:10,490
So here's GloVe results for the word frog.

952
00:57:10,490 --> 00:57:13,985
And frogs and toad are obvious.

953
00:57:13,985 --> 00:57:16,675
But there are these different kinds of words, uh,

954
00:57:16,675 --> 00:57:21,205
various kinds of pretty tree frogs and things like that.

955
00:57:21,205 --> 00:57:26,440
Okay. Um, so I'll then go from here and say a little

956
00:57:26,440 --> 00:57:31,180
bit more about some of the work on evaluating word vectors.

957
00:57:31,180 --> 00:57:36,505
And this is maybe also a chance just talk a little bit about evaluation altogether.

958
00:57:36,505 --> 00:57:40,165
So, normally in NLP when we do a valuation,

959
00:57:40,165 --> 00:57:45,320
the first thing that comes up is intrinsic versus extrinsic evaluation.

960
00:57:45,320 --> 00:57:50,170
So, normally if there's something we trying to do like model, um,

961
00:57:50,170 --> 00:57:54,790
word similarity with word vectors or we're trying to, um,

962
00:57:54,790 --> 00:57:57,889
put parts of speech on words or something,

963
00:57:57,889 --> 00:58:02,860
we can just have an intrinsic evaluation of saying how good a job did you get.

964
00:58:02,860 --> 00:58:05,105
Are you guessing the right part of speech?

965
00:58:05,105 --> 00:58:07,480
Are you putting synonyms close together?

966
00:58:07,480 --> 00:58:12,265
And that's sort of normally very easy to do and fast to compute.

967
00:58:12,265 --> 00:58:16,585
And it's useful to do because it helps us understand the system.

968
00:58:16,585 --> 00:58:20,440
On the other hand, a lot of the time those intrinsic evaluations,

969
00:58:20,440 --> 00:58:26,285
it's not very clear where- where they're having done well on that task is really going to

970
00:58:26,285 --> 00:58:30,460
help us build the amazing natural language understanding robots

971
00:58:30,460 --> 00:58:32,230
that we so ardently desire.

972
00:58:32,230 --> 00:58:37,660
Um, so, people are also very interested in extrinsic evaluations.

973
00:58:37,660 --> 00:58:41,470
And so extrinsically is then saying well suppose you use

974
00:58:41,470 --> 00:58:47,455
this new stuff in a real system doesn't make performance go up.

975
00:58:47,455 --> 00:58:50,200
And it's then sort of definitional what counts

976
00:58:50,200 --> 00:58:52,835
to you as a real system that normally that's

977
00:58:52,835 --> 00:58:58,675
meaning it's some application that human beings actually care about and liked to use.

978
00:58:58,675 --> 00:59:02,980
So that's something like web search, or question answering,

979
00:59:02,980 --> 00:59:07,030
or phone dialog system or something like that, um,

980
00:59:07,030 --> 00:59:11,135
hat you can put it into that system and the numbers get- go up.

981
00:59:11,135 --> 00:59:13,300
So, that seems what you want to do.

982
00:59:13,300 --> 00:59:15,575
You want to have stuff that works in real tasks.

983
00:59:15,575 --> 00:59:20,500
Of course, there are sort of on the other hand a lot of things are a lot harder than.

984
00:59:20,500 --> 00:59:27,340
So much more work to do such an evaluation and run different variance of a system.

985
00:59:27,340 --> 00:59:30,580
And even when the results, uh,

986
00:59:30,580 --> 00:59:34,600
poor or great sometimes it's hard to diagnose.

987
00:59:34,600 --> 00:59:39,095
You know, if- if your great new word vectors don't work better in the system, you know,

988
00:59:39,095 --> 00:59:41,800
it might be for sort of some extraneous reason about

989
00:59:41,800 --> 00:59:44,825
how the system was built at sort of hiding all your magic.

990
00:59:44,825 --> 00:59:49,300
And if you just change the rest of the system and suddenly show its good effects.

991
00:59:49,300 --> 00:59:51,130
So, it's kind of hard to do,

992
00:59:51,130 --> 00:59:53,035
um, sort of, um,

993
00:59:53,035 --> 00:59:58,060
apportionment of goodness and badness Okay.

994
00:59:58,060 --> 01:00:01,780
So, um, so, today I'm mainly going to say a little bit more about

995
01:00:01,780 --> 01:00:05,980
these intrinsic word vector evaluations that we've talked about.

996
01:00:05,980 --> 01:00:09,835
So we've talked quite a bit about these analogies.

997
01:00:09,835 --> 01:00:12,370
So if we're actually working out the analogies,

998
01:00:12,370 --> 01:00:16,210
it turns out that normally what people are doing is working out

999
01:00:16,210 --> 01:00:20,890
a cosine distance and angle between, um,

1000
01:00:20,890 --> 01:00:23,740
different word candidates, um,

1001
01:00:23,740 --> 01:00:26,950
to work out which is the word that solves the analogy which

1002
01:00:26,950 --> 01:00:30,565
is an Norbert little tiny wrinkle of difference there.

1003
01:00:30,565 --> 01:00:34,000
And there's also one other trick that people commonly use.

1004
01:00:34,000 --> 01:00:37,620
They forbid the system from returning one of the three word she put

1005
01:00:37,620 --> 01:00:41,620
into the analogy Okay.

1006
01:00:41,620 --> 01:00:45,265
But nevertheless, so, this is something that you can evaluate.

1007
01:00:45,265 --> 01:00:48,520
Here now some GloVe visualizations.

1008
01:00:48,520 --> 01:00:52,240
And so these GloVe visualizations show exactly the same kind of

1009
01:00:52,240 --> 01:00:57,970
linearity property that Doug Rohde discovered which means that analogy's work.

1010
01:00:57,970 --> 01:00:59,259
Sort of by construction,

1011
01:00:59,259 --> 01:01:03,295
because our vector space wanted to make meaning components linear.

1012
01:01:03,295 --> 01:01:05,275
So, this is then, um,

1013
01:01:05,275 --> 01:01:08,425
showing a gender display.

1014
01:01:08,425 --> 01:01:13,450
This is showing one between companies and their CEOs, kind of cool.

1015
01:01:13,450 --> 01:01:16,150
And you can also do more syntactic facts.

1016
01:01:16,150 --> 01:01:17,770
So this is showing, um,

1017
01:01:17,770 --> 01:01:21,440
positive comparative and superlative of adjectives.

1018
01:01:21,440 --> 01:01:28,645
Yeah. So, Tomas Mikolov came up with this idea of doing these analogy tasks.

1019
01:01:28,645 --> 01:01:32,815
And so he built a data-set with a lot of analogies in it.

1020
01:01:32,815 --> 01:01:37,330
It's sort of- it's a bit of a weirdo data-set because it's sort of tests a few

1021
01:01:37,330 --> 01:01:42,380
random different things which may have been things that his system worked well on, um,

1022
01:01:42,380 --> 01:01:46,185
but you know, it tests countries and capitals,

1023
01:01:46,185 --> 01:01:52,780
country, cities and states, countries and currency.

1024
01:01:52,780 --> 01:01:55,535
So there are a bunch of semantic things that tests.

1025
01:01:55,535 --> 01:01:58,630
And then there are some, um,

1026
01:01:58,630 --> 01:02:01,360
syntactic things that tests so bad, worst,

1027
01:02:01,360 --> 01:02:04,535
fast fastest for superlatives.

1028
01:02:04,535 --> 01:02:07,780
But, you know, even some of the ones I was showing before, you know,

1029
01:02:07,780 --> 01:02:10,210
there's no- there's no Obama is to

1030
01:02:10,210 --> 01:02:15,400
Clinton kind of ones that are actually in this evaluation set.

1031
01:02:15,400 --> 01:02:18,579
Um, here's a big table of results,

1032
01:02:18,579 --> 01:02:20,830
um, that comes from our GloVe paper.

1033
01:02:20,830 --> 01:02:25,270
So not surprisingly the GloVe paper perform best in this evaluation.

1034
01:02:25,270 --> 01:02:28,150
Because that was our paper. Um, [LAUGHTER]

1035
01:02:28,150 --> 01:02:31,450
[LAUGHTER] But I mean perhaps- you know,

1036
01:02:31,450 --> 01:02:34,180
perhaps the things to start to notice is,

1037
01:02:34,180 --> 01:02:37,660
yeah, if you just do a plain SVD on counts.

1038
01:02:37,660 --> 01:02:44,170
You know that that works abominably badly for these, um, analogy tasks.

1039
01:02:44,170 --> 01:02:46,825
But, you know, kind of as Doug Rohde showed,

1040
01:02:46,825 --> 01:02:53,875
if you start then doing manipulations of the count matrix before you do an SVD,

1041
01:02:53,875 --> 01:02:55,690
you can actually start to produce

1042
01:02:55,690 --> 01:03:01,090
an SVD based system that actually performs quite well on these tasks.

1043
01:03:01,090 --> 01:03:05,410
Um, you know, not badly against other things.

1044
01:03:05,410 --> 01:03:07,990
Um, other things that you will discover,

1045
01:03:07,990 --> 01:03:10,480
right at the top there are a 100 dimensional ones,

1046
01:03:10,480 --> 01:03:13,360
and at the bottom there are some 1000 dimensional ones,

1047
01:03:13,360 --> 01:03:15,355
and other 300 dimensional ones.

1048
01:03:15,355 --> 01:03:17,725
At least when you're training on a big amount of text,

1049
01:03:17,725 --> 01:03:20,530
bigger dimensionality definitely works better.

1050
01:03:20,530 --> 01:03:22,420
And I'll come back to that in a minute.

1051
01:03:22,420 --> 01:03:25,630
Um, the amount of text makes a difference as well, right?

1052
01:03:25,630 --> 01:03:30,520
So we're going up from- so one to 1,5 billion words at the beginning,

1053
01:03:30,520 --> 01:03:34,480
to these ones down here are being trained over 42 billion words of text,

1054
01:03:34,480 --> 01:03:40,450
and perhaps unsurprisingly, the 42 billion words of texts ones work better.

1055
01:03:40,450 --> 01:03:42,660
Um, so it's big data.

1056
01:03:42,660 --> 01:03:45,525
Um, here are a couple more steps from this paper.

1057
01:03:45,525 --> 01:03:50,100
So this is a graph of dimensionality and what the performance is.

1058
01:03:50,100 --> 01:03:52,979
So for the three lines the green one's semantic,

1059
01:03:52,979 --> 01:03:57,715
the blue one's the syntactic analogies and so red's the overall score.

1060
01:03:57,715 --> 01:04:00,790
So sort of what you see is up to dimensionality

1061
01:04:00,790 --> 01:04:04,375
300 things that clearly increasing quite a bit,

1062
01:04:04,375 --> 01:04:06,085
and then it gets fairly flat,

1063
01:04:06,085 --> 01:04:09,325
which is precisely why you find a lot of word vectors,

1064
01:04:09,325 --> 01:04:11,770
um, that are of dimensionality 300.

1065
01:04:11,770 --> 01:04:15,505
Um, this one's showing what window size.

1066
01:04:15,505 --> 01:04:20,680
So this is sort of what we talked about symmetric on both sides window size,

1067
01:04:20,680 --> 01:04:23,455
and as it goes from 246810.

1068
01:04:23,455 --> 01:04:25,240
And sort of what you see is,

1069
01:04:25,240 --> 01:04:29,635
if you use a very small window like two, that actually works.

1070
01:04:29,635 --> 01:04:33,610
That the, the syntactic prediction is stronger because well,

1071
01:04:33,610 --> 01:04:35,770
syntactic effects are very local.

1072
01:04:35,770 --> 01:04:37,345
Whereas as you go out,

1073
01:04:37,345 --> 01:04:39,805
the semantic prediction gets better and better.

1074
01:04:39,805 --> 01:04:42,370
Actually this syntactic gets a bit better as well,

1075
01:04:42,370 --> 01:04:45,100
but it's especially the semantic that gains.

1076
01:04:45,100 --> 01:04:50,080
Um, the right graph shows that if you only use context on one side,

1077
01:04:50,080 --> 01:04:52,570
um, your numbers aren't as good.

1078
01:04:52,570 --> 01:05:00,700
Okay, um, so, I sort of just wanted to sort of sneak in a little cameos of a couple of,

1079
01:05:00,700 --> 01:05:03,040
um, recent bits of work,

1080
01:05:03,040 --> 01:05:05,815
as sort of a first of what things people are doing,

1081
01:05:05,815 --> 01:05:07,570
um, with word vectors.

1082
01:05:07,570 --> 01:05:09,670
Um, so this one, um,

1083
01:05:09,670 --> 01:05:12,475
was actually by two Stanford people.

1084
01:05:12,475 --> 01:05:15,010
Um, now the best- this would be the best story.

1085
01:05:15,010 --> 01:05:17,995
If I could say that this was a final project,

1086
01:05:17,995 --> 01:05:19,540
um, in this class last year,

1087
01:05:19,540 --> 01:05:21,025
but unfortunately that's not true.

1088
01:05:21,025 --> 01:05:25,330
This paper has nothing to do with this class [LAUGHTER].

1089
01:05:25,330 --> 01:05:26,710
But it-- right.

1090
01:05:26,710 --> 01:05:30,490
Um, Zin Yin and Yuanyuan,

1091
01:05:30,490 --> 01:05:33,910
um, actually had, um,

1092
01:05:33,910 --> 01:05:37,690
some sort of clever and very mathy ideas,

1093
01:05:37,690 --> 01:05:41,200
where they're using matrix perturbation theory.

1094
01:05:41,200 --> 01:05:44,140
Um, and sort of just showing how, um,

1095
01:05:44,140 --> 01:05:49,030
dimensionality in word vectors actually sort of feeds into the bias-variance trade-off.

1096
01:05:49,030 --> 01:05:50,335
If you've seen that,

1097
01:05:50,335 --> 01:05:52,825
um, in other parts of machine learning.

1098
01:05:52,825 --> 01:05:56,185
And I'm not even going to attempt to explain their paper.

1099
01:05:56,185 --> 01:05:57,535
Um, but here it is,

1100
01:05:57,535 --> 01:05:59,230
that they did really well with this paper,

1101
01:05:59,230 --> 01:06:01,570
they gone all talk in Europe's from it.

1102
01:06:01,570 --> 01:06:03,580
Um, and so- but there's sort of

1103
01:06:03,580 --> 01:06:07,720
an interesting result of what you see with these word vectors,

1104
01:06:07,720 --> 01:06:10,015
which is in a way kind of surprising.

1105
01:06:10,015 --> 01:06:16,210
So this is showing doing word vector dimensions from zero up to 10,000.

1106
01:06:16,210 --> 01:06:19,270
So we're going way higher than we talked about before.

1107
01:06:19,270 --> 01:06:23,440
And so what you discover which people have known for ages is,

1108
01:06:23,440 --> 01:06:27,745
that there's sort of a little blip that somewhere around two or 300,

1109
01:06:27,745 --> 01:06:30,235
which seems to optimize performance.

1110
01:06:30,235 --> 01:06:32,275
So, I've used those sizes.

1111
01:06:32,275 --> 01:06:35,920
But the thing that they were sort of doing a lot of their theory about,

1112
01:06:35,920 --> 01:06:38,185
and it's kind of surprising is, well,

1113
01:06:38,185 --> 01:06:42,250
surely if you have a humongous humongous number, like,

1114
01:06:42,250 --> 01:06:44,290
if you are using 10,000,

1115
01:06:44,290 --> 01:06:46,720
um, dimensional vectors, you know,

1116
01:06:46,720 --> 01:06:53,320
you're trying to estimate another two orders of magnitude more numbers for every word,

1117
01:06:53,320 --> 01:06:56,455
surely things should just fall apart, um,

1118
01:06:56,455 --> 01:06:59,920
because you've got hopelessly many parameters relative to

1119
01:06:59,920 --> 01:07:04,060
the amount of training data that you're trying to estimate these numbers from.

1120
01:07:04,060 --> 01:07:07,330
And so the interesting result that they show is,

1121
01:07:07,330 --> 01:07:10,270
that things don't fall apart.

1122
01:07:10,270 --> 01:07:15,280
Um, and that you can essentially go out to these huge huge dimensionalities,

1123
01:07:15,280 --> 01:07:17,365
and the performance stays flat.

1124
01:07:17,365 --> 01:07:19,285
And that they've got a lot of theory,

1125
01:07:19,285 --> 01:07:24,670
sort of for predicting why that that's actually going to end up being the case.

1126
01:07:24,670 --> 01:07:26,785
Um, yeah.

1127
01:07:26,785 --> 01:07:28,975
So for training these models iteratively,

1128
01:07:28,975 --> 01:07:33,775
this is- orange is showing, um, GloVe training.

1129
01:07:33,775 --> 01:07:36,370
You know, they keep on getting better for a while.

1130
01:07:36,370 --> 01:07:37,870
So you know, just go out,

1131
01:07:37,870 --> 01:07:41,020
go sleep and see in the morning how it's doing, right?

1132
01:07:41,020 --> 01:07:42,550
So that if you were running it, um,

1133
01:07:42,550 --> 01:07:47,290
for 24 hours your numbers are better than if you only ran it for six hours.

1134
01:07:47,290 --> 01:07:51,610
Um, and that's true for a lot of deep learning models, sorry.

1135
01:07:51,610 --> 01:07:54,730
So this is the key reason why you don't want

1136
01:07:54,730 --> 01:07:57,910
to start your assignment the night before it's due.

1137
01:07:57,910 --> 01:08:00,880
Because even if you program it perfectly,

1138
01:08:00,880 --> 01:08:03,865
you might just not have enough time for it to run,

1139
01:08:03,865 --> 01:08:07,430
um, so that you produce good numbers at the end of it.

1140
01:08:08,160 --> 01:08:12,280
Um, okay. Uh, yeah so,

1141
01:08:12,280 --> 01:08:14,965
so couple of more, um,

1142
01:08:14,965 --> 01:08:17,710
things, on that, um.

1143
01:08:17,710 --> 01:08:21,850
Yes. So, um, what are we showing here?

1144
01:08:21,850 --> 01:08:26,185
So these are again semantics in tactic and overall numbers.

1145
01:08:26,185 --> 01:08:29,950
So there are sort of two things that are sort of being mixed together here.

1146
01:08:29,950 --> 01:08:33,010
One is, if we just look at the overall numbers,

1147
01:08:33,010 --> 01:08:35,095
they're highest over here, um,

1148
01:08:35,095 --> 01:08:39,730
which is this 42 billion Common Crawl web-pages corpus,

1149
01:08:39,730 --> 01:08:42,085
that gives us the highest overall number.

1150
01:08:42,085 --> 01:08:46,735
But there's sort of something else that's interesting in this graph, which is,

1151
01:08:46,735 --> 01:08:51,730
um, that using Wikipedia works frequently well.

1152
01:08:51,730 --> 01:08:57,040
So that you actually find that 1,6 billion tokens of Wikipedia works

1153
01:08:57,040 --> 01:09:03,640
better than 4,3 billion tokens of News-wire newspaper article data.

1154
01:09:03,640 --> 01:09:08,125
And so I, I think that's sort of actually make sense,

1155
01:09:08,125 --> 01:09:09,280
which is well, you know,

1156
01:09:09,280 --> 01:09:11,620
the job of encyclopedias is to just sort of

1157
01:09:11,620 --> 01:09:14,860
explain concepts and how they relate to each other, right?

1158
01:09:14,860 --> 01:09:16,870
So that encyclopedias are

1159
01:09:16,870 --> 01:09:22,000
just much more expository text that show all the connections between things,

1160
01:09:22,000 --> 01:09:27,730
whereas newspapers in general aren't trying to expose at how things fit together.

1161
01:09:27,730 --> 01:09:29,530
They're just telling you about, you know,

1162
01:09:29,530 --> 01:09:32,980
who got shot dead last night or something like that, right?

1163
01:09:32,980 --> 01:09:37,600
So, um, so this is sort of interesting fact, um,

1164
01:09:37,600 --> 01:09:39,955
that this Wikipedia data kind of really,

1165
01:09:39,955 --> 01:09:42,655
it sort of is differentially useful, um,

1166
01:09:42,655 --> 01:09:46,180
for, um, making word vectors.

1167
01:09:46,180 --> 01:09:48,385
And you know, in fact, you know,

1168
01:09:48,385 --> 01:09:53,935
when we did very well without GloVe word vectors and lots of people use those.

1169
01:09:53,935 --> 01:09:58,060
You know, I think actually one of the reasons why they work so well is that

1170
01:09:58,060 --> 01:10:03,880
the original word2vec vectors that Google distributes are built only on Google News data,

1171
01:10:03,880 --> 01:10:05,770
where else sort of have this,

1172
01:10:05,770 --> 01:10:07,915
um, Wikipedia data inside them.

1173
01:10:07,915 --> 01:10:11,650
Okay, um, rushing ahead.

1174
01:10:11,650 --> 01:10:14,500
Um, yes, so the- there's all the work on analogy,

1175
01:10:14,500 --> 01:10:20,380
but the other more basic evaluation is this one of capturing similarity judgments.

1176
01:10:20,380 --> 01:10:23,020
And I haven't said much about this, but you know,

1177
01:10:23,020 --> 01:10:28,135
there is this sort of large sub-literature in the psychology community,

1178
01:10:28,135 --> 01:10:32,935
where people have wanted to model humans judgments of similarity.

1179
01:10:32,935 --> 01:10:36,265
So like a good psych person, what you do,

1180
01:10:36,265 --> 01:10:39,655
is you find your classroom of Psych one undergrads,

1181
01:10:39,655 --> 01:10:42,520
and you show them pairs of words and say rate

1182
01:10:42,520 --> 01:10:45,745
these things for similarity on a scale of one to 10.

1183
01:10:45,745 --> 01:10:48,265
And lots of that data has been collected,

1184
01:10:48,265 --> 01:10:50,875
and you work out the mean over human beings,

1185
01:10:50,875 --> 01:10:55,255
and they give numbers like this of tiger and cat, 7,35.

1186
01:10:55,255 --> 01:10:58,945
Tiger's similar to Tiger 10, book and paper,

1187
01:10:58,945 --> 01:11:01,225
plane and car, stock and phone,

1188
01:11:01,225 --> 01:11:03,895
stock and CD, and you get numbers.

1189
01:11:03,895 --> 01:11:07,000
So then, what we're doing is wanting to say,

1190
01:11:07,000 --> 01:11:13,140
well let's use distance in the space to map directly onto these similarity judgments,

1191
01:11:13,140 --> 01:11:15,540
and how well does it map?

1192
01:11:15,540 --> 01:11:18,570
And so that's sort of similarity judging has

1193
01:11:18,570 --> 01:11:22,350
also then being used for evaluating these systems.

1194
01:11:22,350 --> 01:11:24,570
So again, here are a lot of models.

1195
01:11:24,570 --> 01:11:26,550
This is again from our GloVe paper.

1196
01:11:26,550 --> 01:11:29,500
But so there are these various similarity data-sets.

1197
01:11:29,500 --> 01:11:36,190
So one of the best-known ones that I had on the slide before is this, um, Wordsim 353.

1198
01:11:36,190 --> 01:11:38,140
It has 353, um,

1199
01:11:38,140 --> 01:11:40,120
different ones in it,

1200
01:11:40,120 --> 01:11:44,140
and so you are sort of then modeling a correlation

1201
01:11:44,140 --> 01:11:49,225
between your judgments of similarity and the ones that came from the human beings.

1202
01:11:49,225 --> 01:11:52,960
Okay. Two more things I want to say. Um, yes.

1203
01:11:52,960 --> 01:11:56,860
So, we had that problem right at the beginning

1204
01:11:56,860 --> 01:12:01,765
of Clinton and how that could be various people.

1205
01:12:01,765 --> 01:12:07,030
And that's perhaps in some sense the simplest case of words being ambiguous,

1206
01:12:07,030 --> 01:12:10,585
when you have names which have reference to different people.

1207
01:12:10,585 --> 01:12:13,810
Um, but it's not only true of names.

1208
01:12:13,810 --> 01:12:15,595
So by and large,

1209
01:12:15,595 --> 01:12:21,820
words in human languages are ambiguous and have lots of meanings.

1210
01:12:21,820 --> 01:12:24,640
Um, that's especially true of common words.

1211
01:12:24,640 --> 01:12:26,500
They always have lots of meaning.

1212
01:12:26,500 --> 01:12:30,625
It's especially true of words that have existed for a long time.

1213
01:12:30,625 --> 01:12:34,315
It's not true of new very technical words, you know, carcinoma.

1214
01:12:34,315 --> 01:12:35,965
I think that only has one meaning.

1215
01:12:35,965 --> 01:12:37,090
Um, but, you know,

1216
01:12:37,090 --> 01:12:40,585
if you think of any relatively, um,

1217
01:12:40,585 --> 01:12:43,690
common word and starts, um,

1218
01:12:43,690 --> 01:12:45,955
scratching your head for a moment,

1219
01:12:45,955 --> 01:12:48,430
you'll find it has lots of meanings.

1220
01:12:48,430 --> 01:12:50,680
I- maybe this isn't even such a common word,

1221
01:12:50,680 --> 01:12:53,110
but my random word I've got here is Pike.

1222
01:12:53,110 --> 01:12:55,270
Um, pike has lots of meanings,

1223
01:12:55,270 --> 01:12:57,610
it has meanings like?

1224
01:12:57,610 --> 01:12:59,230
Fish.

1225
01:12:59,230 --> 01:13:00,760
Fish, it's a kind of fish, yeah.

1226
01:13:00,760 --> 01:13:02,140
So there's a fish that's a pike.

1227
01:13:02,140 --> 01:13:03,220
What else is a pike?

1228
01:13:03,220 --> 01:13:04,480
A large spear.

1229
01:13:04,480 --> 01:13:05,740
A large spear.

1230
01:13:05,740 --> 01:13:08,200
Yes, so a large spear is a pike.

1231
01:13:08,200 --> 01:13:09,940
Other kinds of pike's?

1232
01:13:09,940 --> 01:13:10,535
Gymnastics move.

1233
01:13:10,535 --> 01:13:10,920
It's a road.

1234
01:13:10,920 --> 01:13:13,350
Gymnastics move or in diving move.

1235
01:13:13,350 --> 01:13:14,595
It's a road.

1236
01:13:14,595 --> 01:13:18,240
Um, yeah. Um, so there are lots of meanings.

1237
01:13:18,240 --> 01:13:19,845
Um, there are other meanings.

1238
01:13:19,845 --> 01:13:21,240
Um, in Australian English,

1239
01:13:21,240 --> 01:13:23,760
pike is also used as a verb to mean,

1240
01:13:23,760 --> 01:13:25,935
um, to pull out from doing something.

1241
01:13:25,935 --> 01:13:31,590
Like, "We were all going to go out to a nightclub later, but Joe piked."

1242
01:13:31,590 --> 01:13:35,160
[LAUGHTER] Um, I don't think that usage is common in this country,

1243
01:13:35,160 --> 01:13:38,255
but, um, you can try that, um. [LAUGHTER]

1244
01:13:38,255 --> 01:13:41,355
Right. But lots of meanings and, you know,

1245
01:13:41,355 --> 01:13:44,940
this isn't only true of the word pike, right?

1246
01:13:44,940 --> 01:13:47,540
Pick any other simple word, right?

1247
01:13:47,540 --> 01:13:52,040
You can pick a word like shell or field or house or make,

1248
01:13:52,040 --> 01:13:55,065
you know, they have lots of meanings when it comes down to it.

1249
01:13:55,065 --> 01:13:56,550
So, you know, but, uh,

1250
01:13:56,550 --> 01:14:01,069
how can this work if we just have one meaningful words?

1251
01:14:01,069 --> 01:14:04,380
And that's the interesting question and it was something that

1252
01:14:04,380 --> 01:14:08,120
[NOISE] we were actually interested in early on.

1253
01:14:08,120 --> 01:14:13,815
So, I'm even before the Word2Vec paper came out back in 2012,

1254
01:14:13,815 --> 01:14:15,910
um, we were playing around, um,

1255
01:14:15,910 --> 01:14:20,370
with neural word vectors and, um, we thought,

1256
01:14:20,370 --> 01:14:23,790
boy this is so broken having only one,

1257
01:14:23,790 --> 01:14:26,605
um, cents, for a word.

1258
01:14:26,605 --> 01:14:30,045
Why don't we come up with a model that has multiple sensors for a word?

1259
01:14:30,045 --> 01:14:33,565
And so we did that and we did it in a pretty crude way,

1260
01:14:33,565 --> 01:14:34,920
I guess, [NOISE] um,

1261
01:14:34,920 --> 01:14:36,710
the way we did it is say,

1262
01:14:36,710 --> 01:14:40,145
well, let's for each common word,

1263
01:14:40,145 --> 01:14:44,285
let's cluster all the contexts in which it occurs.

1264
01:14:44,285 --> 01:14:47,365
And then we'll see if there seem to be

1265
01:14:47,365 --> 01:14:51,980
multiple clear clusters by some criterion for that word.

1266
01:14:51,980 --> 01:14:56,220
And if so, we'll just sort of split the word into pseudo words.

1267
01:14:56,220 --> 01:14:59,180
So, if it seems like that there are five clusters,

1268
01:14:59,180 --> 01:15:00,960
um, for the word,

1269
01:15:00,960 --> 01:15:03,020
the example I meant to use here is jaguar.

1270
01:15:03,020 --> 01:15:05,090
Five clusters for the word jaguar,

1271
01:15:05,090 --> 01:15:08,240
I will just call them jaguar_1, jaguar_2, jaguar_3, four,

1272
01:15:08,240 --> 01:15:10,570
five, so it's just literally changed

1273
01:15:10,570 --> 01:15:13,945
the word in our corpus according to its cluster number.

1274
01:15:13,945 --> 01:15:16,990
And then we run our word vectoring algorithm and so we get

1275
01:15:16,990 --> 01:15:20,870
a representation that each of those sensors of the word.

1276
01:15:20,870 --> 01:15:22,380
And basically, that works,

1277
01:15:22,380 --> 01:15:24,570
right up the top is jaguar_1 next,

1278
01:15:24,570 --> 01:15:26,825
uh, luxury and convertible.

1279
01:15:26,825 --> 01:15:33,260
Um, here is, I guess there's a very old version of MacOS called Jaguar,

1280
01:15:33,260 --> 01:15:34,860
any remem- remember that one?

1281
01:15:34,860 --> 01:15:40,435
Um. Right. So, jaguars right next to software and Microsoft up there, so that's hopeful.

1282
01:15:40,435 --> 01:15:44,715
Um, here's the jaguar that's right next to the Hunter, um,

1283
01:15:44,715 --> 01:15:47,070
and I'm being confused on this one,

1284
01:15:47,070 --> 01:15:50,550
is jaguar as near solo musical keyboard and string.

1285
01:15:50,550 --> 01:15:53,545
Is there a band, [NOISE] a brand of keyboard called jaguar?

1286
01:15:53,545 --> 01:15:55,015
I'm not quite sure about that one,

1287
01:15:55,015 --> 01:15:57,380
but anyway, it's sort of basically works.

1288
01:15:57,380 --> 01:16:02,080
Um, but that was sort of crude and it's also perhaps problematic,

1289
01:16:02,080 --> 01:16:06,450
so a lot of time, the divisions between sensors aren't very clear, right?

1290
01:16:06,450 --> 01:16:10,330
A lot of sensors are actually related to each other and overlapping because

1291
01:16:10,330 --> 01:16:14,790
when how sensors normally arrive is that people stretch the meanings of words.

1292
01:16:14,790 --> 01:16:16,860
It's not that they just sort of randomly

1293
01:16:16,860 --> 01:16:19,685
wake up the next morning and say, "I know carpet.

1294
01:16:19,685 --> 01:16:23,005
I could also refer to that as stone," um,

1295
01:16:23,005 --> 01:16:25,990
and given a new sense to the word stone, right?

1296
01:16:25,990 --> 01:16:28,600
You so take something that you know about like

1297
01:16:28,600 --> 01:16:33,085
a web and you extend it metaphorically to other uses of webbing.

1298
01:16:33,085 --> 01:16:37,000
Um, so here's a perhaps more interesting things,

1299
01:16:37,000 --> 01:16:39,475
so this is the other Sanjeev Arora,

1300
01:16:39,475 --> 01:16:41,980
um, paper that I was going to mention.

1301
01:16:41,980 --> 01:16:44,710
So, that what happens if you don't,

1302
01:16:44,710 --> 01:16:50,095
um, if you don't have more than one cents for each word?

1303
01:16:50,095 --> 01:16:53,100
Well, effectively what you get is that

1304
01:16:53,100 --> 01:16:57,260
the word vector that you learn is what's referred to by

1305
01:16:57,260 --> 01:17:01,040
physicists and fancy people as a superposition

1306
01:17:01,040 --> 01:17:05,910
of the word vectors of the different sentence, different sensors.

1307
01:17:05,910 --> 01:17:09,995
By supersitio- superposition just means a weighted average.

1308
01:17:09,995 --> 01:17:14,930
Um, um, [LAUGHTER] so that effectively

1309
01:17:14,930 --> 01:17:17,040
my meaning of pike is sort of

1310
01:17:17,040 --> 01:17:20,865
a weighted average of the vectors for the different sensors of pike,

1311
01:17:20,865 --> 01:17:24,645
and the components are just weighted by their frequency.

1312
01:17:24,645 --> 01:17:28,265
Um, so that part maybe is perhaps not too surprising,

1313
01:17:28,265 --> 01:17:31,830
but the part that's really surprising is well,

1314
01:17:31,830 --> 01:17:34,375
if we just averaging these word vectors,

1315
01:17:34,375 --> 01:17:38,380
you'd think you couldn't get anything out of the average, right|?

1316
01:17:38,380 --> 01:17:43,180
Like if I tell you I'm thinking of two numbers and they're here,

1317
01:17:43,180 --> 01:17:46,010
weighted sum is 54,

1318
01:17:46,010 --> 01:17:47,870
what are my two numbers, right?

1319
01:17:47,870 --> 01:17:51,990
You are sort of really short of information to be able to answer my question.

1320
01:17:51,990 --> 01:17:53,610
But, well, you know,

1321
01:17:53,610 --> 01:17:56,820
for these word vectors, um,

1322
01:17:56,820 --> 01:18:02,625
we have these high dimensional spaces and even though there

1323
01:18:02,625 --> 01:18:07,980
are a lot of words that the space is so vast for thoughts dimensions,

1324
01:18:07,980 --> 01:18:13,535
that actual words or sensors are very sparse in that space.

1325
01:18:13,535 --> 01:18:17,650
And so it turns out that there's this whole literature on, um,

1326
01:18:17,650 --> 01:18:20,330
sparse coding, compressed sensing,

1327
01:18:20,330 --> 01:18:23,740
um, some of which has actually done by people in the stats department here,

1328
01:18:23,740 --> 01:18:29,325
um, which shows that in these cases where you have these sort of sparse,

1329
01:18:29,325 --> 01:18:32,115
um, codes in these high dimensional spaces,

1330
01:18:32,115 --> 01:18:36,885
you can actually commonly reconstruct out the components of a superposition,

1331
01:18:36,885 --> 01:18:40,240
even though all you've done is sort of done this weighted average,

1332
01:18:40,240 --> 01:18:45,010
and so, um, this paper looks at how you can do this and so they have,

1333
01:18:45,010 --> 01:18:48,105
um, these underlying meaning components,

1334
01:18:48,105 --> 01:18:49,910
and they sort of separated out.

1335
01:18:49,910 --> 01:18:52,810
So, tie has one meaning component,

1336
01:18:52,810 --> 01:18:55,975
there's in this space of trousers, blouse, waistcoat,

1337
01:18:55,975 --> 01:19:00,300
that makes sense, and other one in this meaning component of seasoned teams,

1338
01:19:00,300 --> 01:19:02,645
winning league, makes sense.

1339
01:19:02,645 --> 01:19:06,015
Um, scoreline goal with equalizer clinching scorers,

1340
01:19:06,015 --> 01:19:08,355
this one seems to overlap with this one a bit.

1341
01:19:08,355 --> 01:19:10,045
Um, but here tie,

1342
01:19:10,045 --> 01:19:13,800
this is sort of cable ties and wire ties and things like that.

1343
01:19:13,800 --> 01:19:17,245
So, they are actually able to pull out the different sense meanings,

1344
01:19:17,245 --> 01:19:21,055
um, from outside, out of the meaning of the word.

1345
01:19:21,055 --> 01:19:24,045
Um, so that is a kind of a cool thing.

1346
01:19:24,045 --> 01:19:26,085
I just wanna, um,

1347
01:19:26,085 --> 01:19:28,195
say one more thing.

1348
01:19:28,195 --> 01:19:32,010
Okay. [NOISE] All the evaluations so far was intrinsic,

1349
01:19:32,010 --> 01:19:35,735
um, you also might wanna do extrinsic evaluation.

1350
01:19:35,735 --> 01:19:40,010
Why, why word vectors excited people on NLP so much?

1351
01:19:40,010 --> 01:19:42,300
Is it turned out that having this meaning,

1352
01:19:42,300 --> 01:19:45,965
having this representation meaning just turned out to be

1353
01:19:45,965 --> 01:19:49,810
very useful and sort of improve all of your tasks after that.

1354
01:19:49,810 --> 01:19:52,280
Um, and so, um,

1355
01:19:52,280 --> 01:19:55,360
this is doing named entity recognition which is labeling

1356
01:19:55,360 --> 01:19:58,860
persons and locations and organizations, but, you know,

1357
01:19:58,860 --> 01:20:01,755
it's typical of many tasks of what people found,

1358
01:20:01,755 --> 01:20:06,610
was if you started with a model without sort of word representations

1359
01:20:06,610 --> 01:20:11,875
and you throw in your word vectors regardless of whether they were to vehicle GloVe ones,

1360
01:20:11,875 --> 01:20:15,505
just kind of your numbers go up a couple of percent or more?

1361
01:20:15,505 --> 01:20:19,570
And so word vectors were just sort of this useful source that you could

1362
01:20:19,570 --> 01:20:24,190
throw into any NLP system that you build and your numbers went up.

1363
01:20:24,190 --> 01:20:27,335
So, that there was just a very effective technology, um,

1364
01:20:27,335 --> 01:20:28,980
which actually did work in

1365
01:20:28,980 --> 01:20:34,490
basically any extrinsic tasks you type tried it on. Okay. Thanks a lot.

