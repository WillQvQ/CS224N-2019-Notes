1
00:00:04,430 --> 00:00:07,410
Okay. Hello everyone.

2
00:00:07,410 --> 00:00:11,265
[LAUGHTER] Okay we should get started.

3
00:00:11,265 --> 00:00:14,640
Um, they're actually are still quite a few seats left.

4
00:00:14,640 --> 00:00:15,960
If you wanna be really bold,

5
00:00:15,960 --> 00:00:18,525
there are a couple of seats right in front of me in the front row.

6
00:00:18,525 --> 00:00:20,445
If you're less bolder a few over there.

7
00:00:20,445 --> 00:00:23,940
Um, but they're also on some of the rows are quite a few middle seat.

8
00:00:23,940 --> 00:00:28,080
So if people wanted to be really civic minded some people could sort of

9
00:00:28,080 --> 00:00:32,280
squeeze towards the edges and make  more accessible um,

10
00:00:32,280 --> 00:00:35,685
some of the seats that still exist in the classroom.

11
00:00:35,685 --> 00:00:39,435
Okay. Um, so, um,

12
00:00:39,435 --> 00:00:42,890
it's really exciting and great to see so many people here.

13
00:00:42,890 --> 00:00:47,390
So I'm a hearty welcome to CS224N and occasionally also

14
00:00:47,390 --> 00:00:52,625
known as Ling 284 which is Natural Language Processing with Deep Learning.

15
00:00:52,625 --> 00:00:55,420
Um, as just a sort of a  personal anecdote,

16
00:00:55,420 --> 00:00:59,720
is still sort of blows my mind that so many people turn up to this class these days.

17
00:00:59,720 --> 00:01:03,980
So, for about the first decade that I taught NLP here,

18
00:01:03,980 --> 00:01:08,180
you know the number of people I got each year was approximately 45.

19
00:01:08,180 --> 00:01:11,240
[LAUGHTER] So it's an order of [LAUGHTER] magnitude smaller than

20
00:01:11,240 --> 00:01:14,360
it is now but guess it says quite a lot

21
00:01:14,360 --> 00:01:17,450
on about what a revolutionary   impact

22
00:01:17,450 --> 00:01:20,870
that artificial intelligence in general and machine learning,

23
00:01:20,870 --> 00:01:25,600
deep learning, NLP are starting to have in modern society.

24
00:01:25,600 --> 00:01:28,860
Okay. So this is our plan for today.

25
00:01:28,860 --> 00:01:32,750
So, um, um, we're really gonna get straight down to business today.

26
00:01:32,750 --> 00:01:37,970
So they'll be a brief, very brief introduction some of the sort of course logistics,

27
00:01:37,970 --> 00:01:42,140
very brief discussion and talk about human language and

28
00:01:42,140 --> 00:01:46,370
word meaning and then we wanna get right into talking about um,

29
00:01:46,370 --> 00:01:50,540
the first thing that we're doing which is coming up with word vectors and looking

30
00:01:50,540 --> 00:01:55,010
at the word2vec algorithm and that will then sort of fill up the rest of the class.

31
00:01:55,010 --> 00:01:56,840
There are still two seats right in

32
00:01:56,840 --> 00:01:59,480
the front row for someone who wants to sit right in front of me,

33
00:01:59,480 --> 00:02:02,760
just letting you know [LAUGHTER].

34
00:02:02,760 --> 00:02:06,365
Okay. Okay. So here are the course logistics in brief.

35
00:02:06,365 --> 00:02:08,345
So I'm Christopher Manning,

36
00:02:08,345 --> 00:02:15,290
the person who bravely became the head TA is Abigail See is right there.

37
00:02:15,290 --> 00:02:18,920
And then we have quite a lot of wonderful TA's.

38
00:02:18,920 --> 00:02:22,700
To the people who are wonderful TA's just sort of stand up for one moment.

39
00:02:22,700 --> 00:02:26,810
So, um, [LAUGHTER] we have some sense for wonderful TAs.

40
00:02:26,810 --> 00:02:28,900
[LAUGHTER] Okay great.

41
00:02:28,900 --> 00:02:31,320
Um, okay.

42
00:02:31,320 --> 00:02:33,260
So you know when the lecture is because you made it

43
00:02:33,260 --> 00:02:37,100
here and so welcome also to SCPD people.

44
00:02:37,100 --> 00:02:41,300
This is also an SCPD class and you can watch it on video.

45
00:02:41,300 --> 00:02:44,300
But we love for Stanford students to turn

46
00:02:44,300 --> 00:02:47,300
up and show their beautiful faces in the classroom.

47
00:02:47,300 --> 00:02:52,810
Okay. So, um, the web-page has all the info about syllabus et cetera et cetera.

48
00:02:52,810 --> 00:02:56,175
Okay. So this class what do we hope to teach?

49
00:02:56,175 --> 00:02:59,240
So, one thing that we wanna teach is, uh, you know,

50
00:02:59,240 --> 00:03:02,495
an understanding of effective modern methods for deep learning.

51
00:03:02,495 --> 00:03:05,090
Starting off by reviewing some of the basics and then

52
00:03:05,090 --> 00:03:08,780
particularly talking about the kinds of techniques including um,

53
00:03:08,780 --> 00:03:11,450
recurrent networks and attention that are widely

54
00:03:11,450 --> 00:03:14,570
used for natural language processing models.

55
00:03:14,570 --> 00:03:18,770
A second thing we wanna teach is a big picture understanding of

56
00:03:18,770 --> 00:03:23,075
human languages and some of the difficulties in understanding and producing them.

57
00:03:23,075 --> 00:03:25,490
Of course if you wanna know a lot about human languages,

58
00:03:25,490 --> 00:03:29,060
there's a whole linguistics department and you can do a lot of courses of that.

59
00:03:29,060 --> 00:03:33,590
Um, but so I wanna give at least some appreciation so you have some clue of what are

60
00:03:33,590 --> 00:03:38,235
the challenges and difficulties and varieties of human languages.

61
00:03:38,235 --> 00:03:41,315
And then this is also kind of a practical class.

62
00:03:41,315 --> 00:03:44,960
Like we actually wanna teach you how you can

63
00:03:44,960 --> 00:03:49,670
build practical systems that work for some of the major parts of NLP.

64
00:03:49,670 --> 00:03:53,750
So if you go and get a job at one of those tech firms and they say "Hey,

65
00:03:53,750 --> 00:03:55,789
could you build us a named entity recognizer?"

66
00:03:55,789 --> 00:03:58,130
You can say "Sure, I can do that."

67
00:03:58,130 --> 00:04:00,530
And so for a bunch of problems,

68
00:04:00,530 --> 00:04:02,090
obviously we can't do everything,

69
00:04:02,090 --> 00:04:03,230
we're gonna do word meaning,

70
00:04:03,230 --> 00:04:07,579
dependency parsing, machine translation and you have an option to do question answering,

71
00:04:07,579 --> 00:04:10,340
I'm actually building systems for those.

72
00:04:10,340 --> 00:04:15,080
If you'd been talking to friends who did the class in the last couple of years,

73
00:04:15,080 --> 00:04:18,860
um, here are the differences for this year just to get things straight.

74
00:04:18,860 --> 00:04:21,830
Um, so we've updated some of the content of the course.

75
00:04:21,830 --> 00:04:26,285
So, uh, between me and guest lectures there's new content.

76
00:04:26,285 --> 00:04:28,470
Well that look bad.

77
00:04:29,030 --> 00:04:32,505
Wonder if that will keep happening, we'll find out.

78
00:04:32,505 --> 00:04:38,165
There's new content and on various topics that are sort of developing areas.

79
00:04:38,165 --> 00:04:41,300
One of the problems with this course is really big area of deep learning at

80
00:04:41,300 --> 00:04:44,755
the moment is still just developing really really quickly.

81
00:04:44,755 --> 00:04:47,480
So, it's sort of seems like one-year-old content is already

82
00:04:47,480 --> 00:04:51,290
things kind of data and we're trying to update things.

83
00:04:51,290 --> 00:04:54,140
A big change that we're making this year is we're

84
00:04:54,140 --> 00:04:56,930
having five-one week assignments instead of

85
00:04:56,930 --> 00:04:59,450
three-two week assignments at the beginning of

86
00:04:59,450 --> 00:05:02,795
the course and I'll say a bit more about that in a minute.

87
00:05:02,795 --> 00:05:06,215
Um, this year we're gonna use PyTorch instead of TensorFlow,

88
00:05:06,215 --> 00:05:08,860
and we'll talk about that more later too.

89
00:05:08,860 --> 00:05:13,880
Um, we're having the assignments due before class on either Tuesday or Thursday.

90
00:05:13,880 --> 00:05:16,585
So you're not distracted and can come to class.

91
00:05:16,585 --> 00:05:20,355
So starting off, um, yeah.

92
00:05:20,355 --> 00:05:22,680
So we're trying to give an easier,

93
00:05:22,680 --> 00:05:26,510
gentler ramp-up but on the other hand a fast ramp-up.

94
00:05:26,510 --> 00:05:29,555
So we've got this first assignment which is sort of easy, uh,

95
00:05:29,555 --> 00:05:34,040
but it's available right now and is due next Tuesday.

96
00:05:34,040 --> 00:05:37,460
And the final thing is we're not having a midterm this year.

97
00:05:37,460 --> 00:05:39,395
Um, okay.

98
00:05:39,395 --> 00:05:40,790
So this is what we're doing.

99
00:05:40,790 --> 00:05:44,300
So there are five of these assignments that I just mentioned.

100
00:05:44,300 --> 00:05:46,340
Um, So six percent for the first one,

101
00:05:46,340 --> 00:05:49,090
12 percent for each of the other ones,

102
00:05:49,090 --> 00:05:52,185
um, and, I already said that.

103
00:05:52,185 --> 00:05:54,230
We're gonna use gradescope for grading.

104
00:05:54,230 --> 00:05:56,780
It'll be really help out the TAs if you could use

105
00:05:56,780 --> 00:06:01,010
your SUnet ID as your gradescope account ID.

106
00:06:01,010 --> 00:06:04,205
Um, so then for the second part of the course,

107
00:06:04,205 --> 00:06:08,795
people do a final project and there are two choices for the final project.

108
00:06:08,795 --> 00:06:12,080
You can either do our default final project,

109
00:06:12,080 --> 00:06:14,030
which is a good option for many people,

110
00:06:14,030 --> 00:06:15,890
or you can do a custom final project and I'll

111
00:06:15,890 --> 00:06:19,010
talk about that in the more in the beginning.

112
00:06:19,010 --> 00:06:21,200
This is not working right.

113
00:06:21,200 --> 00:06:25,130
Um, and so then at the end we have

114
00:06:25,130 --> 00:06:30,425
a final poster presentation session at which your attendance is expected,

115
00:06:30,425 --> 00:06:34,580
and we're gonna be having that Wednesday in the evening.

116
00:06:34,580 --> 00:06:37,460
Probably not quite five hours but it'll be within that window,

117
00:06:37,460 --> 00:06:39,485
we'll work out the details in a bit.

118
00:06:39,485 --> 00:06:41,510
Three percent for participation,

119
00:06:41,510 --> 00:06:43,390
see the website for details.

120
00:06:43,390 --> 00:06:45,885
Six late days, um,

121
00:06:45,885 --> 00:06:50,330
collaboration, like always in computer science classes,

122
00:06:50,330 --> 00:06:55,340
we want you to do your own work and not borrow stuff from other people's Githubs and

123
00:06:55,340 --> 00:06:57,650
so we really do emphasize that you should

124
00:06:57,650 --> 00:07:01,150
read and pay attention to collaboration policies.

125
00:07:01,150 --> 00:07:04,700
Okay. So here's the high level plan for the problem sets.

126
00:07:04,700 --> 00:07:07,790
So, homework one available right now,

127
00:07:07,790 --> 00:07:10,130
is a hopefully easy on ramp.

128
00:07:10,130 --> 00:07:11,720
That's on iPython notebook,

129
00:07:11,720 --> 00:07:13,565
just help get everyone up to speed.

130
00:07:13,565 --> 00:07:17,750
Homework two is pure Python plus numpy but that

131
00:07:17,750 --> 00:07:22,190
will start to kind of teach you more about the sort of underlying,

132
00:07:22,190 --> 00:07:24,260
how do we do deep learning.

133
00:07:24,260 --> 00:07:29,430
If you're not so good or a bit rusty or never seen um,

134
00:07:29,430 --> 00:07:31,155
Python or numpy, um,

135
00:07:31,155 --> 00:07:34,730
we're gonna have an extra section on Friday.

136
00:07:34,730 --> 00:07:38,210
So Friday from 1:30 to 2:50 um,

137
00:07:38,210 --> 00:07:42,710
in Skilling Auditorium, we'll have a section that's a Python review.

138
00:07:42,710 --> 00:07:44,610
That's our only plan section at the moment,

139
00:07:44,610 --> 00:07:46,595
we're not gonna have a regular section.

140
00:07:46,595 --> 00:07:49,550
Um, so encourage to go to that and that will also be

141
00:07:49,550 --> 00:07:53,515
recorded for SCPD and available for video as well.

142
00:07:53,515 --> 00:07:56,790
Um, then Homework three um,

143
00:07:56,790 --> 00:08:00,850
will start us on using PyTorch.

144
00:08:00,850 --> 00:08:04,760
And then homeworks four and five we're then gonna be using

145
00:08:04,760 --> 00:08:08,720
py- PyTorch on GPU and we're actually gonna be using

146
00:08:08,720 --> 00:08:13,520
Microsoft Azure with big thank yous to the kind Microsoft Azure people who have

147
00:08:13,520 --> 00:08:19,165
sponsored our GPU computing for the last um, three years.

148
00:08:19,165 --> 00:08:24,995
Um, yes. So basically I mean all of modern deep learning has moved to the use

149
00:08:24,995 --> 00:08:30,589
of one or other of the large deep learning libraries like PyTorch TensorFlow,

150
00:08:30,589 --> 00:08:32,210
Chainer or MXNet um,

151
00:08:32,210 --> 00:08:36,440
et cetera and then doing the computing on GPU.

152
00:08:36,440 --> 00:08:38,600
So of course since we're in the one building,

153
00:08:38,600 --> 00:08:40,450
we should of course be using, um,

154
00:08:40,450 --> 00:08:42,620
GPUs [LAUGHTER] but I mean in general

155
00:08:42,620 --> 00:08:48,830
the so parallelisms scalability of GPUs is what's powered most of modern deep learning.

156
00:08:48,830 --> 00:08:50,720
Okay. The final project.

157
00:08:50,720 --> 00:08:55,460
So for the final project there are two things that you can do.

158
00:08:55,460 --> 00:09:00,665
So we have a default final project which is essentially our final project in a box.

159
00:09:00,665 --> 00:09:06,215
And so this is building a question answering system and we do it over the squad dataset.

160
00:09:06,215 --> 00:09:11,450
So what you build and how you can improve your performance is completely up to you.

161
00:09:11,450 --> 00:09:14,480
It is open-ended but it has an easier start,

162
00:09:14,480 --> 00:09:16,910
a clearly defined objective and we can

163
00:09:16,910 --> 00:09:19,775
have a leaderboard for how well things are working.

164
00:09:19,775 --> 00:09:24,680
Um, so if you don't have a clear research objective that can be a good choice for you

165
00:09:24,680 --> 00:09:29,600
or you can propose the custom Final Project and  assuming it's sensible,

166
00:09:29,600 --> 00:09:32,539
we will approve your custom final project,

167
00:09:32,539 --> 00:09:34,190
we will give you feedback, um,

168
00:09:34,190 --> 00:09:36,755
form someone as a mentor, um,

169
00:09:36,755 --> 00:09:42,410
and either way for only the final project we allow teams of one, two or three.

170
00:09:42,410 --> 00:09:45,200
For the homework should expect it to do them yourself.

171
00:09:45,200 --> 00:09:50,020
Of course you can chat to people in a general way about the problems.

172
00:09:50,020 --> 00:09:53,010
Okay. So that is the course.

173
00:09:53,010 --> 00:09:55,700
All good, and not even behind schedule yet.

174
00:09:55,700 --> 00:10:01,730
Okay. So the next section is human language and word meaning.Um.

175
00:10:01,730 --> 00:10:04,745
You know, if I was um,

176
00:10:04,745 --> 00:10:10,265
really going to tell you a lot about human language that would take a lot of time um,

177
00:10:10,265 --> 00:10:12,110
which I don't really have here.

178
00:10:12,110 --> 00:10:14,015
So I'm just going to tell you um,

179
00:10:14,015 --> 00:10:16,655
two anecdotes about human language.

180
00:10:16,655 --> 00:10:19,970
And the first is this XKCD cartoon.

181
00:10:19,970 --> 00:10:22,520
Um, and I mean this isn't,

182
00:10:22,520 --> 00:10:25,110
and I don't know why that's happening.

183
00:10:26,050 --> 00:10:28,250
I'm not sure what to make of that.

184
00:10:28,250 --> 00:10:34,070
Um, so, I actually really liked this XKCD cartoon.

185
00:10:34,070 --> 00:10:37,310
It's not one of the classic ones that you see most often around the place,

186
00:10:37,310 --> 00:10:42,140
but I actually think it says a lot about language and is worth thinking about.

187
00:10:42,140 --> 00:10:45,650
Like I think a lot of the time for the kind of people who come

188
00:10:45,650 --> 00:10:49,384
to this class who are mainly people like CS people,

189
00:10:49,384 --> 00:10:51,950
and EE people and random others.

190
00:10:51,950 --> 00:10:55,250
There's some other people I know since these people linguists and so on around.

191
00:10:55,250 --> 00:10:57,050
But for a lot of those people like,

192
00:10:57,050 --> 00:11:01,610
you've sort of spent your life looking at formal languages and the impression

193
00:11:01,610 --> 00:11:06,185
is that sort of human language as a sort of somehow a little bit broken formal languages,

194
00:11:06,185 --> 00:11:08,570
but there's really a lot more to it than that, right?

195
00:11:08,570 --> 00:11:11,165
That language is this amazing um,

196
00:11:11,165 --> 00:11:15,110
human created system that is used for

197
00:11:15,110 --> 00:11:19,520
all sorts of purposes and is adaptable to all sorts of purposes.

198
00:11:19,520 --> 00:11:23,750
So you can do everything from describing mathematics and human language

199
00:11:23,750 --> 00:11:28,520
um to sort of nuzzling up to your best friend and getting them to understand you better.

200
00:11:28,520 --> 00:11:31,910
So there's actually an amazing thing of human language. Anyway, I'll just read it.

201
00:11:31,910 --> 00:11:34,655
Um, so it's the first person,

202
00:11:34,655 --> 00:11:36,185
the dark haired person says,

203
00:11:36,185 --> 00:11:38,105
"Anyway, I could care less."

204
00:11:38,105 --> 00:11:40,010
And her friend says,

205
00:11:40,010 --> 00:11:42,440
"I think you mean you couldn't care less."

206
00:11:42,440 --> 00:11:46,490
Saying you could care less implies you care at least some amount.

207
00:11:46,490 --> 00:11:49,775
And the dark haired person says, "I don't know,

208
00:11:49,775 --> 00:11:54,590
we're these unbelievably complicated brains drifting through a void trying

209
00:11:54,590 --> 00:11:59,630
in vain to connect with one another by blindly flinging words out into the darkness."

210
00:11:59,630 --> 00:12:02,720
Every choice of phrasing and spelling, and tone,

211
00:12:02,720 --> 00:12:07,775
and timing carries countless signals and contexts and subtexts and more.

212
00:12:07,775 --> 00:12:11,435
And every listener interprets those signals in their own way.

213
00:12:11,435 --> 00:12:13,565
Language isn't a formal system,

214
00:12:13,565 --> 00:12:16,235
language is glorious chaos.

215
00:12:16,235 --> 00:12:20,750
You can never know for sure what any words will mean to anyone.

216
00:12:20,750 --> 00:12:26,150
All you can do is try to get better at guessing how your words affect people so

217
00:12:26,150 --> 00:12:28,790
you can have a chance of finding the ones that will make

218
00:12:28,790 --> 00:12:31,790
them feel something like what you want them to feel.

219
00:12:31,790 --> 00:12:34,235
Everything else is pointless.

220
00:12:34,235 --> 00:12:37,390
I assume you're giving me tips on how you interpret

221
00:12:37,390 --> 00:12:41,065
words because you want me to feel less alone.

222
00:12:41,065 --> 00:12:43,510
If so, thank you.

223
00:12:43,510 --> 00:12:45,585
That means a lot.

224
00:12:45,585 --> 00:12:48,440
But if you're just running my sentences past

225
00:12:48,440 --> 00:12:51,785
some mental checklist so you can show off how well you know it,

226
00:12:51,785 --> 00:12:53,180
then I could care less.

227
00:12:53,180 --> 00:13:02,825
[NOISE] Um, and so I think um,

228
00:13:02,825 --> 00:13:07,790
I think actually this has some nice messages about how language is this uncertain

229
00:13:07,790 --> 00:13:13,340
evolved system of communication but somehow we have enough agreed meaning that you know,

230
00:13:13,340 --> 00:13:15,500
we can kind of pretty much communicate.

231
00:13:15,500 --> 00:13:16,865
But we're doing some kind of you know

232
00:13:16,865 --> 00:13:20,540
probabilistic inference of guessing what people mean and we're

233
00:13:20,540 --> 00:13:22,070
using language not just for

234
00:13:22,070 --> 00:13:26,195
the information functions but for the social functions etc etc.

235
00:13:26,195 --> 00:13:32,310
Okay. And then here's my one other thought I had review about language.

236
00:13:33,490 --> 00:13:40,565
So, essentially if we want to have artificial intelligence that's intelligent,

237
00:13:40,565 --> 00:13:43,940
what we need to somehow get to the point of having

238
00:13:43,940 --> 00:13:48,560
compu- computers that have the knowledge of human beings, right?

239
00:13:48,560 --> 00:13:52,430
Because human beings have knowledge that gives them intelligence.

240
00:13:52,430 --> 00:13:55,460
And if you think about how we sort of

241
00:13:55,460 --> 00:13:59,270
convey knowledge around the place in our human world,

242
00:13:59,270 --> 00:14:04,025
mainly the way we do it is through human language.

243
00:14:04,025 --> 00:14:06,410
You know, some kinds of knowledge you can sort of

244
00:14:06,410 --> 00:14:09,260
work out for yourself by doing physical stuff right,

245
00:14:09,260 --> 00:14:11,900
I can hold this and drop that and I've learnt something.

246
00:14:11,900 --> 00:14:13,760
So I have to learn a bit of knowledge there.

247
00:14:13,760 --> 00:14:17,180
But sort of most of the knowledge in your heads and why you're sitting in

248
00:14:17,180 --> 00:14:21,980
this classroom has come from people communicating in human language to you.

249
00:14:21,980 --> 00:14:24,260
Um, so one of the famous,

250
00:14:24,260 --> 00:14:26,990
most famous steep learning people Yann Le Cun,

251
00:14:26,990 --> 00:14:29,165
he likes to say this line about,

252
00:14:29,165 --> 00:14:33,380
oh, you know really I think that you know there's not much difference

253
00:14:33,380 --> 00:14:37,965
between the intelligence of human being and orangutan.

254
00:14:37,965 --> 00:14:40,510
And I actually think he's really wrong on that.

255
00:14:40,510 --> 00:14:42,790
Like the sense in which he means that is,

256
00:14:42,790 --> 00:14:45,835
an orangutan has a really good vision system.

257
00:14:45,835 --> 00:14:48,610
Orangutans have very good you know control of

258
00:14:48,610 --> 00:14:52,060
their arms just like human beings for picking things up.

259
00:14:52,060 --> 00:14:58,970
Orangutans um can use tools um and orangutans can make plans so

260
00:14:58,970 --> 00:15:02,270
that if you sort of put the food somewhere where they have to sort of move

261
00:15:02,270 --> 00:15:05,960
the plank to get to the island with the food they can do a plan like that.

262
00:15:05,960 --> 00:15:09,890
So yeah, in a sense they've got a fair bit of intelligence but you know,

263
00:15:09,890 --> 00:15:13,385
sort of orangutans just aren't like human beings.

264
00:15:13,385 --> 00:15:16,100
And why aren't they like human beings?

265
00:15:16,100 --> 00:15:21,605
And I'd like to suggest to you the reason for that is what human beings have achieved is,

266
00:15:21,605 --> 00:15:25,070
we don't just have sort of one computer like

267
00:15:25,070 --> 00:15:29,825
a you know dusty old IBM PC in your mother's garage.

268
00:15:29,825 --> 00:15:33,740
What we have is a human computer network.

269
00:15:33,740 --> 00:15:37,520
And the way that we've achieved that human computer network is that,

270
00:15:37,520 --> 00:15:41,285
we use human languages as our networking language.

271
00:15:41,285 --> 00:15:44,690
Um, and so, when you think about it um,

272
00:15:44,690 --> 00:15:51,815
so on any kind of evolutionary scale language is super super super super recent, right?

273
00:15:51,815 --> 00:15:57,470
That um, creatures have had vision for people don't quite know but you know,

274
00:15:57,470 --> 00:16:00,980
maybe it's 75 million years or maybe it's longer, right?

275
00:16:00,980 --> 00:16:03,845
A huge length of time.

276
00:16:03,845 --> 00:16:07,295
How long have human beings have had language?

277
00:16:07,295 --> 00:16:09,860
You know people don't know that either because it turns out you know,

278
00:16:09,860 --> 00:16:11,015
when you have fossils,

279
00:16:11,015 --> 00:16:13,490
you can't knock the skull on the side and say,

280
00:16:13,490 --> 00:16:15,050
do you not have language.

281
00:16:15,050 --> 00:16:19,100
Um, but you know, most people estimate that sort of language is

282
00:16:19,100 --> 00:16:25,985
a very recent invention before current human beings moved out of um, out of Africa.

283
00:16:25,985 --> 00:16:28,550
So that many people think that we've only had language for

284
00:16:28,550 --> 00:16:31,460
something like a 100,000 years or something like that.

285
00:16:31,460 --> 00:16:35,450
So that's sort of you know blink of an eye on the evolutionary timescale.

286
00:16:35,450 --> 00:16:39,740
But you know, it was the development of language [inaudible]

287
00:16:39,740 --> 00:16:43,970
that sort of made human beings invisible- [NOISE] in invincible, right?

288
00:16:43,970 --> 00:16:46,475
It wasn't that, human beings um,

289
00:16:46,475 --> 00:16:51,410
developed poison fangs or developed ability to run

290
00:16:51,410 --> 00:16:53,660
faster than any other creature or

291
00:16:53,660 --> 00:16:56,210
put a big horn on their heads or something like that, right?

292
00:16:56,210 --> 00:16:59,060
You know, humans are basically pretty puny um,

293
00:16:59,060 --> 00:17:01,190
but they had this um,

294
00:17:01,190 --> 00:17:04,310
unbeatable advantage that they could communicate with

295
00:17:04,310 --> 00:17:07,880
each other and therefore work much more effectively in teams.

296
00:17:07,880 --> 00:17:11,495
And that sort of basically made human beings invincible.

297
00:17:11,495 --> 00:17:15,575
But you know, even then humans were kind of limited, right?

298
00:17:15,575 --> 00:17:18,140
That kind of got you to about the Stone Age right,

299
00:17:18,140 --> 00:17:20,390
where you could bang on your stones and with

300
00:17:20,390 --> 00:17:23,240
the right kind of stone make something sharp to cut with.

301
00:17:23,240 --> 00:17:25,685
Um, what got humans beyond that,

302
00:17:25,685 --> 00:17:28,100
was that they invented writing.

303
00:17:28,100 --> 00:17:32,915
So writing was then an ability where you could take knowledge

304
00:17:32,915 --> 00:17:37,730
not only communicated um mouth to mouth to people that you saw.

305
00:17:37,730 --> 00:17:41,660
You could put it down on your piece of papyrus so your clay tablet or whatever

306
00:17:41,660 --> 00:17:45,620
it was at first and that knowledge could then be sent places.

307
00:17:45,620 --> 00:17:50,270
It could be sent spatially around the world and it could then

308
00:17:50,270 --> 00:17:55,430
be sent temporally through time.

309
00:17:55,430 --> 00:17:57,290
And well, how old is writing?

310
00:17:57,290 --> 00:18:00,890
I mean, we sort of basically know about how old writing is, right?

311
00:18:00,890 --> 00:18:04,115
That writing is about 5,000 years old.

312
00:18:04,115 --> 00:18:09,740
It's incredibly incredibly recent on this scale of evolution but you know,

313
00:18:09,740 --> 00:18:16,730
essentially writing was so powerful as a way of having knowledge that then in those 5,000

314
00:18:16,730 --> 00:18:24,035
years that enabled human beings to go from stone age sharp piece or flint to you know,

315
00:18:24,035 --> 00:18:26,240
having iPhones and all of these things,

316
00:18:26,240 --> 00:18:28,790
all these incredibly sophisticated devices.

317
00:18:28,790 --> 00:18:32,960
So, language is pretty special thing I'd like to suggest.

318
00:18:32,960 --> 00:18:37,910
Um, but you know, if I go back to my analogy that sort of it's allowed humans to

319
00:18:37,910 --> 00:18:43,280
construct a networked computer that is way way more powerful than um,

320
00:18:43,280 --> 00:18:47,600
just having individual creatures as sort of intelligent like an orangutan.

321
00:18:47,600 --> 00:18:50,525
Um, and you compare it to our computer networks,

322
00:18:50,525 --> 00:18:53,045
it's a really funny kind of network, right?

323
00:18:53,045 --> 00:18:55,745
You know that these days um,

324
00:18:55,745 --> 00:19:01,805
we have networks that run around where we have sort of large network bandwidth, right?

325
00:19:01,805 --> 00:19:03,770
You know, we might be frustrated sometimes with

326
00:19:03,770 --> 00:19:06,530
our Netflix downloads but by and large you know,

327
00:19:06,530 --> 00:19:09,755
we can download hundreds of megabytes really easily and quickly.

328
00:19:09,755 --> 00:19:11,570
And we don't think that's fast enough,

329
00:19:11,570 --> 00:19:13,670
so we're going to be rolling out 5G networks.

330
00:19:13,670 --> 00:19:16,400
So it's an order of magnitude faster again.

331
00:19:16,400 --> 00:19:18,800
I mean, by comparison to that, I mean,

332
00:19:18,800 --> 00:19:23,540
human language is a pathetically slow network, right?

333
00:19:23,540 --> 00:19:29,465
That the amount of information you can convey by human language is very slow.

334
00:19:29,465 --> 00:19:33,950
I mean you know, whatever it is I sort of speak at about 15 words a second right,

335
00:19:33,950 --> 00:19:35,420
you can start doing um,

336
00:19:35,420 --> 00:19:37,550
your information theory if you know some right?

337
00:19:37,550 --> 00:19:41,060
But um, you don't actually get much bandwidth at all.

338
00:19:41,060 --> 00:19:44,405
And that then leads- so you can think of,

339
00:19:44,405 --> 00:19:45,980
how does it work then?

340
00:19:45,980 --> 00:19:47,570
So, humans have come up with

341
00:19:47,570 --> 00:19:53,390
this incredibly impressive system which is essentially form of compression.

342
00:19:53,390 --> 00:19:56,120
Sort of a very adaptive form of compression,

343
00:19:56,120 --> 00:19:58,070
so that when we're talking to people,

344
00:19:58,070 --> 00:20:02,870
we assume that they have an enormous amount of knowledge in their heads which

345
00:20:02,870 --> 00:20:07,640
isn't the same as but it's broadly similar to mine when I'm talking to you right?

346
00:20:07,640 --> 00:20:10,565
That you know what English words mean,

347
00:20:10,565 --> 00:20:13,850
and you know a lot about how the wor- world works.

348
00:20:13,850 --> 00:20:17,149
And therefore, I can say a short message and communicate

349
00:20:17,149 --> 00:20:22,820
only a relatively short bit string and you can actually understand a lot. All right?

350
00:20:22,820 --> 00:20:26,030
So, I can say sort of whatever you know,

351
00:20:26,030 --> 00:20:28,850
imagine a busy shopping mall and that

352
00:20:28,850 --> 00:20:31,630
there are two guys standing in front of a makeup counter,

353
00:20:31,630 --> 00:20:36,290
and you know I've only said whatever that was sort of about 200 bits of

354
00:20:36,290 --> 00:20:38,960
information but that's enabled you to construct

355
00:20:38,960 --> 00:20:42,340
a whole visual scene that we're taking megabytes to um,

356
00:20:42,340 --> 00:20:44,385
represent as an image.

357
00:20:44,385 --> 00:20:46,625
So, that's why language is good.

358
00:20:46,625 --> 00:20:49,100
Um, so from that more authorial level,

359
00:20:49,100 --> 00:20:51,425
I'll now move back to the concrete stuff.

360
00:20:51,425 --> 00:20:55,925
What we wanna do in this class is not solve the whole of language,

361
00:20:55,925 --> 00:20:57,950
but we want to represent, um,

362
00:20:57,950 --> 00:21:00,380
the meaning of words, right?

363
00:21:00,380 --> 00:21:03,230
So, a lot of language is bound up in words and their meanings

364
00:21:03,230 --> 00:21:06,200
and words can have really rich meanings, right?

365
00:21:06,200 --> 00:21:07,970
As soon as you say a word teacher,

366
00:21:07,970 --> 00:21:12,530
that's kinda quite a lot of rich meaning or you can have actions that have rich meaning.

367
00:21:12,530 --> 00:21:17,225
So, if I say a word like prognosticate or,

368
00:21:17,225 --> 00:21:19,070
um, total or something you know,

369
00:21:19,070 --> 00:21:22,385
these words that have rich meanings and a lot of nuance on them.

370
00:21:22,385 --> 00:21:24,395
And so we wanna represent meaning.

371
00:21:24,395 --> 00:21:26,510
And so, the question is what is meaning?

372
00:21:26,510 --> 00:21:29,360
So, you can of course you can- dictionaries are meant to tell you about meanings.

373
00:21:29,360 --> 00:21:31,490
So, you can look up dictionaries um,

374
00:21:31,490 --> 00:21:35,720
and Webster says sort of tries to relate meaning to idea.

375
00:21:35,720 --> 00:21:39,515
The idea that is represented by a word or a phrase.

376
00:21:39,515 --> 00:21:44,240
The idea that a person wants to express by word signs et cetera.

377
00:21:44,240 --> 00:21:46,190
I mean, you know,

378
00:21:46,190 --> 00:21:49,730
you could think that these definitions are kind of a cop-out because it seems

379
00:21:49,730 --> 00:21:53,015
like they're rewriting meaning in terms of the word idea,

380
00:21:53,015 --> 00:21:55,040
and is that really gotten you anywhere.

381
00:21:55,040 --> 00:21:58,370
Um, how do linguists think about meaning?

382
00:21:58,370 --> 00:22:03,110
I mean, the most common way that linguists have thought about

383
00:22:03,110 --> 00:22:05,660
meaning is an idea that's called denotational

384
00:22:05,660 --> 00:22:08,420
semantics which is also used in programming languages.

385
00:22:08,420 --> 00:22:14,810
So, the idea of that is we think of meaning as what things represent.

386
00:22:14,810 --> 00:22:16,955
So, if I say the word chair,

387
00:22:16,955 --> 00:22:21,140
the denotation of the word chair includes this one here and that one,

388
00:22:21,140 --> 00:22:22,325
that one, that one, that one.

389
00:22:22,325 --> 00:22:24,919
And so, the word chair is sort of representing

390
00:22:24,919 --> 00:22:28,580
all the things that are chairs and you can sort of, um,

391
00:22:28,580 --> 00:22:33,410
you can then think of something like running as well that you know there's sort of sets

392
00:22:33,410 --> 00:22:37,985
of actions that people can partake that- that's their denotation.

393
00:22:37,985 --> 00:22:42,200
And that's sort of what you most commonly see in philosophy or linguistics as denotation.

394
00:22:42,200 --> 00:22:47,135
It's kind of a hard thing to get your hands on, um, computationally.

395
00:22:47,135 --> 00:22:50,480
So, um, what type of people most commonly

396
00:22:50,480 --> 00:22:54,020
do or use the most commonly do I guess I should say now

397
00:22:54,020 --> 00:22:57,530
for working out the meaning of words on the computer that

398
00:22:57,530 --> 00:23:01,115
commonly that turn to something that was a bit like a dictionary.

399
00:23:01,115 --> 00:23:06,200
In particular favorite online thing was this online thesaurus called WordNet which

400
00:23:06,200 --> 00:23:11,510
sort of tells you about word meanings and relationships between word meanings.

401
00:23:11,510 --> 00:23:16,445
Um, so this is just giving you the very slices sense of,

402
00:23:16,445 --> 00:23:19,820
um, of what's in WordNet.

403
00:23:19,820 --> 00:23:24,485
Um, so this is an actual bit of Python code up there which you can,

404
00:23:24,485 --> 00:23:28,370
um, type into your computer and run and do this for yourself.

405
00:23:28,370 --> 00:23:31,040
Um, so this uses a thing called NLTK.

406
00:23:31,040 --> 00:23:33,725
Um, so NLTK is sort of like

407
00:23:33,725 --> 00:23:39,364
the "Swiss Army Knife of NLP" meaning that it's not terribly good for anything,

408
00:23:39,364 --> 00:23:41,570
but it has a lot of basic tools.

409
00:23:41,570 --> 00:23:46,460
So, if you wanted to do something like just get some stuff out of WordNet and show it,

410
00:23:46,460 --> 00:23:49,625
it's the perfect thing to use. Um, okay.

411
00:23:49,625 --> 00:23:54,830
So, um, from NLTK I'm importing WordNet and so then I can say,

412
00:23:54,830 --> 00:24:01,355
"Okay, um, for the word good tell me about the synonym sets with good participates in."

413
00:24:01,355 --> 00:24:03,440
And there's good goodness as a noun.

414
00:24:03,440 --> 00:24:04,760
There is an adjective good.

415
00:24:04,760 --> 00:24:08,330
There's one estimable good, honorable, respectable.

416
00:24:08,330 --> 00:24:11,150
Um, this looks really complex and hard to understand.

417
00:24:11,150 --> 00:24:13,700
But the idea of word- WordNet makes

418
00:24:13,700 --> 00:24:18,080
these very fine grain distinctions between senses of a word.

419
00:24:18,080 --> 00:24:20,675
So, what sort of saying for good, um,

420
00:24:20,675 --> 00:24:23,570
there's what some sensors where it's a noun, right?

421
00:24:23,570 --> 00:24:24,755
That's where you sort of,

422
00:24:24,755 --> 00:24:27,200
I bought some goods for my trip, right?

423
00:24:27,200 --> 00:24:28,880
So, that's sort of, um,

424
00:24:28,880 --> 00:24:32,780
one of these noun sensors like this one I guess.

425
00:24:32,780 --> 00:24:35,480
Um, then there are adjective sensors and it's trying to

426
00:24:35,480 --> 00:24:38,840
distinguish- there's a basic adjective sense of good being good,

427
00:24:38,840 --> 00:24:41,270
and then in certain, um, sensors,

428
00:24:41,270 --> 00:24:44,750
there are these extended sensors of good in different directions.

429
00:24:44,750 --> 00:24:48,515
So, I guess this is good in the sense of beneficial, um,

430
00:24:48,515 --> 00:24:52,925
and this one is sort of person who is respectable or something.

431
00:24:52,925 --> 00:24:55,580
He's a good man or something like that, right?

432
00:24:55,580 --> 00:24:56,855
So, um, but you know,

433
00:24:56,855 --> 00:24:59,660
part of what's kind of makes us

434
00:24:59,660 --> 00:25:02,630
think very problematic and practice to use is it tries to make

435
00:25:02,630 --> 00:25:06,850
all these very fine-grain differences between sensors that are a human being can

436
00:25:06,850 --> 00:25:11,410
barely understand the difference between them um, and relate to.

437
00:25:11,410 --> 00:25:13,690
Um, so you can then do other things with WordNet.

438
00:25:13,690 --> 00:25:18,460
So, this bit of code you can sort of well walk up and is a kind of hierarchy.

439
00:25:18,460 --> 00:25:21,635
So, it's kinda like a traditional, um, database.

440
00:25:21,635 --> 00:25:29,030
So, if I start with a panda and say- [NOISE] if I start with a panda.

441
00:25:29,030 --> 00:25:32,180
Um, and walk up, um,

442
00:25:32,180 --> 00:25:35,330
the pandas are [inaudible].

443
00:25:35,330 --> 00:25:37,640
Maybe you'd guys to bio which are carnivores,

444
00:25:37,640 --> 00:25:39,545
placentals, mammals, blah, blah, blah.

445
00:25:39,545 --> 00:25:44,135
Okay, so, um, that's the kind of stuff you can get out to- out of WordNet.

446
00:25:44,135 --> 00:25:47,105
Um, you know, in practice WordNet has been.

447
00:25:47,105 --> 00:25:49,580
Everyone sort of used to use it because it gave

448
00:25:49,580 --> 00:25:51,995
you some sort of sense of the meaning of the word.

449
00:25:51,995 --> 00:25:54,125
But you know it's also sort of well-known.

450
00:25:54,125 --> 00:25:56,540
It never worked that well.

451
00:25:56,540 --> 00:26:02,720
Um, so you know that sort of the synonym sets miss a lot of nuance.

452
00:26:02,720 --> 00:26:05,270
So, you know one of the synonym sets for good has

453
00:26:05,270 --> 00:26:08,240
proficient in it and good sort of like proficient

454
00:26:08,240 --> 00:26:11,495
but doesn't proficient have some more connotations and nuance?

455
00:26:11,495 --> 00:26:13,250
I think it does.

456
00:26:13,250 --> 00:26:18,080
Um, WordNet like most hand built resources is sort of very incomplete.

457
00:26:18,080 --> 00:26:21,290
So, as soon as you're coming to new meanings of words,

458
00:26:21,290 --> 00:26:23,705
or new words and slang words,

459
00:26:23,705 --> 00:26:25,310
well then, that gives you nothing.

460
00:26:25,310 --> 00:26:28,985
Um, it's sort of built with human labor,

461
00:26:28,985 --> 00:26:35,030
um, in ways that you know it's hard to sort of create and adapt.

462
00:26:35,030 --> 00:26:37,670
And in particular, what we want to focus on is,

463
00:26:37,670 --> 00:26:41,870
seems like a basic thing you'd like to do with words and it's actually at least

464
00:26:41,870 --> 00:26:45,920
understand similarities and relations between the meaning of words.

465
00:26:45,920 --> 00:26:49,520
And it turns out that you know WordNet doesn't actually do that that well

466
00:26:49,520 --> 00:26:53,600
because it just has these sort of fixed discrete synonym sets.

467
00:26:53,600 --> 00:26:56,090
So, if you have a words in a synonym said that there's

468
00:26:56,090 --> 00:26:59,075
sort of a synonym and maybe not exactly the same meaning,

469
00:26:59,075 --> 00:27:00,800
they're not in the same synonyms set,

470
00:27:00,800 --> 00:27:04,580
you kind of can't really measure the partial resemblance as a meaning for them.

471
00:27:04,580 --> 00:27:08,435
So, if something like good and marvelous aren't in the same synonym set,

472
00:27:08,435 --> 00:27:11,960
but there's something that they share in common that you'd like to represent.

473
00:27:11,960 --> 00:27:16,880
Okay. So, um, that's kinda turn to lead into

474
00:27:16,880 --> 00:27:21,935
us wanting to do something different and better for word meaning.

475
00:27:21,935 --> 00:27:25,730
And, um, before getting there I just sort of wanna again sort

476
00:27:25,730 --> 00:27:29,495
of build a little from traditional NLP.

477
00:27:29,495 --> 00:27:33,275
So, traditional NLP in the context of this course sort of means

478
00:27:33,275 --> 00:27:39,275
Natural Language Processing up until approximately 2012.

479
00:27:39,275 --> 00:27:43,640
There were some earlier antecedents but as basically, um,

480
00:27:43,640 --> 00:27:47,600
in 2013 that things really began to change with

481
00:27:47,600 --> 00:27:53,060
people starting to use neural net style representations for natural language processing.

482
00:27:53,060 --> 00:27:55,430
So, up until 2012,

483
00:27:55,430 --> 00:27:58,055
um, standardly you know we had words.

484
00:27:58,055 --> 00:28:02,210
They are just words. So, we had hotel conference motel.

485
00:28:02,210 --> 00:28:06,650
They were words, and we'd have you know lexicons and put words into our model.

486
00:28:06,650 --> 00:28:12,290
Um, and in neural networks land this is referred to as a localist representation.

487
00:28:12,290 --> 00:28:14,960
I'll come back to those terms again next time.

488
00:28:14,960 --> 00:28:20,015
But that's sort of meaning that for any concept there's sort of one particular,

489
00:28:20,015 --> 00:28:24,080
um, place which is the word hotel or the word motel.

490
00:28:24,080 --> 00:28:26,465
A way of thinking about that is to think

491
00:28:26,465 --> 00:28:29,615
about what happens when you build a machine learning model.

492
00:28:29,615 --> 00:28:34,759
So, if you have a categorical variable like you have words with the choice of word

493
00:28:34,759 --> 00:28:40,130
and you want to stick that into some kind of classifier in a Machine Learning Model,

494
00:28:40,130 --> 00:28:42,905
somehow you have to code that categorical variable,

495
00:28:42,905 --> 00:28:46,550
and the standard way of doing it is that you code it by having

496
00:28:46,550 --> 00:28:51,275
different levels of the variable which means that you have a vector,

497
00:28:51,275 --> 00:28:53,840
and you have, this is the word house.

498
00:28:53,840 --> 00:28:55,670
This is the word cat. This is the word dog.

499
00:28:55,670 --> 00:28:57,020
This is the word some chairs.

500
00:28:57,020 --> 00:28:58,190
This is the word agreeable.

501
00:28:58,190 --> 00:28:59,465
This is the word something else.

502
00:28:59,465 --> 00:29:01,415
This is the word, um,

503
00:29:01,415 --> 00:29:05,750
hotel, um, and this is another word for something different, right?

504
00:29:05,750 --> 00:29:08,075
So that you have put a one at the position

505
00:29:08,075 --> 00:29:11,120
and neural net land we call these one-hot vectors,

506
00:29:11,120 --> 00:29:12,470
and so these might be, ah,

507
00:29:12,470 --> 00:29:16,250
one-hot vectors for hotel and motel.

508
00:29:16,250 --> 00:29:19,040
So, there are a couple of things that are bad here.

509
00:29:19,040 --> 00:29:21,005
Um, the one that's sort of, ah,

510
00:29:21,005 --> 00:29:27,140
practical nuisance is you know languages have a lot of words.

511
00:29:27,140 --> 00:29:30,590
Ah, so, it's sort of one of those dictionaries that you might have still had in

512
00:29:30,590 --> 00:29:35,450
school that you probably have about 250,000 words in them.

513
00:29:35,450 --> 00:29:37,400
But you know, if you start getting into

514
00:29:37,400 --> 00:29:41,855
more technical and scientific English it's easy to get to a million words.

515
00:29:41,855 --> 00:29:45,690
I mean, actually the number of words that you have in a language, um,

516
00:29:45,690 --> 00:29:48,620
like English is actually infinite because we have

517
00:29:48,620 --> 00:29:52,220
these processes which are called derivational morphology,

518
00:29:52,220 --> 00:29:56,930
um, where you can make more words by adding endings onto existing words.

519
00:29:56,930 --> 00:29:59,660
So, you know you can start with something like paternalist,

520
00:29:59,660 --> 00:30:03,470
fatherly, and then you can sort of say from maternal,

521
00:30:03,470 --> 00:30:06,275
you can say paternalist, or paternalistic,

522
00:30:06,275 --> 00:30:10,070
paternalism and pa- I did it paternalistically.

523
00:30:10,070 --> 00:30:14,255
Right? Now all of these ways that you can bake bigger words by adding more stuff into it.

524
00:30:14,255 --> 00:30:18,905
Um, and so really you end up with an infinite space of words.

525
00:30:18,905 --> 00:30:22,880
Um, yeah. So that's a minor problem, right?

526
00:30:22,880 --> 00:30:28,275
We have very big vectors if we want to represent a sensible size vocabulary.

527
00:30:28,275 --> 00:30:31,990
Um, but there's a much bigger problem than that, which is, well,

528
00:30:31,990 --> 00:30:35,200
precisely what we want to do all the time, is we want to,

529
00:30:35,200 --> 00:30:38,590
sort of, understand relationships and the meaning of words.

530
00:30:38,590 --> 00:30:42,380
So, you know, an obvious example of this is web search.

531
00:30:42,380 --> 00:30:45,350
So, if I do a search for Seattle motel,

532
00:30:45,350 --> 00:30:48,710
it'd be useful if it also showed me results that had

533
00:30:48,710 --> 00:30:52,655
Seattle hotel on the page and vice versa because,

534
00:30:52,655 --> 00:30:55,415
you know, hotels and motels pretty much the same thing.

535
00:30:55,415 --> 00:30:59,900
Um, but, you know, if we have these one-hot vectors like we had before they have

536
00:30:59,900 --> 00:31:04,250
no s- similarity relationship between them, right?

537
00:31:04,250 --> 00:31:05,675
So, in math terms,

538
00:31:05,675 --> 00:31:07,775
these two vectors are orthogonal.

539
00:31:07,775 --> 00:31:10,865
No similarity relationship between them.

540
00:31:10,865 --> 00:31:12,650
Um, and so you,

541
00:31:12,650 --> 00:31:14,705
kind of, get nowhere.

542
00:31:14,705 --> 00:31:16,880
Now, you know, there are things that you could do,

543
00:31:16,880 --> 00:31:18,710
I- I just showed you WordNet's.

544
00:31:18,710 --> 00:31:20,840
WordNet's shows you some synonyms and stuff.

545
00:31:20,840 --> 00:31:22,610
So that might help a bit.

546
00:31:22,610 --> 00:31:24,035
There are other things you could do.

547
00:31:24,035 --> 00:31:25,415
You could sort of say, well wait,

548
00:31:25,415 --> 00:31:29,645
why don't we just build up a big table where we have a big table of,

549
00:31:29,645 --> 00:31:32,675
um, word similarities, and we could work with that.

550
00:31:32,675 --> 00:31:34,910
And, you know, people used to try and do that, right?

551
00:31:34,910 --> 00:31:39,770
You know, that's sort of what Google did in 2005 or something.

552
00:31:39,770 --> 00:31:42,080
You know, it had word similarity tables.

553
00:31:42,080 --> 00:31:44,510
The problem with doing that is you know,

554
00:31:44,510 --> 00:31:48,290
we were talking about how maybe we want 500,000 words.

555
00:31:48,290 --> 00:31:52,040
And if you want to build up then a word similarity table out

556
00:31:52,040 --> 00:31:56,060
of our pairs of words from one-hot representations,

557
00:31:56,060 --> 00:31:58,640
um, you- that means that the size of that table,

558
00:31:58,640 --> 00:32:00,380
as my math is pretty bad,

559
00:32:00,380 --> 00:32:02,315
is it 2,5 trillion?

560
00:32:02,315 --> 00:32:07,130
It's some very big number of cells in your similarity, um, matrix.

561
00:32:07,130 --> 00:32:09,230
So that's almost impossible to do.

562
00:32:09,230 --> 00:32:13,715
So, what we're gonna instead do is explore a method in which,

563
00:32:13,715 --> 00:32:16,670
um, we are going to represent words as vectors,

564
00:32:16,670 --> 00:32:18,140
in a way I'll show you just, um,

565
00:32:18,140 --> 00:32:21,770
a minute in such a way that just the representation of

566
00:32:21,770 --> 00:32:26,480
a word gives you their similarity with no further work.

567
00:32:26,480 --> 00:32:30,635
Okay. And so that's gonna lead into these different ideas.

568
00:32:30,635 --> 00:32:34,175
So, I mentioned before denotational semantics.

569
00:32:34,175 --> 00:32:39,115
Here's another idea for representing the meaning of words,

570
00:32:39,115 --> 00:32:41,980
um, which is called distributional semantics.

571
00:32:41,980 --> 00:32:45,140
And so the idea of distributional semantics is, well,

572
00:32:45,140 --> 00:32:50,900
how are we going to represent the meaning of a word is by looking at the contexts,

573
00:32:50,900 --> 00:32:52,925
um, in which it appears.

574
00:32:52,925 --> 00:32:56,510
So, this is a picture of JR Firth who was a British linguist.

575
00:32:56,510 --> 00:32:58,400
Um, he's famous for this saying,

576
00:32:58,400 --> 00:33:01,535
"You shall know a word by the company it keeps."

577
00:33:01,535 --> 00:33:06,950
Um, but another person who's very famous for developing this notion of meaning is, um,

578
00:33:06,950 --> 00:33:10,670
the philosopher Ludwig- Ludwig Wittgenstein in his later writings,

579
00:33:10,670 --> 00:33:13,445
which he referred to as a use theory of meeting- meaning.

580
00:33:13,445 --> 00:33:16,070
Well, actually he's- he used some big German word that I don't know,

581
00:33:16,070 --> 00:33:18,530
but, um, we'll call it a use theory of meaning.

582
00:33:18,530 --> 00:33:22,535
And, you know, essentially the point was, well, you know,

583
00:33:22,535 --> 00:33:26,780
if you can explain every- if- if you can

584
00:33:26,780 --> 00:33:31,160
explain what contexts it's correct to use a certain word,

585
00:33:31,160 --> 00:33:34,595
versus in what contexts would be the wrong word to use,

586
00:33:34,595 --> 00:33:38,135
this maybe gives you bad memories of doing English in high school,

587
00:33:38,135 --> 00:33:40,490
when people said, ah, that's the wrong word to use there,

588
00:33:40,490 --> 00:33:43,205
um, well, then you understand the meaning of the word, right?

589
00:33:43,205 --> 00:33:47,045
Um, and so that's the idea of distributional semantics.

590
00:33:47,045 --> 00:33:49,790
And it's been- so one of the most successful ideas in

591
00:33:49,790 --> 00:33:54,005
modern statistical NLP because it gives you a great way to learn about word meaning.

592
00:33:54,005 --> 00:33:56,615
And so what we're gonna do is we're going to say,

593
00:33:56,615 --> 00:33:58,925
haha, I want to know what the word banking means.

594
00:33:58,925 --> 00:34:01,730
So, I'm gonna grab a lot of texts,

595
00:34:01,730 --> 00:34:04,520
which is easy to do now when we have the World Wide Web,

596
00:34:04,520 --> 00:34:07,955
I'll find lots of sentences where the word banking is used,

597
00:34:07,955 --> 00:34:12,770
Government debt problems turning into banking crises as happened in 2009.

598
00:34:12,770 --> 00:34:15,845
And both these- I'm just going to say all of

599
00:34:15,845 --> 00:34:19,115
this stuff is the meaning of the word banking.

600
00:34:19,115 --> 00:34:23,750
Um, that those are the contexts in which the word banking is used.

601
00:34:23,750 --> 00:34:29,495
And that seems like very simple and perhaps even not quite right idea,

602
00:34:29,495 --> 00:34:34,880
but it turns out to be a very usable idea that does a great job at capturing meaning.

603
00:34:34,880 --> 00:34:38,300
And so what we're gonna do is say rather than

604
00:34:38,300 --> 00:34:42,950
our old localist representation we're now gonna

605
00:34:42,950 --> 00:34:48,215
represent words in what we call a distributed representation.

606
00:34:48,215 --> 00:34:51,830
And so, for the distributed representation we're still going

607
00:34:51,830 --> 00:34:55,655
to [NOISE] represent the meaning of a word as a numeric vector.

608
00:34:55,655 --> 00:34:59,480
But now we're going to say that the meaning of each word is,

609
00:34:59,480 --> 00:35:01,520
ah, smallish vector, um,

610
00:35:01,520 --> 00:35:07,760
but it's going to be a dense vector where by all of the numbers are non-zero.

611
00:35:07,760 --> 00:35:10,010
So the meaning of banking is going to be

612
00:35:10,010 --> 00:35:13,340
distributed over the dim- dimensions of this vector.

613
00:35:13,340 --> 00:35:19,190
Um, now, my vector here is of dimension nine because I want to keep the slide, um, nice.

614
00:35:19,190 --> 00:35:23,195
Um, life isn't quite that good in practice.

615
00:35:23,195 --> 00:35:25,970
When we do this we use a larger dimensionality,

616
00:35:25,970 --> 00:35:29,075
kinda, solid the minimum that people use is 50.

617
00:35:29,075 --> 00:35:32,330
Um, a typical number that you might use on your laptop is

618
00:35:32,330 --> 00:35:35,945
300 if you want to really max out performance,

619
00:35:35,945 --> 00:35:38,885
um, maybe 1,000, 2,000, 4,000.

620
00:35:38,885 --> 00:35:42,020
But, you know, nevertheless [NOISE] orders of magnitude is

621
00:35:42,020 --> 00:35:46,320
smaller compared to a length 500,000 vector.

622
00:35:46,810 --> 00:35:51,890
Okay. So we have words with their vector representations.

623
00:35:51,890 --> 00:35:55,790
And so since each word is going to have a vector, um,

624
00:35:55,790 --> 00:36:01,160
representation we then have a vector space in which we can place all of the words.

625
00:36:01,160 --> 00:36:03,980
Um, and that's completely unreadable, um,

626
00:36:03,980 --> 00:36:08,135
but if you zoom into the vector space it's still completely unreadable.

627
00:36:08,135 --> 00:36:10,115
But if you zoom in a bit further,

628
00:36:10,115 --> 00:36:13,100
um, you can find different parts of this space.

629
00:36:13,100 --> 00:36:16,820
So here's the part that where countries attending to,

630
00:36:16,820 --> 00:36:18,950
um, exist Japanese, German,

631
00:36:18,950 --> 00:36:21,950
French, Russian, British Australian American,

632
00:36:21,950 --> 00:36:25,130
um, France, Britain, Germany et cetera.

633
00:36:25,130 --> 00:36:27,770
And you can shift over to a different part of the space.

634
00:36:27,770 --> 00:36:31,040
So here's a part of the space where various verbs are,

635
00:36:31,040 --> 00:36:33,485
so has have, had, been, be.

636
00:36:33,485 --> 00:36:40,880
Oops. Um, um, [inaudible] be always was where.

637
00:36:40,880 --> 00:36:43,970
You can even see that some morphological forms are grouping together,

638
00:36:43,970 --> 00:36:46,100
and things that sort of go together like say,

639
00:36:46,100 --> 00:36:48,770
think expect to things that take those, kind of, compliment.

640
00:36:48,770 --> 00:36:50,795
He said or thought something.

641
00:36:50,795 --> 00:36:52,415
Um, they group together.

642
00:36:52,415 --> 00:36:55,010
Now, what am I actually showing you here?

643
00:36:55,010 --> 00:36:57,755
Um, you know, really this was built from,

644
00:36:57,755 --> 00:37:00,575
ah, 100 dimensional word vectors.

645
00:37:00,575 --> 00:37:05,630
And there is this problem is really hard to visualize 100 dimensional word vectors.

646
00:37:05,630 --> 00:37:09,860
So, what is actually happening here is these, um,

647
00:37:09,860 --> 00:37:15,110
100 dimensional word vectors are being projected down into two-dimensions,

648
00:37:15,110 --> 00:37:17,990
and you're so- seeing the two-dimensional view,

649
00:37:17,990 --> 00:37:19,790
which I'll get back to later.

650
00:37:19,790 --> 00:37:22,400
Um, so, on the one hand, um,

651
00:37:22,400 --> 00:37:24,410
whenever you see these pictures you should hold on to

652
00:37:24,410 --> 00:37:26,840
the your wallet because there's a huge amount of

653
00:37:26,840 --> 00:37:31,535
detail on the original vector space that got completely killed and went away, um,

654
00:37:31,535 --> 00:37:32,839
in the 2D projection,

655
00:37:32,839 --> 00:37:37,070
and indeed some of what push things together in the 2D,

656
00:37:37,070 --> 00:37:39,875
um, projection may really, really,

657
00:37:39,875 --> 00:37:42,590
really misrepresent what's in the original space.

658
00:37:42,590 --> 00:37:45,740
Um, but even looking at these 2D representations,

659
00:37:45,740 --> 00:37:46,850
the overall feeling is,

660
00:37:46,850 --> 00:37:48,920
my gosh this actually sort of works, doesn't it?

661
00:37:48,920 --> 00:37:54,365
Um, we can sort of see similarities, um, between words.

662
00:37:54,365 --> 00:38:02,375
Okay. So, um, ha- so that was the idea of what we want to do.

663
00:38:02,375 --> 00:38:04,310
Um, the next part, um,

664
00:38:04,310 --> 00:38:07,940
is then how do we actually go about doing it?

665
00:38:07,940 --> 00:38:10,445
I'll pause for breath for half a minute.

666
00:38:10,445 --> 00:38:12,710
Has anyone got a question they're dying to ask?

667
00:38:12,710 --> 00:38:20,300
[NOISE] Yeah.

668
00:38:20,300 --> 00:38:26,720
Where were the- the vectors is each, um,

669
00:38:26,720 --> 00:38:28,460
had a different order in each contact,

670
00:38:28,460 --> 00:38:30,530
like, say the first decimal vector,

671
00:38:30,530 --> 00:38:32,840
second decimal vector, are those standard

672
00:38:32,840 --> 00:38:35,475
across all theory or people choose them themselves?

673
00:38:35,475 --> 00:38:42,340
Um, they're not standards across NLP um and they're not chosen at all.

674
00:38:42,340 --> 00:38:45,055
So what we're gonna present is a learning algorithm.

675
00:38:45,055 --> 00:38:48,430
So where we just sort of shuffle in lots of text

676
00:38:48,430 --> 00:38:51,970
and miraculously these word vectors come out.

677
00:38:51,970 --> 00:38:57,760
And so the l- learning algorithm itself decides the dimensions.

678
00:38:57,760 --> 00:39:03,085
But um that actually reminds me of something I sort of meant to say which was yeah,

679
00:39:03,085 --> 00:39:05,425
I mean, since this is a vector space,

680
00:39:05,425 --> 00:39:09,580
in some sense the dimensions over the arbitrary right,

681
00:39:09,580 --> 00:39:12,570
because you can you know just have your basis vectors in

682
00:39:12,570 --> 00:39:15,945
any different direction and you could sort of re-represent,

683
00:39:15,945 --> 00:39:19,715
um the words in the vector space with a different set of basics,

684
00:39:19,715 --> 00:39:22,930
basis vectors and it'd be exactly the same vector space

685
00:39:22,930 --> 00:39:26,380
just sort of rotate around to your new um, vectors.

686
00:39:26,380 --> 00:39:30,580
So, you know, you shouldn't read too much into the sort of elements.

687
00:39:30,580 --> 00:39:32,860
So, it actually turns out that because of the way a lot of

688
00:39:32,860 --> 00:39:36,070
deep learning um operations work,

689
00:39:36,070 --> 00:39:38,170
some things they do, do element-wise.

690
00:39:38,170 --> 00:39:42,775
So that the dimensions do actually tend to get some meaning to them it turns out.

691
00:39:42,775 --> 00:39:46,900
But um, though I think I really wanted to say was,

692
00:39:46,900 --> 00:39:52,240
that you know one thing we can just think of is how close things

693
00:39:52,240 --> 00:39:54,250
are in the vector space and that's

694
00:39:54,250 --> 00:39:57,805
a notion of meaning similarity that we are going to exploit.

695
00:39:57,805 --> 00:40:00,640
But you might hope that you get more than that,

696
00:40:00,640 --> 00:40:03,010
and you might actually think that there's meaning in

697
00:40:03,010 --> 00:40:06,925
different dimensions and directions in the word vector space.

698
00:40:06,925 --> 00:40:11,335
And the answer to that is there is and I'll come back to that a bit later.

699
00:40:11,335 --> 00:40:17,770
Okay. Um, so in some sense this thing that had

700
00:40:17,770 --> 00:40:22,240
the biggest impact um in sort of turning the world of

701
00:40:22,240 --> 00:40:27,625
NLP in a neural networks direction was that picture.

702
00:40:27,625 --> 00:40:32,260
Um, was this um algorithm that um

703
00:40:32,260 --> 00:40:37,345
Thomas Mikolov came up with in 2013 called the word2vec algorithm.

704
00:40:37,345 --> 00:40:43,210
So it wasn't the first work and having distributed representations of words.

705
00:40:43,210 --> 00:40:45,730
So there was older work from Yoshua Bengio that went

706
00:40:45,730 --> 00:40:48,370
back to about the sort of turn on the millennium,

707
00:40:48,370 --> 00:40:52,780
that somehow it's sort of hadn't really sort of hit the world over their head and had

708
00:40:52,780 --> 00:40:57,730
a huge impact and has really sort of Thomas Mikolov showed this very simple,

709
00:40:57,730 --> 00:41:00,070
very scalable way of learning

710
00:41:00,070 --> 00:41:05,005
vector representations of um words and that sort of really opened the flood gates.

711
00:41:05,005 --> 00:41:08,650
And so that's the algorithm that I'm going to um show now.

712
00:41:08,650 --> 00:41:15,775
Okay. So the idea of this algorithm is you start with a big pile of text.

713
00:41:15,775 --> 00:41:20,650
Um, so wherever you find you know web pages on newspaper articles or something,

714
00:41:20,650 --> 00:41:22,480
a lot of continuous text, right?

715
00:41:22,480 --> 00:41:26,350
Actual sentences because we want to learn wo- word meaning context.

716
00:41:26,350 --> 00:41:32,470
Um, NLP people call a large pile of text a corpus.

717
00:41:32,470 --> 00:41:35,890
And I mean that's just the Latin word for body, right?

718
00:41:35,890 --> 00:41:37,915
It's a body of text.

719
00:41:37,915 --> 00:41:43,224
Important things to note if you want to seem really educated is in Latin,

720
00:41:43,224 --> 00:41:46,690
this is a fourth declensions noun.

721
00:41:46,690 --> 00:41:49,900
So the plural of corpus is corpora.

722
00:41:49,900 --> 00:41:51,190
And whereas if you say

723
00:41:51,190 --> 00:41:55,390
core Pi everyone will know that you didn't study Latin in high school.

724
00:41:55,390 --> 00:42:00,490
[LAUGHTER] Um, okay.

725
00:42:00,490 --> 00:42:06,460
Um, so right- so we then want to say that every word um

726
00:42:06,460 --> 00:42:08,890
in a- in a fixed vocabulary which would just be

727
00:42:08,890 --> 00:42:12,445
the vocabulary the corpus is um represented by a vector.

728
00:42:12,445 --> 00:42:16,615
And we just start those vectors off as random vectors.

729
00:42:16,615 --> 00:42:18,340
And so then what we're going to do is do

730
00:42:18,340 --> 00:42:22,585
this big iterative algorithm where we go through each position in the text.

731
00:42:22,585 --> 00:42:24,715
We say, here's a word in the text.

732
00:42:24,715 --> 00:42:30,520
Let's look at the words around it and what we're going to want to do is say well,

733
00:42:30,520 --> 00:42:32,890
the meaning of a word is its contexts of use.

734
00:42:32,890 --> 00:42:35,290
So we want the representation of the word

735
00:42:35,290 --> 00:42:37,870
in the middle to be able to predict the words that are

736
00:42:37,870 --> 00:42:43,720
around it and so we're gonna achieve that by moving the position of the word vector.

737
00:42:43,720 --> 00:42:47,500
And we just repeat that a billion times and

738
00:42:47,500 --> 00:42:51,190
somehow a miracle occurs and outcomes at the end we have

739
00:42:51,190 --> 00:42:54,790
a word vector space that looks like a picture I showed where it has

740
00:42:54,790 --> 00:42:59,530
a good meaning of word meet good representation of word meaning.

741
00:42:59,530 --> 00:43:03,090
So slightly more, um,

742
00:43:03,090 --> 00:43:07,240
um, slightly more um graphically right.

743
00:43:07,240 --> 00:43:08,440
So here's the situation.

744
00:43:08,440 --> 00:43:12,835
So we've got part of our corpus problems turning into banking crisis,

745
00:43:12,835 --> 00:43:14,290
and so what we want to say is well,

746
00:43:14,290 --> 00:43:17,725
we want to know the meaning of the word into and so we're going to hope that

747
00:43:17,725 --> 00:43:21,400
its representation can be used in a way that'll

748
00:43:21,400 --> 00:43:24,820
make precise to predict what words appear in

749
00:43:24,820 --> 00:43:28,600
the context of into because that's the meaning of into.

750
00:43:28,600 --> 00:43:31,525
And so we're going to try and make those predictions,

751
00:43:31,525 --> 00:43:34,855
see how well we can predict and then change

752
00:43:34,855 --> 00:43:39,475
the vector representations of words in a way that we can do that prediction better.

753
00:43:39,475 --> 00:43:41,320
And then once we've dealt with into,

754
00:43:41,320 --> 00:43:43,765
we just go onto the next word and we say,

755
00:43:43,765 --> 00:43:46,060
okay, let's take banking as the word.

756
00:43:46,060 --> 00:43:49,795
The meaning of banking is predicting the contexts in which banking occurs.

757
00:43:49,795 --> 00:43:51,265
Here's one context.

758
00:43:51,265 --> 00:43:54,550
Let's try and predict these words that occur around banking and

759
00:43:54,550 --> 00:43:58,735
see how we do and then we'll move on again from there.

760
00:43:58,735 --> 00:44:02,470
Okay. Um, sounds easy so far.

761
00:44:02,470 --> 00:44:06,100
Um, [NOISE] now we go on and sort of do a bit more stuff.

762
00:44:06,100 --> 00:44:12,460
Okay. So overall, we have a big long corpus of capital T words.

763
00:44:12,460 --> 00:44:17,125
So if we have a whole lot of documents we just concatenate them all together and we say,

764
00:44:17,125 --> 00:44:19,014
okay, here's a billion words,

765
00:44:19,014 --> 00:44:21,745
and so big long list of words.

766
00:44:21,745 --> 00:44:23,305
And so what we're gonna do,

767
00:44:23,305 --> 00:44:26,875
is for the first um product we're going to sort of

768
00:44:26,875 --> 00:44:30,954
go through all the words and then for the second product,

769
00:44:30,954 --> 00:44:34,630
we're gonna say- we're gonna choose some fixed size window, you know,

770
00:44:34,630 --> 00:44:37,990
it might be five words on each side or something and we're going to try and

771
00:44:37,990 --> 00:44:42,010
predict the 10 words that are around that center word.

772
00:44:42,010 --> 00:44:44,200
And we're going to predict in the sense of trying to

773
00:44:44,200 --> 00:44:46,780
predict that word given the center word.

774
00:44:46,780 --> 00:44:48,460
That's our probability model.

775
00:44:48,460 --> 00:44:51,175
And so if we multiply all those things together,

776
00:44:51,175 --> 00:44:54,610
that's our model likelihood is how good a job it

777
00:44:54,610 --> 00:44:58,375
does at predicting the words around every word.

778
00:44:58,375 --> 00:45:01,600
And that model likelihood is going to depend

779
00:45:01,600 --> 00:45:05,185
on the parameters of our model which we write as theta.

780
00:45:05,185 --> 00:45:07,855
And in this particular model,

781
00:45:07,855 --> 00:45:10,690
the only parameters in it is actually

782
00:45:10,690 --> 00:45:13,810
going to be the vector representations we give the words.

783
00:45:13,810 --> 00:45:16,945
The model has absolutely no other parameters to it.

784
00:45:16,945 --> 00:45:20,050
So, we're just going to say we're representing

785
00:45:20,050 --> 00:45:23,695
a word with a vector in a vector space and that

786
00:45:23,695 --> 00:45:27,880
representation of it is its meaning and we're then going to be able to

787
00:45:27,880 --> 00:45:32,335
use that to predict what other words occur in a way I'm about to show you.

788
00:45:32,335 --> 00:45:37,240
Okay. So, um, that's our likelihood and so what we do in all of

789
00:45:37,240 --> 00:45:42,280
these models is we sort of define an objective function and then we're going to be,

790
00:45:42,280 --> 00:45:45,880
I want to come up with vector representations of words in

791
00:45:45,880 --> 00:45:50,740
such a way as to minimize our objective function.

792
00:45:50,740 --> 00:45:56,380
Um, so objective function is basically the same as what's on the top half of the slide,

793
00:45:56,380 --> 00:45:58,045
but we change a couple of things.

794
00:45:58,045 --> 00:46:03,040
We stick a minus sign in front of it so we can do minimization rather than maximization.

795
00:46:03,040 --> 00:46:05,515
Completely arbitrary makes no difference.

796
00:46:05,515 --> 00:46:08,125
Um, we stick a one and T in front of it,

797
00:46:08,125 --> 00:46:11,800
so that we're working out the sort of average

798
00:46:11,800 --> 00:46:16,150
as of a goodness of predicting for each choice of center word.

799
00:46:16,150 --> 00:46:19,360
Again, that sort of makes no difference but it kinda keeps the scale of

800
00:46:19,360 --> 00:46:23,095
things ah not dependent on the size of the corpus.

801
00:46:23,095 --> 00:46:27,235
Um, the bit that's actually important is we stick a log in front of

802
00:46:27,235 --> 00:46:31,690
the function that was up there um because it turns out that everything always gets nice.

803
00:46:31,690 --> 00:46:33,805
So when you stick logs and find the products

804
00:46:33,805 --> 00:46:36,370
um when you're doing things like optimization.

805
00:46:36,370 --> 00:46:38,860
So, when we do that we then got a log of

806
00:46:38,860 --> 00:46:42,430
all these products which will allow us to turn things you know,

807
00:46:42,430 --> 00:46:46,300
into a sums of the log of this probability

808
00:46:46,300 --> 00:46:50,755
and we'll go through that again um in just a minute.

809
00:46:50,755 --> 00:46:55,420
Okay. Um, and so if we can mi- if we can change

810
00:46:55,420 --> 00:47:00,865
our vector representations of these words so as to minimize this J of theta,

811
00:47:00,865 --> 00:47:06,110
that means we'll be good at predicting words in the context of another word.

812
00:47:06,540 --> 00:47:10,450
So then, that all sounded good but it was all

813
00:47:10,450 --> 00:47:13,960
dependent on having this probability function where you wanna

814
00:47:13,960 --> 00:47:17,020
predict the probability of a word in

815
00:47:17,020 --> 00:47:20,635
the context given the center word and the question is,

816
00:47:20,635 --> 00:47:23,620
how can you possibly do that?

817
00:47:23,620 --> 00:47:28,390
Um, well um, remember what I said is actually our model is just gonna

818
00:47:28,390 --> 00:47:33,655
have vector representations of words and that was the only parameters of the model.

819
00:47:33,655 --> 00:47:35,650
Now, that's, that's almost true.

820
00:47:35,650 --> 00:47:37,105
It's not quite true.

821
00:47:37,105 --> 00:47:39,220
Um, we actually cheat slightly.

822
00:47:39,220 --> 00:47:42,400
Since we actually propose two vector representations for

823
00:47:42,400 --> 00:47:46,600
each word and this makes it simpler to do this.

824
00:47:46,600 --> 00:47:48,070
Um, you cannot do this,

825
00:47:48,070 --> 00:47:50,620
there are ways to get around it but this is the simplest way to do it.

826
00:47:50,620 --> 00:47:54,610
So we have one vector for word when it's the center word that's predicting

827
00:47:54,610 --> 00:47:59,500
other words but we have a second vector for each word when it's a context word,

828
00:47:59,500 --> 00:48:01,225
so that's one of the words in context.

829
00:48:01,225 --> 00:48:02,680
So for each word type,

830
00:48:02,680 --> 00:48:06,850
we have these two vectors as center word, as context word.

831
00:48:06,850 --> 00:48:12,700
Um, so then we're gonna work out this probability of a word in the context,

832
00:48:12,700 --> 00:48:14,574
given the center word,

833
00:48:14,574 --> 00:48:22,105
purely in terms of these vectors and the way we do it is with this equation right here,

834
00:48:22,105 --> 00:48:25,165
which I'll explain more in just a moment.

835
00:48:25,165 --> 00:48:29,650
So we're still on exactly the same situation, right?

836
00:48:29,650 --> 00:48:32,050
That we're wanting to work out probabilities of

837
00:48:32,050 --> 00:48:35,665
words occurring in the context of our center word.

838
00:48:35,665 --> 00:48:38,785
So the center word is C and the context words represented with

839
00:48:38,785 --> 00:48:42,370
O and these [inaudible] slide notation but sort of,

840
00:48:42,370 --> 00:48:44,890
we're basically saying there's one kind of

841
00:48:44,890 --> 00:48:47,590
vector for center words is a different kind of vector

842
00:48:47,590 --> 00:48:53,665
for context words and we're gonna work out this probabilistic prediction um,

843
00:48:53,665 --> 00:48:56,470
in terms of these word vectors.

844
00:48:56,470 --> 00:48:59,260
Okay. So how can we do that?

845
00:48:59,260 --> 00:49:02,950
Well, the way we do it is with this um,

846
00:49:02,950 --> 00:49:07,870
formula here which is the sort of shape that you see over and over again um,

847
00:49:07,870 --> 00:49:10,300
in deep learning with categorical staff.

848
00:49:10,300 --> 00:49:12,670
So for the very center bit of it,

849
00:49:12,670 --> 00:49:17,815
the bit in orange are more the same thing occurs in the um, denominator.

850
00:49:17,815 --> 00:49:21,130
What we're doing there is calculating a dot product.

851
00:49:21,130 --> 00:49:24,460
So, we're gonna go through the components of our vector and we're gonna

852
00:49:24,460 --> 00:49:28,750
multiply them together and that means if um,

853
00:49:28,750 --> 00:49:32,800
different words have B components of the same sign,

854
00:49:32,800 --> 00:49:35,620
plus or minus, in the same positions,

855
00:49:35,620 --> 00:49:38,920
the dot product will be big and if

856
00:49:38,920 --> 00:49:42,460
they have different signs or one is big and one is small,

857
00:49:42,460 --> 00:49:44,410
the dot product will be a lot smaller.

858
00:49:44,410 --> 00:49:48,100
So that orange part directly calculates uh,

859
00:49:48,100 --> 00:49:51,670
sort of a similarity between words where

860
00:49:51,670 --> 00:49:55,345
the similarity is the sort of vectors looking the same, right?

861
00:49:55,345 --> 00:49:57,610
Um, and so that's the heart of it, right?

862
00:49:57,610 --> 00:50:00,130
So we're gonna have words that have similar vectors,

863
00:50:00,130 --> 00:50:04,240
IS close together in the vector space have similar meaning.

864
00:50:04,240 --> 00:50:06,580
Um, so for the rest of it- um,

865
00:50:06,580 --> 00:50:10,330
so the next thing we do is take that number and put an X around it.

866
00:50:10,330 --> 00:50:12,100
So, um, the exponential has

867
00:50:12,100 --> 00:50:15,295
this nice property that no matter what number you stick into it,

868
00:50:15,295 --> 00:50:17,845
because the dot product might be positive or negative,

869
00:50:17,845 --> 00:50:20,890
it's gonna come out as a positive number and if

870
00:50:20,890 --> 00:50:24,160
we eventually wanna get a probability, um, that's really good.

871
00:50:24,160 --> 00:50:28,450
If we have positive numbers and not negative numbers, um, so that's good.

872
00:50:28,450 --> 00:50:33,370
Um, then the third part of which is the bid in blue is we wanted to have

873
00:50:33,370 --> 00:50:36,070
probabilities and probabilities are meant to add up to

874
00:50:36,070 --> 00:50:39,970
one and so we do that in the standard, dumbest possible way.

875
00:50:39,970 --> 00:50:42,205
We sum up what this quantity is,

876
00:50:42,205 --> 00:50:47,080
that every different word in our vocabulary and we divide through by

877
00:50:47,080 --> 00:50:52,315
it and so that normalizes things and turns them into a probability distribution.

878
00:50:52,315 --> 00:50:54,685
Yeah, so there's sort of in practice,

879
00:50:54,685 --> 00:50:55,990
there are two parts.

880
00:50:55,990 --> 00:50:59,110
There's the orange part which is this idea of using

881
00:50:59,110 --> 00:51:03,580
dot product and a vector space as our similarity measure between words

882
00:51:03,580 --> 00:51:07,480
and then the second part is all the rest of it where we feed it

883
00:51:07,480 --> 00:51:11,665
through what we refer to a news all the time as a softmax distribution.

884
00:51:11,665 --> 00:51:17,530
So the two parts of the expen normalizing gives you a softmax distribution.

885
00:51:17,530 --> 00:51:22,120
Um, and softmax functions will sort of map any numbers into

886
00:51:22,120 --> 00:51:26,950
a probability distribution always for the two reasons that I gave and so,

887
00:51:26,950 --> 00:51:30,000
it's referred to as a softmax um,

888
00:51:30,000 --> 00:51:33,525
because it works like a softmax, right?

889
00:51:33,525 --> 00:51:35,040
So if you have numbers,

890
00:51:35,040 --> 00:51:39,735
you could just say what's the max of these numbers, um,

891
00:51:39,735 --> 00:51:46,810
and you know that's sort of a hot- if you sort of map your original numbers into,

892
00:51:46,810 --> 00:51:49,390
if it's the max of the max and everything else is zero,

893
00:51:49,390 --> 00:51:51,160
that's sort of a hard max.

894
00:51:51,160 --> 00:51:56,935
Um, soft- this is a softmax because the exponenti- you know,

895
00:51:56,935 --> 00:52:00,310
if you sort of imagine this but- if we just ignore the problem

896
00:52:00,310 --> 00:52:04,315
negative numbers for a moment and you got rid of the exp, um,

897
00:52:04,315 --> 00:52:06,220
then you'd sort of coming out with

898
00:52:06,220 --> 00:52:09,640
a probability distribution but by and large it's so be fairly

899
00:52:09,640 --> 00:52:12,070
flat and wouldn't particularly pick out the max of

900
00:52:12,070 --> 00:52:15,310
the different XI numbers whereas when you exponentiate them,

901
00:52:15,310 --> 00:52:18,670
that sort of makes big numbers way bigger and so, this,

902
00:52:18,670 --> 00:52:25,990
this softmax sort of mainly puts mass where the max's or the couple of max's are.

903
00:52:25,990 --> 00:52:29,920
Um, so that's the max part and a soft part is that this isn't

904
00:52:29,920 --> 00:52:34,900
a hard decisions still spreads a little bit of probability mass everywhere else.

905
00:52:34,900 --> 00:52:40,540
Okay, so now we have uh, loss function.

906
00:52:40,540 --> 00:52:45,160
We have a loss function with a probability model on the inside that we can

907
00:52:45,160 --> 00:52:50,230
build and so what we want to be able to do is then um,

908
00:52:50,230 --> 00:52:55,690
move our vector representations of words around

909
00:52:55,690 --> 00:53:01,075
so that they are good at predicting what words occur in the context of other words.

910
00:53:01,075 --> 00:53:06,400
Um, and so, at this point what we're gonna do is optimization.

911
00:53:06,400 --> 00:53:10,465
So, we have vector components of different words.

912
00:53:10,465 --> 00:53:13,180
We have a very high-dimensional space again but here,

913
00:53:13,180 --> 00:53:16,270
I've just got two for the picture and we're gonna wanna

914
00:53:16,270 --> 00:53:19,510
say how- how can we minimize this function and we're going to

915
00:53:19,510 --> 00:53:23,920
want to jiggle the numbers that are used in the word representations in

916
00:53:23,920 --> 00:53:28,990
such a way that we're walking down the slope of this space.

917
00:53:28,990 --> 00:53:32,095
I walking down the gradient and um,

918
00:53:32,095 --> 00:53:37,330
then we're gonna minimize the function we found good representations for words.

919
00:53:37,330 --> 00:53:39,775
So doing this for this case,

920
00:53:39,775 --> 00:53:42,070
we want to make a very big vector in

921
00:53:42,070 --> 00:53:45,400
a very high-dimensional vector space of all the parameters of

922
00:53:45,400 --> 00:53:48,730
our model and the only parameters that this model

923
00:53:48,730 --> 00:53:53,095
has is literally the vector space representations of words.

924
00:53:53,095 --> 00:53:56,170
So if there are a 100 dimensional word representations,

925
00:53:56,170 --> 00:53:59,320
they're sort of a 100 parameters for aardvark and context,

926
00:53:59,320 --> 00:54:03,400
100 parameters for the word a- in context et cetera going through,

927
00:54:03,400 --> 00:54:08,020
100 parameters for the word aardvark [NOISE] as a center word et cetera,

928
00:54:08,020 --> 00:54:12,520
et cetera through that gives us a high big vector of parameters to

929
00:54:12,520 --> 00:54:18,265
optimize and we're gonna run this optimization and then um, move them down.

930
00:54:18,265 --> 00:54:23,740
Um, [NOISE] yeah so that's essentially what you do.

931
00:54:23,740 --> 00:54:26,365
Um, I sort of wanted to go through um,

932
00:54:26,365 --> 00:54:28,990
the details of this um,

933
00:54:28,990 --> 00:54:32,440
just so we've kind of gone through things concretely to

934
00:54:32,440 --> 00:54:36,070
make sure everyone is on the same page.

935
00:54:36,070 --> 00:54:39,475
Um, so I suspect that, you know,

936
00:54:39,475 --> 00:54:43,510
if I try and do this concretely,

937
00:54:43,510 --> 00:54:45,865
um, there are a lot of people um,

938
00:54:45,865 --> 00:54:50,830
that this will bore and some people that are- will bore very badly,

939
00:54:50,830 --> 00:54:54,415
um, so I apologize for you,

940
00:54:54,415 --> 00:54:55,810
um, but you know,

941
00:54:55,810 --> 00:54:59,140
I'm hoping and thinking that there's probably

942
00:54:59,140 --> 00:55:02,650
some people who haven't done as much of this stuff recently

943
00:55:02,650 --> 00:55:05,740
and it might just actually be good to do it concretely

944
00:55:05,740 --> 00:55:09,760
and get everyone up to speed right at the beginning. Yeah?

945
00:55:09,760 --> 00:55:14,680
[inaudible] how do we calculate [inaudible] specifically?

946
00:55:14,680 --> 00:55:20,275
Well, so, we- so the way we calculate the,

947
00:55:20,275 --> 00:55:26,050
the U and V vectors is we're literally going to start with a random vector for

948
00:55:26,050 --> 00:55:33,010
each word and then we iteratively going to change those vectors a little bit as we learn.

949
00:55:33,010 --> 00:55:37,135
And the way we're going to work out how to change them is we're gonna say,

950
00:55:37,135 --> 00:55:42,400
"I want to do optimization," and that is going to be implemented as okay.

951
00:55:42,400 --> 00:55:44,830
We have the current vectors for each word.

952
00:55:44,830 --> 00:55:51,550
Let me do some calculus to work out how I could change the word vectors, um, to mean,

953
00:55:51,550 --> 00:55:55,780
that the word vectors would calculate a higher probability for

954
00:55:55,780 --> 00:56:00,160
the words that actually occur in contexts of this center word.

955
00:56:00,160 --> 00:56:01,855
And we will do that,

956
00:56:01,855 --> 00:56:03,925
and we'll do it again and again and again,

957
00:56:03,925 --> 00:56:06,760
and then will eventually end up with good word vectors.

958
00:56:06,760 --> 00:56:08,260
Thank you for that question,

959
00:56:08,260 --> 00:56:10,780
cause that's a concept that you're meant to have understood.

960
00:56:10,780 --> 00:56:13,330
Is that how this works and maybe I didn't

961
00:56:13,330 --> 00:56:16,645
explain that high-level recipe well enough, yeah.

962
00:56:16,645 --> 00:56:20,410
Okay, so yeah, so let's just go through it. So, we've seen it, right?

963
00:56:20,410 --> 00:56:24,070
So, we had this formula that we wanted to maximize, you know,

964
00:56:24,070 --> 00:56:32,410
our original function which was the product of T equals one to T,

965
00:56:32,410 --> 00:56:35,995
and then the product of the words, uh,

966
00:56:35,995 --> 00:56:40,720
position minus M less than or equal to J,

967
00:56:40,720 --> 00:56:42,460
less than or equal to M,

968
00:56:42,460 --> 00:56:46,000
J not equal to zero of, um,

969
00:56:46,000 --> 00:56:51,640
the probability of W. At prime at T

970
00:56:51,640 --> 00:56:57,700
plus J given WT according to the parameters of our model.

971
00:56:57,700 --> 00:57:01,330
Okay, and then we'd already seen that we were gonna convert that

972
00:57:01,330 --> 00:57:05,515
into the function that we're going to use where we have J of Theta,

973
00:57:05,515 --> 00:57:15,490
where we had the minus one on T. Of the sum of T equals one to T of the sum of minus M,

974
00:57:15,490 --> 00:57:17,770
less than or equal to J less than or equal to M,

975
00:57:17,770 --> 00:57:27,400
J not equal to zero of the log of the probability of W times T, plus J, W,

976
00:57:27,400 --> 00:57:31,840
T. Okay, so we had that and then we'd had

977
00:57:31,840 --> 00:57:36,490
this formula that the probability of the outside word given

978
00:57:36,490 --> 00:57:46,360
the context word is this formula we just went through of xu ot vc over

979
00:57:46,360 --> 00:57:56,770
the sum of W equals one to the vocabulary size of xu wt vc.

980
00:57:56,770 --> 00:57:59,530
Okay, so that's sort of our model.

981
00:57:59,530 --> 00:58:03,835
We want to min- minimize this.

982
00:58:03,835 --> 00:58:11,230
So, we wanna minimize this and we want to minimize that by changing these parameters.

983
00:58:11,230 --> 00:58:15,415
And these parameters are the contents of these vectors.

984
00:58:15,415 --> 00:58:17,635
And so, what we want to do now,

985
00:58:17,635 --> 00:58:23,560
is do calculus and we wanna say let's work out in terms of these parameters which are,

986
00:58:23,560 --> 00:58:25,960
u and v vectors, um,

987
00:58:25,960 --> 00:58:30,115
for the current values of the parameters which we initialized randomly.

988
00:58:30,115 --> 00:58:32,050
Like what's the slope of the space?

989
00:58:32,050 --> 00:58:33,490
Where is downhill?

990
00:58:33,490 --> 00:58:35,770
Because if we can work out downhill is,

991
00:58:35,770 --> 00:58:39,115
we got just gotta walk downhill and our model gets better.

992
00:58:39,115 --> 00:58:42,010
So, we're gonna take derivatives and work out what

993
00:58:42,010 --> 00:58:45,610
direction downhill is and then we wanna walk that way, yeah.

994
00:58:45,610 --> 00:58:50,230
So, why do we wanna maximize that probable edge and like,

995
00:58:50,230 --> 00:58:51,805
like going through every word,

996
00:58:51,805 --> 00:58:57,640
it's like [inaudible] given the [inaudible]

997
00:58:57,640 --> 00:58:59,665
So, well, so, so,

998
00:58:59,665 --> 00:59:02,905
I'm wanting to achieve this, um,

999
00:59:02,905 --> 00:59:08,395
what I want to achieve for my distributional notion of meaning is,

1000
00:59:08,395 --> 00:59:11,500
I have a meaningful word, a vector.

1001
00:59:11,500 --> 00:59:16,810
And that vector knows what words occur in the context of,

1002
00:59:16,810 --> 00:59:19,525
um, a word- of itself.

1003
00:59:19,525 --> 00:59:23,020
And knowing what words occur in its context means,

1004
00:59:23,020 --> 00:59:24,790
it can accurately give

1005
00:59:24,790 --> 00:59:28,945
a high probability estimate to those words that occur in the context,

1006
00:59:28,945 --> 00:59:32,319
and it will give low probability estimates

1007
00:59:32,319 --> 00:59:35,050
to words that don't typically occur in the context.

1008
00:59:35,050 --> 00:59:37,240
So, you know, if the word is bank,

1009
00:59:37,240 --> 00:59:39,549
I'm hoping that words like branch,

1010
00:59:39,549 --> 00:59:41,575
and open, and withdrawal,

1011
00:59:41,575 --> 00:59:43,360
will be given high probability,

1012
00:59:43,360 --> 00:59:45,445
cause they tend to occur with the word bank.

1013
00:59:45,445 --> 00:59:49,950
And I'm hoping that some other words, um,

1014
00:59:49,950 --> 00:59:52,740
like neural network or something have

1015
00:59:52,740 --> 00:59:57,970
a lower probability because they don't tend to occur with the word bank.

1016
00:59:58,290 --> 01:00:01,525
Okay, um, does that make sense?

1017
01:00:01,525 --> 01:00:01,780
Yeah.

1018
01:00:01,780 --> 01:00:03,730
Yeah. And the other thing I was,

1019
01:00:03,730 --> 01:00:06,865
I'd forgotten meant to comment was, you know, obviously,

1020
01:00:06,865 --> 01:00:10,480
we're not gonna be able to do this super well or it's just not gonna be able,

1021
01:00:10,480 --> 01:00:13,180
that we can say all the words in the context is going to

1022
01:00:13,180 --> 01:00:15,880
be this word with probability 0,97, right?

1023
01:00:15,880 --> 01:00:19,750
Because we're using this one simple probability distribution

1024
01:00:19,750 --> 01:00:23,230
to predict all words in our context.

1025
01:00:23,230 --> 01:00:27,880
So, in particular, we're using it to predict 10 different words generally, right?

1026
01:00:27,880 --> 01:00:32,425
So, at best, we can kind of be giving sort of five percent chance to one of them, right?

1027
01:00:32,425 --> 01:00:33,820
We can't possibly be,

1028
01:00:33,820 --> 01:00:35,950
so guessing right every time.

1029
01:00:35,950 --> 01:00:37,390
Um, and well, you know,

1030
01:00:37,390 --> 01:00:40,255
they're gonna be different contexts with different words in them.

1031
01:00:40,255 --> 01:00:44,610
So, you know, it's gonna be a very loose model,

1032
01:00:44,610 --> 01:00:48,660
but nevertheless, we wanna capture the fact that, you know,

1033
01:00:48,660 --> 01:00:51,330
withdrawal is much more likely, um,

1034
01:00:51,330 --> 01:00:57,580
to occur near the word bank than something like football.

1035
01:00:57,580 --> 01:01:01,030
That's, you know, basically what our goal is.

1036
01:01:01,030 --> 01:01:07,360
Okay, um, yes, so we want to maximize this,

1037
01:01:07,360 --> 01:01:12,610
by minimizing this, which means we then want to do some calculus to work this out.

1038
01:01:12,610 --> 01:01:14,740
So, what we're then gonna do is,

1039
01:01:14,740 --> 01:01:16,720
that we're going to say, well,

1040
01:01:16,720 --> 01:01:19,495
these parameters are our word vectors

1041
01:01:19,495 --> 01:01:22,630
and we're gonna sort of want to move these word vectors,

1042
01:01:22,630 --> 01:01:28,180
um, to, um, work things out as to how to, um, walk downhill.

1043
01:01:28,180 --> 01:01:32,440
So, the case that I'm going to do now is gonna look at the parameters of

1044
01:01:32,440 --> 01:01:38,275
this center word vc and work out how to do things with respect to it.

1045
01:01:38,275 --> 01:01:40,750
Um, now, that's not the only thing that you wanna do,

1046
01:01:40,750 --> 01:01:44,905
you also want to work out the slope with respect to the uo vector.

1047
01:01:44,905 --> 01:01:47,965
Um, but I'm not gonna do that because time in class is going to run out.

1048
01:01:47,965 --> 01:01:49,750
So, it'd be really good if you did that one at

1049
01:01:49,750 --> 01:01:51,715
home and then you'd feel much more competent.

1050
01:01:51,715 --> 01:01:57,130
Right, so then, um, so what I'm wanting you to do is work out the partial derivative with

1051
01:01:57,130 --> 01:02:03,205
respect to my vc vector representation of this quantity,

1052
01:02:03,205 --> 01:02:04,810
that we were just looking at.

1053
01:02:04,810 --> 01:02:08,290
Which is, um, the quantity in here,

1054
01:02:08,290 --> 01:02:11,980
um, where we're taking the log of that quantity.

1055
01:02:11,980 --> 01:02:17,560
Right, the log of the x of u,

1056
01:02:17,560 --> 01:02:20,140
o, T, v, c,

1057
01:02:20,140 --> 01:02:26,830
over the sum of W equals one to V of the x of u,

1058
01:02:26,830 --> 01:02:30,220
o, T, v, c. Okay,

1059
01:02:30,220 --> 01:02:33,219
so this, um, so now we have a log of the division,

1060
01:02:33,219 --> 01:02:35,695
so that's easy to rewrite, um,

1061
01:02:35,695 --> 01:02:39,595
that we have a partial derivative of the log of

1062
01:02:39,595 --> 01:02:47,560
the numerator minus and

1063
01:02:47,560 --> 01:02:49,690
I can distribute the partial derivative.

1064
01:02:49,690 --> 01:02:53,395
So, I can have minus the partial derivative,

1065
01:02:53,395 --> 01:02:56,680
um, of the denominator,

1066
01:02:56,680 --> 01:02:59,710
um, which is log of this thing.

1067
01:02:59,710 --> 01:03:08,865
[NOISE]

1068
01:03:08,865 --> 01:03:18,190
Okay. Um, so this is sort of what was the numerator and this is what was the denominator.

1069
01:03:19,190 --> 01:03:27,060
Okay. So, um, the part that was the numerator is really easy.

1070
01:03:27,060 --> 01:03:29,130
In fact maybe I can fit it in here.

1071
01:03:29,130 --> 01:03:33,450
Um, so log on exp are just inverses of each other,

1072
01:03:33,450 --> 01:03:34,800
so they cancel out.

1073
01:03:34,800 --> 01:03:43,650
So, we've got the partial derivative of U_o T V_c.

1074
01:03:43,650 --> 01:03:47,460
Okay, so this point I should, um, just, um,

1075
01:03:47,460 --> 01:03:51,630
remind people right that this V_c here's a vector of- um,

1076
01:03:51,630 --> 01:03:56,130
it's still a vector right because we had a 100 dimensional representation of a word.

1077
01:03:56,130 --> 01:04:00,330
Um, so this is doing multivariate calculus.

1078
01:04:00,330 --> 01:04:02,790
Um, so you know, if you're,

1079
01:04:02,790 --> 01:04:04,530
if you at all, um,

1080
01:04:04,530 --> 01:04:06,105
remember any of this stuff,

1081
01:04:06,105 --> 01:04:08,175
you can say, "Ha this is trivial".

1082
01:04:08,175 --> 01:04:12,390
The answer to that is you are done, um and that's great.

1083
01:04:12,390 --> 01:04:14,955
But you know, if you're, um, feeling, um,

1084
01:04:14,955 --> 01:04:17,550
not so good on all of this stuff, um,

1085
01:04:17,550 --> 01:04:19,125
and you wanna sort of, um,

1086
01:04:19,125 --> 01:04:22,440
cheat a little on the side and try and work out what it is,

1087
01:04:22,440 --> 01:04:24,180
um, you can sort of say,

1088
01:04:24,180 --> 01:04:25,980
"Well, let me um,,

1089
01:04:25,980 --> 01:04:28,380
work out the partial derivative,

1090
01:04:28,380 --> 01:04:34,200
um with respect to one element of this vector like the first element of this vector".

1091
01:04:34,200 --> 01:04:42,869
Well, what I actually got here for this dot product is I have U_o one times V_c one,

1092
01:04:42,869 --> 01:04:49,560
plus U_o two times V_c two plus dot, dot,

1093
01:04:49,560 --> 01:04:56,910
dot plus U_o 100 times V_c 100, right,

1094
01:04:56,910 --> 01:05:02,535
and I'm finding the partial derivative of this with respect to V_c one,

1095
01:05:02,535 --> 01:05:05,490
and hopefully remember that much calculus from high school

1096
01:05:05,490 --> 01:05:09,135
of none of these terms involve V_c one.

1097
01:05:09,135 --> 01:05:12,660
So, the only thing that's left is this U_o one,

1098
01:05:12,660 --> 01:05:15,960
and that's what I've got there for this dimension.

1099
01:05:15,960 --> 01:05:17,850
So, this particular parameter.

1100
01:05:17,850 --> 01:05:23,265
But I don't only want to do the first component of the V_c vector,

1101
01:05:23,265 --> 01:05:26,745
I also want to do the second component of the V_c vector et cetera,

1102
01:05:26,745 --> 01:05:30,630
which means I'm going to end up with all of them

1103
01:05:30,630 --> 01:05:35,685
turning up in precisely one of these things.

1104
01:05:35,685 --> 01:05:41,190
Um, and so the end result is I get the vector U_o.

1105
01:05:41,190 --> 01:05:43,620
Okay. Um, but you know,

1106
01:05:43,620 --> 01:05:47,220
if you're sort of getting confused and your brain is falling apart,

1107
01:05:47,220 --> 01:05:52,050
I think it can be sort of kind of useful to re- reduce things to sort of um,

1108
01:05:52,050 --> 01:05:58,275
single dimensional calculus and actually sort of play out what's actually happening.

1109
01:05:58,275 --> 01:06:00,840
Um, anyway, this part was easy.

1110
01:06:00,840 --> 01:06:03,540
The numerator, we get um, U_o.

1111
01:06:03,540 --> 01:06:08,085
Um, so things aren't quite so nice when we do the denominator.

1112
01:06:08,085 --> 01:06:11,640
So we now want to have this, um, B_d,

1113
01:06:11,640 --> 01:06:17,010
V_c of the log of the sum of W equals

1114
01:06:17,010 --> 01:06:22,845
one to the P_x of U_o T V_c.

1115
01:06:22,845 --> 01:06:25,800
Okay. So, now at this point,

1116
01:06:25,800 --> 01:06:27,450
I'm not quite so pretty.

1117
01:06:27,450 --> 01:06:31,035
We've got this log sum X combination that you see a lot,

1118
01:06:31,035 --> 01:06:35,640
and so at this point you have to remember that there was E, the chain rule.

1119
01:06:35,640 --> 01:06:38,520
Okay. So, what we can say is here's you know,

1120
01:06:38,520 --> 01:06:42,540
our function F and here is the body of the function,

1121
01:06:42,540 --> 01:06:46,245
and so what we want to do is um,

1122
01:06:46,245 --> 01:06:48,630
do it in two stages.

1123
01:06:48,630 --> 01:06:51,570
Um, so that at the end of the day,

1124
01:06:51,570 --> 01:06:53,430
we've got this V_c at the end.

1125
01:06:53,430 --> 01:06:57,105
So, we have sort of some function here.

1126
01:06:57,105 --> 01:06:59,910
There's ultimately a function of V_c,

1127
01:06:59,910 --> 01:07:02,220
and so we gonna do with a chain rule.

1128
01:07:02,220 --> 01:07:05,040
We'll say the chain rule is we first take

1129
01:07:05,040 --> 01:07:09,135
the derivative of this outside thing putting in this body,

1130
01:07:09,135 --> 01:07:13,680
and then we remember that the derivative of log is one on X.

1131
01:07:13,680 --> 01:07:22,920
So, we have one over the sum of W equals one to V of the exp of U_o T V_c

1132
01:07:22,920 --> 01:07:26,640
and then we need to multiply that by then taking

1133
01:07:26,640 --> 01:07:32,610
the derivative of the inside part which is um,

1134
01:07:32,610 --> 01:07:34,750
what we have here.

1135
01:07:40,490 --> 01:07:44,850
Okay. Times the derivative of the inside part with

1136
01:07:44,850 --> 01:07:48,600
the important reminder that you need to do a change of variables,

1137
01:07:48,600 --> 01:07:53,460
and for the inside part use a different variable that you're summing over.

1138
01:07:53,460 --> 01:08:00,810
Okay. So, now we're trying to find the derivative of a sum of X.

1139
01:08:00,810 --> 01:08:05,055
The first thing that we can do is v-very easy.

1140
01:08:05,055 --> 01:08:08,865
We can move the derivative inside a sum.

1141
01:08:08,865 --> 01:08:14,430
So, we can rewrite that and have at the sum first of the X equals one to

1142
01:08:14,430 --> 01:08:20,430
V of the partial derivatives with respect to V_c of the [inaudible].

1143
01:08:20,430 --> 01:08:22,575
Um, so that's a little bit of progress.

1144
01:08:22,575 --> 01:08:26,730
Um and that point we have to sort of do the chain rule again, right.

1145
01:08:26,730 --> 01:08:33,210
So, here is our function and here's the thing in it again which is some function of V_c.

1146
01:08:33,210 --> 01:08:37,605
So, we again want to do um, the chain rule.

1147
01:08:37,605 --> 01:08:41,340
So, [NOISE] we then have well,

1148
01:08:41,340 --> 01:08:45,720
the derivative of X um, is exp.

1149
01:08:45,720 --> 01:08:54,630
So, we gonna have the sum of X equals one to V of exp of U_x T V_c,

1150
01:08:54,630 --> 01:09:00,150
and then we're going to multiply that by the partial derivative with

1151
01:09:00,150 --> 01:09:05,700
respect to T V_c of the inside U_x T V_c.

1152
01:09:05,700 --> 01:09:08,160
Well, we saw that one before, so,

1153
01:09:08,160 --> 01:09:13,200
the derivative of that is U- well,

1154
01:09:13,200 --> 01:09:16,320
yeah, U_x because we're doing it through a different X, right.

1155
01:09:16,320 --> 01:09:18,780
This then becomes out as U_x,

1156
01:09:18,780 --> 01:09:23,850
and so we have the sum of the X equals one to

1157
01:09:23,850 --> 01:09:30,030
V of this exp U X T B C times the U_of X.

1158
01:09:30,030 --> 01:09:34,995
Okay. So, by doing the chain rule twice, we've got that.

1159
01:09:34,995 --> 01:09:38,190
So, now if we put it together, you know,

1160
01:09:38,190 --> 01:09:43,050
the derivative of V_c with respect of the whole thing,

1161
01:09:43,050 --> 01:09:46,500
this log of the probability of O given C, right.

1162
01:09:46,500 --> 01:09:51,210
That for the numerator it was just U_o,

1163
01:09:51,210 --> 01:09:54,030
and then we're subtracting,

1164
01:09:54,030 --> 01:09:57,645
we had this term here, um,

1165
01:09:57,645 --> 01:09:59,730
which is sort of a denominator,

1166
01:09:59,730 --> 01:10:03,870
and then we have this term here which is the numerator.

1167
01:10:03,870 --> 01:10:07,725
So, we're subtracting in the numerator,

1168
01:10:07,725 --> 01:10:12,270
we have the sum of X equals one to V of

1169
01:10:12,270 --> 01:10:18,765
the exp of U_x T V_c times U_x,

1170
01:10:18,765 --> 01:10:25,395
and then in the denominator, we have um,

1171
01:10:25,395 --> 01:10:35,920
the sum of W equals one to V of exp of U_w T V_c.

1172
01:10:36,350 --> 01:10:40,035
Um, okay, so we kind of get that.

1173
01:10:40,035 --> 01:10:44,025
Um, oh wait. Yeah. Yeah, I've gotten.

1174
01:10:44,025 --> 01:10:45,900
Yeah, that's right. Um, okay.

1175
01:10:45,900 --> 01:10:52,170
We kind of get that and then we can sort of just re-arrange this a little.

1176
01:10:52,170 --> 01:10:56,475
So, we can have this sum right out front,

1177
01:10:56,475 --> 01:11:03,284
and we can say that this is sort of a big sum of X equals one to V,

1178
01:11:03,284 --> 01:11:09,870
and we can sort of take that U_x out the end and say, okay.

1179
01:11:09,870 --> 01:11:13,155
Let's call that put over here a U_x,

1180
01:11:13,155 --> 01:11:15,090
and if we do that,

1181
01:11:15,090 --> 01:11:20,295
sort of an interesting thing has happened because look right here,

1182
01:11:20,295 --> 01:11:25,830
we've rediscovered exactly the same form

1183
01:11:25,830 --> 01:11:31,425
that we use as our probability distribution for predicting the probability of words.

1184
01:11:31,425 --> 01:11:37,860
So, this is now simply the probability of X given C according to our model.

1185
01:11:37,860 --> 01:11:46,155
Um, so we can rewrite this and say that what we're getting is U_o minus the sum of

1186
01:11:46,155 --> 01:11:54,795
X equals one to V of the probability of X given C times U_x.

1187
01:11:54,795 --> 01:11:58,755
This has a kind of an interesting meaning if you think about it.

1188
01:11:58,755 --> 01:12:01,365
So, this is actually giving us, you know,

1189
01:12:01,365 --> 01:12:04,200
our slope in this multi-dimensional space

1190
01:12:04,200 --> 01:12:07,215
and how we're getting that slope is we're taking

1191
01:12:07,215 --> 01:12:11,280
the observed representation of

1192
01:12:11,280 --> 01:12:18,450
the context word and we're subtracting from that what our model thinks um,

1193
01:12:18,450 --> 01:12:20,955
the context should look like.

1194
01:12:20,955 --> 01:12:24,465
What does the model think that the context should look like?

1195
01:12:24,465 --> 01:12:27,330
This part here is formal in expectation.

1196
01:12:27,330 --> 01:12:31,395
So, what you're doing is you're finding the weighted average

1197
01:12:31,395 --> 01:12:36,375
of the models of the representations of each word,

1198
01:12:36,375 --> 01:12:39,990
multiplied by the probability of it in the current model.

1199
01:12:39,990 --> 01:12:45,315
So, this is sort of the expected context word according to our current model,

1200
01:12:45,315 --> 01:12:47,460
and so we're taking the difference between

1201
01:12:47,460 --> 01:12:52,170
the expected context word and the actual context word that showed up,

1202
01:12:52,170 --> 01:12:55,560
and that difference then turns out to exactly give

1203
01:12:55,560 --> 01:12:58,890
us the slope as to which direction we should be

1204
01:12:58,890 --> 01:13:01,050
walking changing the words

1205
01:13:01,050 --> 01:13:06,715
representation in order to improve our model's ability to predict.

1206
01:13:06,715 --> 01:13:11,565
Okay. Um, so we'll,

1207
01:13:11,565 --> 01:13:14,100
um, assignment two, um, yeah.

1208
01:13:14,100 --> 01:13:18,060
So, um, it'll be a great exercise for you guys,

1209
01:13:18,060 --> 01:13:20,115
um, to in- um,

1210
01:13:20,115 --> 01:13:22,830
to try and do that for the cen-, wait,

1211
01:13:22,830 --> 01:13:26,640
um I did the center words trying to look context words as well

1212
01:13:26,640 --> 01:13:31,125
and show you that you can do the same kind of piece of math and have it work out.

1213
01:13:31,125 --> 01:13:35,655
Um, if I've just got a few minutes left at the end.

1214
01:13:35,655 --> 01:13:43,320
Um, what I just wanted to show you if I can get all of this to work right.

1215
01:13:43,320 --> 01:13:49,950
Um, let's go [inaudible] this way.

1216
01:13:49,950 --> 01:13:53,590
Okay, find my.

1217
01:13:54,200 --> 01:14:00,075
Okay. Um, so I just wanted to just show you a quick example.

1218
01:14:00,075 --> 01:14:01,935
So, for the first assignment,

1219
01:14:01,935 --> 01:14:04,170
um, again it's an iPython Notebook.

1220
01:14:04,170 --> 01:14:09,015
So, if you're all set up you sort of can do Jupyter Notebook.

1221
01:14:09,015 --> 01:14:12,945
Um, and you have some notebook.

1222
01:14:12,945 --> 01:14:16,630
Um, here's my little notebook I'm gonna show you,

1223
01:14:17,180 --> 01:14:23,620
um, and the trick will be to make this big enough that people can see it.

1224
01:14:30,940 --> 01:14:35,530
That readable? [LAUGHTER] Okay, um,

1225
01:14:35,530 --> 01:14:39,210
so right so, so Numpy is the sort of,

1226
01:14:39,210 --> 01:14:41,930
um, do math package in Python.

1227
01:14:41,930 --> 01:14:43,120
You'll want to know about that.

1228
01:14:43,120 --> 01:14:44,445
If you don't know about it.

1229
01:14:44,445 --> 01:14:46,440
Um, Matplotlib is sort of the,

1230
01:14:46,440 --> 01:14:49,040
one of the most basic graphing package

1231
01:14:49,040 --> 01:14:51,755
if you don't know about that you're going to want to know about it.

1232
01:14:51,755 --> 01:14:55,900
This is sort of an IPython or Jupyter special that

1233
01:14:55,900 --> 01:14:59,755
lets you have an interactive matplotlib um, inside.

1234
01:14:59,755 --> 01:15:03,675
And if you want to get fancy you can play it- play with your graphic styles.

1235
01:15:03,675 --> 01:15:06,615
Um, there's that.

1236
01:15:06,615 --> 01:15:10,465
Scikit-learn is kind of a general machine learning package.

1237
01:15:10,465 --> 01:15:13,350
Um, Gensim isn't a deep learning package.

1238
01:15:13,350 --> 01:15:17,590
Gensim is kind of a word similarity package which started off um,

1239
01:15:17,590 --> 01:15:20,760
with um, methods like Latent Dirichlet analysis.

1240
01:15:20,760 --> 01:15:22,530
If you know about that from modelling words

1241
01:15:22,530 --> 01:15:25,940
similarities that sort of grown as a good package um,

1242
01:15:25,940 --> 01:15:28,570
for doing um, word vectors as well.

1243
01:15:28,570 --> 01:15:31,650
So, it's quite often used for word vectors and

1244
01:15:31,650 --> 01:15:36,105
word similarities that sort of efficient for doing things at large-scale.

1245
01:15:36,105 --> 01:15:37,720
Um, yeah.

1246
01:15:37,720 --> 01:15:41,360
So, I haven't yet told you about will next time we have

1247
01:15:41,360 --> 01:15:46,400
our own homegrown form of word vectors which are the GloVe word vectors.

1248
01:15:46,400 --> 01:15:51,270
I'm using them not because it really matters for what I'm showing but you know,

1249
01:15:51,270 --> 01:15:55,745
these vectors are conveniently small.

1250
01:15:55,745 --> 01:16:00,470
It turns out that the vectors that Facebook and Google

1251
01:16:00,470 --> 01:16:05,940
distribute are extremely large vocabulary and extremely high dimensional.

1252
01:16:05,940 --> 01:16:08,940
So take me just too long to load them in

1253
01:16:08,940 --> 01:16:12,860
the last five minutes of this class where conveniently uh,

1254
01:16:12,860 --> 01:16:16,865
in our Stanford vectors we have a 100 dimensional vectors, um,

1255
01:16:16,865 --> 01:16:19,160
and 50 dimensional vectors which are kinda

1256
01:16:19,160 --> 01:16:21,755
good for doing small things on a laptop frankly.

1257
01:16:21,755 --> 01:16:27,330
Um, so, what I'm doing here is Gensim doesn't natively support

1258
01:16:27,330 --> 01:16:30,210
GloVe vectors but they actually provide a utility that

1259
01:16:30,210 --> 01:16:33,390
converts the GloVe file format to the word2vec file format.

1260
01:16:33,390 --> 01:16:40,285
So I've done that. And then I've loaded a pre-trained model of word vectors.

1261
01:16:40,285 --> 01:16:44,430
Um, and, so this is what they call a keyed vector.

1262
01:16:44,430 --> 01:16:46,890
And so, the keyed vector is nothing fancy.

1263
01:16:46,890 --> 01:16:51,660
It's just you have words like potato and there's a vector that hangs off each one.

1264
01:16:51,660 --> 01:16:55,435
So it's really just sort of a big dictionary with a vector for each thing.

1265
01:16:55,435 --> 01:16:58,690
But, so this model has been a trained model where

1266
01:16:58,690 --> 01:17:02,230
we just use the kind of algorithm we looked at and,

1267
01:17:02,230 --> 01:17:06,730
you know, trained at billions of times fiddling our word vectors.

1268
01:17:06,730 --> 01:17:11,255
Um, and once we have one we can then, um,

1269
01:17:11,255 --> 01:17:14,265
ask questions like, we can say,

1270
01:17:14,265 --> 01:17:17,110
what is the most similar word to some other words?

1271
01:17:17,110 --> 01:17:19,650
So we could take something like, um,

1272
01:17:19,650 --> 01:17:23,175
what are the most similar words to Obama let's say?

1273
01:17:23,175 --> 01:17:25,770
And we get back Barrack, Bush, Clinton,

1274
01:17:25,770 --> 01:17:29,040
McCain, Gore, Hillary Dole, Martin, Henry.

1275
01:17:29,040 --> 01:17:31,425
That seems actually kind of interesting.

1276
01:17:31,425 --> 01:17:34,050
These factors from a few years ago.

1277
01:17:34,050 --> 01:17:37,150
So we don't have a post- post-Obama staff.

1278
01:17:37,150 --> 01:17:40,750
I mean if you put in another word, um, you know,

1279
01:17:40,750 --> 01:17:44,100
we can put in something like banana and we get coconut,

1280
01:17:44,100 --> 01:17:46,600
mango, bananas, potato, pineapple.

1281
01:17:46,600 --> 01:17:49,430
We get kind of tropical food.

1282
01:17:49,430 --> 01:17:54,070
So, you can actually- you can actually ask uh,

1283
01:17:54,070 --> 01:17:56,995
for being dissimilar to words.

1284
01:17:56,995 --> 01:17:59,700
By itself dissimilar isn't very useful.

1285
01:17:59,700 --> 01:18:04,550
So if I ask most similar and I say um,

1286
01:18:04,550 --> 01:18:09,290
negative equals, um, banana,

1287
01:18:09,290 --> 01:18:14,720
um, I'm not sure what your concept of what's most dissimilar to,

1288
01:18:14,720 --> 01:18:16,620
um, banana is, but you know,

1289
01:18:16,620 --> 01:18:22,655
actually by itself you don't get anything useful out of this, um,

1290
01:18:22,655 --> 01:18:28,000
because, um, you just so get these weird really rare words um,

1291
01:18:28,000 --> 01:18:31,440
which, um, [LAUGHTER] definitely weren't the ones who are thinking of.

1292
01:18:31,440 --> 01:18:37,570
Um, but it turns out you can do something really useful with this negative idea

1293
01:18:37,570 --> 01:18:39,000
which was one of

1294
01:18:39,000 --> 01:18:44,175
the highly celebrated results of word vectors when they first started off.

1295
01:18:44,175 --> 01:18:50,205
And that was this idea that there is actually dimensions of meaning in this space.

1296
01:18:50,205 --> 01:18:54,820
And so this was the most celebrated example um, which was look,

1297
01:18:54,820 --> 01:18:59,985
what we could do is we could start with the word king and subtract

1298
01:18:59,985 --> 01:19:05,355
from it the meaning of man and then we could add to it the meaning of woman.

1299
01:19:05,355 --> 01:19:09,110
And then we could say which word in our vector space as

1300
01:19:09,110 --> 01:19:13,060
most similar in meaning to that word.

1301
01:19:13,060 --> 01:19:15,810
And that would be a way of sort of doing analogies.

1302
01:19:15,810 --> 01:19:18,640
Would be able to do the, um, analogy,

1303
01:19:18,640 --> 01:19:22,050
man is the king as woman is to what?

1304
01:19:22,050 --> 01:19:26,500
And so, the way we're gonna do that is to say we want to be similar to king

1305
01:19:26,500 --> 01:19:31,220
and woman because they're both positive ones and far away from man.

1306
01:19:31,220 --> 01:19:35,185
And so, we could do that manually,

1307
01:19:35,185 --> 01:19:36,950
here is said manually,

1308
01:19:36,950 --> 01:19:41,050
most similar positive woman king, negative man.

1309
01:19:41,050 --> 01:19:45,410
And we can run this and lo and behold it produces queen.

1310
01:19:45,410 --> 01:19:48,570
To make that a little bit easier I defined this analogy,

1311
01:19:48,570 --> 01:19:53,485
um, analogy predicates so I can run other ones.

1312
01:19:53,485 --> 01:19:59,095
And so I can run another one like analogy Japan Japanese,

1313
01:19:59,095 --> 01:20:01,160
Austria is to Austrian.

1314
01:20:01,160 --> 01:20:03,125
Um, and you know,

1315
01:20:03,125 --> 01:20:07,150
I think it's fair to say that when people first

1316
01:20:07,150 --> 01:20:10,950
saw that you could have this simple piece of math and run it,

1317
01:20:10,950 --> 01:20:12,950
and learn meanings of words.

1318
01:20:12,950 --> 01:20:18,465
I mean it actually just sort of blew people's minds how effective this was.

1319
01:20:18,465 --> 01:20:22,030
You know. Like there- there's is no mirrors and strings here, right?

1320
01:20:22,030 --> 01:20:24,225
You know it's not that I have a separate-

1321
01:20:24,225 --> 01:20:28,330
a special sort of list in my Python where there's a difficult I'm looking up,

1322
01:20:28,330 --> 01:20:30,240
er, for Austria Austrian,

1323
01:20:30,240 --> 01:20:31,910
uh, and things like that.

1324
01:20:31,910 --> 01:20:35,310
But somehow these vector representations are

1325
01:20:35,310 --> 01:20:38,760
such that it is actually encoding these semantic relationships,

1326
01:20:38,760 --> 01:20:40,920
you know, so you can try different ones,

1327
01:20:40,920 --> 01:20:43,360
you know, like it's not that only this one works.

1328
01:20:43,360 --> 01:20:46,185
I can put in France, it says French.

1329
01:20:46,185 --> 01:20:49,780
I can put in Germany, it says German,

1330
01:20:49,780 --> 01:20:54,590
I can put in Australia not Austria and it says Australian,

1331
01:20:54,590 --> 01:20:59,480
you know that somehow if you want this vector representations of words that

1332
01:20:59,480 --> 01:21:04,815
for sort of these ideas like understanding the relationships between words,

1333
01:21:04,815 --> 01:21:10,595
you're just doing this vector space manipulation on these 100 dimensional numbers,

1334
01:21:10,595 --> 01:21:15,830
that it actually knows about them.This not only the similarities of word meanings but

1335
01:21:15,830 --> 01:21:18,255
actually different semantic relationships

1336
01:21:18,255 --> 01:21:21,635
between words like country names and their peoples.

1337
01:21:21,635 --> 01:21:23,850
And yeah that's actually pretty amazing.

1338
01:21:23,850 --> 01:21:31,340
It really-you know, it's sort of surprising that running such a dumb algorithm on um,

1339
01:21:31,340 --> 01:21:35,100
vectors of numbers could capture so well the meaning of words.

1340
01:21:35,100 --> 01:21:38,160
And so that's sort of became the foundation of a lot of sort

1341
01:21:38,160 --> 01:21:41,355
of modern distributed neural representations of words.

1342
01:21:41,355 --> 01:21:42,630
Okay I'll stop there.

1343
01:21:42,630 --> 01:21:46,700
Thanks a lot guys and see you on Thursday. [NOISE]

