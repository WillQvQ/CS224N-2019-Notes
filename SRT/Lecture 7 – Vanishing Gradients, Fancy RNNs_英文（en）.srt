1
00:00:04,700 --> 00:00:07,140
Hi, everyone. I'm Abby.

2
00:00:07,140 --> 00:00:08,355
If you weren't here last week,

3
00:00:08,355 --> 00:00:10,190
I'm the head TA of this course.

4
00:00:10,190 --> 00:00:13,325
And this is the second [NOISE] of three lectures that I'm

5
00:00:13,325 --> 00:00:17,180
going to be giving on RNNs and related topics.

6
00:00:17,360 --> 00:00:20,430
Okay. So, welcome to week four.

7
00:00:20,430 --> 00:00:22,960
Today, we're going to be learning about vanishing gradients,

8
00:00:22,960 --> 00:00:25,120
and some more complex types of RNNs.

9
00:00:25,120 --> 00:00:26,815
So, before we get started,

10
00:00:26,815 --> 00:00:28,000
I've got a few announcements.

11
00:00:28,000 --> 00:00:33,150
Uh, the first announcement is that assignment four is released today, uh,

12
00:00:33,150 --> 00:00:35,835
it's due Thursday of next week, not Tuesday,

13
00:00:35,835 --> 00:00:39,795
so that means you have two days more to do it than you did for all the other homeworks.

14
00:00:39,795 --> 00:00:41,410
And the reason for that is assignment four is

15
00:00:41,410 --> 00:00:43,360
probably more work than the other homework so far,

16
00:00:43,360 --> 00:00:45,415
so don't be surprised by that.

17
00:00:45,415 --> 00:00:48,575
Uh, assignment four is all about Neural Machine Translation.

18
00:00:48,575 --> 00:00:52,135
Uh, we're gonna learn about NMT on Thursday's lecture this week.

19
00:00:52,135 --> 00:00:54,270
And, uh, this is really exciting,

20
00:00:54,270 --> 00:00:57,600
because actually CS 224 has never had an NMT assignment before,

21
00:00:57,600 --> 00:00:58,740
so this is all new this year,

22
00:00:58,740 --> 00:01:02,420
and you're gonna be the first year students who are going to be doing an NMT assignment.

23
00:01:02,420 --> 00:01:04,910
Uh, something else that's different about

24
00:01:04,910 --> 00:01:07,610
assignment four is that you're going to be using Azure, which is, uh,

25
00:01:07,610 --> 00:01:09,170
a cloud computing service,

26
00:01:09,170 --> 00:01:13,145
in order to train your NMT systems on a virtual machine with a GPU.

27
00:01:13,145 --> 00:01:17,365
And, uh, this is necessary in order to be able to do it in a reasonable amount of time.

28
00:01:17,365 --> 00:01:19,250
So, I have a warning which is,

29
00:01:19,250 --> 00:01:20,930
if you're a person who perhaps doesn't have, ah,

30
00:01:20,930 --> 00:01:24,145
learnt- a lot of experience working on remote machines,

31
00:01:24,145 --> 00:01:26,360
so for example if you're not very familiar with SSH,

32
00:01:26,360 --> 00:01:28,880
or tmux, or remote text editing,

33
00:01:28,880 --> 00:01:31,580
then I advise you to budget some extra time for assignment four,

34
00:01:31,580 --> 00:01:34,945
because that's probably gonna take you a little while to set up and get used to.

35
00:01:34,945 --> 00:01:37,280
So, again, I'm going to emphasize,

36
00:01:37,280 --> 00:01:40,250
do get started early on assignment four, because, uh,

37
00:01:40,250 --> 00:01:43,775
the NMT system takes about four hours to train on your virtual machine,

38
00:01:43,775 --> 00:01:47,230
so you really can't start it the night before and expect to get it in on time.

39
00:01:47,230 --> 00:01:51,245
Uh, and assignment four is really quite a lot more complicated than assignment three.

40
00:01:51,245 --> 00:01:55,250
So, uh, don't get into a false sense of security if you found assignment three easy.

41
00:01:55,250 --> 00:02:00,640
Um, so Thursday's slides on NMT are ready on the website today,

42
00:02:00,640 --> 00:02:02,610
so you can even start looking at it today if you

43
00:02:02,610 --> 00:02:05,910
want- if you wanna get started on assignment four early.

44
00:02:05,910 --> 00:02:08,310
Uh, so, I have a few more announcements, uh,

45
00:02:08,310 --> 00:02:10,080
on the subject of projects, uh,

46
00:02:10,080 --> 00:02:11,860
next week's lectures are going to be all about projects.

47
00:02:11,860 --> 00:02:14,630
So, you're going to hear about, uh, question answering,

48
00:02:14,630 --> 00:02:16,445
and the default final projects,

49
00:02:16,445 --> 00:02:18,620
and then you're also gonna get some tips about how you might,

50
00:02:18,620 --> 00:02:21,205
uh, choose and define your own custom projects.

51
00:02:21,205 --> 00:02:24,180
So, it's fine if you're not thinking about a project this week, that's okay.

52
00:02:24,180 --> 00:02:27,130
You can delay until next week to start thinking about it for the first time.

53
00:02:27,130 --> 00:02:29,630
But if you are a person who is already thinking about your projects,

54
00:02:29,630 --> 00:02:32,200
for example, if you're trying to choose your custom projects, uh,

55
00:02:32,200 --> 00:02:34,295
then you should check out the website's project page,

56
00:02:34,295 --> 00:02:36,230
because it has quite a lot of information about, uh,

57
00:02:36,230 --> 00:02:39,170
how to choose your projects, and also some inspiration.

58
00:02:39,170 --> 00:02:41,645
And that includes- we've collected some, uh,

59
00:02:41,645 --> 00:02:44,650
project ideas from various members of the Stanford AI Lab.

60
00:02:44,650 --> 00:02:48,155
So, these are faculty and PhD students and postdocs,

61
00:02:48,155 --> 00:02:49,760
who have ideas for, uh,

62
00:02:49,760 --> 00:02:51,560
NLP deep learning projects that they would like

63
00:02:51,560 --> 00:02:54,150
CS224n students such as yourself to work on.

64
00:02:54,150 --> 00:02:57,095
So, especially, if you're looking to maybe get into research later,

65
00:02:57,095 --> 00:02:59,180
this is a really great opportunity, uh,

66
00:02:59,180 --> 00:03:00,905
to work with someone in the Stanford AI Lab,

67
00:03:00,905 --> 00:03:03,270
and maybe get some mentorship as well.

68
00:03:03,700 --> 00:03:06,840
Okay. So here's an overview.

69
00:03:06,840 --> 00:03:10,080
Uh, last week, we learned about Recurrent Neural Networks,

70
00:03:10,080 --> 00:03:12,725
um, we learned about why they're really great for Language Modeling.

71
00:03:12,725 --> 00:03:15,335
And today, we're gonna learn about some problems with RNNs,

72
00:03:15,335 --> 00:03:16,880
and we're gonna learn about how to fix them.

73
00:03:16,880 --> 00:03:22,035
And this is gonna motiva- motivate us to learn about some more complex RNN variants.

74
00:03:22,035 --> 00:03:24,300
And then, uh, next lecture on Thursday,

75
00:03:24,300 --> 00:03:27,570
we're going to, uh, have some more application-based, uh, contents,

76
00:03:27,570 --> 00:03:29,750
so we are going to be learning about Neural Machine Translation,

77
00:03:29,750 --> 00:03:31,700
which is a really important task in, uh,

78
00:03:31,700 --> 00:03:33,830
NLP and deep learning, and in particular,

79
00:03:33,830 --> 00:03:37,435
we're gonna learn about this architecture called sequence-to-sequence with attention.

80
00:03:37,435 --> 00:03:40,080
But in more detail,

81
00:03:40,080 --> 00:03:41,640
today's lecture, uh, first,

82
00:03:41,640 --> 00:03:43,650
we are going to learn about the vanishing gradient problem.

83
00:03:43,650 --> 00:03:46,220
And this is gonna motivate us to learn about two new types of

84
00:03:46,220 --> 00:03:48,890
RNN called Long Short-Term Memory,

85
00:03:48,890 --> 00:03:50,855
and Gated Recurrent Unit.

86
00:03:50,855 --> 00:03:52,940
We're also going to learn about some other kind of

87
00:03:52,940 --> 00:03:55,640
miscellaneous fixes for the vanishing gradient problem,

88
00:03:55,640 --> 00:03:57,190
or the exploding gradient problem.

89
00:03:57,190 --> 00:03:58,370
Uh, so in particular,

90
00:03:58,370 --> 00:03:59,790
we're going to learn about gradient clipping,

91
00:03:59,790 --> 00:04:02,190
which is, uh, fairly simple, but quite important.

92
00:04:02,190 --> 00:04:04,710
Uh, we're also going to learn about skip connections,

93
00:04:04,710 --> 00:04:07,080
which is a fairly new neural architecture,

94
00:04:07,080 --> 00:04:08,150
which tries to, uh,

95
00:04:08,150 --> 00:04:09,350
fix the vanishing gradient problem.

96
00:04:09,350 --> 00:04:11,640
[NOISE] And then, at the end of the lecture,

97
00:04:11,640 --> 00:04:14,210
we're gonna learn about some more fancy RNN variants such as, uh,

98
00:04:14,210 --> 00:04:17,780
bidirectional RN- RNNs, those are the ones which go not just left to right,

99
00:04:17,780 --> 00:04:18,980
but also right to left,

100
00:04:18,980 --> 00:04:21,740
and we're going to learn about multi-layer RNNs.

101
00:04:21,740 --> 00:04:24,880
And that's when you stack multiple RNNs on top of each other.

102
00:04:24,880 --> 00:04:27,560
So, there's a lot of important definitions today.

103
00:04:27,560 --> 00:04:29,870
Um, so, you're gonna find that the information in

104
00:04:29,870 --> 00:04:31,160
this lecture is pretty important for

105
00:04:31,160 --> 00:04:34,080
assignment four and probably for your project as well.

106
00:04:35,450 --> 00:04:40,250
Okay. So, let's get started thinking about the vanishing gradients.

107
00:04:40,250 --> 00:04:42,350
Uh, so here we have an RNN,

108
00:04:42,350 --> 00:04:44,390
with, let say, ah, four steps,

109
00:04:44,390 --> 00:04:46,955
and suppose that we have some kind of loss that's, uh,

110
00:04:46,955 --> 00:04:50,915
J4, and that's computed based on the four hidden states.

111
00:04:50,915 --> 00:04:56,300
So, let's suppose we're interested in asking what is the derivative of this loss J4,

112
00:04:56,300 --> 00:04:58,340
with respect to the hidden states,

113
00:04:58,340 --> 00:05:00,865
uh, h1, the first hidden state?

114
00:05:00,865 --> 00:05:03,390
So, I'm representing that with this, uh,

115
00:05:03,390 --> 00:05:05,720
blue arrow notation to kind of represent how we have

116
00:05:05,720 --> 00:05:09,070
to make the gradients flow backwards in order to complete this.

117
00:05:09,070 --> 00:05:11,750
So, if we're interested in what this gradient is,

118
00:05:11,750 --> 00:05:13,850
we can apply the chain rule and say, "Well,

119
00:05:13,850 --> 00:05:15,035
it's the product of the, uh,

120
00:05:15,035 --> 00:05:17,560
gradient of the loss with respect to h2,

121
00:05:17,560 --> 00:05:20,975
and then gradient of h2, with respect to h1."

122
00:05:20,975 --> 00:05:23,670
And then, similarly, we can decompose that

123
00:05:23,670 --> 00:05:27,255
again using the chain rule, and we can do it again.

124
00:05:27,255 --> 00:05:31,790
So, what we've done here is we've decomposed the gradient that we were interested in,

125
00:05:31,790 --> 00:05:35,375
into the products of these various intermediate gradients.

126
00:05:35,375 --> 00:05:39,290
And in particular, we're seeing all these ht by ht minus 1,

127
00:05:39,290 --> 00:05:41,975
uh, adjacent gradients of the hidden states.

128
00:05:41,975 --> 00:05:43,910
So, the thing I want to ask you is,

129
00:05:43,910 --> 00:05:47,130
what happens if these gradients are small?

130
00:05:47,130 --> 00:05:49,020
Given that there's a lot of them,

131
00:05:49,020 --> 00:05:52,090
uh, what happens if they're small in magnitude?

132
00:05:52,130 --> 00:05:56,865
So, the overall problem of the vanishing gradient problem,

133
00:05:56,865 --> 00:05:59,340
is that when these gradients are small,

134
00:05:59,340 --> 00:06:02,525
then our overall gradient is gonna get smaller and smaller,

135
00:06:02,525 --> 00:06:04,375
as it back propagates further.

136
00:06:04,375 --> 00:06:09,150
Because the accumulated gradient is the product of all of these intermediate gradients.

137
00:06:09,150 --> 00:06:11,275
And when you multiply something by something small,

138
00:06:11,275 --> 00:06:12,970
then the whole thing gets smaller.

139
00:06:12,970 --> 00:06:15,220
So, that's what I'm representing here with these, uh,

140
00:06:15,220 --> 00:06:18,230
smaller and smaller blue arrows going backwards.

141
00:06:18,290 --> 00:06:21,565
So, that's the general idea of the vanishing gradient problem.

142
00:06:21,565 --> 00:06:23,735
Here's a slightly more formal definition.

143
00:06:23,735 --> 00:06:25,905
So, if you remember from last time,

144
00:06:25,905 --> 00:06:28,095
uh, if we have a null RNN,

145
00:06:28,095 --> 00:06:30,490
then the hidden state ht is

146
00:06:30,490 --> 00:06:33,490
computed as a function of the previous hidden state ht minus 1,

147
00:06:33,490 --> 00:06:34,810
and the current input xt.

148
00:06:34,810 --> 00:06:37,880
Uh, so you might remember in the previous lecture we

149
00:06:37,880 --> 00:06:41,075
said that xt were one-hot vectors representing words,

150
00:06:41,075 --> 00:06:42,620
and then ET is the embedding.

151
00:06:42,620 --> 00:06:44,060
Uh, this lecture we're going to be,

152
00:06:44,060 --> 00:06:45,110
uh, getting rid of that detail,

153
00:06:45,110 --> 00:06:47,060
and we're just gonna be thinking very abstractly about

154
00:06:47,060 --> 00:06:49,455
an RNN that has some kind of input xt,

155
00:06:49,455 --> 00:06:51,210
and xt is just any kind of vector.

156
00:06:51,210 --> 00:06:52,260
Probably a dense vector,

157
00:06:52,260 --> 00:06:53,925
but you know, it could be words or not.

158
00:06:53,925 --> 00:06:55,500
It could be one-hot or dense.

159
00:06:55,500 --> 00:06:57,810
Uh, but that's just the input.

160
00:06:57,810 --> 00:07:00,210
So, that's the, uh,

161
00:07:00,210 --> 00:07:03,365
the definition that we learned last time for Vanilla RNNs.

162
00:07:03,365 --> 00:07:05,930
So, this means that the derivative of ht,

163
00:07:05,930 --> 00:07:08,480
hidden state on step t with respect to the previous hidden state,

164
00:07:08,480 --> 00:07:10,075
uh, is this expression here.

165
00:07:10,075 --> 00:07:13,880
Uh, so this is just an application of the chain rule, and, uh,

166
00:07:13,880 --> 00:07:16,010
if you looked long enough or refer back to

167
00:07:16,010 --> 00:07:18,590
the backprop lecture you'll see, uh, that that make sense.

168
00:07:18,590 --> 00:07:20,585
So, in particular, we're, um,

169
00:07:20,585 --> 00:07:23,210
multiplying by Wh at the end, uh,

170
00:07:23,210 --> 00:07:27,400
because we have the multiplication of Wh and ht minus 1 on the inside.

171
00:07:27,400 --> 00:07:30,590
Okay. So, if you remember, on the previous slide,

172
00:07:30,590 --> 00:07:33,650
we were thinking about what's the gradient of the loss on some step,

173
00:07:33,650 --> 00:07:34,910
step i I'd say,

174
00:07:34,910 --> 00:07:36,980
with respect to a hidden state hj,

175
00:07:36,980 --> 00:07:38,795
on some previous step j.

176
00:07:38,795 --> 00:07:41,950
And maybe J is quite a few steps before i.

177
00:07:41,950 --> 00:07:44,235
So, we can now write this,

178
00:07:44,235 --> 00:07:46,130
uh, in the following way.

179
00:07:46,130 --> 00:07:48,455
So just by applying the chain rule,

180
00:07:48,455 --> 00:07:51,365
now on the first line we're saying that this derivative that we're interested in

181
00:07:51,365 --> 00:07:54,920
can be decomposed into the derivative with respect to step i,

182
00:07:54,920 --> 00:07:56,270
which is kind of the last step,

183
00:07:56,270 --> 00:08:00,265
and then do all of those intermediate gradients of the adjacent hidden states as well.

184
00:08:00,265 --> 00:08:04,100
So, that- that first slide is just exactly the same thing as we were looking at on the,

185
00:08:04,100 --> 00:08:08,160
uh, the picture, uh, the diagram on the previous slide.

186
00:08:08,300 --> 00:08:11,435
Okay. And then, given that we figured out what is, uh,

187
00:08:11,435 --> 00:08:14,030
dht by dht minus one,

188
00:08:14,030 --> 00:08:15,050
ah, further on the slide,

189
00:08:15,050 --> 00:08:16,775
then we can just substitute that in.

190
00:08:16,775 --> 00:08:19,370
So, what we're finding is that this overall gradient that we're

191
00:08:19,370 --> 00:08:21,795
interested in, in particular,

192
00:08:21,795 --> 00:08:23,445
has this term, uh,

193
00:08:23,445 --> 00:08:26,790
Wh, the weight matrix, and it's, uh,

194
00:08:26,790 --> 00:08:29,020
multiplied by itself, i minus j times,

195
00:08:29,020 --> 00:08:32,535
because there's i minus j many steps between, uh,

196
00:08:32,535 --> 00:08:33,825
step j and step i,

197
00:08:33,825 --> 00:08:37,520
which is the- the distance that we're traveling with this gradient.

198
00:08:37,520 --> 00:08:39,755
So, the big problem here is,

199
00:08:39,755 --> 00:08:42,665
if this weight matrix Wh is small,

200
00:08:42,665 --> 00:08:45,445
then this term is gonna get vanishingly small,

201
00:08:45,445 --> 00:08:49,960
exponentially small, as i and j get further apart.

202
00:08:49,960 --> 00:08:53,740
So, to give this a little more detail, uh,

203
00:08:53,740 --> 00:08:55,150
we can think about the, uh,

204
00:08:55,150 --> 00:08:58,360
L2 matrix norms of all of these matrices, right?

205
00:08:58,360 --> 00:09:03,840
And, uh, as a- as a- uh, as a- sorry.

206
00:09:03,840 --> 00:09:06,405
I'm- it's a known fact of,

207
00:09:06,405 --> 00:09:08,340
uh, L2 norms that you have this, um,

208
00:09:08,340 --> 00:09:10,605
inequality that's the, uh,

209
00:09:10,605 --> 00:09:12,450
norm of the products of

210
00:09:12,450 --> 00:09:15,740
some matrices is less and equal to the product of the norms of the matrices.

211
00:09:15,740 --> 00:09:19,705
So, in particular, we're seeing that the norm of this gradient that we're interested in,

212
00:09:19,705 --> 00:09:21,905
is less than or equal to, uh,

213
00:09:21,905 --> 00:09:26,690
the product i minus j many times of the norm of the weight matrix Wh.

214
00:09:26,690 --> 00:09:30,415
So, this is what we mean when we say we're concerned about Wh being small,

215
00:09:30,415 --> 00:09:34,180
because if it's small, then the thing on the left has to be exponentially small.

216
00:09:34,180 --> 00:09:36,000
So in particular in this,

217
00:09:36,000 --> 00:09:37,065
uh, paper that, uh,

218
00:09:37,065 --> 00:09:40,080
you can take a look at the bottom if you're interested, um, uh,

219
00:09:40,080 --> 00:09:41,960
Pascanu et al showed that if

220
00:09:41,960 --> 00:09:46,265
the largest eigenvalue of the weight matrix Wh is less than one,

221
00:09:46,265 --> 00:09:49,910
then this gradient on the left is going to shrink exponentially.

222
00:09:49,910 --> 00:09:52,385
And you can probably see intuitively why this is true.

223
00:09:52,385 --> 00:09:53,960
So if, you know, as a simplifying assumption,

224
00:09:53,960 --> 00:09:56,090
we suppose that Wh was not a matrix,

225
00:09:56,090 --> 00:09:58,765
but simply a scalar that was just a single number,

226
00:09:58,765 --> 00:10:01,760
then you can see why if that number was greater than one,

227
00:10:01,760 --> 00:10:03,350
then the whole thing is gonna explode.

228
00:10:03,350 --> 00:10:04,955
And if that number is less than one,

229
00:10:04,955 --> 00:10:06,305
then it is going to shrink

230
00:10:06,305 --> 00:10:09,070
exponentially as you multiply by the same number again and again.

231
00:10:09,070 --> 00:10:12,710
Uh, so you can check out the paper for more details,

232
00:10:12,710 --> 00:10:14,960
but here, uh, the bound is one,

233
00:10:14,960 --> 00:10:17,725
partially because we have the sigmoid nonlinearity.

234
00:10:17,725 --> 00:10:21,140
And that's, uh, based on the bounds of what we know as the,

235
00:10:21,140 --> 00:10:24,300
uh, norm of the sigmoid function to be.

236
00:10:24,490 --> 00:10:29,400
So, uh, this shows you why if the, uh,

237
00:10:29,400 --> 00:10:31,020
Wh matrix is small,

238
00:10:31,020 --> 00:10:32,600
or if its largest eigenvalue was small,

239
00:10:32,600 --> 00:10:34,280
then we're going to have vanishing gradients.

240
00:10:34,280 --> 00:10:35,930
And similarly, if you check out the paper,

241
00:10:35,930 --> 00:10:38,140
you can see that there's a similar proof, uh,

242
00:10:38,140 --> 00:10:40,975
relating if the largest eigenvalue is greater than one,

243
00:10:40,975 --> 00:10:43,000
to having exploding gradients.

244
00:10:43,000 --> 00:10:45,145
So that's when the gradients get bigger and bigger,

245
00:10:45,145 --> 00:10:47,240
as you backprop further.

246
00:10:48,270 --> 00:10:52,075
Okay. So hopefully I've convinced you that

247
00:10:52,075 --> 00:10:55,240
vanishing gradients is a phenomenon that happens in our norms.

248
00:10:55,240 --> 00:10:57,475
But I haven't yet said why this is a problem.

249
00:10:57,475 --> 00:11:00,340
So, why should we view this as a bad thing,

250
00:11:00,340 --> 00:11:02,515
if the gradients are getting larger and larger,

251
00:11:02,515 --> 00:11:04,570
or smaller and smaller as you backprop?

252
00:11:04,570 --> 00:11:08,795
So here's, uh, here's a picture that might illustrate why it's a bad thing.

253
00:11:08,795 --> 00:11:10,500
So, uh, as before,

254
00:11:10,500 --> 00:11:11,790
suppose that we're thinking about,

255
00:11:11,790 --> 00:11:14,130
what's the derivative of the loss on

256
00:11:14,130 --> 00:11:16,740
the fourth step with respect to the first hidden state?

257
00:11:16,740 --> 00:11:18,270
And we have this situation where

258
00:11:18,270 --> 00:11:21,560
the gradient is getting smaller and smaller as it goes backwards.

259
00:11:21,560 --> 00:11:24,820
But then, think about what is the gradient of let's say

260
00:11:24,820 --> 00:11:28,270
the loss in the second step also with respect to the first hidden state.

261
00:11:28,270 --> 00:11:30,745
So I'm representing that with the orange arrows.

262
00:11:30,745 --> 00:11:32,635
And what my point is here,

263
00:11:32,635 --> 00:11:37,765
is that the magnitude of the gradient signal from close by,

264
00:11:37,765 --> 00:11:42,175
is a lot bigger than the magnitude of the gradient signal from far away.

265
00:11:42,175 --> 00:11:45,790
And this means that when you update your model weights,

266
00:11:45,790 --> 00:11:48,520
the signal that you're getting from close by is gonna

267
00:11:48,520 --> 00:11:50,770
be so much bigger than the signal from far away,

268
00:11:50,770 --> 00:11:52,585
that essentially you're only going to learn,

269
00:11:52,585 --> 00:11:54,610
you're only going to optimize with respect to

270
00:11:54,610 --> 00:11:57,535
these nearby effects and not the long-term effects.

271
00:11:57,535 --> 00:12:02,200
So you're gonna, you're gonna lose the long-term effects, er, inside the,

272
00:12:02,200 --> 00:12:07,385
the nearby effects. Any questions about this, yeah?

273
00:12:07,385 --> 00:12:11,490
So, uh, where they say there that you do actual updates.

274
00:12:11,490 --> 00:12:15,235
You know, there are actually some that are multiple chains, not just one chain.

275
00:12:15,235 --> 00:12:17,815
So the nearer term should cover it.

276
00:12:17,815 --> 00:12:19,330
Sorry, what's the last part?

277
00:12:19,330 --> 00:12:22,405
The nearer term should have a larger effect considering you're

278
00:12:22,405 --> 00:12:25,870
updating the sum of the weights over different chains.

279
00:12:25,870 --> 00:12:29,035
Okay. So I think, ah, the observation was that,

280
00:12:29,035 --> 00:12:30,250
given that, for example,

281
00:12:30,250 --> 00:12:32,800
in Language Modeling you might be summing over multiple losses.

282
00:12:32,800 --> 00:12:35,860
There is a loss in every step and you sum all of them and that's your overall loss.

283
00:12:35,860 --> 00:12:40,600
Then you do want to update more with respect to the nearby losses than the far losses.

284
00:12:40,600 --> 00:12:41,950
So I think, uh, yeah,

285
00:12:41,950 --> 00:12:43,750
so if the design of your objective function

286
00:12:43,750 --> 00:12:45,460
is that it's the sum of the loss in every step,

287
00:12:45,460 --> 00:12:46,765
then you do want to, uh,

288
00:12:46,765 --> 00:12:48,325
weight all of them equally.

289
00:12:48,325 --> 00:12:50,410
I think, uh, my point was more about,

290
00:12:50,410 --> 00:12:52,675
what is the influence of, uh,

291
00:12:52,675 --> 00:12:55,285
the action of the weight matrix at this early stage.

292
00:12:55,285 --> 00:12:57,595
What is its influence on a loss that's nearby?

293
00:12:57,595 --> 00:13:00,250
And what is its influence on a loss that's far away?

294
00:13:00,250 --> 00:13:02,875
Um, and due to, uh,

295
00:13:02,875 --> 00:13:05,290
the dynamics of how the vanishing gradient, uh,

296
00:13:05,290 --> 00:13:07,420
problem works, then, uh,

297
00:13:07,420 --> 00:13:09,250
the influence on the loss that's far away

298
00:13:09,250 --> 00:13:11,185
is gonna be much less than the influence nearby.

299
00:13:11,185 --> 00:13:15,040
And I'm gonna give some more linguistics examples later of why you might want to learn,

300
00:13:15,040 --> 00:13:17,065
uh, the connections that are farther away.

301
00:13:17,065 --> 00:13:18,190
So essentially the problem is,

302
00:13:18,190 --> 00:13:20,260
in situations where you do want to learn the connection

303
00:13:20,260 --> 00:13:23,125
between something that happens early and something that happens later,

304
00:13:23,125 --> 00:13:25,090
then you're going to be unable to learn that connection.

305
00:13:25,090 --> 00:13:28,330
Uh, so we'll see some motivating examples in a minute.

306
00:13:28,330 --> 00:13:36,130
Any other questions on this? Yeah?

307
00:13:36,130 --> 00:13:39,970
Um, I'm getting confused like, why are you talking about like dh, dj dh.

308
00:13:39,970 --> 00:13:46,509
Uh, it's like H parameter, like, are we going-

309
00:13:46,509 --> 00:13:46,835
Yeah.

310
00:13:46,835 --> 00:13:47,020
from-

311
00:13:47,020 --> 00:13:48,685
Okay. That's a great question.

312
00:13:48,685 --> 00:13:52,510
So you're asking why are we interested in some kind of dj by

313
00:13:52,510 --> 00:13:56,935
dh given that we're not updating H. H is an activation not a weight.

314
00:13:56,935 --> 00:14:00,520
Um, so the reason why we're thinking about that,

315
00:14:00,520 --> 00:14:04,330
is because when you think about what is dj by dw,

316
00:14:04,330 --> 00:14:05,800
which is a thing that we're going to update.

317
00:14:05,800 --> 00:14:10,300
That's always gonna be in terms of dj by dh at some point, right?

318
00:14:10,300 --> 00:14:12,040
So if we're thinking about W, you know,

319
00:14:12,040 --> 00:14:13,615
and how it acts on, uh,

320
00:14:13,615 --> 00:14:15,550
the transmission from h_1 to h_2,

321
00:14:15,550 --> 00:14:21,355
then dj4 by W in that position is going to have to go through dj4 by dh_2.

322
00:14:21,355 --> 00:14:23,620
So if we're getting vanishing gradients,

323
00:14:23,620 --> 00:14:25,795
uh, as we back propagate further,

324
00:14:25,795 --> 00:14:27,190
then it's kind of like a bottleneck.

325
00:14:27,190 --> 00:14:29,740
Then you're certainly going to have vanishing gradients as they affect, uh,

326
00:14:29,740 --> 00:14:31,540
the recurrence matrix there,

327
00:14:31,540 --> 00:14:35,540
and indeed the matrix that's applied to the inputs.

328
00:14:38,880 --> 00:14:41,990
Okay. I'm gonna move off now.

329
00:14:42,150 --> 00:14:46,300
Uh, so another way to explain why vanishing gradients is a problem,

330
00:14:46,300 --> 00:14:49,435
is you can think of it as, uh, a gradient.

331
00:14:49,435 --> 00:14:53,185
You can think of it as a measure of the effect of the past on the future.

332
00:14:53,185 --> 00:14:54,760
So we've already talked about this little bit.

333
00:14:54,760 --> 00:14:57,460
Uh, gradient is like saying, if I change, uh,

334
00:14:57,460 --> 00:14:59,575
this weight or this activation a little bit,

335
00:14:59,575 --> 00:15:03,010
then how much and how does it affect this thing in the future.

336
00:15:03,010 --> 00:15:08,650
So in particular, if our gradient is becoming vanishingly small over longer distances,

337
00:15:08,650 --> 00:15:12,130
let say from step T, step T to step T plus N,

338
00:15:12,130 --> 00:15:15,790
then we can't tell whether in one of two situations.

339
00:15:15,790 --> 00:15:18,970
So the first situation is maybe there's no dependency between

340
00:15:18,970 --> 00:15:22,165
step T and step T plus N in the data.

341
00:15:22,165 --> 00:15:24,715
So perhaps we're learning on a task where,

342
00:15:24,715 --> 00:15:27,190
in the task there truly is no collect, uh,

343
00:15:27,190 --> 00:15:29,080
connection or relationship to be

344
00:15:29,080 --> 00:15:31,510
learned between what happens on step T and what happens on

345
00:15:31,510 --> 00:15:33,580
step T plus N. So there truly is nothing to be

346
00:15:33,580 --> 00:15:35,980
learned and it's actually correct that there should be,

347
00:15:35,980 --> 00:15:38,800
you know, small gradients with respect to those two things.

348
00:15:38,800 --> 00:15:41,620
But the second possibility is that, yes,

349
00:15:41,620 --> 00:15:44,965
that is a true connection between those two things in the data and in the task.

350
00:15:44,965 --> 00:15:47,650
And really ideally we should be learning that connection.

351
00:15:47,650 --> 00:15:52,480
Um, but we have the wrong parameters in our model to capture this thing,

352
00:15:52,480 --> 00:15:54,115
and therefore that is why the,

353
00:15:54,115 --> 00:15:55,165
the gradients are small.

354
00:15:55,165 --> 00:15:57,340
Because the model doesn't see them as connected.

355
00:15:57,340 --> 00:16:00,940
So we are not learning the true dependency between these two things.

356
00:16:00,940 --> 00:16:03,970
And the problem with the vanishing gradient problem is that it's,

357
00:16:03,970 --> 00:16:05,740
we're unable to tell in this situation,

358
00:16:05,740 --> 00:16:07,795
which of these two situations we're in.

359
00:16:07,795 --> 00:16:09,820
Okay. So this is all pretty theoretical.

360
00:16:09,820 --> 00:16:11,365
I think this example should make it a little more,

361
00:16:11,365 --> 00:16:14,590
more clear why the vanishing gradient problem is bad.

362
00:16:14,590 --> 00:16:17,695
So, uh, last week we learned about RNN-Language Models.

363
00:16:17,695 --> 00:16:20,680
And if you remember Language Modeling is a task where you have some kind of

364
00:16:20,680 --> 00:16:24,010
text and then you're trying to predict what word should come next.

365
00:16:24,010 --> 00:16:25,630
So, uh, here's a piece of text.

366
00:16:25,630 --> 00:16:28,420
It says, um, ''When she tried to print her tickets,

367
00:16:28,420 --> 00:16:30,310
she found that the printer was out of toner.

368
00:16:30,310 --> 00:16:32,665
She went to the stationery store to buy more toner.

369
00:16:32,665 --> 00:16:34,150
It was very overpriced.

370
00:16:34,150 --> 00:16:36,115
After installing the toner into the printer,

371
00:16:36,115 --> 00:16:38,110
she finally printed her,'' and

372
00:16:38,110 --> 00:16:40,524
can someone shout out what word you think should come next?

373
00:16:40,524 --> 00:16:41,395
Tickets.

374
00:16:41,395 --> 00:16:42,790
Tickets. Yes, exactly.

375
00:16:42,790 --> 00:16:44,920
So that was easy for you to do because, uh,

376
00:16:44,920 --> 00:16:47,275
it makes sense logically that if that was the thing she was trying to do,

377
00:16:47,275 --> 00:16:51,055
that's the thing she's gonna do once she's gone the whole detour for the, for the toner.

378
00:16:51,055 --> 00:16:53,830
Um, so the question is,

379
00:16:53,830 --> 00:16:57,400
can RNN-Language Models easily answer this question.

380
00:16:57,400 --> 00:17:00,429
Would they do well at this particular Language Modeling example?

381
00:17:00,429 --> 00:17:04,060
So for an RNN-Language Model to do well at this kind of example,

382
00:17:04,060 --> 00:17:07,360
then they need to learn from this kind of example in the Training Data.

383
00:17:07,360 --> 00:17:09,745
So if it solves the example in the Training Data,

384
00:17:09,745 --> 00:17:12,475
then the RNN-Language Model will need to model the dependency.

385
00:17:12,475 --> 00:17:14,560
Learn the connection between the appearance of

386
00:17:14,560 --> 00:17:17,065
the word tickets early on on the 7th step,

387
00:17:17,065 --> 00:17:20,200
and the target word tickets at the end.

388
00:17:20,200 --> 00:17:22,945
But if we have the vanishing gradient problem,

389
00:17:22,945 --> 00:17:25,855
then these gradients, uh, if they know the step,

390
00:17:25,855 --> 00:17:27,955
the, the last step with respect to the early step,

391
00:17:27,955 --> 00:17:29,380
it's gonna be very small because it's,

392
00:17:29,380 --> 00:17:31,120
it's a fairly long distance, right?

393
00:17:31,120 --> 00:17:33,310
And this means that the model is going to be unable to

394
00:17:33,310 --> 00:17:36,040
learn this dependency, easily or at all.

395
00:17:36,040 --> 00:17:39,340
So if the model can't learn this kind of dependency during training,

396
00:17:39,340 --> 00:17:41,590
then the model is going to be unable to predict

397
00:17:41,590 --> 00:17:44,845
similar kinds of long distance dependencies at test-time.

398
00:17:44,845 --> 00:17:47,545
Okay, here's another example.

399
00:17:47,545 --> 00:17:50,200
Um, here's a piece of text.

400
00:17:50,200 --> 00:17:52,780
Uh, the text says and this isn't a full sentence.

401
00:17:52,780 --> 00:17:54,160
This is just a partial sentence.

402
00:17:54,160 --> 00:17:56,935
It says, the writer of the books, blank.

403
00:17:56,935 --> 00:17:58,570
And I'm gonna give you two options.

404
00:17:58,570 --> 00:18:03,325
It's either, the writer of the books is or the writer of the books are.

405
00:18:03,325 --> 00:18:07,480
So, uh, again shout out which one do you think it is, is or are?

406
00:18:07,480 --> 00:18:08,200
Is.

407
00:18:08,200 --> 00:18:10,930
Is, that's right. So, uh, the correct answer,

408
00:18:10,930 --> 00:18:13,435
a correct possible continuation of the sentence would be,

409
00:18:13,435 --> 00:18:15,700
uh, the writer of the books is planning a sequel.

410
00:18:15,700 --> 00:18:18,910
I can't think of a continuation that goes the writer of the books are,

411
00:18:18,910 --> 00:18:21,910
that would be, uh, grammatically correct.

412
00:18:21,910 --> 00:18:24,655
So the reason why I'm bringing up this example,

413
00:18:24,655 --> 00:18:27,490
is because this shows a kind of tension between, uh,

414
00:18:27,490 --> 00:18:28,600
two things called, uh,

415
00:18:28,600 --> 00:18:32,725
syntactic recency and sem- uh, sequential recency.

416
00:18:32,725 --> 00:18:36,790
So syntactic recency is the idea that in

417
00:18:36,790 --> 00:18:40,780
order to correctly predict the next word should be more is than are,

418
00:18:40,780 --> 00:18:45,370
is that the word writer is the kind of syntactically close word here.

419
00:18:45,370 --> 00:18:48,640
So we say the writer of the books is because it's the writer is.

420
00:18:48,640 --> 00:18:51,310
So you can see this as the word writer and is,

421
00:18:51,310 --> 00:18:53,425
are, uh, syntactically close.

422
00:18:53,425 --> 00:18:55,975
Because if you looked at the dependency paths for example,

423
00:18:55,975 --> 00:18:59,050
then there would be a short path in that tree.

424
00:18:59,050 --> 00:19:05,155
So by contrast, se- sequential recency is the,

425
00:19:05,155 --> 00:19:10,960
uh, simpler concepts of how close words are just in the sentence as a sequence of words.

426
00:19:10,960 --> 00:19:12,534
So in this example,

427
00:19:12,534 --> 00:19:15,865
books and are, are very sequentially recent because they're right next to each other.

428
00:19:15,865 --> 00:19:18,475
So the reason I'm bringing this up is because,

429
00:19:18,475 --> 00:19:22,660
the second one would be incorrect but it's kind of a tempting option.

430
00:19:22,660 --> 00:19:26,590
Because if you're mostly only paying attention to things that happened recently,

431
00:19:26,590 --> 00:19:29,155
um, then you might get distracted and think,

432
00:19:29,155 --> 00:19:31,090
"Oh, the books are, that sounds right."

433
00:19:31,090 --> 00:19:35,500
So the problem here is that RNN-Language Models

434
00:19:35,500 --> 00:19:41,170
are better at learning from sequential recency than sicta- syntactic recency.

435
00:19:41,170 --> 00:19:42,655
And this is partially due,

436
00:19:42,655 --> 00:19:44,455
due to the vanishing gradient problem.

437
00:19:44,455 --> 00:19:47,290
Because especially perhaps, if your syntactically,

438
00:19:47,290 --> 00:19:49,765
uh, related word is actually kind of far away,

439
00:19:49,765 --> 00:19:54,355
then it might get really hard to use the information from the syntactically recent word,

440
00:19:54,355 --> 00:19:58,390
especially if there's a lot of strong signal from the sequentially recent word.

441
00:19:58,390 --> 00:20:03,520
So, uh, there are some papers that show that RNN-Language Models make this kind of error,

442
00:20:03,520 --> 00:20:05,200
of saying are, rather than is.

443
00:20:05,200 --> 00:20:08,440
Uh, they make this kind of error more often than you would like, uh,

444
00:20:08,440 --> 00:20:11,860
especially if you have multiple of these distracting words such as books, uh,

445
00:20:11,860 --> 00:20:14,380
in between, uh, the word you're trying to predict

446
00:20:14,380 --> 00:20:17,900
and the true word that you should be, uh, referring to.

447
00:20:19,470 --> 00:20:27,490
Okay, any questions on this? All right, moving on.

448
00:20:27,490 --> 00:20:31,780
So, we briefly mentioned that exploding gradients, uh, is a problem.

449
00:20:31,780 --> 00:20:34,960
So, I'm briefly going to justify why is exploding gradients a problem,

450
00:20:34,960 --> 00:20:36,970
and why does it, uh, what does it look like?

451
00:20:36,970 --> 00:20:40,015
[NOISE] So, the reason why exploding gradients are a problem,

452
00:20:40,015 --> 00:20:42,460
is if you remember this is how SGD works.

453
00:20:42,460 --> 00:20:44,860
Uh, we say that the new parameters of the model,

454
00:20:44,860 --> 00:20:46,495
which we represent by Theta,

455
00:20:46,495 --> 00:20:48,430
is equal to the old premises,

456
00:20:48,430 --> 00:20:50,620
and then you take some step in the direction of

457
00:20:50,620 --> 00:20:54,040
negative gradients because you're trying to minimize the loss of J.

458
00:20:54,040 --> 00:20:58,050
So, the problem is if your gradient gets really big, uh,

459
00:20:58,050 --> 00:21:01,890
then your SGD update step is going to become really big too.

460
00:21:01,890 --> 00:21:03,690
So, you're going to be taking a very big step,

461
00:21:03,690 --> 00:21:07,170
and you're going to be drastically changing your model parameters, Theta.

462
00:21:07,170 --> 00:21:10,705
And this means that you can end up with some bad updates.

463
00:21:10,705 --> 00:21:13,075
We end up taking too large a step.

464
00:21:13,075 --> 00:21:15,580
And we're changing the parameters too much.

465
00:21:15,580 --> 00:21:16,780
And this means that, uh,

466
00:21:16,780 --> 00:21:18,145
we kind of take a big step,

467
00:21:18,145 --> 00:21:19,840
and we end up in some, uh,

468
00:21:19,840 --> 00:21:21,940
area where the parameters are actually very bad.

469
00:21:21,940 --> 00:21:25,450
Uh, with example the- for example,

470
00:21:25,450 --> 00:21:27,805
they might have a much larger loss than they had before.

471
00:21:27,805 --> 00:21:29,860
So, in the worst case,

472
00:21:29,860 --> 00:21:32,755
this can often manifest as seeing, uh,

473
00:21:32,755 --> 00:21:37,920
infinities or NaNs, not a number in your network when you're training it in practice.

474
00:21:37,920 --> 00:21:41,485
[NOISE] So, this can happen because if you take such a big step

475
00:21:41,485 --> 00:21:45,445
that maybe you update your parameters so much that now they're infinity,

476
00:21:45,445 --> 00:21:47,290
or minus infinity, something like that,

477
00:21:47,290 --> 00:21:50,245
then you're gonna have all of these infinities within your activations as well,

478
00:21:50,245 --> 00:21:52,195
and then all of your losses are going to be infinity,

479
00:21:52,195 --> 00:21:54,385
and the whole thing just isn't going to work, at all.

480
00:21:54,385 --> 00:21:56,170
So, it's very annoying when this happens,

481
00:21:56,170 --> 00:21:58,465
and unfortunately it happens, uh, fairly often.

482
00:21:58,465 --> 00:22:00,355
And if it does then you have to essentially

483
00:22:00,355 --> 00:22:02,590
restart training from some earlier checkpoint before you

484
00:22:02,590 --> 00:22:04,480
got the NaNs and the infinities because there's

485
00:22:04,480 --> 00:22:06,580
no kind of salvaging it from its new state.

486
00:22:06,580 --> 00:22:10,900
[NOISE] So, what's the solution to this exploding gradient problem?

487
00:22:10,900 --> 00:22:13,300
[NOISE] Uh, the solution is actually pretty

488
00:22:13,300 --> 00:22:16,315
simple and it's this technique called gradient clipping.

489
00:22:16,315 --> 00:22:18,580
So, the main idea of gradient clipping,

490
00:22:18,580 --> 00:22:21,610
[NOISE] is that if the norm of your gradient is

491
00:22:21,610 --> 00:22:25,600
greater than some threshold and the threshold is a hyperparameter that you choose.

492
00:22:25,600 --> 00:22:29,110
uh, then you want to scale down that gradient,

493
00:22:29,110 --> 00:22:32,035
um, before you apply the SGD update.

494
00:22:32,035 --> 00:22:35,410
So, the intuition is yo- you're still gonna take a step in the same direction.

495
00:22:35,410 --> 00:22:37,030
But you're gonna make sure that it's a smaller step.

496
00:22:37,030 --> 00:22:38,950
[NOISE] So, here, um,

497
00:22:38,950 --> 00:22:41,995
I've got a screenshot of some pseudocode from, uh,

498
00:22:41,995 --> 00:22:43,375
the related paper that, uh,

499
00:22:43,375 --> 00:22:46,180
proposed gradient clipping, or at least some version of gradient clipping.

500
00:22:46,180 --> 00:22:48,640
[NOISE] And, um, it's pretty simple as you can see.

501
00:22:48,640 --> 00:22:51,310
Uh, g hat is the vector which is the, uh,

502
00:22:51,310 --> 00:22:54,475
derivative of the error with respect to the premises,

503
00:22:54,475 --> 00:22:56,770
and it's saying that if the norm of

504
00:22:56,770 --> 00:23:00,070
this gradient is greater than the threshold's, then you just scale it down.

505
00:23:00,070 --> 00:23:03,429
But the important thing to note is that it's still pointing in the same direction,

506
00:23:03,429 --> 00:23:06,290
it's just a smaller step.

507
00:23:06,420 --> 00:23:10,615
So, here's a picture to show how that might work out in practice.

508
00:23:10,615 --> 00:23:13,105
And, uh, this is a diagram from the, uh,

509
00:23:13,105 --> 00:23:16,255
deep learning textbook which is also linked on [NOISE] the website.

510
00:23:16,255 --> 00:23:19,615
So, what's going on here, is that, uh,

511
00:23:19,615 --> 00:23:23,200
the picture here is the loss surface of a simple RNN.

512
00:23:23,200 --> 00:23:27,250
So, they made a very simple RNN that instead of having, uh,

513
00:23:27,250 --> 00:23:29,275
a sequence of vectors as the hidden states,

514
00:23:29,275 --> 00:23:32,545
it just suppose that each hidden state is simply just a single scalar.

515
00:23:32,545 --> 00:23:35,020
So, this means that instead of having a weight matrix, w,

516
00:23:35,020 --> 00:23:36,490
and the bias vector, b,

517
00:23:36,490 --> 00:23:38,605
you have a scalar w and a scalar b.

518
00:23:38,605 --> 00:23:42,550
So, that's why in the picture, you just have this like two-dimensional parameter space.

519
00:23:42,550 --> 00:23:45,805
And then the, the z-axis is your, is your loss.

520
00:23:45,805 --> 00:23:47,515
So here, high loss is,

521
00:23:47,515 --> 00:23:50,335
is bad and low loss is good in what you're trying to get.

522
00:23:50,335 --> 00:23:52,825
So, uh, here in this picture,

523
00:23:52,825 --> 00:23:56,890
you've got this kind of cliff, right, where you have this very steep cliff face,

524
00:23:56,890 --> 00:23:59,290
uh, where the loss changes very quickly.

525
00:23:59,290 --> 00:24:03,715
[NOISE] And this cliff is really dangerous because it has steep, steep gradients.

526
00:24:03,715 --> 00:24:06,160
And you might be in danger of taking a really big,

527
00:24:06,160 --> 00:24:09,970
[NOISE] uh, update step because you're on the area with a really steep gradient.

528
00:24:09,970 --> 00:24:12,505
[NOISE] So, on the left,

529
00:24:12,505 --> 00:24:17,410
you've got a possible scenario of what might happen if you don't have gradient clipping.

530
00:24:17,410 --> 00:24:19,330
[NOISE] So, on the left, uh,

531
00:24:19,330 --> 00:24:22,615
you can see that you start kind of at the bottom of the cliff,

532
00:24:22,615 --> 00:24:25,465
and you have a f- a si- a few small updates.

533
00:24:25,465 --> 00:24:28,150
And then, in particular makes a bad update because you

534
00:24:28,150 --> 00:24:30,760
see there's a small kind of dip before it goes off the cliff.

535
00:24:30,760 --> 00:24:32,590
So, th- the true local minimum,

536
00:24:32,590 --> 00:24:36,145
the optimal you're trying to get to is that the bottom of that small kind of ditch.

537
00:24:36,145 --> 00:24:40,375
And, um, it starts off kind of near the edge of that ditch,

538
00:24:40,375 --> 00:24:42,670
and then there's a negative gradient going into it.

539
00:24:42,670 --> 00:24:45,775
But unfortunately, the, the update kind of overshoots,

540
00:24:45,775 --> 00:24:47,785
and it ends up going a long way off the cliff.

541
00:24:47,785 --> 00:24:50,470
So now, it's in this bad situation where it's taken a bad update,

542
00:24:50,470 --> 00:24:52,930
and now it's got a much bigger loss than it had [NOISE] before.

543
00:24:52,930 --> 00:24:54,655
So now that it's on the cliff.

544
00:24:54,655 --> 00:24:56,485
Again it, it measures the gradient,

545
00:24:56,485 --> 00:24:58,180
and the gradient is very steep, right?

546
00:24:58,180 --> 00:24:59,425
The gradient is very large.

547
00:24:59,425 --> 00:25:01,360
So, when it takes a, uh,

548
00:25:01,360 --> 00:25:03,115
update with respect to that gradient,

549
00:25:03,115 --> 00:25:04,300
then because the gradient is so big,

550
00:25:04,300 --> 00:25:05,905
it takes a really huge step.

551
00:25:05,905 --> 00:25:08,020
And that's, um, the, the one to the right.

552
00:25:08,020 --> 00:25:09,580
You can see the step going to the right.

553
00:25:09,580 --> 00:25:12,190
So, that's also a very bad update because it's just throwing

554
00:25:12,190 --> 00:25:15,310
it really far to some probably fairly random,

555
00:25:15,310 --> 00:25:17,635
uh, configuration of w and b.

556
00:25:17,635 --> 00:25:20,740
So, on the left, you can see what can go wrong if you're taking

557
00:25:20,740 --> 00:25:24,825
these really big steps because you were in areas with a very steep gradient.

558
00:25:24,825 --> 00:25:26,520
So, by contrast on the right,

559
00:25:26,520 --> 00:25:29,610
you can see what might happen if you do have a gradient clipping.

560
00:25:29,610 --> 00:25:32,550
[NOISE] [NOISE] And, um, it's much less drastic, right?

561
00:25:32,550 --> 00:25:35,775
You've got a similar kind of pattern where it takes a few steps into the ditch,

562
00:25:35,775 --> 00:25:37,745
and then ends up going off the cliff a little bit,

563
00:25:37,745 --> 00:25:39,835
but not too much because the gradient was clipped.

564
00:25:39,835 --> 00:25:42,400
And then, it's on the cliff and there's again a really steep gradient,

565
00:25:42,400 --> 00:25:45,490
but it doesn't take such a big step because again the gradient was clipped,

566
00:25:45,490 --> 00:25:47,095
so that it kind of comes back down.

567
00:25:47,095 --> 00:25:51,085
So, you can see that plausibly by using this gradient clipping method,

568
00:25:51,085 --> 00:25:53,215
you've got a, a kind of safer update rule,

569
00:25:53,215 --> 00:25:54,310
where you're not gonna take any,

570
00:25:54,310 --> 00:25:57,355
any big crazy steps and you're more likely to kind of find the,

571
00:25:57,355 --> 00:25:59,590
the true minimum which is at the bottom of the ditch.

572
00:25:59,590 --> 00:26:02,245
[NOISE] I think there was a question earlier.

573
00:26:02,245 --> 00:26:03,940
Was there a question over here? [NOISE]

574
00:26:03,940 --> 00:26:05,380
I just want to see the value. [NOISE] [NOISE]

575
00:26:05,380 --> 00:26:07,870
Okay. Anyone else?

576
00:26:07,870 --> 00:26:09,460
[NOISE]

577
00:26:09,460 --> 00:26:21,190
Yeah?

578
00:26:21,190 --> 00:26:21,460
[NOISE] [inaudible]

579
00:26:21,460 --> 00:26:23,005
So, the question is, in assignment three,

580
00:26:23,005 --> 00:26:26,320
y- you saw the atom optimization algorithm which, uh,

581
00:26:26,320 --> 00:26:27,925
has this thing called momentum,

582
00:26:27,925 --> 00:26:31,015
which essentially says that kind of like physical momentum in,

583
00:26:31,015 --> 00:26:35,755
in the real world, that if you've been traveling in the same direction for a while,

584
00:26:35,755 --> 00:26:39,190
then you can take bigger steps,

585
00:26:39,190 --> 00:26:41,320
I think, and if you've recently kind of changed direction,

586
00:26:41,320 --> 00:26:42,835
then you should take smaller steps.

587
00:26:42,835 --> 00:26:47,620
And I think there's another element as well, where you divide by some factor.

588
00:26:47,620 --> 00:26:49,600
[NOISE] So, it is a similar kind of idea.

589
00:26:49,600 --> 00:26:51,190
I suppose it's a different criterion, right?

590
00:26:51,190 --> 00:26:54,430
So, what they both have in common is it's a kind of criterion for when to

591
00:26:54,430 --> 00:26:58,045
scale up or scale down the size of your update step.

592
00:26:58,045 --> 00:27:00,190
Um, and I think they're based on different notions

593
00:27:00,190 --> 00:27:03,160
of when should you take bigger steps and when should you take smaller steps.

594
00:27:03,160 --> 00:27:05,020
When should you be cautious or less cautious?

595
00:27:05,020 --> 00:27:07,510
So, I guess here the criterion is different.

596
00:27:07,510 --> 00:27:09,880
It's kind of a simple criterion saying, like if it's really steep,

597
00:27:09,880 --> 00:27:13,250
then be careful. Yeah. Another question?

598
00:27:26,730 --> 00:27:31,885
Uh, so the [inaudible]. [NOISE]

599
00:27:31,885 --> 00:27:34,060
Okay. So the question is,

600
00:27:34,060 --> 00:27:37,150
is this similar to regularization of some kind, right?

601
00:27:37,150 --> 00:27:39,460
So, I suppose, yeah, there is- there are some things in common.

602
00:27:39,460 --> 00:27:43,690
Say for, example, L2 regularization says that you want, for example,

603
00:27:43,690 --> 00:27:49,405
your weight matrices to have a small L2 norm, right?

604
00:27:49,405 --> 00:27:51,340
And the idea is that you're trying to prevent

605
00:27:51,340 --> 00:27:53,920
your model from over-fitting the data by, um,

606
00:27:53,920 --> 00:27:57,190
having some kind of constraint that says you have to keep your weights fairly simple,

607
00:27:57,190 --> 00:27:59,140
that is keep them, you know, small.

608
00:27:59,140 --> 00:28:01,390
So, I suppose the relationship is that here we're

609
00:28:01,390 --> 00:28:03,490
saying that we don't want the norm of the gradients to be too big.

610
00:28:03,490 --> 00:28:07,500
Ah, I don't know if this is related to overfitting.

611
00:28:07,500 --> 00:28:09,495
Um, I guess I have to think more carefully about that,

612
00:28:09,495 --> 00:28:14,175
but I guess it's a similar kind of constraint that you're placing.

613
00:28:14,175 --> 00:28:16,440
Okay. I'm gonna move on for now.

614
00:28:16,440 --> 00:28:19,660
Uh, so we've talked

615
00:28:19,660 --> 00:28:23,080
about how you might fix the exploding gradient problem with gradient clipping,

616
00:28:23,080 --> 00:28:26,740
but we haven't talked about how we might fix the vanishing gradient problem.

617
00:28:26,740 --> 00:28:29,410
So, um, to recap,

618
00:28:29,410 --> 00:28:34,270
I think one way to characterize the problem with the- the vanishing gradients in RNNs is

619
00:28:34,270 --> 00:28:39,620
that it's too difficult for the RNN to learn to preserve information over many timesteps.

620
00:28:39,620 --> 00:28:41,340
So, in our example with printing

621
00:28:41,340 --> 00:28:44,475
the tickets and re- remembering that it's the tickets that she wants to print,

622
00:28:44,475 --> 00:28:48,420
you could think of it as it's hard for the RNN language model to correctly

623
00:28:48,420 --> 00:28:52,260
predict tickets because in a way, it's too hard for the RNN language model to,

624
00:28:52,260 --> 00:28:56,345
uh, learn to retain the tickets information and use it later.

625
00:28:56,345 --> 00:28:58,900
So, um, if you look at the equation

626
00:28:58,900 --> 00:29:01,630
for vanilla RNNs and how we compute the hidden state, uh,

627
00:29:01,630 --> 00:29:03,955
based on the previous hidden state and- and the inputs,

628
00:29:03,955 --> 00:29:07,495
you can see that the hidden state is in a way constantly being rewritten.

629
00:29:07,495 --> 00:29:09,910
It's always computed based on these, uh,

630
00:29:09,910 --> 00:29:11,650
linear transformations and the,

631
00:29:11,650 --> 00:29:13,105
you know, the non-linearity.

632
00:29:13,105 --> 00:29:15,010
So, it's not all that easy to

633
00:29:15,010 --> 00:29:17,965
preserve the information from one hidden state to the other,

634
00:29:17,965 --> 00:29:21,145
in particular, because we are putting it through this non-linearity function.

635
00:29:21,145 --> 00:29:26,980
So, this motivates us to ask what about an RNN with some kind of separate memory?

636
00:29:26,980 --> 00:29:31,510
If we have some kind of separate place to store information that we want to use later,

637
00:29:31,510 --> 00:29:34,630
then would this make it easier for our RNN

638
00:29:34,630 --> 00:29:38,290
to learn to preserve information over many timesteps?

639
00:29:38,290 --> 00:29:45,205
So, this is the motivating idea behind LSTMs or Long Short-Term Memory RNNs.

640
00:29:45,205 --> 00:29:51,550
So, the idea here is that an LSTM is a type of RNN and it was proposed back in, uh, 1997.

641
00:29:51,550 --> 00:29:53,335
And the idea is that this is, uh,

642
00:29:53,335 --> 00:29:56,800
this was proposed as an explicit solution to the vanishing gradients problem.

643
00:29:56,800 --> 00:30:00,280
[NOISE] So, one of the main differences here is

644
00:30:00,280 --> 00:30:03,880
that on each step T instead of just having a hidden state h_t,

645
00:30:03,880 --> 00:30:08,320
we have both the hidden state h_t and the cell state which we denote c_t.

646
00:30:08,320 --> 00:30:12,085
And both of these are vectors of some same length,

647
00:30:12,085 --> 00:30:15,430
n, and the idea there is that the cell is meant to

648
00:30:15,430 --> 00:30:20,110
sto- store our long-term information that, that's on memory units.

649
00:30:20,110 --> 00:30:23,605
Another super important thing is that the LSTM can

650
00:30:23,605 --> 00:30:26,980
erase and write [NOISE] and read information from the cell.

651
00:30:26,980 --> 00:30:29,995
So, you kind of think of this a bit like memory in a computer,

652
00:30:29,995 --> 00:30:33,715
in that you can do these operations, reading and writing and erasing,

653
00:30:33,715 --> 00:30:37,090
um, and that's how you're gonna keep your information.

654
00:30:37,090 --> 00:30:39,220
[NOISE].

655
00:30:39,220 --> 00:30:43,254
Another super important thing is that the way the LSTM decides,

656
00:30:43,254 --> 00:30:45,490
whether it wants to erase, write, read,

657
00:30:45,490 --> 00:30:48,565
information and decide how much and which information,

658
00:30:48,565 --> 00:30:51,430
uh, that's all controlled by these [NOISE] gates.

659
00:30:51,430 --> 00:30:56,560
So, the idea is [NOISE] that the gates are themselves also vectors of length n,

660
00:30:56,560 --> 00:30:59,590
and the idea there is that on each timestep,

661
00:30:59,590 --> 00:31:04,960
each element of these gates which are vectors are somewhere between zero and one.

662
00:31:04,960 --> 00:31:10,165
So here, uh, one represents an open gate and zero represents a closed gate,

663
00:31:10,165 --> 00:31:12,715
and you can have values anywhere in between.

664
00:31:12,715 --> 00:31:15,475
So, the overall idea, which we're gonna firm up on the next slide,

665
00:31:15,475 --> 00:31:17,770
but the overall idea is that if the gate is open,

666
00:31:17,770 --> 00:31:20,590
that represents some kind of information being passed through,

667
00:31:20,590 --> 00:31:21,670
and if the gate is closed,

668
00:31:21,670 --> 00:31:24,320
it [NOISE] means that information does not pass through.

669
00:31:24,360 --> 00:31:28,705
Okay. So, the last really important thing is that the gates are dynamic.

670
00:31:28,705 --> 00:31:32,950
They're not just set at some constant value for the whole sequence.

671
00:31:32,950 --> 00:31:34,330
[NOISE] Um, they're dynamic,

672
00:31:34,330 --> 00:31:36,790
which means that they're different on each timestep T,

673
00:31:36,790 --> 00:31:41,200
and the value that is the decision of whether they're open or closed and in which ways,

674
00:31:41,200 --> 00:31:45,100
[NOISE] um, that is computed based on the current context.

675
00:31:45,100 --> 00:31:46,945
Okay. So here's, um,

676
00:31:46,945 --> 00:31:50,125
here's the- the equations for the LSTM which might make it clearer.

677
00:31:50,125 --> 00:31:54,160
So, uh, suppose we have some sequence of i- inputs x_t and we

678
00:31:54,160 --> 00:31:58,435
want to compute a sequence of hidden state h_t and cell states c_t.

679
00:31:58,435 --> 00:32:02,740
So, this is what happens on timestep t. Uh,

680
00:32:02,740 --> 00:32:07,210
this process equation shows you the three gates that I talked about before.

681
00:32:07,210 --> 00:32:09,910
So, the first one is called the Forget Gates.

682
00:32:09,910 --> 00:32:14,545
And the idea is that this one is controlling what is kept versus what is forgotten,

683
00:32:14,545 --> 00:32:18,130
um, from the previous cell state, the previous memory.

684
00:32:18,130 --> 00:32:22,510
And you can see that this forget gate is computed based on, uh,

685
00:32:22,510 --> 00:32:26,465
the previous hidden state h_t minus one and the current input x_t.

686
00:32:26,465 --> 00:32:29,110
Um, so that's what I meant when I said that it's

687
00:32:29,110 --> 00:32:31,930
dynamic and it's computed based on the- the current context.

688
00:32:31,930 --> 00:32:36,190
[NOISE] Um, you can also see that it's computed using,

689
00:32:36,190 --> 00:32:37,390
uh, the sigmoid function,

690
00:32:37,390 --> 00:32:39,920
which means that it is somewhere between zero and one.

691
00:32:39,920 --> 00:32:43,050
Okay. The next gate is called the input gate,

692
00:32:43,050 --> 00:32:48,645
and this one controls what parts of the new cell contents are written to the cell.

693
00:32:48,645 --> 00:32:52,020
So, the idea there is that you have this- this memory cell and this is kind of, um,

694
00:32:52,020 --> 00:32:57,235
controlling like ho- how and what you get to write to the memory cell.

695
00:32:57,235 --> 00:32:59,995
Okay. And the last one is called the upper gate.

696
00:32:59,995 --> 00:33:01,780
So, this one is controlling, uh,

697
00:33:01,780 --> 00:33:04,930
what parts of the cell are outputs to the hidden state,

698
00:33:04,930 --> 00:33:08,620
[NOISE] so you could view this as kind of like the read function, right?

699
00:33:08,620 --> 00:33:10,090
We're going to read some information from

700
00:33:10,090 --> 00:33:12,685
our memory cell and that's gonna get put into our hidden states,

701
00:33:12,685 --> 00:33:14,050
and this gate is gonna control that.

702
00:33:14,050 --> 00:33:16,990
[NOISE] Okay.

703
00:33:16,990 --> 00:33:22,060
[NOISE] Uh, yeah, that's just the sigmoid function as we noted before.

704
00:33:22,060 --> 00:33:25,870
All right. So, the next set of equation shows how we use these gates.

705
00:33:25,870 --> 00:33:28,120
[NOISE] So, the first line, uh,

706
00:33:28,120 --> 00:33:29,335
you could regard this, uh,

707
00:33:29,335 --> 00:33:32,095
c_tilde as the new [NOISE] cell content.

708
00:33:32,095 --> 00:33:34,960
So, uh, this is the new content that you want to write to the cell,

709
00:33:34,960 --> 00:33:37,540
[NOISE] and this is also computed based on, uh,

710
00:33:37,540 --> 00:33:39,490
your previous hidden state and your current inputs,

711
00:33:39,490 --> 00:33:41,920
and this goes through your tan h non-linearity.

712
00:33:41,920 --> 00:33:45,250
So, uh, this is kind of the- the main contents that

713
00:33:45,250 --> 00:33:49,570
you are computing based on the context and you want to write this into memory.

714
00:33:49,570 --> 00:33:54,550
So, on the next line what's happening is that we're going to use

715
00:33:54,550 --> 00:34:00,070
the forget gate to selectively forget some of the information from the previous,

716
00:34:00,070 --> 00:34:01,930
[NOISE] uh, memory cell.

717
00:34:01,930 --> 00:34:04,780
And you can see that we're doing these element-wise products,

718
00:34:04,780 --> 00:34:06,340
that's what the little circle is.

719
00:34:06,340 --> 00:34:08,950
So, the idea is that if you remember that f_t is

720
00:34:08,950 --> 00:34:11,980
a vector full of values between zero and one,

721
00:34:11,980 --> 00:34:14,380
when you do an element-wise product between f_t and

722
00:34:14,380 --> 00:34:16,885
the previous cell state c_t minus one,

723
00:34:16,885 --> 00:34:19,000
then what you're essentially doing is you're kind of masking

724
00:34:19,000 --> 00:34:21,865
out some of the information from the previous hidden state.

725
00:34:21,865 --> 00:34:23,680
Sorry, no. Previous cell state.

726
00:34:23,680 --> 00:34:26,155
So, when f is one,

727
00:34:26,155 --> 00:34:27,805
then you're copying over the information,

728
00:34:27,805 --> 00:34:30,340
but when f is zero, then you're getting rid of that information,

729
00:34:30,340 --> 00:34:32,840
you are erasing it or forgetting it.

730
00:34:33,930 --> 00:34:37,030
Okay. And then the other half of this equation,

731
00:34:37,030 --> 00:34:39,550
um, i_t times c tilde t, uh,

732
00:34:39,550 --> 00:34:41,500
that's the input gate controlling

733
00:34:41,500 --> 00:34:44,215
which parts of the new cell contents are gonna get written,

734
00:34:44,215 --> 00:34:46,910
written to the, to the cell.

735
00:34:47,130 --> 00:34:50,095
Okay. And then the last thing we do is we, uh,

736
00:34:50,095 --> 00:34:52,900
pass the cell through a tan h,

737
00:34:52,900 --> 00:34:55,390
that's just adding another non-linearity,

738
00:34:55,390 --> 00:34:56,530
and then you pass that through

739
00:34:56,530 --> 00:34:59,380
the output gates and that gives you [NOISE] the hidden state.

740
00:34:59,380 --> 00:35:02,950
So, in LSTMs, we often think of the hidden states as being,

741
00:35:02,950 --> 00:35:05,095
uh, like the outputs of the RNN.

742
00:35:05,095 --> 00:35:07,570
And the reason for this is that you kind of view

743
00:35:07,570 --> 00:35:09,910
the cell states as being this kind of

744
00:35:09,910 --> 00:35:12,985
internal memory that's not generally accessible to the outside,

745
00:35:12,985 --> 00:35:15,280
but the hidden states are the parts that you're

746
00:35:15,280 --> 00:35:17,905
gonna pa- pass on to the next part of the model.

747
00:35:17,905 --> 00:35:20,620
So, that's why we view it as kind of like the output of the model.

748
00:35:20,620 --> 00:35:24,520
[NOISE] Uh, and this is, yeah,

749
00:35:24,520 --> 00:35:26,410
x just to remind the- there is- circles are

750
00:35:26,410 --> 00:35:29,095
element-wise products and that's how we apply the gates.

751
00:35:29,095 --> 00:35:30,910
Uh, did anyone have any questions about this?

752
00:35:30,910 --> 00:35:40,990
[NOISE].

753
00:35:40,990 --> 00:35:43,960
Okay. [NOISE] Um, so as a reminder,

754
00:35:43,960 --> 00:35:46,420
all of these are vectors of some same length n.

755
00:35:46,420 --> 00:35:49,510
[NOISE] Okay.

756
00:35:49,510 --> 00:35:52,914
So, some people learn better from diagrams than equations,

757
00:35:52,914 --> 00:35:55,660
and here's a diagram presentation of the same idea.

758
00:35:55,660 --> 00:35:57,895
So, this is a really nice diagram from a blog post,

759
00:35:57,895 --> 00:35:59,665
uh, by Chris Olah about LSTMs,

760
00:35:59,665 --> 00:36:01,420
and that was a good place to start if you want to

761
00:36:01,420 --> 00:36:04,450
get an intuitive understanding of what LSTMs are.

762
00:36:04,450 --> 00:36:06,505
So, in this diagram, uh,

763
00:36:06,505 --> 00:36:09,055
the green boxes represent timesteps,

764
00:36:09,055 --> 00:36:12,550
um, and let's zoom in on the middle one and see what's happening here.

765
00:36:12,550 --> 00:36:14,860
So, within one timestep,

766
00:36:14,860 --> 00:36:17,650
you can see that this diagram is showing exactly the same thing as

767
00:36:17,650 --> 00:36:20,755
those six equations showed on the previous slide.

768
00:36:20,755 --> 00:36:25,810
So, uh, the first thing we do is we use the, uh, the current input x_t,

769
00:36:25,810 --> 00:36:29,350
which is at the bottom and the previous hidden state h_t minus the one on the left,

770
00:36:29,350 --> 00:36:31,420
and we can use that to compute the forget gate.

771
00:36:31,420 --> 00:36:34,720
[NOISE] And you can see f_t is on that arrow there.

772
00:36:34,720 --> 00:36:39,385
And then you apply the forget gate to the previous, uh, cell,

773
00:36:39,385 --> 00:36:42,970
and that's the same thing as forgetting some of the- the cell content from last time.

774
00:36:42,970 --> 00:36:44,695
[NOISE] Okay.

775
00:36:44,695 --> 00:36:47,290
And then after that, you can compute the input gate, uh,

776
00:36:47,290 --> 00:36:50,155
and that's computed in much the same way as the forget gate.

777
00:36:50,155 --> 00:36:55,240
And then you use the input gate to decide which parts of this,

778
00:36:55,240 --> 00:36:58,765
uh, new cell content get written to the cell,

779
00:36:58,765 --> 00:37:00,565
and that gives you the cell c_t.

780
00:37:00,565 --> 00:37:04,240
So, here you can see that you computed the impu ga- input gates and

781
00:37:04,240 --> 00:37:08,770
the new content and then you use that to gate that and write it to the cell.

782
00:37:08,770 --> 00:37:10,600
So, now we've got our new cell c_t,

783
00:37:10,600 --> 00:37:15,370
and then the last things we need to do is to compute our new output gate, that's o_t.

784
00:37:15,370 --> 00:37:19,780
And then lastly, use the output gate to select which parts of

785
00:37:19,780 --> 00:37:25,090
the cell contents you're gonna read and put in the new hidden state h_t.

786
00:37:25,090 --> 00:37:27,310
So, that's, that's, uh, that's

787
00:37:27,310 --> 00:37:30,440
the same thing as the equations we saw on the previous slide.

788
00:37:32,400 --> 00:37:35,035
Okay. So, that's LSTMs.

789
00:37:35,035 --> 00:37:37,510
Um, is there a question?

790
00:37:37,510 --> 00:37:50,110
What's the importance [NOISE]  [inaudible]

791
00:37:50,110 --> 00:37:52,840
The question is, why are we applying a tan h

792
00:37:52,840 --> 00:37:55,720
on the very last equation on this, on this slide?

793
00:37:55,720 --> 00:38:01,120
Why we're planning a tan h to the cell before applying the output gate?

794
00:38:01,120 --> 00:38:05,800
Let's see. Um.

795
00:38:05,800 --> 00:38:10,330
Yeah. So, your question is, the- the cell,

796
00:38:10,330 --> 00:38:19,330
the new cell content already went through a tan h. Um, I'm not sure.

797
00:38:19,330 --> 00:38:21,580
So, I suppose a- a- a general answer is that it must

798
00:38:21,580 --> 00:38:23,980
be giving some kind of more expressivity in some way,

799
00:38:23,980 --> 00:38:26,410
and that it's not just applying

800
00:38:26,410 --> 00:38:31,420
tan h's sequentially because you do have the gates in between.

801
00:38:31,420 --> 00:38:34,240
Um, so I suppose there must be a reason,

802
00:38:34,240 --> 00:38:36,445
kind of similarly to when you apply- apply

803
00:38:36,445 --> 00:38:39,640
a linear layer you won't have a non-linearity before the next linear layer.

804
00:38:39,640 --> 00:38:43,075
I suppose maybe we're viewing these cases as a kind of linear layer?

805
00:38:43,075 --> 00:38:44,920
I'm not sure. I'll look it up.

806
00:38:44,920 --> 00:38:49,450
[NOISE] Okay.

807
00:38:49,450 --> 00:38:52,075
So, uh, that's LSTMs.

808
00:38:52,075 --> 00:38:54,220
And, um, re- if you recall,

809
00:38:54,220 --> 00:38:55,795
we were- oh, question?

810
00:38:55,795 --> 00:38:58,615
Yeah. Why is it that in the forget gate,

811
00:38:58,615 --> 00:39:02,185
you don't look at the previous cell state but you just look at the new hidden state?

812
00:39:02,185 --> 00:39:04,180
Like it seems like if you're this- instead of

813
00:39:04,180 --> 00:39:07,090
deciding what to forget from the cell state, you should look at it.

814
00:39:07,090 --> 00:39:09,670
So the question is, why is the forget gate

815
00:39:09,670 --> 00:39:12,610
computed only for the previous hidden state and the current input,

816
00:39:12,610 --> 00:39:16,840
why is it not computed based on ct minus one itself, right?

817
00:39:16,840 --> 00:39:18,820
Because surely you want to look at the thing to figure

818
00:39:18,820 --> 00:39:21,880
out whether you want to forget it or not?

819
00:39:21,880 --> 00:39:24,295
Um, that's a pretty good question.

820
00:39:24,295 --> 00:39:29,740
Uh, so, I suppose one reason why you might think that this- this works fine is that

821
00:39:29,740 --> 00:39:33,039
the LSTM might be learning a general algorithm

822
00:39:33,039 --> 00:39:36,730
for where it stores different types of information in the cell, right?

823
00:39:36,730 --> 00:39:39,370
So, maybe it's learning that in this particular position in the cell,

824
00:39:39,370 --> 00:39:44,230
I learn information about this particular semantic thing and then in this situation,

825
00:39:44,230 --> 00:39:48,295
I want to use that or not use that, forget it or keep it.

826
00:39:48,295 --> 00:39:51,580
But, yeah, I haven't entirely convinced myself why you don't want to

827
00:39:51,580 --> 00:39:55,090
look at the contents of the cell itself in order to decide.

828
00:39:55,090 --> 00:40:02,530
I suppose another thing to notice is that ht minus one was read from ct minus one.

829
00:40:02,530 --> 00:40:06,010
So, I suppose there is some information there but not necessarily all of the information.

830
00:40:06,010 --> 00:40:11,095
Ah, yeah.

831
00:40:11,095 --> 00:40:13,420
I'm not sure, that's another thing I need to look up I guess.

832
00:40:13,420 --> 00:40:18,530
[NOISE] Any other questions?

833
00:40:21,060 --> 00:40:26,990
Okay. Ah, so, that's LSTMs and,

834
00:40:27,780 --> 00:40:31,825
um, LSTMs were introduced to try to solve the vanishing gradient problem.

835
00:40:31,825 --> 00:40:33,640
So, the question is, ah,

836
00:40:33,640 --> 00:40:37,840
how exactly is this architecture making the vanishing gradient problem any better?

837
00:40:37,840 --> 00:40:41,710
So, you could, ah, see that the LSTM architecture

838
00:40:41,710 --> 00:40:45,925
actually makes it easier for RNNs to preserve information over many time steps.

839
00:40:45,925 --> 00:40:48,340
So, while it w as kind of difficult for

840
00:40:48,340 --> 00:40:52,285
the vanilla RNN to preserve the information over all of the hidden states,

841
00:40:52,285 --> 00:40:54,730
there's actually a fairly easy strategy that makes

842
00:40:54,730 --> 00:40:57,265
it simple for the LSTM to preserve the information.

843
00:40:57,265 --> 00:41:02,290
So, namely, if the forget gate is set to remember everything on every step, um,

844
00:41:02,290 --> 00:41:05,290
that's a fairly simple strategy that will ensure that

845
00:41:05,290 --> 00:41:09,820
the information in the cell is going to be preserved indefinitely over many time steps.

846
00:41:09,820 --> 00:41:13,105
So, I don't know if that's actually a good strategy for whatever task you're trying to do,

847
00:41:13,105 --> 00:41:15,985
but my point is that there is at least, um,

848
00:41:15,985 --> 00:41:20,680
a fairly straightforward way for the LSTM to keep the information over many steps.

849
00:41:20,680 --> 00:41:25,175
And as we noted that's relatively harder for the vanilla RNN to do.

850
00:41:25,175 --> 00:41:29,640
So, you can think of this as the key reason why LSTMs are more able,

851
00:41:29,640 --> 00:41:31,859
ah, to preserve the information

852
00:41:31,859 --> 00:41:34,820
and thus are more robust to the vanishing gradient problem.

853
00:41:34,820 --> 00:41:38,590
Ah, however, I think you should still know that LSTMs don't

854
00:41:38,590 --> 00:41:42,175
necessarily guarantee that we don't have a vanishing or exploding gradient problem.

855
00:41:42,175 --> 00:41:43,615
You could still have that problem,

856
00:41:43,615 --> 00:41:47,630
but the thing to remember is that it's easier to avoid it anyway.

857
00:41:48,080 --> 00:41:51,420
Okay. So, um, LSTMs, ah,

858
00:41:51,420 --> 00:41:54,825
have been shown to be more robust to the vanishing gradient problem,

859
00:41:54,825 --> 00:41:57,180
ah but I'm going to tell you a little about how they've

860
00:41:57,180 --> 00:42:00,110
actually been more successful in real life. You have a question?

861
00:42:00,110 --> 00:42:21,600
Yeah,  [inaudible]

862
00:42:21,600 --> 00:42:26,110
Okay. So it's a great question.

863
00:42:26,110 --> 00:42:28,480
The question is, why is it that just because you

864
00:42:28,480 --> 00:42:31,390
have these LSTM defined forward equations,

865
00:42:31,390 --> 00:42:33,385
why do you not have the vanishing gradient problem?

866
00:42:33,385 --> 00:42:36,280
Why does the- the logic about, ah,

867
00:42:36,280 --> 00:42:40,060
the chain rule kind of getting smaller and smaller or bigger and bigger not apply?

868
00:42:40,060 --> 00:42:43,795
So, I think the key here is that, um,

869
00:42:43,795 --> 00:42:45,715
in the vanilla RNN,

870
00:42:45,715 --> 00:42:47,920
the hidden states are kind of like a bottleneck, right?

871
00:42:47,920 --> 00:42:50,620
Like all gradients must pass through them.

872
00:42:50,620 --> 00:42:52,360
So, if that gradient is small then,

873
00:42:52,360 --> 00:42:54,595
all downstream gradients will be small,

874
00:42:54,595 --> 00:42:58,405
whereas here you could regard the cell as being kind of like

875
00:42:58,405 --> 00:43:00,820
a shortcut connection at least in

876
00:43:00,820 --> 00:43:04,300
the case where the forget gate is set to remember things,

877
00:43:04,300 --> 00:43:07,330
um, then that's kind of like a shortcut connection where

878
00:43:07,330 --> 00:43:11,020
the cell will stay the same if you have the forget gate set to remember things.

879
00:43:11,020 --> 00:43:14,080
So, if the cell is staying mostly the same,

880
00:43:14,080 --> 00:43:17,530
then you are not going to be,

881
00:43:17,530 --> 00:43:20,200
ah, having the vanishing gradient via the cell.

882
00:43:20,200 --> 00:43:22,510
So, that means that to get a connection from

883
00:43:22,510 --> 00:43:25,465
the gradient of something in the future with respect to something in the past,

884
00:43:25,465 --> 00:43:27,760
there is a potential route for the gradient to

885
00:43:27,760 --> 00:43:30,865
go via the cell that doesn't necessarily vanish.

886
00:43:30,865 --> 00:43:32,860
So in that, I have one more question.

887
00:43:32,860 --> 00:43:33,120
Um-uh.

888
00:43:33,120 --> 00:43:49,830
Since we have a shortcut [inaudible]

889
00:43:49,830 --> 00:43:53,680
So I think the question was how do you check that your gradients are correct given that

890
00:43:53,680 --> 00:43:57,895
there are now multiple routes for information to travel?

891
00:43:57,895 --> 00:43:58,450
Right.

892
00:43:58,450 --> 00:44:01,780
So, I suppose this somewhat relates to what we talked about last time with

893
00:44:01,780 --> 00:44:04,660
the multivariable chain rule about what is

894
00:44:04,660 --> 00:44:08,815
the derivative of the loss with respect to a repeated weight matrix and we saw that,

895
00:44:08,815 --> 00:44:10,675
if there are multiple routes then

896
00:44:10,675 --> 00:44:13,330
the multivariable chain rule says that you add up the gradients.

897
00:44:13,330 --> 00:44:16,675
So, if your question is how do you do the calculus correctly and make sure it's correct,

898
00:44:16,675 --> 00:44:18,190
I guess you just kind of apply

899
00:44:18,190 --> 00:44:19,660
the multi-variable chain rule and it's more

900
00:44:19,660 --> 00:44:21,460
complicated than assessing with the LSTMs.

901
00:44:21,460 --> 00:44:24,085
Ah if you're using PyTorch 14 you do not have to do that yourself,

902
00:44:24,085 --> 00:44:25,780
if you're going to implement it yourself then,

903
00:44:25,780 --> 00:44:27,535
you might have a more difficult time.

904
00:44:27,535 --> 00:44:30,775
Um, yeah. So, I guess, yeah.

905
00:44:30,775 --> 00:44:38,560
Okay. All right, so, what do we get to. All right.

906
00:44:38,560 --> 00:44:41,395
So, let's talk about LSTMs and how they work in the- in the real world.

907
00:44:41,395 --> 00:44:44,950
So, in the pretty recent past,

908
00:44:44,950 --> 00:44:48,490
2013-2015 um LSTM started achieving a lot of state of

909
00:44:48,490 --> 00:44:52,315
the art results on a variety of different tasks including for example,

910
00:44:52,315 --> 00:44:54,370
handwriting recognition, speech recognition,

911
00:44:54,370 --> 00:44:57,805
machine translation, parsing, image captioning.

912
00:44:57,805 --> 00:44:59,380
So, over this period,

913
00:44:59,380 --> 00:45:02,020
LSTMs became the dominant approach in a lot of

914
00:45:02,020 --> 00:45:08,230
these application areas because they worked convincingly a lot better than vanilla RNNs.

915
00:45:08,230 --> 00:45:10,705
However, today in 2019,

916
00:45:10,705 --> 00:45:13,465
things changed pretty fast in deep learning.

917
00:45:13,465 --> 00:45:16,240
So, other approaches for example,

918
00:45:16,240 --> 00:45:18,400
transformers which you're going to learn about later in the class.

919
00:45:18,400 --> 00:45:20,410
Ah, in some of these application areas,

920
00:45:20,410 --> 00:45:21,715
they seem to have become,

921
00:45:21,715 --> 00:45:23,935
ah, the dominant approach.

922
00:45:23,935 --> 00:45:25,480
So, to look into this,

923
00:45:25,480 --> 00:45:29,350
I had a look at WMT which is a machine translation conference and

924
00:45:29,350 --> 00:45:33,865
also competition where people submit their MT systems to be evaluated.

925
00:45:33,865 --> 00:45:35,620
And I looked at the report,

926
00:45:35,620 --> 00:45:39,760
the summary report for WMT 2016 and in this report,

927
00:45:39,760 --> 00:45:41,125
I did a quick Ctrl+F,

928
00:45:41,125 --> 00:45:44,005
and I found the word RNN appeared 44 times.

929
00:45:44,005 --> 00:45:46,990
So, it seems that most people entering this competition were building

930
00:45:46,990 --> 00:45:50,800
their MT systems based on RNNs and in particular LSTMs.

931
00:45:50,800 --> 00:45:52,945
And then I looked at the report from 2018,

932
00:45:52,945 --> 00:45:55,210
just two years later and I found that the RNN,

933
00:45:55,210 --> 00:45:59,785
the word RNN only appeared nine times and the word transformer appeared 63 times,

934
00:45:59,785 --> 00:46:02,155
and in fact the organizers noted that everyone,

935
00:46:02,155 --> 00:46:04,330
well, most people seem to using transformers now.

936
00:46:04,330 --> 00:46:07,930
So um, this shows that things change pretty fast in deep learning.

937
00:46:07,930 --> 00:46:11,350
The thing that was hot and new just a few years ago um,

938
00:46:11,350 --> 00:46:15,655
is- is now being passed by perhaps by other kinds of approaches.

939
00:46:15,655 --> 00:46:17,260
So, you're going to learn more about transformers

940
00:46:17,260 --> 00:46:19,315
later but I guess that gives you a kind of

941
00:46:19,315 --> 00:46:24,385
idea of where LSTMs are currently in applications.

942
00:46:24,385 --> 00:46:29,845
Okay. So, the second kind of RNN we're going to learn about is gated recurrent units.

943
00:46:29,845 --> 00:46:32,605
So, these fortunately are simpler than LSTMs,

944
00:46:32,605 --> 00:46:36,070
in fact that was the motivation for them being proposed.

945
00:46:36,070 --> 00:46:40,150
They were proposed in 2014 as a way to try to retain

946
00:46:40,150 --> 00:46:45,055
the strengths of LSTMs by getting rid of any unnecessary complexities.

947
00:46:45,055 --> 00:46:46,825
So, in a GRU,

948
00:46:46,825 --> 00:46:48,520
we don't have a cell state.

949
00:46:48,520 --> 00:46:50,470
We again just have a hidden state.

950
00:46:50,470 --> 00:46:54,070
But the thing it has in ah in common with LSTMs is that we're going to be

951
00:46:54,070 --> 00:46:57,460
using gates to control the flow of information.

952
00:46:57,460 --> 00:47:00,415
So, here are the equations for GRU.

953
00:47:00,415 --> 00:47:02,890
We start off with two gates.

954
00:47:02,890 --> 00:47:06,430
So the first gate is called the update gate and this

955
00:47:06,430 --> 00:47:11,050
controls what parts of the hidden states are going to be updated versus preserved.

956
00:47:11,050 --> 00:47:13,750
So, you can kind of view this as playing

957
00:47:13,750 --> 00:47:17,170
the role of both the forget gate and the input gate in

958
00:47:17,170 --> 00:47:24,580
the LSTM and it's computed in much the same way as the gates in the LSTM were.

959
00:47:24,580 --> 00:47:27,565
The second gate is called the reset gate rt,

960
00:47:27,565 --> 00:47:30,550
and this gate is controlling which parts of

961
00:47:30,550 --> 00:47:34,915
the previous hidden state are going to be used to compute new contents.

962
00:47:34,915 --> 00:47:37,735
So, you can think of the- the reset gate as kind of selecting

963
00:47:37,735 --> 00:47:41,305
which parts of the previous hidden states are useful versus not useful.

964
00:47:41,305 --> 00:47:45,010
So, it's going to discard some things and select some other things.

965
00:47:45,010 --> 00:47:48,055
Okay. So, here's how those gates get used.

966
00:47:48,055 --> 00:47:49,690
Um, h tilde here.

967
00:47:49,690 --> 00:47:54,400
This is you can think of it as the new hidden state contents and what's

968
00:47:54,400 --> 00:47:56,440
going on in that equation is that we are applying

969
00:47:56,440 --> 00:47:59,410
the reset gate to the previous hidden state ht minus

970
00:47:59,410 --> 00:48:04,150
one um and then putting all of that through some linear transformations and

971
00:48:04,150 --> 00:48:07,420
a tan H and then this gives us the new content

972
00:48:07,420 --> 00:48:11,305
which we want to write to the hidden cell.

973
00:48:11,305 --> 00:48:15,655
And then lastly our new hidden cell is going to be a combination

974
00:48:15,655 --> 00:48:20,065
of ah this new content and the previous hidden state.

975
00:48:20,065 --> 00:48:24,790
So, the important thing to notice here is that we have this one minus u and u term.

976
00:48:24,790 --> 00:48:27,370
So um, it's kind of like a balance right?

977
00:48:27,370 --> 00:48:31,079
U is ah is setting the balance between

978
00:48:31,079 --> 00:48:35,085
preserving things from the previous hidden state versus writing new stuff.

979
00:48:35,085 --> 00:48:36,360
So, whereas in the LSTM,

980
00:48:36,360 --> 00:48:39,195
those were two completely separate gates that could be whatever value.

981
00:48:39,195 --> 00:48:42,240
Here we have this constraint that U is being uh, balanced.

982
00:48:42,240 --> 00:48:44,765
So, if you have more of one, you have to have less of the other.

983
00:48:44,765 --> 00:48:51,160
So, this is one way in which the creators of the GRU sought to make LSTMs more simple.

984
00:48:51,160 --> 00:48:54,520
Was by having a single gate play both of these roles.

985
00:48:54,520 --> 00:48:59,410
Okay. So, that's GRUs and I think it's a little less obvious just looking at it.

986
00:48:59,410 --> 00:49:04,870
Why GRUs help the vanishing gradients problem because there is no explicit ah memory

987
00:49:04,870 --> 00:49:06,520
cell, like there is in LSTMs.

988
00:49:06,520 --> 00:49:10,360
So, I think the way to look at this here is um GRUs,

989
00:49:10,360 --> 00:49:12,160
you can view this as also being a solution to

990
00:49:12,160 --> 00:49:15,220
the vanishing gradient problem because like LSTMs,

991
00:49:15,220 --> 00:49:19,425
GRUs make it easier to retain information ah long-term.

992
00:49:19,425 --> 00:49:21,240
So, for example here,

993
00:49:21,240 --> 00:49:25,094
if the update gate ut is set to zero,

994
00:49:25,094 --> 00:49:30,385
then we're going to be ah keeping the hidden state the same on every step.

995
00:49:30,385 --> 00:49:33,640
And again that's maybe not a good idea but at least that is a strategy you can easily

996
00:49:33,640 --> 00:49:37,315
do in order to retain information over long distances.

997
00:49:37,315 --> 00:49:40,420
So that's kind of like- like the same explanation of how GRUs make it

998
00:49:40,420 --> 00:49:44,660
potentially easier for RNNs to retain information long-term.

999
00:49:46,410 --> 00:49:51,490
Okay. So, we've learned about these two different types of RNNs. Yes.

1000
00:49:51,490 --> 00:50:08,230
[inaudible]

1001
00:50:08,230 --> 00:50:10,105
I think the question was,

1002
00:50:10,105 --> 00:50:12,790
if we view the two gates in the GRU, as being, uh,

1003
00:50:12,790 --> 00:50:20,440
a precise, um, analogy to the gates in the LSTM or are they more of a fuzzy analogy.

1004
00:50:20,440 --> 00:50:22,794
I'd say probably more of a fuzzy analogy

1005
00:50:22,794 --> 00:50:26,080
because there are other changes going on in here, like,

1006
00:50:26,080 --> 00:50:28,510
for example, the fact that there's no separate, um,

1007
00:50:28,510 --> 00:50:32,260
memory cell, it means they're not performing exactly the same functions.

1008
00:50:32,260 --> 00:50:39,895
Yeah. Okay. So, we've learned about LSTMs and GRUs which are both,

1009
00:50:39,895 --> 00:50:41,650
um, more complicated forms of RNNs,

1010
00:50:41,650 --> 00:50:43,705
more complicated than Vanilla RNNs.

1011
00:50:43,705 --> 00:50:45,610
And they are both,

1012
00:50:45,610 --> 00:50:48,865
uh, more robust to the vanishing gradient problem.

1013
00:50:48,865 --> 00:50:53,950
So, um, it would be useful to know which of these should we be using in practice?

1014
00:50:53,950 --> 00:50:55,120
Which one is more successful,

1015
00:50:55,120 --> 00:50:56,770
the LSTM or GRU?

1016
00:50:56,770 --> 00:50:59,770
Uh, so, I- I did a little reading and it looks like researchers have

1017
00:50:59,770 --> 00:51:02,890
proposed a lot of different types of gated RNNs.

1018
00:51:02,890 --> 00:51:04,450
So, it's not just GRUs and LSTMs,

1019
00:51:04,450 --> 00:51:07,585
there's many other papers with lots of other different variants.

1020
00:51:07,585 --> 00:51:11,845
Uh, but these are definitely the two that are most widely used.

1021
00:51:11,845 --> 00:51:15,340
And, ah, you can probably say that the biggest difference between the two, um,

1022
00:51:15,340 --> 00:51:17,920
for sure is the fact that GRUs are simpler

1023
00:51:17,920 --> 00:51:21,115
and quicker to compute and they have fewer parameters.

1024
00:51:21,115 --> 00:51:23,500
So, this makes an actual practical difference to you as, uh,

1025
00:51:23,500 --> 00:51:27,970
a deep learning practitioner because if you build your net based on GRUs,

1026
00:51:27,970 --> 00:51:29,965
then it's gonna be faster to run forwards and,

1027
00:51:29,965 --> 00:51:32,170
you know, faster to train and so on.

1028
00:51:32,170 --> 00:51:34,540
So, other than that, there appears to be

1029
00:51:34,540 --> 00:51:38,680
no very conclusive evidence that one of these LSTM or GRUs,

1030
00:51:38,680 --> 00:51:42,790
uh, is consistently outperforming the other on lots of different tasks.

1031
00:51:42,790 --> 00:51:45,955
Uh, it seems that often, uh,

1032
00:51:45,955 --> 00:51:48,250
sometimes GRUs do perform as well as LSTMs,

1033
00:51:48,250 --> 00:51:51,520
but there are cases where one of them performs better than the other.

1034
00:51:51,520 --> 00:51:53,440
So, as a rule of thumb,

1035
00:51:53,440 --> 00:51:57,190
it seems like LSTM is often a good default choice to start with, uh,

1036
00:51:57,190 --> 00:51:58,840
especially if your data has

1037
00:51:58,840 --> 00:52:01,150
particularly long dependencies because there's evidence to think

1038
00:52:01,150 --> 00:52:05,500
that LSTMs might be slightly better at keeping information over very long distances.

1039
00:52:05,500 --> 00:52:07,570
And also, if you have a lot of training data,

1040
00:52:07,570 --> 00:52:09,670
you might think that LSTMs are a better choice because they

1041
00:52:09,670 --> 00:52:12,340
have more parameters which means that,

1042
00:52:12,340 --> 00:52:15,860
um, maybe you need more train data to learn them.

1043
00:52:17,940 --> 00:52:21,700
So, a rule of thumb is that maybe you want to start with LSTMs

1044
00:52:21,700 --> 00:52:23,500
and if you're happy with their performance and you're

1045
00:52:23,500 --> 00:52:25,675
happy with how long it takes to train, then you stick with that.

1046
00:52:25,675 --> 00:52:27,610
But if you feel like you need it to be more efficient,

1047
00:52:27,610 --> 00:52:30,850
then maybe you should switch to GRUs and see how that goes with the performance

1048
00:52:30,850 --> 00:52:34,690
and if it's faster. All right.

1049
00:52:34,690 --> 00:52:36,970
So, um, we've talked so far about how

1050
00:52:36,970 --> 00:52:40,720
the vanishing/exploding gradients are a problem that occur a lot in RNNs.

1051
00:52:40,720 --> 00:52:42,415
But, um, the question is,

1052
00:52:42,415 --> 00:52:43,735
is it only an RNN problem?

1053
00:52:43,735 --> 00:52:46,330
Does this occur in other kinds of neural networks as well?

1054
00:52:46,330 --> 00:52:48,115
And the answer is,

1055
00:52:48,115 --> 00:52:50,170
uh, no, it's not just an RNN problem.

1056
00:52:50,170 --> 00:52:52,390
In fact, vanishing and exploding gradients are a

1057
00:52:52,390 --> 00:52:55,150
pretty significant problem for

1058
00:52:55,150 --> 00:52:58,419
most neural architecture such as feed-forward and convolutional,

1059
00:52:58,419 --> 00:52:59,740
especially when they're deep.

1060
00:52:59,740 --> 00:53:02,410
And this is a really serious problem because there's no point having

1061
00:53:02,410 --> 00:53:06,655
a really cool neural architecture if you can't learn it efficiently because of the,

1062
00:53:06,655 --> 00:53:08,500
uh, vanishing gradient problem.

1063
00:53:08,500 --> 00:53:13,030
So, in particular, uh, in these feed-forward and convolutional networks, uh,

1064
00:53:13,030 --> 00:53:15,280
you often have a gradient becoming vanishingly

1065
00:53:15,280 --> 00:53:18,520
small over back-propagation, uh, because of the Chain Rule,

1066
00:53:18,520 --> 00:53:20,200
because of this multiplying by

1067
00:53:20,200 --> 00:53:22,390
all these different intermediate gradients or

1068
00:53:22,390 --> 00:53:25,120
sometimes due to your choice of non-linearity function.

1069
00:53:25,120 --> 00:53:29,260
So, if this happens, this means that your- the lower layers of your, let's say,

1070
00:53:29,260 --> 00:53:31,285
convolutional or feed-forward network,

1071
00:53:31,285 --> 00:53:32,950
they have a much smaller,

1072
00:53:32,950 --> 00:53:35,935
uh, gradient than the high levels.

1073
00:53:35,935 --> 00:53:39,970
And this means that they get changed very slowly during SGD.

1074
00:53:39,970 --> 00:53:41,290
So, this means that, overall,

1075
00:53:41,290 --> 00:53:43,990
your network is very slow to train because when you take updates,

1076
00:53:43,990 --> 00:53:47,080
then your lower layers are changing very slowly.

1077
00:53:47,080 --> 00:53:49,300
So, one solution, uh,

1078
00:53:49,300 --> 00:53:51,280
the kind of like a family of solutions that we've seen in

1079
00:53:51,280 --> 00:53:53,620
recent years is that there's been lots of

1080
00:53:53,620 --> 00:53:58,720
proposals for new types of deep feed-forward or convolutional architectures.

1081
00:53:58,720 --> 00:54:02,740
And what they do is, they add more direct connections in the network.

1082
00:54:02,740 --> 00:54:04,330
And the- the idea,

1083
00:54:04,330 --> 00:54:05,755
kind of as we talked about before,

1084
00:54:05,755 --> 00:54:08,530
is that if you add all of these direct connections between layers,

1085
00:54:08,530 --> 00:54:12,085
like maybe not just adjacent layers but further apart layers,

1086
00:54:12,085 --> 00:54:14,680
then it makes it much easier for the gradients to flow,

1087
00:54:14,680 --> 00:54:17,635
and you're going to find it easier to train your network overall.

1088
00:54:17,635 --> 00:54:19,660
So, I'm going to show you some examples of these in

1089
00:54:19,660 --> 00:54:21,670
particular because it's fairly likely you're going to

1090
00:54:21,670 --> 00:54:25,705
run into these kinds of architectures when you're doing your projects and reading papers.

1091
00:54:25,705 --> 00:54:29,410
So, one example is something called residual connections or,

1092
00:54:29,410 --> 00:54:32,515
uh, the network itself is sometimes referred to as ResNet.

1093
00:54:32,515 --> 00:54:36,350
And here we've got a figure from the related paper.

1094
00:54:36,350 --> 00:54:41,070
So, what's going on in this diagram is that you have, uh,

1095
00:54:41,070 --> 00:54:42,930
the usual kind of you've got weight layer and

1096
00:54:42,930 --> 00:54:45,480
a non-linearity which is ReLU, and another weight layer.

1097
00:54:45,480 --> 00:54:49,180
So, if you regard that function as being f of x, ah,

1098
00:54:49,180 --> 00:54:50,695
what they're doing is instead of just, ah,

1099
00:54:50,695 --> 00:54:52,540
transforming x to f of x,

1100
00:54:52,540 --> 00:54:55,150
the- they're taking f of x plus x.

1101
00:54:55,150 --> 00:54:58,435
So they're adding this identity skip connection where

1102
00:54:58,435 --> 00:55:01,990
the input x is skipped over those two layers and then,

1103
00:55:01,990 --> 00:55:05,470
um, added to the output of the two layers.

1104
00:55:05,470 --> 00:55:07,510
So, the reason why this is a good idea,

1105
00:55:07,510 --> 00:55:10,150
uh, also known as skip connections,

1106
00:55:10,150 --> 00:55:15,280
is that the identity connection is going to preserve information by default, right?

1107
00:55:15,280 --> 00:55:18,010
So, if you imagine perhaps if you, um,

1108
00:55:18,010 --> 00:55:20,110
initialize your network and you

1109
00:55:20,110 --> 00:55:22,600
initialize your weight layers to have small random values,

1110
00:55:22,600 --> 00:55:24,835
then if they're small and kind of close to zero,

1111
00:55:24,835 --> 00:55:28,885
then you're going to have something like a noisy identity function, right?

1112
00:55:28,885 --> 00:55:32,245
So you're going to be preserving information by default through all of your layers.

1113
00:55:32,245 --> 00:55:33,520
And if you have a very deep network,

1114
00:55:33,520 --> 00:55:35,110
that means that even often many,

1115
00:55:35,110 --> 00:55:39,410
um, many layers, you're still gonna have something like your original input.

1116
00:55:40,320 --> 00:55:44,215
So, uh, the- the people who wrote this paper, they show that, uh,

1117
00:55:44,215 --> 00:55:46,660
if you don't have something like skip connections then

1118
00:55:46,660 --> 00:55:49,570
actually you can find that deep layers- uh,

1119
00:55:49,570 --> 00:55:53,245
deep networks perform worse on some tasks than shallow networks.

1120
00:55:53,245 --> 00:55:54,955
Not because they're not expressive enough,

1121
00:55:54,955 --> 00:55:56,665
but because they're too difficult to learn.

1122
00:55:56,665 --> 00:55:58,285
So, when you attempt to learn deep networks,

1123
00:55:58,285 --> 00:55:59,950
it just doesn't learn effectively and you end up

1124
00:55:59,950 --> 00:56:01,795
getting worse performance in the shallow network.

1125
00:56:01,795 --> 00:56:03,070
So, the people who wrote this paper,

1126
00:56:03,070 --> 00:56:05,050
they show that when they add these skip connections,

1127
00:56:05,050 --> 00:56:07,105
then they made the deep networks, uh,

1128
00:56:07,105 --> 00:56:10,550
much more effective and they managed to get good performance.

1129
00:56:12,000 --> 00:56:15,280
Uh, so another example which kinda take this- this idea

1130
00:56:15,280 --> 00:56:18,220
further is something called dense connections or DenseNet.

1131
00:56:18,220 --> 00:56:19,585
And again, this was, uh,

1132
00:56:19,585 --> 00:56:23,965
something proposed I think in a feed-forward or or convolutional setting.

1133
00:56:23,965 --> 00:56:26,770
And, ah, it's just kind of the same as skip connections but except ,

1134
00:56:26,770 --> 00:56:28,030
um, connects everything to everything.

1135
00:56:28,030 --> 00:56:30,070
So, add more of these skip connections kind of

1136
00:56:30,070 --> 00:56:32,680
from all layers to all layers and they showed that this,

1137
00:56:32,680 --> 00:56:34,480
uh, performs even better.

1138
00:56:34,480 --> 00:56:37,450
And, uh, the last one I want to talk about which I don't have a picture

1139
00:56:37,450 --> 00:56:40,210
for is something called highway connections.

1140
00:56:40,210 --> 00:56:43,180
So, this is similar to the residual or skip connections.

1141
00:56:43,180 --> 00:56:46,585
Ah, but the idea is that instead of just adding your x,

1142
00:56:46,585 --> 00:56:48,640
adding your identity, uh, connection,

1143
00:56:48,640 --> 00:56:52,060
the idea is that you're gonna have a gate that controls the balance between, um,

1144
00:56:52,060 --> 00:56:55,960
adding the identity and computing, ah, the transformation.

1145
00:56:55,960 --> 00:56:58,405
So, instead of f of x plus x, you're gonna have, you know,

1146
00:56:58,405 --> 00:56:59,995
gate times f of x plus, you know,

1147
00:56:59,995 --> 00:57:02,110
one minus gate times x, something like that.

1148
00:57:02,110 --> 00:57:05,560
Um, so, this work was actually inspired by LSTMs,

1149
00:57:05,560 --> 00:57:07,510
but instead of applying it to a recurrent setting,

1150
00:57:07,510 --> 00:57:11,570
they were seeking to apply it to a feed-forward setting.

1151
00:57:13,860 --> 00:57:16,120
Okay. I'm gonna keep going for now.

1152
00:57:16,120 --> 00:57:19,390
Um. So, overall the question was,

1153
00:57:19,390 --> 00:57:20,620
you know, how much uh,

1154
00:57:20,620 --> 00:57:23,650
vanishing and exploding gradients a problem outside of the setting of RNNs?

1155
00:57:23,650 --> 00:57:26,710
And I think uh, the important takeaway is that it is a big problem

1156
00:57:26,710 --> 00:57:30,595
but you should notice that it is particularly a problem for RNNs.

1157
00:57:30,595 --> 00:57:34,000
So, um, RNNs are particularly unstable and

1158
00:57:34,000 --> 00:57:37,525
this is essentially due to the repeated multiplication by the same weight matrix.

1159
00:57:37,525 --> 00:57:39,175
If you remember from last time, um,

1160
00:57:39,175 --> 00:57:41,890
the characteristic thing about RNNs that makes them recurrent is

1161
00:57:41,890 --> 00:57:44,770
the fact that you are applying the same weight matrix over and over again.

1162
00:57:44,770 --> 00:57:47,095
So, this is actually the core reason

1163
00:57:47,095 --> 00:57:49,865
why they are so prone to the vanishing and exploding gradients,

1164
00:57:49,865 --> 00:57:53,590
and ah, you can see some more information about that in the paper.

1165
00:57:53,670 --> 00:57:57,730
Okay. So, I know there's been a lot of dense information today,

1166
00:57:57,730 --> 00:57:59,815
a lot of um, lot of notation.

1167
00:57:59,815 --> 00:58:01,690
So, here's a recap, if I've lost you at any point.

1168
00:58:01,690 --> 00:58:03,280
Now's a good time to jump back in because it's gonna

1169
00:58:03,280 --> 00:58:05,515
get a little easier to understand perhaps.

1170
00:58:05,515 --> 00:58:07,600
So, okay, recap. What have we learned about today?

1171
00:58:07,600 --> 00:58:10,435
Um, the first thing we learned about was the vanishing gradient problem.

1172
00:58:10,435 --> 00:58:11,830
We learned uh, what it is.

1173
00:58:11,830 --> 00:58:15,580
We learned why it happens and we saw why it's bad for RNNs,

1174
00:58:15,580 --> 00:58:18,085
for example, RNN language models.

1175
00:58:18,085 --> 00:58:21,640
Ah, and we also learned about LSTMs and GRUs which are

1176
00:58:21,640 --> 00:58:25,480
more complicated RNNs and they use gates to control the flow of information.

1177
00:58:25,480 --> 00:58:29,395
And by doing that, they are more resilient to the vanishing gradient problem.

1178
00:58:29,395 --> 00:58:31,390
Okay. So, if the remainder of this lecture,

1179
00:58:31,390 --> 00:58:32,740
I think we've got about 20 minutes left,

1180
00:58:32,740 --> 00:58:36,445
ah, we're going to be learning about two more advanced type of RNNs.

1181
00:58:36,445 --> 00:58:39,040
So, the first one is bidirectional RNNs and that's all

1182
00:58:39,040 --> 00:58:43,105
about information flowing left to right and right to left.

1183
00:58:43,105 --> 00:58:44,905
And then we're also going to learn about

1184
00:58:44,905 --> 00:58:49,780
multi-layer RNNs which is when you apply multiple RNNs on top of each other.

1185
00:58:49,780 --> 00:58:54,100
So, I'd say that both of these are pretty simple conceptually.

1186
00:58:54,100 --> 00:58:56,905
Um, so it shouldn't be too hard to understand.

1187
00:58:56,905 --> 00:59:00,160
All right, so let's start with bidirectional RNNs.

1188
00:59:00,160 --> 00:59:04,225
Um, this is a picture which you saw at the end of last lecture.

1189
00:59:04,225 --> 00:59:05,305
So, if you remember,

1190
00:59:05,305 --> 00:59:07,690
sentiment classification is the task when you have

1191
00:59:07,690 --> 00:59:10,150
some kind of input sentence such as the movie was

1192
00:59:10,150 --> 00:59:15,460
terribly exciting and you want to classify this as a positive or negative sentiment.

1193
00:59:15,460 --> 00:59:19,160
So, in this example, it should be seen as positive sentiment.

1194
00:59:19,680 --> 00:59:23,650
So, um, this is an example of how you might try to

1195
00:59:23,650 --> 00:59:26,860
solve sentiment classification using a fairly simple RNN model.

1196
00:59:26,860 --> 00:59:29,830
Ah, here we're using the RNN as a kind of encoder of

1197
00:59:29,830 --> 00:59:32,890
the sentence and the hidden states represent the sentence.

1198
00:59:32,890 --> 00:59:35,740
And we'll do some kind of combination of the hidden states to compute uh,

1199
00:59:35,740 --> 00:59:37,765
what we think the sentiment is.

1200
00:59:37,765 --> 00:59:40,165
So, my question is, if we look at let's say,

1201
00:59:40,165 --> 00:59:44,350
the hidden state that corresponds to the word terribly and we're regarding

1202
00:59:44,350 --> 00:59:46,420
this hidden state as a representation of the word

1203
00:59:46,420 --> 00:59:49,510
terribly in the context of the sentence.

1204
00:59:49,510 --> 00:59:53,455
So, for this reason we- we sometimes call hidden states in this kind of situation

1205
00:59:53,455 --> 00:59:55,930
a contextual representation because the idea is that it's

1206
00:59:55,930 --> 00:59:59,785
a representation of the word terribly in the context of the sentence.

1207
00:59:59,785 --> 01:00:04,150
So, thing to think about here is that this contextual representation,

1208
01:00:04,150 --> 01:00:07,165
it only contains information about the left context.

1209
01:00:07,165 --> 01:00:10,150
So, for terribly, the left context is the words um,

1210
01:00:10,150 --> 01:00:13,120
the movie was and this hidden state the one that's got

1211
01:00:13,120 --> 01:00:16,435
a blue box around it has only seen information to the left.

1212
01:00:16,435 --> 01:00:20,485
It hasn't seen the information of the words exciting or exclamation mark.

1213
01:00:20,485 --> 01:00:24,715
So, what we're asking is what about the right context?

1214
01:00:24,715 --> 01:00:28,690
The right context of terribly is- is what exciting and the exclamation mark.

1215
01:00:28,690 --> 01:00:33,040
And do we think that the right context is useful here?

1216
01:00:33,040 --> 01:00:35,230
Do we think that this is something we want to know about?

1217
01:00:35,230 --> 01:00:37,405
And I would argue that in this example,

1218
01:00:37,405 --> 01:00:41,695
it is actually kind of important because we've got the phrase terribly exciting.

1219
01:00:41,695 --> 01:00:44,829
And if you look at the word terribly in isolation,

1220
01:00:44,829 --> 01:00:47,005
terrible or terribly usually means something bad, right?

1221
01:00:47,005 --> 01:00:50,650
But terribly exciting, you can mean something good because it just means very exciting.

1222
01:00:50,650 --> 01:00:53,230
So, if you know about the right context,

1223
01:00:53,230 --> 01:00:56,680
the word exciting then this might quite significantly

1224
01:00:56,680 --> 01:00:58,900
modify your perception of the meaning of the word

1225
01:00:58,900 --> 01:01:01,210
terribly in the context of the sentence.

1226
01:01:01,210 --> 01:01:03,790
And especially given that we're trying to do sentiment classification,

1227
01:01:03,790 --> 01:01:05,815
this is- this is kind of important.

1228
01:01:05,815 --> 01:01:10,150
So this motivates why you might want to have information

1229
01:01:10,150 --> 01:01:13,915
from both the left and the right when you're making your representations.

1230
01:01:13,915 --> 01:01:16,000
Ah, if when you were a kid,

1231
01:01:16,000 --> 01:01:18,535
your parents told you to look both ways before you cross the street.

1232
01:01:18,535 --> 01:01:20,620
You might regard it as the same kind of idea that there's

1233
01:01:20,620 --> 01:01:22,510
useful information to the left and the right that

1234
01:01:22,510 --> 01:01:24,970
you'd like to know about ah, before you do anything.

1235
01:01:24,970 --> 01:01:27,925
Okay. So that's the motivation and um,

1236
01:01:27,925 --> 01:01:31,900
here is how a bidirectional RNN might work in practice.

1237
01:01:31,900 --> 01:01:35,065
I have a kind of accidentally festive color scheme here.

1238
01:01:35,065 --> 01:01:38,755
And so the idea is that you have two RNNs going on.

1239
01:01:38,755 --> 01:01:42,880
You have the forward RNN as before that encodes the sentence left to right.

1240
01:01:42,880 --> 01:01:46,000
And then separately, you also have a backwards RNN.

1241
01:01:46,000 --> 01:01:49,135
And this has completely separate weights to the forward RNN.

1242
01:01:49,135 --> 01:01:52,660
So, the backward RNN is just doing the same thing

1243
01:01:52,660 --> 01:01:56,110
except that it's encoding the sequence from right to left.

1244
01:01:56,110 --> 01:01:59,980
So, each of the hidden states is computed based on the one to the right.

1245
01:01:59,980 --> 01:02:02,500
And then finally, you just take the hidden states from

1246
01:02:02,500 --> 01:02:06,700
the two RNNs and then you concatenate them together and you've got your uh,

1247
01:02:06,700 --> 01:02:09,385
your final kind of representations.

1248
01:02:09,385 --> 01:02:12,025
So, in particular, if we now think about

1249
01:02:12,025 --> 01:02:16,330
this contextual representation of the word terribly in the context,

1250
01:02:16,330 --> 01:02:22,180
um, this- this vector has information from both the left and the right, right?

1251
01:02:22,180 --> 01:02:24,235
Because you had the forwards and backwards RNNs that

1252
01:02:24,235 --> 01:02:27,295
respectively had information from both left and right.

1253
01:02:27,295 --> 01:02:30,190
So the idea is that these concatenated hidden states,

1254
01:02:30,190 --> 01:02:34,735
those can be regarded as kind of like the outputs of the bidirectional RNN.

1255
01:02:34,735 --> 01:02:36,460
Like if you're going to use these hidden states for

1256
01:02:36,460 --> 01:02:38,680
any kind of further computation, then ah,

1257
01:02:38,680 --> 01:02:40,780
it's these concatenated hidden states that you are going to be

1258
01:02:40,780 --> 01:02:44,210
passing on to the next part of the network.

1259
01:02:44,250 --> 01:02:48,205
Um, here- here are the equations that just say the same thing.

1260
01:02:48,205 --> 01:02:51,790
So, you have your forward RNN and here we've got ah,

1261
01:02:51,790 --> 01:02:53,995
a notation that you might not have seen before

1262
01:02:53,995 --> 01:02:57,010
this kind of notation where it says RNN and then in brackets,

1263
01:02:57,010 --> 01:03:00,775
the previous hidden state and the input that's simply saying that you know,

1264
01:03:00,775 --> 01:03:04,180
HT is computed from the previous hidden state and the input.

1265
01:03:04,180 --> 01:03:08,590
And RNN forward could be a vanilla or a GRU or an LSTM.

1266
01:03:08,590 --> 01:03:10,810
It doesn't really matter, we're looking at it abstractly.

1267
01:03:10,810 --> 01:03:16,104
So, you have these two separate RNNs,

1268
01:03:16,104 --> 01:03:19,540
RNN forwards and RNN backwards and generally, these have separate weights.

1269
01:03:19,540 --> 01:03:21,910
Although I have seen some papers where they have shared weights.

1270
01:03:21,910 --> 01:03:23,875
So, it seems that sometimes that does work better,

1271
01:03:23,875 --> 01:03:26,600
perhaps maybe when you have enough training data.

1272
01:03:26,790 --> 01:03:32,020
And then finally, we regard these concatenated hidden states which you might just

1273
01:03:32,020 --> 01:03:37,610
notice ht as being like the hidden state of the bidirectional RNN.

1274
01:03:38,550 --> 01:03:42,550
So, um, the previous diagram is pretty unwieldy.

1275
01:03:42,550 --> 01:03:44,395
So here's a simplified diagram.

1276
01:03:44,395 --> 01:03:46,240
And this is probably the only kind of diagram you're going to

1277
01:03:46,240 --> 01:03:48,700
see from now on to denote bidirectional RNNs.

1278
01:03:48,700 --> 01:03:50,770
Um, so, what we've done here is you've just

1279
01:03:50,770 --> 01:03:53,575
made all of the horizontal arrows go left and right ah,

1280
01:03:53,575 --> 01:03:56,260
to represent that this is a bidirectional RNN.

1281
01:03:56,260 --> 01:04:00,370
So, the other thing you should assume is that the hidden states depicted here, you know,

1282
01:04:00,370 --> 01:04:04,240
these red- red trying- red rectangles with the dots.

1283
01:04:04,240 --> 01:04:06,580
You can assume that those are the concatenated forwards,

1284
01:04:06,580 --> 01:04:08,590
backwards hidden states from the bidirectional RNN.

1285
01:04:08,590 --> 01:04:16,000
[inaudible]

1286
01:04:16,000 --> 01:04:18,415
Okay. So the question is, um,

1287
01:04:18,415 --> 01:04:22,060
would you train your forwards and backwards RNNs kind of separately,

1288
01:04:22,060 --> 01:04:23,890
um, on some kind of task and then

1289
01:04:23,890 --> 01:04:26,665
maybe concatenate them together once they're separately trained networks,

1290
01:04:26,665 --> 01:04:28,285
or would you train them all together?

1291
01:04:28,285 --> 01:04:32,200
Um, it seems to me that it's much more common to train them together,

1292
01:04:32,200 --> 01:04:35,230
but I don- I don't think I've heard of anyone training them separately.

1293
01:04:35,230 --> 01:04:37,270
Uh, so yeah, it seems like the standard practice is usually

1294
01:04:37,270 --> 01:04:39,160
to train them together. Does that make sense?

1295
01:04:39,160 --> 01:04:53,290
[inaudible].

1296
01:04:53,290 --> 01:04:55,690
So, let's suppose that we were trying to build

1297
01:04:55,690 --> 01:04:59,440
a sentiment classification system using the bidirectional RNN.

1298
01:04:59,440 --> 01:05:03,460
Then what you do, which maybe I should have pictured but I didn't have space, is uh,

1299
01:05:03,460 --> 01:05:07,420
you would do the same thing that you were doing with the unidirectional RNN, uh,

1300
01:05:07,420 --> 01:05:10,045
which was, let's say an element y is min or max,

1301
01:05:10,045 --> 01:05:11,665
um, to get your sentence encoding.

1302
01:05:11,665 --> 01:05:16,490
Maybe you just do that but over the concatenated, um, n states.

1303
01:05:16,950 --> 01:05:20,560
Okay. So, an important thing to note is that, uh,

1304
01:05:20,560 --> 01:05:23,020
when talking about applying bidirectional RNNs,

1305
01:05:23,020 --> 01:05:26,935
we've assumed that we actually have access to the entire input sequence.

1306
01:05:26,935 --> 01:05:28,780
So, we assume that we have the full sentence,

1307
01:05:28,780 --> 01:05:31,565
uh, the movie was very exciting, and,

1308
01:05:31,565 --> 01:05:34,740
uh, that, that was a necessary assumption in order to

1309
01:05:34,740 --> 01:05:37,935
be able to run the forwards and the backwards RNN, right?

1310
01:05:37,935 --> 01:05:40,935
Um, so there are some situations where you can't assume this.

1311
01:05:40,935 --> 01:05:43,185
Like, for example, in Language Modeling,

1312
01:05:43,185 --> 01:05:47,355
you only have access to the left context kind of by definition of the task.

1313
01:05:47,355 --> 01:05:48,990
You only know the words that have come so far.

1314
01:05:48,990 --> 01:05:50,405
You don't know what's coming next.

1315
01:05:50,405 --> 01:05:54,070
So, you can't use a bidirectional RNN, uh,

1316
01:05:54,070 --> 01:05:55,525
to do Language Modeling, uh,

1317
01:05:55,525 --> 01:05:57,760
in the way that we've depicted here because uh,

1318
01:05:57,760 --> 01:05:59,815
you don't have the full sequence.

1319
01:05:59,815 --> 01:06:03,115
However, if you do have access to the entire sequence.

1320
01:06:03,115 --> 01:06:05,230
Uh, so, for example, if you're doing any kind of encoding

1321
01:06:05,230 --> 01:06:07,495
similar to the sentiment example,

1322
01:06:07,495 --> 01:06:11,680
uh, then bidirectionally- bidirectionality is pretty powerful.

1323
01:06:11,680 --> 01:06:14,815
And you should probably regard it as a good thing to do by default uh,

1324
01:06:14,815 --> 01:06:16,870
because it turns out that getting this information from

1325
01:06:16,870 --> 01:06:18,805
both the left and the right, uh,

1326
01:06:18,805 --> 01:06:23,725
makes it a lot easier to learn these more useful contextual representations.

1327
01:06:23,725 --> 01:06:25,870
So, in particular, as a preview of

1328
01:06:25,870 --> 01:06:28,030
something you're going to learn about later in the class, uh,

1329
01:06:28,030 --> 01:06:30,610
there's a model called BERT, B-E-R-T,

1330
01:06:30,610 --> 01:06:34,330
and that stands for Bidirectional Encoder Representations from Transformers.

1331
01:06:34,330 --> 01:06:36,010
And this is a pretty recently.

1332
01:06:36,010 --> 01:06:39,070
Like, a few months ago, uh, proposed system,

1333
01:06:39,070 --> 01:06:42,460
and it's this pre-trained contextual representation system.

1334
01:06:42,460 --> 01:06:46,450
Um, and it's heavily reliant on the idea of bidirectionality.

1335
01:06:46,450 --> 01:06:48,760
It turns out that the bidirectional, uh,

1336
01:06:48,760 --> 01:06:51,565
nature of BERT is pretty important to its success.

1337
01:06:51,565 --> 01:06:53,290
So, you're gonna learn more about that later,

1338
01:06:53,290 --> 01:06:55,990
but that's just an example of how bidirectionality can give you much

1339
01:06:55,990 --> 01:06:59,875
more uh, powerful contextual representations.

1340
01:06:59,875 --> 01:07:04,390
Okay. So the last thing we're going to talk about today is multi-layer RNNs.

1341
01:07:04,390 --> 01:07:08,800
Uh, so you could regard RNNs as already being deep

1342
01:07:08,800 --> 01:07:14,200
in some sense because you've already unrolled them over potentially very many timesteps,

1343
01:07:14,200 --> 01:07:16,630
and you could regard that as a kind of depth, right?

1344
01:07:16,630 --> 01:07:19,390
But there's another way that RNNs could be deep.

1345
01:07:19,390 --> 01:07:25,210
So, for example, if you applied multiple RNNs kind of one after another,

1346
01:07:25,210 --> 01:07:28,555
then this would be a different way to make your RNN deep,

1347
01:07:28,555 --> 01:07:30,490
and this is the idea between, uh,

1348
01:07:30,490 --> 01:07:33,775
behind a multi-layer RNN.

1349
01:07:33,775 --> 01:07:37,315
So, the reason why you would want to do this is because uh,

1350
01:07:37,315 --> 01:07:40,660
this might allow the network to compute more complex representations.

1351
01:07:40,660 --> 01:07:43,840
So, this is the logic betwe- behind deep networks in general.

1352
01:07:43,840 --> 01:07:45,280
So, if you're familiar with the idea of why

1353
01:07:45,280 --> 01:07:47,620
deeper is better for let's say convolutional networks,

1354
01:07:47,620 --> 01:07:49,195
then this is kind of the same logic.

1355
01:07:49,195 --> 01:07:54,760
It's saying that, uh, your lower RNNs might be computing lower-level features like,

1356
01:07:54,760 --> 01:07:56,770
let's suppose maybe it's keeping track of syntax,

1357
01:07:56,770 --> 01:08:01,700
and your higher  level RNN's gonna compute higher-level features like maybe semantics.

1358
01:08:02,100 --> 01:08:06,775
And a note on terminology, these are sometimes called stacked RNNs.

1359
01:08:06,775 --> 01:08:09,640
So, this works much as you'd imagine.

1360
01:08:09,640 --> 01:08:13,630
So here's an example of how a multi-layer RNN might work.

1361
01:08:13,630 --> 01:08:15,610
Uh, if it's three layers.

1362
01:08:15,610 --> 01:08:18,115
So this is a unidirectional RNN,

1363
01:08:18,115 --> 01:08:20,530
but it could be bidirectional,

1364
01:08:20,530 --> 01:08:23,680
um, If you have access to the entire input sequence.

1365
01:08:23,680 --> 01:08:29,290
So, I guess the, the main thing is that the hidden states from one RNN layer are going to

1366
01:08:29,290 --> 01:08:35,390
be used as the inputs to the RNN layer that's coming next.

1367
01:08:35,400 --> 01:08:38,800
Um, any questions on this?

1368
01:08:38,800 --> 01:08:45,270
Yeah.

1369
01:08:45,270 --> 01:08:46,450
[inaudible].

1370
01:08:46,450 --> 01:08:49,450
That's a great question. So the question I think it's about the order of computation.

1371
01:08:49,450 --> 01:08:52,390
What order will you compute all of these hidden states in?

1372
01:08:52,390 --> 01:08:56,100
I suppose there's some flexibility, right?

1373
01:08:56,100 --> 01:08:59,640
But you could compute all of the step one ones,

1374
01:08:59,640 --> 01:09:02,295
like all of the V ones and then all of the movie ones,

1375
01:09:02,295 --> 01:09:05,970
or you could do all of RNN layer one and then all of RNN layer two.

1376
01:09:05,970 --> 01:09:08,880
So, it's- I think that, um, when you- you know,

1377
01:09:08,880 --> 01:09:11,055
call the PyTorch function to do a multi-layer RNN,

1378
01:09:11,055 --> 01:09:13,380
it will do all of RNN layer one, then two, then three.

1379
01:09:13,380 --> 01:09:14,580
That's what I think happens.

1380
01:09:14,580 --> 01:09:16,270
But it seems like logically,

1381
01:09:16,270 --> 01:09:19,000
there's no reason why you couldn't do it the other way.

1382
01:09:19,000 --> 01:09:30,190
Yep?  [inaudible].

1383
01:09:30,190 --> 01:09:32,110
Yes, yes. That's a great point as well.

1384
01:09:32,110 --> 01:09:35,950
Um, so uh, someone pointed out that if they were bidirectional,

1385
01:09:35,950 --> 01:09:37,480
then you no longer have that flexibility.

1386
01:09:37,480 --> 01:09:39,955
You would have to do all of layer one before layer two.

1387
01:09:39,955 --> 01:09:44,090
Yeah, good point. Anyone else?

1388
01:09:47,040 --> 01:09:52,480
Okay. Uh, so mostly RNNs in practice,

1389
01:09:52,480 --> 01:09:56,065
um, this tends to perform pretty well,

1390
01:09:56,065 --> 01:09:58,090
uh, in that when I look at, um,

1391
01:09:58,090 --> 01:10:00,970
RNN-based systems that are doing very well on some kind of task,

1392
01:10:00,970 --> 01:10:04,330
they usually are some kind of multi-layer RNN, um,

1393
01:10:04,330 --> 01:10:06,400
but they certainly aren't as deep as

1394
01:10:06,400 --> 01:10:09,430
the deep convolutional or feed-forward networks you might have seen in,

1395
01:10:09,430 --> 01:10:10,795
for example, image tasks.

1396
01:10:10,795 --> 01:10:12,550
So whereas, you know, very deep convolutional networks,

1397
01:10:12,550 --> 01:10:14,470
I think hundreds of layers now, um,

1398
01:10:14,470 --> 01:10:16,795
you certainly aren't getting RNNs that are that deep.

1399
01:10:16,795 --> 01:10:18,790
So, for example, um,

1400
01:10:18,790 --> 01:10:22,300
in this paper from, uh, Google, uh,

1401
01:10:22,300 --> 01:10:25,165
they're doing this kind of large hyperparameter search for

1402
01:10:25,165 --> 01:10:29,740
neural machine translation to find which kinds of hyperparameters work well for NMT.

1403
01:10:29,740 --> 01:10:31,720
And in this paper, they found that um,

1404
01:10:31,720 --> 01:10:34,165
two to four layers was best for the encoder RNN,

1405
01:10:34,165 --> 01:10:36,280
and four layers was best for the decoder RNN.

1406
01:10:36,280 --> 01:10:39,430
Uh, you'll find out more about what encoder and decoder mean next time.

1407
01:10:39,430 --> 01:10:41,275
Um, but those are fairly small numbers.

1408
01:10:41,275 --> 01:10:43,330
Although they did find that if you add these skip

1409
01:10:43,330 --> 01:10:45,355
connections or these dense connections, um,

1410
01:10:45,355 --> 01:10:49,750
then it makes it much easier to learn some even deeper RNNs more effectively,

1411
01:10:49,750 --> 01:10:51,055
like, maybe up to eight layers,

1412
01:10:51,055 --> 01:10:53,605
but these certainly aren'tx  hundreds of layers deep.

1413
01:10:53,605 --> 01:10:55,750
And one of the reasons why, uh,

1414
01:10:55,750 --> 01:10:59,095
RNNs don't tend to be nearly as deep as these other kinds of networks,

1415
01:10:59,095 --> 01:11:01,675
is that because as we commented before,

1416
01:11:01,675 --> 01:11:03,205
RNNs have to be computed, uh,

1417
01:11:03,205 --> 01:11:05,380
sequentially; they can't be computed in parallel.

1418
01:11:05,380 --> 01:11:07,330
This means that they're pretty expensive to compute.

1419
01:11:07,330 --> 01:11:09,519
If you have this depth in like, two-dimensions,

1420
01:11:09,519 --> 01:11:13,675
you have the depth over the timesteps and then the depth over the RNN layer is two,

1421
01:11:13,675 --> 01:11:15,160
then it beco- it becomes very,

1422
01:11:15,160 --> 01:11:17,830
very expensive to compute these, these RNNs.

1423
01:11:17,830 --> 01:11:19,885
So, that's another reason why they don't get very deep.

1424
01:11:19,885 --> 01:11:23,290
Uh, so again, we just mentioned transformers.

1425
01:11:23,290 --> 01:11:25,165
Uh, you gonna learn about transformers later.

1426
01:11:25,165 --> 01:11:27,280
But these, it seems, um,

1427
01:11:27,280 --> 01:11:30,400
can be deeper fro- from what I can tell of,

1428
01:11:30,400 --> 01:11:31,900
of what people are using these days.

1429
01:11:31,900 --> 01:11:33,580
Transformer-based networks can be pretty deep.

1430
01:11:33,580 --> 01:11:35,530
So, uh, but for example,

1431
01:11:35,530 --> 01:11:38,425
there's a 24-layer version and a 12-layer version, um,

1432
01:11:38,425 --> 01:11:39,820
and admittedly, that was trained by Google,

1433
01:11:39,820 --> 01:11:41,860
and they have a lot of computational power.

1434
01:11:41,860 --> 01:11:43,600
Um, but I think part of the reason why

1435
01:11:43,600 --> 01:11:45,685
these transformer-based networks can be quite deep,

1436
01:11:45,685 --> 01:11:48,220
is that they have a lot of these skipping like connections.

1437
01:11:48,220 --> 01:11:49,750
In fact, the whole um,

1438
01:11:49,750 --> 01:11:52,570
innovation of transformers is that they're built on a lot of, kind of,

1439
01:11:52,570 --> 01:11:57,520
skip connections. Okay, any questions?

1440
01:11:57,520 --> 01:12:01,450
We're almost done. Okay. All right.

1441
01:12:01,450 --> 01:12:04,030
So, uh, here's a summary of what we've learned today.

1442
01:12:04,030 --> 01:12:05,965
I know it's been a lot of information.

1443
01:12:05,965 --> 01:12:11,530
Um, but I think here are four practical takeaways from today that, uh,

1444
01:12:11,530 --> 01:12:13,795
are probably useful to you in your projects,

1445
01:12:13,795 --> 01:12:14,920
even if you, um,

1446
01:12:14,920 --> 01:12:17,800
uh, even if you

1447
01:12:17,800 --> 01:12:21,130
didn't find them very interesting in themselves they're probably pretty useful.

1448
01:12:21,130 --> 01:12:24,190
So, the first one is that LSTMs are very powerful.

1449
01:12:24,190 --> 01:12:25,990
They're certainly a lot powerful than,

1450
01:12:25,990 --> 01:12:27,910
uh, more powerful than Vanila RNNs.

1451
01:12:27,910 --> 01:12:31,450
Um, GRUs are also more powerful than, uh, Vanila RNNs.

1452
01:12:31,450 --> 01:12:34,210
Uh, and the only difference that is consistently the

1453
01:12:34,210 --> 01:12:37,480
same is that GRUs are faster than LSTMs.

1454
01:12:37,480 --> 01:12:40,465
The next one is that you should probably clip your gradients,

1455
01:12:40,465 --> 01:12:42,055
because if you don't clip your gradients,

1456
01:12:42,055 --> 01:12:47,050
you're in danger of walking off cliffs and then ending up with NaNs in your model.

1457
01:12:47,050 --> 01:12:52,105
Uh, the next tip is that bidirectionality is useful if you can apply it.

1458
01:12:52,105 --> 01:12:56,125
And, basically, anytime when you have access to the entire input sequence,

1459
01:12:56,125 --> 01:12:57,850
you can apply bidirectionality,

1460
01:12:57,850 --> 01:12:59,790
so you should probably do that by default.

1461
01:12:59,790 --> 01:13:04,140
And then the last tip is that multi-layer RNNs are pretty powerful.

1462
01:13:04,140 --> 01:13:06,330
And again, you should probably do that if you,

1463
01:13:06,330 --> 01:13:08,355
uh, have enough computational power to do so.

1464
01:13:08,355 --> 01:13:11,405
But if you're going to make your multi-layer RNN pretty deep,

1465
01:13:11,405 --> 01:13:13,255
then you might need skip connections.

1466
01:13:13,255 --> 01:13:22,840
All right. Thanks [NOISE].

