1
00:00:05,210 --> 00:00:07,950
Hi, everyone. I'm Abby,

2
00:00:07,950 --> 00:00:09,544
I'm the head TA for this class

3
00:00:09,544 --> 00:00:12,510
and I'm also a PhD student in the Stanford NLP group.

4
00:00:12,510 --> 00:00:14,670
And today I'm gonna be telling you about

5
00:00:14,670 --> 00:00:17,040
language models and recurrent neural networks.

6
00:00:17,040 --> 00:00:19,980
So, here's an overview of what we're gonna do today.

7
00:00:19,980 --> 00:00:24,885
Today, first, we're going to introduce a new NLP task, that's language modelling,

8
00:00:24,885 --> 00:00:29,504
and that's going to motivate us to learn about a new family of neural networks,

9
00:00:29,504 --> 00:00:32,730
that is recurrent neural networks or RNNs.

10
00:00:32,730 --> 00:00:34,440
So, I'd say that these are two of

11
00:00:34,440 --> 00:00:37,415
the most important ideas you're going to learn for the rest of the course.

12
00:00:37,415 --> 00:00:41,070
So, we're going to be covering some fairly cool material today.

13
00:00:41,070 --> 00:00:44,305
So, let's start off with language modeling.

14
00:00:44,305 --> 00:00:48,775
Language modeling is the task of predicting what word comes next.

15
00:00:48,775 --> 00:00:52,230
So, given this piece of text the students opens their blank,

16
00:00:52,230 --> 00:00:56,200
could anyone shout out a word which you think might be coming next?

17
00:00:58,370 --> 00:00:59,550
Purpose. [NOISE].

18
00:00:59,550 --> 00:01:03,840
[OVERLAPPING] Mind, what else? I didn't quite hear them,

19
00:01:03,840 --> 00:01:06,720
but, uh, yeah, these are all likely things, right?

20
00:01:06,720 --> 00:01:08,150
So, these are some things which I thought,

21
00:01:08,150 --> 00:01:09,490
students might be opening, uh,

22
00:01:09,490 --> 00:01:11,200
students open their books, seems likely.

23
00:01:11,200 --> 00:01:13,280
Uh, students open their laptops,

24
00:01:13,280 --> 00:01:15,040
students open their exams,

25
00:01:15,040 --> 00:01:16,580
Students open their minds, incredibly,

26
00:01:16,580 --> 00:01:18,700
someone came up with one, that one just now,

27
00:01:18,700 --> 00:01:20,330
uh, it's kind of a metaphorical meaning of opening.

28
00:01:20,330 --> 00:01:23,675
So, you are all performing language modeling right now.

29
00:01:23,675 --> 00:01:25,475
And thinking about what word comes next,

30
00:01:25,475 --> 00:01:27,250
you are being a language model.

31
00:01:27,250 --> 00:01:31,385
So, here's a more formal definition of what a language model is.

32
00:01:31,385 --> 00:01:34,430
Given a sequence of words X1 up to Xt,

33
00:01:34,430 --> 00:01:37,340
a language model, is something that computes

34
00:01:37,340 --> 00:01:41,200
the probability distribution of the next word, Xt plus 1.

35
00:01:41,200 --> 00:01:44,194
So, a language model comes up with the probability distribution,

36
00:01:44,194 --> 00:01:49,067
the conditional probability, of what X t plus 1 is given the words it found.

37
00:01:49,067 --> 00:01:50,810
And here we're assuming that, Xt plus 1

38
00:01:50,810 --> 00:01:53,960
can be any word w from a fixed vocabulary V.

39
00:01:53,960 --> 00:01:55,520
So we are assuming that there is

40
00:01:55,520 --> 00:01:58,205
a pre-defined list of words that we're considering.

41
00:01:58,205 --> 00:02:00,140
In this way, you can view language modeling

42
00:02:00,140 --> 00:02:01,850
as a type of classification task,

43
00:02:01,850 --> 00:02:04,580
because there's a predefined number of possibilities.

44
00:02:04,580 --> 00:02:09,360
Um, we call a system that does this a language model.

45
00:02:09,850 --> 00:02:12,050
There's an alternative way of thinking

46
00:02:12,050 --> 00:02:13,715
about a language model as well.

47
00:02:13,715 --> 00:02:15,200
You can think of a language model

48
00:02:15,200 --> 00:02:19,060
as a system which assigns probability to a piece of text.

49
00:02:19,060 --> 00:02:21,470
So, for example, if we have some piece of text,

50
00:02:21,470 --> 00:02:23,180
X up to X capital T,

51
00:02:23,180 --> 00:02:25,040
then, the probability of this text

52
00:02:25,040 --> 00:02:27,830
according to the language model can be broken down.

53
00:02:27,830 --> 00:02:29,250
So, just by definition,

54
00:02:29,250 --> 00:02:31,250
you can say that the probability is equal to,

55
00:02:31,250 --> 00:02:34,530
the product of all of these conditional probabilities.

56
00:02:34,530 --> 00:02:37,375
And, uh, the form inside,

57
00:02:37,375 --> 00:02:40,480
the products is exactly what a language model provides.

58
00:02:40,480 --> 00:02:42,465
So, you can think of these things as somewhat equivalent.

59
00:02:42,465 --> 00:02:44,720
Predicting next words, gives you a system,

60
00:02:44,720 --> 00:02:49,110
that can give the probability of a given piece of text.

61
00:02:49,270 --> 00:02:52,745
So, in fact, you, use language models every day.

62
00:02:52,745 --> 00:02:56,120
For example, when you're texting on your phone and you're writing a message,

63
00:02:56,120 --> 00:02:57,610
then most likely if you have a smartphone,

64
00:02:57,610 --> 00:03:00,080
it will be predicting what word you might be about to say.

65
00:03:00,080 --> 00:03:01,790
So, if you say, um, I'll meet you at the-

66
00:03:01,790 --> 00:03:04,250
your phone might suggest perhaps you mean airport or cafe,

67
00:03:04,250 --> 00:03:06,025
or office, for example.

68
00:03:06,025 --> 00:03:08,905
Another situation which you use language models every day

69
00:03:08,905 --> 00:03:11,495
is when you search for something on the internet, for example, Google,

70
00:03:11,495 --> 00:03:12,830
and you start typing your query,

71
00:03:12,830 --> 00:03:15,952
then Google tries to complete your query for you, and that's language modeling.

72
00:03:15,952 --> 00:03:19,020
It's predicting what word or words might come next.

73
00:03:20,480 --> 00:03:23,715
So, that's what a language model is,

74
00:03:23,715 --> 00:03:26,675
and the question is, how would you learn a language model?

75
00:03:26,675 --> 00:03:29,917
So, if I was to ask that question in the pre- deep learning era,

76
00:03:29,917 --> 00:03:31,730
which was really only a few years ago,

77
00:03:31,730 --> 00:03:35,005
the answer would be, you would learn a n-gram language model.

78
00:03:35,005 --> 00:03:38,570
So, today first we're going to learn about n-gram language models.

79
00:03:38,570 --> 00:03:41,330
So, before I can tell you what a n-gram language model is,

80
00:03:41,330 --> 00:03:43,160
you need to know what an n-gram is.

81
00:03:43,160 --> 00:03:47,905
So, by definition an n-gram is a chunk of n  consecutive words.

82
00:03:47,905 --> 00:03:50,400
So, for example, a one gram or unigram,

83
00:03:50,400 --> 00:03:52,050
is just all of the individual words

84
00:03:52,050 --> 00:03:55,020
in the sequence that would be "the students open the-"

85
00:03:55,020 --> 00:03:58,810
A two gram or bigram would be all of the consecutive chunks of pairs of words,

86
00:03:58,810 --> 00:04:00,980
"the students", "students opened", "opened their"

87
00:04:00,980 --> 00:04:04,560
and so on for trigrams and four-grams, etc.

88
00:04:05,050 --> 00:04:08,575
So, the core idea of an n-gram language model

89
00:04:08,575 --> 00:04:11,035
is that in order to predict what word comes next,

90
00:04:11,035 --> 00:04:12,815
you're going to collect a bunch of statistics,

91
00:04:12,815 --> 00:04:14,930
about how frequent different n-grams are,

92
00:04:14,930 --> 00:04:16,490
from some kind of training data,

93
00:04:16,490 --> 00:04:18,110
and then you can use those statistics

94
00:04:18,110 --> 00:04:21,120
to predict what next words might be likely.

95
00:04:21,830 --> 00:04:23,640
Here is some more detail.

96
00:04:23,640 --> 00:04:26,325
So, to make an n-gram language model,

97
00:04:26,325 --> 00:04:28,490
first you need to make a simplifying assumption,

98
00:04:28,490 --> 00:04:30,305
and this your assumption.

99
00:04:30,305 --> 00:04:33,350
You say that the next word Xt plus 1

100
00:04:33,350 --> 00:04:37,535
depends only on the preceding N-1 words.

101
00:04:37,535 --> 00:04:39,900
So, what we're assuming,

102
00:04:39,900 --> 00:04:41,650
is that the probability distribution,

103
00:04:41,650 --> 00:04:45,020
the conditional probability of Xt plus 1 given all of the words they follow,

104
00:04:45,020 --> 00:04:46,160
we're just going to simplify that,

105
00:04:46,160 --> 00:04:50,485
and say it only depends on the last N-1 words,  and that's our assumption.

106
00:04:50,485 --> 00:04:53,950
So, by the definition of conditional probability,

107
00:04:53,950 --> 00:04:55,600
we can say that this probability,

108
00:04:55,600 --> 00:04:58,385
is just the ratio of two different probabilities.

109
00:04:58,385 --> 00:05:01,180
So, on the top, you've got the probability of

110
00:05:01,180 --> 00:05:03,220
a particular n-gram and on the bottom we've

111
00:05:03,220 --> 00:05:06,192
got the probability of a particular N-1 gram

112
00:05:06,192 --> 00:05:08,020
This is a little hard to read because of all the superscripts

113
00:05:08,020 --> 00:05:11,015
but I'm gonna give an example with words on the next slide.

114
00:05:11,015 --> 00:05:15,055
Okay. So, that's the definition of the probability of the next word,

115
00:05:15,055 --> 00:05:17,140
but the question remains, how do we get all of

116
00:05:17,140 --> 00:05:19,980
these n-gram and N-1 gram probabilities?

117
00:05:19,980 --> 00:05:22,300
So, the answer is, we're going to get them by

118
00:05:22,300 --> 00:05:25,050
counting them in some large corpus of text.

119
00:05:25,050 --> 00:05:26,510
So, we're going to approximate,

120
00:05:26,510 --> 00:05:29,560
these probabilities just by the count of the number of times that

121
00:05:29,560 --> 00:05:34,190
these particular n-grams and N-1 grams appeared in our training corpus.

122
00:05:34,410 --> 00:05:37,370
Okay. So, here's an example with some words.

123
00:05:37,370 --> 00:05:40,565
Suppose we are trying to learn a 4-gram language model,

124
00:05:40,565 --> 00:05:42,830
and suppose that we have a piece of text, that says,

125
00:05:42,830 --> 00:05:44,540
"As the proctor started the clock,

126
00:05:44,540 --> 00:05:46,100
the students opened their blank",

127
00:05:46,100 --> 00:05:48,895
and we're trying to predict what word is coming next.

128
00:05:48,895 --> 00:05:51,740
So, because we're learning a 4-gram language model,

129
00:05:51,740 --> 00:05:55,910
a simplifying assumption is that the next word depends only on the last three words,

130
00:05:55,910 --> 00:05:57,605
last N-1 words.

131
00:05:57,605 --> 00:06:01,520
So, we're going to discard all of the context so far except for the last few words,

132
00:06:01,520 --> 00:06:03,780
which is, "Students opened their."

133
00:06:03,800 --> 00:06:07,620
So, as a reminder, n-gram language model says that,

134
00:06:07,620 --> 00:06:08,940
the probability of the next word being,

135
00:06:08,940 --> 00:06:13,230
some particular word W in the vocabulary is equal to the number of times we saw

136
00:06:13,230 --> 00:06:15,510
students opened their W divided by the number of

137
00:06:15,510 --> 00:06:18,655
times we saw students opened their,  in the training corpus.

138
00:06:18,655 --> 00:06:21,440
So, let's suppose that in our training corpus,

139
00:06:21,440 --> 00:06:24,215
we saw the phrase "students open their" 1,000 times.

140
00:06:24,215 --> 00:06:28,340
And suppose that, we saw "students opened their books" 400 times.

141
00:06:28,340 --> 00:06:32,220
This means that the probability of the next word being books is 0,4.

142
00:06:32,220 --> 00:06:36,810
And uh, similarly, let's suppose that we saw students open their exams 100 times,

143
00:06:36,810 --> 00:06:39,260
this means that the probability of exams given students

144
00:06:39,260 --> 00:06:41,930
open their is 0,1. Is there a question?

145
00:06:41,930 --> 00:06:44,900
[inaudible].

146
00:06:44,900 --> 00:06:47,010
The question is, does the order of the words matter?

147
00:06:47,010 --> 00:06:50,340
And the answer is yes, the order of students open there does matter.

148
00:06:50,340 --> 00:06:53,190
It's different to "the students opened."

149
00:06:53,190 --> 00:06:56,985
So, the question I want to raise now is,

150
00:06:56,985 --> 00:07:00,805
was it a good idea for us to discard the proctor context?

151
00:07:00,805 --> 00:07:03,115
If you look at the actual example that we had,

152
00:07:03,115 --> 00:07:06,070
the example was as the proctor started the clock,

153
00:07:06,070 --> 00:07:07,850
the students opened their blank.

154
00:07:07,850 --> 00:07:12,360
So, do we think that books or exams is more likely given the actual context,

155
00:07:12,360 --> 00:07:14,550
the full context? Yep.

156
00:07:14,550 --> 00:07:15,450
Exams.

157
00:07:15,450 --> 00:07:17,795
Right. Exams is more likely because the proctor and

158
00:07:17,795 --> 00:07:20,260
the clock heavily implies that it's an exam scenario, so

159
00:07:20,260 --> 00:07:22,625
they're more likely to be opening the exams than the books,

160
00:07:22,625 --> 00:07:24,400
unless it's an open book exam.

161
00:07:24,400 --> 00:07:26,830
Uh, but I think, overall, it should be exams.

162
00:07:26,830 --> 00:07:29,890
So, the problem that we're seeing here is that in the training corpus,

163
00:07:29,890 --> 00:07:31,240
the fact that students were opening

164
00:07:31,240 --> 00:07:33,990
something means that it's more likely to be books than exams

165
00:07:33,990 --> 00:07:36,305
because overall, books are more common than exams.

166
00:07:36,305 --> 00:07:38,565
But if we know that the context is,

167
00:07:38,565 --> 00:07:41,078
the proctor and the clock, then it should be exams.

168
00:07:41,078 --> 00:07:44,240
So, what I'm highlighting here is a problem with our simplifying assumption.

169
00:07:44,240 --> 00:07:45,860
If we throw away too much context,

170
00:07:45,860 --> 00:07:50,455
then we are not as good as predicting the words as we would be if we kept the context.

171
00:07:50,455 --> 00:07:54,690
Okay. So, that's one problem with n-gram, uh, language models.

172
00:07:54,690 --> 00:07:56,810
Uh, there are some other problems as well.

173
00:07:56,810 --> 00:08:00,470
So, uh, here again is the equation that you saw before.

174
00:08:00,470 --> 00:08:01,880
One problem which we're gonna call

175
00:08:01,880 --> 00:08:05,465
the sparsity problem is what happens if the number on top,

176
00:08:05,465 --> 00:08:08,380
the numerator, what if that count is equal to zero.

177
00:08:08,380 --> 00:08:11,210
So, what if for some particular word W,

178
00:08:11,210 --> 00:08:14,450
the phrase students opened their W never occurred in the data.

179
00:08:14,450 --> 00:08:17,240
So, for example, let's suppose students opened their petri dishes,

180
00:08:17,240 --> 00:08:19,880
is fairly uncommon and it never appears in the data,

181
00:08:19,880 --> 00:08:24,355
then that means our probability of the next word being petri dishes will be zero.

182
00:08:24,355 --> 00:08:27,390
And this is bad, because it might be uncommon but it is,

183
00:08:27,390 --> 00:08:29,385
a valid scenario, right?

184
00:08:29,385 --> 00:08:31,090
If you're a biology student for example.

185
00:08:31,090 --> 00:08:34,085
So, this is a problem and we call it the sparsity problem,

186
00:08:34,085 --> 00:08:37,790
because the problem is that if we'd never seen an event happen in the training data,

187
00:08:37,790 --> 00:08:41,485
then our model assigns zero probability to that event.

188
00:08:41,485 --> 00:08:46,415
So, one partial solution to this problem is that maybe we should add a small delta,

189
00:08:46,415 --> 00:08:48,290
small number delta to the count,

190
00:08:48,290 --> 00:08:50,420
for every word in the vocabulary.

191
00:08:50,420 --> 00:08:53,920
And then this way, every possible word that come next,

192
00:08:53,920 --> 00:08:56,250
has at least some small probability.

193
00:08:56,250 --> 00:08:59,089
So, petri dishes will have some small probability,

194
00:08:59,089 --> 00:09:02,410
but then so, will all of the other words which are possibly bad choices.

195
00:09:02,410 --> 00:09:05,580
So, this, uh, technique is called smoothing, because the idea is,

196
00:09:05,580 --> 00:09:06,945
you're going from a very, uh,

197
00:09:06,945 --> 00:09:10,050
sparse probability distribution, which is zero, almost everywhere,

198
00:09:10,050 --> 00:09:11,550
with a few spikes where there's,

199
00:09:11,550 --> 00:09:13,445
uh, being n-grams that we've seen,

200
00:09:13,445 --> 00:09:16,100
it goes from that to being a more smooth probability distribution

201
00:09:16,100 --> 00:09:19,615
where everything has at least a small probability on it.

202
00:09:19,615 --> 00:09:24,270
So, the second sparsity problem which is possibly worse than the first one is,

203
00:09:24,270 --> 00:09:28,130
what happens if the number in the denominator is zero?

204
00:09:28,130 --> 00:09:30,200
So, in our example, that would mean,

205
00:09:30,200 --> 00:09:34,655
what if we never even saw the trigram "students opened their" in the training data.

206
00:09:34,655 --> 00:09:38,480
If that happens, then we can't even calculate this probability distribution at

207
00:09:38,480 --> 00:09:42,820
all for any word W because we never even saw this context before.

208
00:09:42,820 --> 00:09:45,825
So, a possible solution to this is that

209
00:09:45,825 --> 00:09:48,450
if you can't find "students open their" in the corpus,

210
00:09:48,450 --> 00:09:51,940
then you should back off to just conditioning on the last two words,

211
00:09:51,940 --> 00:09:53,545
rather than the last three words.

212
00:09:53,545 --> 00:09:55,900
So, now you'd be looking at times when you'd seen,

213
00:09:55,900 --> 00:09:58,460
uh, "open their" and seeing what what's come next.

214
00:09:58,460 --> 00:10:01,350
So, this is called back-off because in this failure case,

215
00:10:01,350 --> 00:10:04,025
for when you have no data for your 4-gram language model,

216
00:10:04,025 --> 00:10:06,020
you're backing off to a trigram language model.

217
00:10:06,020 --> 00:10:09,510
Are there any questions at this point?

218
00:10:12,310 --> 00:10:17,570
Okay. So, um, another thing to note is that these sparsity problems

219
00:10:17,570 --> 00:10:22,100
get worse if you increase N. If you make N larger in your n-gram language model,

220
00:10:22,100 --> 00:10:23,870
and you might want to do this, for example,

221
00:10:23,870 --> 00:10:26,390
you might think, uh, I want to have a larger context,

222
00:10:26,390 --> 00:10:28,565
so I can pay attention to words that

223
00:10:28,565 --> 00:10:30,890
happened longer ago and that's gonna make it a better predictor.

224
00:10:30,890 --> 00:10:33,275
So, you might think making N bigger is a good idea.

225
00:10:33,275 --> 00:10:36,410
But the problem is if you do that then the sparsity problems get worse.

226
00:10:36,410 --> 00:10:37,700
Because, let's suppose you say,

227
00:10:37,700 --> 00:10:39,215
I want a 10-gram language model.

228
00:10:39,215 --> 00:10:40,910
Then the problem is that you're going to be counting,

229
00:10:40,910 --> 00:10:43,480
how often you seen process in 9-grams and 10-grams.

230
00:10:43,480 --> 00:10:45,485
But 9-grams and 10-grams, there's so many of them,

231
00:10:45,485 --> 00:10:47,615
that the one you are interested in probably never occurred,

232
00:10:47,615 --> 00:10:51,155
in your training data which means that the whole thing becomes dysfunctional.

233
00:10:51,155 --> 00:10:55,680
So, in practice, we usually can't have N much bigger than five.

234
00:10:56,170 --> 00:10:58,490
Okay. So, that was, uh,

235
00:10:58,490 --> 00:11:00,875
two sparsity problems with n-gram language models.

236
00:11:00,875 --> 00:11:02,770
Here is a problem with storage.

237
00:11:02,770 --> 00:11:04,710
So, if we look at this equation, uh,

238
00:11:04,710 --> 00:11:06,780
you have to think about what do you need to

239
00:11:06,780 --> 00:11:09,365
store in order to use your n-gram language model.

240
00:11:09,365 --> 00:11:12,020
You need to store this count number,

241
00:11:12,020 --> 00:11:14,090
for all of the n-grams that you observed in

242
00:11:14,090 --> 00:11:17,215
the corpus when you were going through the training corpus counting them.

243
00:11:17,215 --> 00:11:19,440
And the problem is, that as you increase N,

244
00:11:19,440 --> 00:11:23,480
then this number of n-grams that you have to store and count increases.

245
00:11:23,480 --> 00:11:27,515
So, another problem with increasing N is that the size of your model,

246
00:11:27,515 --> 00:11:30,750
or your n-gram model, uh, gets bigger.

247
00:11:31,490 --> 00:11:37,215
Okay, so n-gram Language Models in practice. Let's look at an example.

248
00:11:37,215 --> 00:11:42,540
You can actually build a simple trigram Language Model over a 1,7 million word corpus,

249
00:11:42,540 --> 00:11:44,325
uh, in a few seconds on your laptop.

250
00:11:44,325 --> 00:11:46,140
And in fact, the corpus that I used to do this

251
00:11:46,140 --> 00:11:47,970
was the same one that you met in assignment one.

252
00:11:47,970 --> 00:11:49,605
It's Reuters' corpus which is,

253
00:11:49,605 --> 00:11:51,180
uh, business and financial news.

254
00:11:51,180 --> 00:11:52,380
So, if you want to do this yourself,

255
00:11:52,380 --> 00:11:55,005
you can follow that link at the bottom of the slide later.

256
00:11:55,005 --> 00:11:57,000
So, uh, this is, uh,

257
00:11:57,000 --> 00:11:59,280
something which I ran on my laptop in a few second.

258
00:11:59,280 --> 00:12:02,790
So I gave it the context of the bigram today the,

259
00:12:02,790 --> 00:12:06,480
and then I asked the trigram Language Model what word is likely to come next.

260
00:12:06,480 --> 00:12:09,855
So, the Language Model said that the top next most likely words are

261
00:12:09,855 --> 00:12:13,455
company, bank, price, Italian, emirate, et cetera.

262
00:12:13,455 --> 00:12:17,640
So already just looking at these probabilities that are assigned to these different words,

263
00:12:17,640 --> 00:12:19,590
uh, you can see that there is a sparsity problem.

264
00:12:19,590 --> 00:12:21,840
For example, the top two most likely words have

265
00:12:21,840 --> 00:12:24,720
the exact same probability and the reason for that is,

266
00:12:24,720 --> 00:12:26,760
that this number is 4 over 26.

267
00:12:26,760 --> 00:12:28,800
So these are quite small integers, uh,

268
00:12:28,800 --> 00:12:30,270
meaning that we only saw, uh,

269
00:12:30,270 --> 00:12:33,000
today the company and today the bank four times each.

270
00:12:33,000 --> 00:12:34,560
So, uh, this is an example of

271
00:12:34,560 --> 00:12:37,290
the sparsity problem because overall these are quite low counts,

272
00:12:37,290 --> 00:12:39,165
we haven't seen that many different, uh,

273
00:12:39,165 --> 00:12:40,500
versions of this event,

274
00:12:40,500 --> 00:12:43,885
so we don't have a very granular probability distribution.

275
00:12:43,885 --> 00:12:46,385
But in any case ignoring the sparsity problem,

276
00:12:46,385 --> 00:12:47,765
I would say that overall,

277
00:12:47,765 --> 00:12:50,640
these, uh, top suggestions look pretty reasonable.

278
00:12:52,600 --> 00:12:55,670
So you can actually use a Language Model to

279
00:12:55,670 --> 00:12:58,305
generate text and this is how you would do it.

280
00:12:58,305 --> 00:13:00,735
So let's suppose you have your first two words already, uh,

281
00:13:00,735 --> 00:13:04,560
you condition on this and you ask your Language Model what's likely to come next.

282
00:13:04,560 --> 00:13:07,305
So then given this probability distribution over the words,

283
00:13:07,305 --> 00:13:08,850
you can sample from it, that is,

284
00:13:08,850 --> 00:13:11,865
select some words with, you know, the associated probability.

285
00:13:11,865 --> 00:13:14,235
So let's suppose that gives us the word price.

286
00:13:14,235 --> 00:13:17,730
So then price is your next word, and then you just condition on the last two words,

287
00:13:17,730 --> 00:13:20,385
which in this ex- example is now the price.

288
00:13:20,385 --> 00:13:23,790
So now you get a new probability distribution and you can continue this process,

289
00:13:23,790 --> 00:13:27,960
uh, sampling and then conditioning again and sampling.

290
00:13:27,960 --> 00:13:30,150
So if you do this long enough,

291
00:13:30,150 --> 00:13:31,350
you will get a piece of text,

292
00:13:31,350 --> 00:13:33,690
so this is the actual text that I got when

293
00:13:33,690 --> 00:13:37,005
I run this generation process with this trigram Language Model.

294
00:13:37,005 --> 00:13:40,260
So it says, "Today the price of gold per ton,

295
00:13:40,260 --> 00:13:43,260
while production of shoe lasts and shoe industry,

296
00:13:43,260 --> 00:13:46,230
the bank intervened just after it considered and rejected

297
00:13:46,230 --> 00:13:49,365
an IMF demand to rebuild depleted European stocks,

298
00:13:49,365 --> 00:13:52,810
September, 30th end primary 76 counts a share.''

299
00:13:52,810 --> 00:13:55,250
Okay. So, uh, what do we think about this text?

300
00:13:55,250 --> 00:13:59,195
We think it's good? We, uh, surprised?

301
00:13:59,195 --> 00:14:02,370
Um, I would say that in some ways it is good,

302
00:14:02,370 --> 00:14:04,620
it's kind of surprisingly grammatical, you know,

303
00:14:04,620 --> 00:14:07,860
it mostly, uh, kind of pauses,

304
00:14:07,860 --> 00:14:09,150
uh, but you would definitely say that it,

305
00:14:09,150 --> 00:14:10,500
it doesn't really make any sense.

306
00:14:10,500 --> 00:14:12,180
It's pretty incoherent.

307
00:14:12,180 --> 00:14:14,580
And we shouldn't be surprised that it's incoherent I

308
00:14:14,580 --> 00:14:17,715
think because if you remember this is a trigram Language Model,

309
00:14:17,715 --> 00:14:20,265
it has a memory of just the last well,

310
00:14:20,265 --> 00:14:22,635
three or two words depending on how you look at it.

311
00:14:22,635 --> 00:14:24,510
So clearly we need to consider

312
00:14:24,510 --> 00:14:27,990
more than three words at a time if we want to model language well.

313
00:14:27,990 --> 00:14:32,265
But as we already know, increasing n makes the sparsity problem worse,

314
00:14:32,265 --> 00:14:38,370
n-gram Language Models, and it also increases model size. Is that a question?

315
00:14:38,370 --> 00:14:40,320
How does it [inaudible] [NOISE]

316
00:14:40,320 --> 00:14:43,380
So the question is, how does the n-gram Language Model know when to put commas.

317
00:14:43,380 --> 00:14:45,150
Uh, so you can,

318
00:14:45,150 --> 00:14:50,400
[NOISE] decide that commas and other punctuation are just another kind of word,

319
00:14:50,400 --> 00:14:51,705
is that well or token,

320
00:14:51,705 --> 00:14:54,510
and then, to the Language Model it doesn't really make much difference.

321
00:14:54,510 --> 00:14:57,705
It's just used that as another possible world that can be, um, predicted,

322
00:14:57,705 --> 00:14:59,445
that's why we've got the weird spacing around the,

323
00:14:59,445 --> 00:15:01,770
the commas is because it was essentially viewed as a separate word.

324
00:15:01,770 --> 00:15:06,135
[NOISE] Okay.

325
00:15:06,135 --> 00:15:09,195
So this course is called NLP with Deep Learning.

326
00:15:09,195 --> 00:15:12,765
So you probably thinking how do we build a neural Language Model?

327
00:15:12,765 --> 00:15:15,450
So let's just recap, uh, in case you forgot.

328
00:15:15,450 --> 00:15:17,940
Remember that a Language Model is something that takes

329
00:15:17,940 --> 00:15:20,760
inputs which is a sequence of words X1 up to Xt,

330
00:15:20,760 --> 00:15:26,290
and then it outputs a probability distribution of what the next word might be Xt plus 1.

331
00:15:27,470 --> 00:15:32,070
Okay, so when we think about what kind of neural models we've met in this course so far.

332
00:15:32,070 --> 00:15:34,545
Uh, we've already met window-based neural models.

333
00:15:34,545 --> 00:15:36,780
And in lecture three, we saw how you could apply

334
00:15:36,780 --> 00:15:40,035
a window-based neural model to a named entity recognition.

335
00:15:40,035 --> 00:15:43,050
So in that scenario you take some kind of window around the word that you

336
00:15:43,050 --> 00:15:46,125
care about which in this example is Paris, and then, uh,

337
00:15:46,125 --> 00:15:48,780
you get the word embeddings for those, concatenate them put them through

338
00:15:48,780 --> 00:15:52,890
some layers, and then you get your decision which is that Paris is a location not,

339
00:15:52,890 --> 00:15:55,425
you know, a person or organization.

340
00:15:55,425 --> 00:15:57,900
So that's a recap of what we saw in lecture three.

341
00:15:57,900 --> 00:16:03,795
How would we apply a model like this to language modeling? So here's how you would do it.

342
00:16:03,795 --> 00:16:06,930
Here's an example of a fixed-window neural language model.

343
00:16:06,930 --> 00:16:09,420
So, again, we have some kind of context

344
00:16:09,420 --> 00:16:12,060
which is, as the proctor started the clock the students opened their,

345
00:16:12,060 --> 00:16:15,225
um, we're trying to guess what word might come next.

346
00:16:15,225 --> 00:16:18,450
So we have to make a similar simplifying assumption to before.

347
00:16:18,450 --> 00:16:21,255
Uh, because it's a fixed size window, uh,

348
00:16:21,255 --> 00:16:25,500
we have to discard the context except for the window that we're conditioning on.

349
00:16:25,500 --> 00:16:29,070
So let's suppose that our fixed window is of size four.

350
00:16:29,070 --> 00:16:34,390
So what we'll do is similarly to the, ah, NER model.

351
00:16:34,390 --> 00:16:38,400
We're going to represent these words with one-hot vectors,

352
00:16:38,400 --> 00:16:42,745
and then we'll use those to look up the word embeddings for these words using the,

353
00:16:42,745 --> 00:16:44,895
uh, embedding lookup matrix.

354
00:16:44,895 --> 00:16:48,075
So then we get all of our word embeddings E,1, 2, 3, 4,

355
00:16:48,075 --> 00:16:51,270
and then we concatenate them together to get e. We put this through

356
00:16:51,270 --> 00:16:55,215
a linear layer and a nonlinearity function f to get some kind of hidden layer,

357
00:16:55,215 --> 00:16:57,720
and then we put it through another linear layer and

358
00:16:57,720 --> 00:17:01,860
the softmax function and now we have an output probability distribution y hat.

359
00:17:01,860 --> 00:17:05,925
And in our case because we're trying to predict what word comes next, ah, ah,

360
00:17:05,925 --> 00:17:08,430
vector y hat will be of length v where v is

361
00:17:08,430 --> 00:17:10,020
the vocabulary and it will contain

362
00:17:10,020 --> 00:17:12,555
the probabilities of all the different words in the vocabulary.

363
00:17:12,555 --> 00:17:15,600
So here I've represented that as a bar charts where if you suppose

364
00:17:15,600 --> 00:17:18,690
you've got all of the words listed alphabetically from a to z,

365
00:17:18,690 --> 00:17:21,300
and then there's the different probabilities of the words.

366
00:17:21,300 --> 00:17:22,845
So if everything goes well,

367
00:17:22,845 --> 00:17:24,480
then this language model should tell us that

368
00:17:24,480 --> 00:17:27,930
some likely next words are books and laptops, for example.

369
00:17:27,930 --> 00:17:29,940
So none of this should be, um,

370
00:17:29,940 --> 00:17:31,770
unfamiliar to you because you saw it all last week.

371
00:17:31,770 --> 00:17:36,100
We're just applying a Window-based model to a different task,  such as language modeling.

372
00:17:36,470 --> 00:17:38,940
Okay, so what are,

373
00:17:38,940 --> 00:17:42,240
some good things about this model compared to n-gram language models?

374
00:17:42,240 --> 00:17:46,305
So one, ah, advantage I'd say is that there's no sparsity problem.

375
00:17:46,305 --> 00:17:49,695
If you remember an n-gram language model has a sparsity problem

376
00:17:49,695 --> 00:17:53,205
which is that if you've never seen a particular n-gram in training then,

377
00:17:53,205 --> 00:17:55,005
you can't assign any probability to it.

378
00:17:55,005 --> 00:17:56,445
You don't have any data on it.

379
00:17:56,445 --> 00:17:59,340
Whereas at least here you can take any, you know, for example,

380
00:17:59,340 --> 00:18:02,115
4-gram you want and you can feed it into the, ah,

381
00:18:02,115 --> 00:18:03,795
the neural nets and it will give you

382
00:18:03,795 --> 00:18:06,150
an output distribution of what it thinks the next word would be.

383
00:18:06,150 --> 00:18:10,245
It might not be a good prediction but at least it will, it will run.

384
00:18:10,245 --> 00:18:12,930
Another advantage is you don't need to store

385
00:18:12,930 --> 00:18:15,090
all of the observed n-grams that you ever saw.

386
00:18:15,090 --> 00:18:17,280
So, uh, this an advantage by, uh,

387
00:18:17,280 --> 00:18:19,230
comparison you just have to store

388
00:18:19,230 --> 00:18:22,155
all of the word vectors for all the words in your vocabulary.

389
00:18:22,155 --> 00:18:26,085
Uh, but there are quite a lot of problems with this fixed-window language model.

390
00:18:26,085 --> 00:18:29,160
So here are some remaining problems: Uh,

391
00:18:29,160 --> 00:18:31,470
one is that your fixed window is probably too small.

392
00:18:31,470 --> 00:18:33,885
No matter how big you make your fixed window, uh,

393
00:18:33,885 --> 00:18:35,640
you're probably going to be losing some kind of

394
00:18:35,640 --> 00:18:38,490
useful context that you would want to use sometimes.

395
00:18:38,490 --> 00:18:41,745
And in fact, if you try to enlarge the window size,

396
00:18:41,745 --> 00:18:44,175
then you also have to enlarge the size of your,

397
00:18:44,175 --> 00:18:45,480
uh, weight factor, sorry,

398
00:18:45,480 --> 00:18:47,580
your weight matrix W. Uh,

399
00:18:47,580 --> 00:18:49,590
so the width of W because you're multiplying it

400
00:18:49,590 --> 00:18:52,110
by e which is the concatenation of your word embeddings.

401
00:18:52,110 --> 00:18:56,230
The width of W grows as you increase the size of your window.

402
00:18:56,390 --> 00:19:01,210
So in inclusion really your window can never be large enough.

403
00:19:01,280 --> 00:19:05,460
Another problem with this model which is more of a subtle point is that

404
00:19:05,460 --> 00:19:08,820
X1 and X2 and really all of the words in the window they're,

405
00:19:08,820 --> 00:19:11,100
uh, multiplied by completely diffe rent weights in

406
00:19:11,100 --> 00:19:14,565
W. So to demonstrate this you could draw a picture.

407
00:19:14,565 --> 00:19:17,610
So the problem is that if you have

408
00:19:17,610 --> 00:19:21,720
your weight matrix W and then you have

409
00:19:21,720 --> 00:19:26,910
your concatenation of embeddings e and we have, uh, four embeddings.

410
00:19:26,910 --> 00:19:30,390
So we have e_1, e_2, e_3,

411
00:19:30,390 --> 00:19:33,135
e_4, and you multiply, uh,

412
00:19:33,135 --> 00:19:36,615
the concatenated embeddings by the weight matrix.

413
00:19:36,615 --> 00:19:39,120
So really you can see that there are essentially

414
00:19:39,120 --> 00:19:42,449
kind of four sections of the weight matrix,

415
00:19:42,449 --> 00:19:45,570
and the first word embedding e_1 is only

416
00:19:45,570 --> 00:19:48,825
ever multiplied by the weights for it in this section,

417
00:19:48,825 --> 00:19:53,025
and that's completely separate to the weights that multiply by e_2 and so forth.

418
00:19:53,025 --> 00:19:56,700
So the problem with this is that what you

419
00:19:56,700 --> 00:20:00,060
learn in the weight matrix in one section is not shared with the others.

420
00:20:00,060 --> 00:20:03,985
You're kind of learning a lot of similar functions four times.

421
00:20:03,985 --> 00:20:07,910
So the reason why we think this is a problem is because there should be a lot of

422
00:20:07,910 --> 00:20:12,130
commonalities in how you process the incoming word embeddings.

423
00:20:12,130 --> 00:20:14,880
So what you learn about how to process, you know,

424
00:20:14,880 --> 00:20:18,375
the third embedding, some of it at least should be shared with all of the embeddings.

425
00:20:18,375 --> 00:20:21,960
So what I'm saying is it's kind of inefficient that we're learning, uh,

426
00:20:21,960 --> 00:20:24,300
all of these separate weights for these different words

427
00:20:24,300 --> 00:20:27,970
when there's a lot of commonalities between them. Is there a question?

428
00:20:29,840 --> 00:20:31,180
So that's why [inaudible] [NOISE].

429
00:20:31,180 --> 00:20:31,965
Okay-

430
00:20:31,965 --> 00:20:36,560
Yeah, hopefully- hopefully the verbal description is on.

431
00:20:38,280 --> 00:20:42,310
So, in conclusion, I'd say that the biggest problem that we've got with

432
00:20:42,310 --> 00:20:45,280
this fixed-size neural model is that clearly we

433
00:20:45,280 --> 00:20:48,355
need some kind of neural architecture that can process any length input,

434
00:20:48,355 --> 00:20:51,070
because most of the problems here come from the fact that we had to make

435
00:20:51,070 --> 00:20:54,920
this simplifying assumption that there was a fixed window.

436
00:20:56,670 --> 00:21:00,040
Okay. So this motivates, uh,

437
00:21:00,040 --> 00:21:02,590
us to introduce this new family of neural architecture,

438
00:21:02,590 --> 00:21:05,515
it's called recurrent neural networks or RNNs.

439
00:21:05,515 --> 00:21:09,100
So, this is a simplified diagram that shows you the most important,

440
00:21:09,100 --> 00:21:11,320
um, features of an RNN.

441
00:21:11,320 --> 00:21:15,070
So we have again an input sequence of X1, X2,

442
00:21:15,070 --> 00:21:20,245
et cetera, but you can assume that this sequence is of any arbitrary length you like.

443
00:21:20,245 --> 00:21:24,460
The idea is that you have a sequence of hidden states instead of just having,

444
00:21:24,460 --> 00:21:27,175
for example, one hidden state as we did in the previous model.

445
00:21:27,175 --> 00:21:30,940
We have a sequence of hidden states and we have as many of them as we have inputs.

446
00:21:30,940 --> 00:21:35,440
And the important thing is that each hidden state ht is computed based

447
00:21:35,440 --> 00:21:40,315
on the previous hidden state and also the input on that step.

448
00:21:40,315 --> 00:21:44,050
So the reason why they're called hidden states is because you could think of

449
00:21:44,050 --> 00:21:47,425
this as a single state that's mutating over time.

450
00:21:47,425 --> 00:21:50,260
It's kind of like several versions of the same thing.

451
00:21:50,260 --> 00:21:53,830
And for this reason, we often call these time-steps, right?

452
00:21:53,830 --> 00:21:55,540
So these steps that go left to right,

453
00:21:55,540 --> 00:21:57,860
we often call them time-steps.

454
00:21:58,950 --> 00:22:01,870
So the really important thing is that

455
00:22:01,870 --> 00:22:07,210
the same weight matrix W is applied on every time-step of this RNN.

456
00:22:07,210 --> 00:22:11,365
That's what makes us able to process any length input we want.

457
00:22:11,365 --> 00:22:13,930
Is because we don't have to have different weights on every step,

458
00:22:13,930 --> 00:22:17,990
because we just apply the exact same transformation on every step.

459
00:22:18,870 --> 00:22:22,690
So additionally, you can also have some outputs from the RNN.

460
00:22:22,690 --> 00:22:23,995
So these y hats,

461
00:22:23,995 --> 00:22:26,155
these are the outputs on each step.

462
00:22:26,155 --> 00:22:28,735
And they're optional because you don't have to compute them

463
00:22:28,735 --> 00:22:31,210
or you can compute them on just some steps and not others.

464
00:22:31,210 --> 00:22:34,160
It depends on where you want to use your RNN to do.

465
00:22:34,920 --> 00:22:38,260
Okay. So that's a simple diagram of an RNN.

466
00:22:38,260 --> 00:22:39,850
Uh, here I'm going to give you a bit more detail.

467
00:22:39,850 --> 00:22:43,630
So here's how you would apply an RNN to do language modeling.

468
00:22:43,630 --> 00:22:48,175
So, uh, again, let's suppose that we have some kind of text so far.

469
00:22:48,175 --> 00:22:50,860
My text is only four words long,

470
00:22:50,860 --> 00:22:53,320
but you can assume that it could be any length, right?

471
00:22:53,320 --> 00:22:55,420
It's just short because we can't fit more on the slide.

472
00:22:55,420 --> 00:22:58,390
So you have some sequence of tags, which could be kind of long.

473
00:22:58,390 --> 00:23:02,020
And again, we're going to represent these by some kind of one-hot vectors and

474
00:23:02,020 --> 00:23:06,460
use those to look up the word embeddings from our embedding matrix.

475
00:23:06,460 --> 00:23:10,370
So then to compute the first hidden state H1,

476
00:23:10,370 --> 00:23:14,300
we need to compute it based on the previous hidden state and the current input.

477
00:23:14,300 --> 00:23:16,615
We already have the current input, that's E1,

478
00:23:16,615 --> 00:23:19,570
but the question is where do we get this first hidden state from?

479
00:23:19,570 --> 00:23:21,160
All right, what comes before H1?

480
00:23:21,160 --> 00:23:24,670
So we often call the initial hidden state H0, uh, yes,

481
00:23:24,670 --> 00:23:28,015
we call the initial hidden state and it can either be something that you learn,

482
00:23:28,015 --> 00:23:32,065
like it's a parameter of the network and you learn how to initialize it,

483
00:23:32,065 --> 00:23:35,395
or you can assume something like maybe it's the zero vector.

484
00:23:35,395 --> 00:23:40,495
So the formula we use to compute the new hidden state based on the previous one,

485
00:23:40,495 --> 00:23:43,195
and also the current inputs is written on the left.

486
00:23:43,195 --> 00:23:46,690
So you do a linear transformation on the previous hidden state and on

487
00:23:46,690 --> 00:23:48,640
the current input and then you add some kind of

488
00:23:48,640 --> 00:23:50,919
bias and then put it through a non-linearity,

489
00:23:50,919 --> 00:23:52,990
like for example, the sigmoid function.

490
00:23:52,990 --> 00:23:55,700
And that gives you a new hidden state.

491
00:23:56,670 --> 00:23:59,470
Okay. So, once you've done that,

492
00:23:59,470 --> 00:24:01,480
then you can compute the next hidden state and you

493
00:24:01,480 --> 00:24:03,850
can keep unrolling the network like this.

494
00:24:03,850 --> 00:24:06,025
And that's, uh, yeah,

495
00:24:06,025 --> 00:24:07,450
that's called unrolling because you're kind of

496
00:24:07,450 --> 00:24:10,270
computing each step given the previous one.

497
00:24:10,270 --> 00:24:12,160
All right. So finally, if you remember,

498
00:24:12,160 --> 00:24:13,330
we're trying to do language modeling.

499
00:24:13,330 --> 00:24:17,530
So we're trying to predict which words should come next after the students opened their.

500
00:24:17,530 --> 00:24:19,870
So on this fourth step over here,

501
00:24:19,870 --> 00:24:21,205
we can use, uh,

502
00:24:21,205 --> 00:24:22,825
the current hidden state, H4,

503
00:24:22,825 --> 00:24:27,430
and put it through a linear layer and put it through a softmax function and then we get

504
00:24:27,430 --> 00:24:32,800
our output distribution Y-hat 4 which is a distribution over the vocabulary.

505
00:24:32,800 --> 00:24:34,720
And again, hopefully, we'll get some kind of

506
00:24:34,720 --> 00:24:38,080
sensible estimates for what the next word might be.

507
00:24:38,080 --> 00:24:43,210
Any questions at this point. Yep?

508
00:24:43,210 --> 00:24:47,650
Is the- the number of hidden state or is it gonna be the number of words in your input?

509
00:24:47,650 --> 00:24:50,845
The question is, is the number of hidden states the number of words in your input?

510
00:24:50,845 --> 00:24:53,485
Yeah, in this setting here, uh, yes,

511
00:24:53,485 --> 00:24:58,405
or you could say more generally the number of hidden states is the number of inputs. Yep.

512
00:24:58,405 --> 00:24:59,950
And just as with the n-gram model,

513
00:24:59,950 --> 00:25:05,590
we could use the output as the input from the tasks mutation in transformational model?

514
00:25:05,590 --> 00:25:07,000
Yeah, so the question is,

515
00:25:07,000 --> 00:25:08,650
as with the n-gram language model,

516
00:25:08,650 --> 00:25:10,570
could we use the output as the input on the next step?

517
00:25:10,570 --> 00:25:12,715
And the answer is yes, and I'll show you that in a minute.

518
00:25:12,715 --> 00:25:15,700
Any other questions? Yeah.

519
00:25:15,700 --> 00:25:17,995
Are you learning the embedding?

520
00:25:17,995 --> 00:25:20,560
The question is, are you learning the embeddings?

521
00:25:20,560 --> 00:25:21,925
Um, that's a choice.

522
00:25:21,925 --> 00:25:23,770
You could have the embeddings be for example,

523
00:25:23,770 --> 00:25:27,370
pre-generated embeddings that you download and you use those and they're frozen,

524
00:25:27,370 --> 00:25:28,750
or maybe you could download them,

525
00:25:28,750 --> 00:25:30,190
but then you could fine-tune them.

526
00:25:30,190 --> 00:25:32,200
That is,  allow them to be changed as parameters of

527
00:25:32,200 --> 00:25:35,170
the network or you could initialize them to,

528
00:25:35,170 --> 00:25:38,560
you know, small, uh, random values and learn them from scratch.

529
00:25:38,560 --> 00:25:40,570
Any other questions? Yeah.

530
00:25:40,570 --> 00:25:43,690
So you said you use the same delta matrix,

531
00:25:43,690 --> 00:25:45,490
like you do back propagation,

532
00:25:45,490 --> 00:25:48,030
does that you only update like WE,

533
00:25:48,030 --> 00:25:51,080
or do you update both WH and WE?

534
00:25:51,080 --> 00:25:56,085
So the question is, you say we reuse the matrix, do we update WE and WH, or just one?

535
00:25:56,085 --> 00:25:58,980
So you suddenly learn both WE and WH.

536
00:25:58,980 --> 00:26:01,410
I suppose I was emphasizing WH more, but yeah,

537
00:26:01,410 --> 00:26:04,090
they're both matrices that are applied repeatedly.

538
00:26:04,090 --> 00:26:05,500
There was also a question about back-prop,

539
00:26:05,500 --> 00:26:07,675
but we're going to cover that later in this lecture.

540
00:26:07,675 --> 00:26:12,250
Okay, moving on for now. Um, so,

541
00:26:12,250 --> 00:26:17,530
what are some advantages and disadvantages of this RNN language model?

542
00:26:17,530 --> 00:26:23,005
So here are some advantages that we can see in comparison to the fixed window one.

543
00:26:23,005 --> 00:26:28,210
So an obvious advantage is that this RNN can process any length of input.

544
00:26:28,210 --> 00:26:31,180
Another advantage is that the computation for

545
00:26:31,180 --> 00:26:35,050
step t can in theory use information from many steps back.

546
00:26:35,050 --> 00:26:36,730
So in our motivation example,

547
00:26:36,730 --> 00:26:38,650
which was as the proctor started the clock,

548
00:26:38,650 --> 00:26:39,970
the students opened their.

549
00:26:39,970 --> 00:26:42,250
We think that proctor and maybe clock are

550
00:26:42,250 --> 00:26:45,340
both pretty important hints for what might be coming up next.

551
00:26:45,340 --> 00:26:47,275
So, at least in theory,

552
00:26:47,275 --> 00:26:49,390
the hidden state at the end

553
00:26:49,390 --> 00:26:54,950
can have access to the information from the input from many steps ago.

554
00:26:55,350 --> 00:26:59,785
Another advantage is that the model size doesn't increase for longer inputs.

555
00:26:59,785 --> 00:27:02,485
So, uh, the size of the model is actually fixed.

556
00:27:02,485 --> 00:27:05,005
It's just WH and WE,s

557
00:27:05,005 --> 00:27:09,400
and then also the biases and also the embedding matrix, if you're counting that.

558
00:27:09,400 --> 00:27:13,000
None of those get bigger if you want to apply it to more,

559
00:27:13,000 --> 00:27:17,300
uh, longer inputs because you just apply the same weights repeatedly.

560
00:27:18,030 --> 00:27:23,995
And another advantage is that you have the same weights applied on every time-step.

561
00:27:23,995 --> 00:27:29,425
So I said this thing before about how the fixed-sized window neural model,

562
00:27:29,425 --> 00:27:31,720
it was less efficient because it was applying

563
00:27:31,720 --> 00:27:34,270
different weights of the weight matrix to the different,

564
00:27:34,270 --> 00:27:35,905
uh, words in the window.

565
00:27:35,905 --> 00:27:38,470
And the advantage about this RNN is that it's

566
00:27:38,470 --> 00:27:41,650
applying the exact same transformation to each of the inputs.

567
00:27:41,650 --> 00:27:45,835
So this means that if it learns a good way to process one input,

568
00:27:45,835 --> 00:27:48,010
that is applied to every input in the sequence.

569
00:27:48,010 --> 00:27:50,630
So you can see it as more efficient in that way.

570
00:27:51,480 --> 00:27:54,805
Okay, so what are the disadvantages of this model?

571
00:27:54,805 --> 00:27:58,270
One is that recurrent computation is pretty slow.

572
00:27:58,270 --> 00:27:59,995
Uh, as you saw before,

573
00:27:59,995 --> 00:28:03,865
you have to compute the hidden state based on the previous hidden state.

574
00:28:03,865 --> 00:28:06,925
So this means that you can't compute all of the hidden states in parallel.

575
00:28:06,925 --> 00:28:08,665
You have to compute them in sequence.

576
00:28:08,665 --> 00:28:13,120
So, especially if you're trying to compute an RNN over a pretty long sequence of inputs,

577
00:28:13,120 --> 00:28:16,660
this means that the RNN can be pretty slow to compute.

578
00:28:16,660 --> 00:28:20,425
Another disadvantage of RNNs is that it tuns out,

579
00:28:20,425 --> 00:28:24,175
in practice, it's quite difficult to access information from many steps back.

580
00:28:24,175 --> 00:28:26,290
So even though I said we should be able to remember about

581
00:28:26,290 --> 00:28:28,930
the proctor and the clock and use that to predict exams and our books,

582
00:28:28,930 --> 00:28:30,430
it turns out that RNNs,

583
00:28:30,430 --> 00:28:32,470
at least the ones that I've presented in this lecture,

584
00:28:32,470 --> 00:28:35,305
are not as good as that as you would think.

585
00:28:35,305 --> 00:28:39,295
Um, we're gonna learn more about both of these disadvantages later in the course,

586
00:28:39,295 --> 00:28:42,610
and we're going to learn something about how you can try to fix them.

587
00:28:42,610 --> 00:28:46,900
Have we gotten any questions at this point? Yep.

588
00:28:46,900 --> 00:28:48,010
Why do we assume that WH are the same?

589
00:28:48,010 --> 00:28:51,265
Sorry, can you speak up?

590
00:28:51,265 --> 00:28:55,900
Why do we assume that the WH should be the same?

591
00:28:55,900 --> 00:28:59,635
So the question is, why should you assume that the WH are the same?

592
00:28:59,635 --> 00:29:01,450
I suppose, it's not exactly an assumption,

593
00:29:01,450 --> 00:29:04,390
it's more a deliberate decision in the design of an RNN.

594
00:29:04,390 --> 00:29:06,460
So, an RNN is by definition,

595
00:29:06,460 --> 00:29:10,450
a network where you apply the exact same weights on every step.

596
00:29:10,450 --> 00:29:13,800
So, I suppose the question why do you assume maybe should be,

597
00:29:13,800 --> 00:29:15,225
why is that a good idea?

598
00:29:15,225 --> 00:29:17,520
Um, so I spoke a little bit about why it's a good idea,

599
00:29:17,520 --> 00:29:18,690
and this list of advantages,

600
00:29:18,690 --> 00:29:23,950
I suppose, are the reasons why you'd want to do that. Does that answer your question?

601
00:29:24,560 --> 00:29:29,020
Open their books, right? If you assume that WH are the same,

602
00:29:29,020 --> 00:29:31,420
you mean that like, uh,

603
00:29:31,420 --> 00:29:34,660
Markov chain, it's like a Markov chain.

604
00:29:34,660 --> 00:29:37,780
Uh, the trans- transmit, uh,

605
00:29:37,780 --> 00:29:42,955
trans- transfer probability for the human moods open,

606
00:29:42,955 --> 00:29:44,890
they are the same,

607
00:29:44,890 --> 00:29:50,940
but actually the Markov chain.

608
00:29:50,940 --> 00:29:56,535
The model, [inaudible] the transfer probability for that is the same,

609
00:29:56,535 --> 00:30:00,895
so [inaudible] probability,

610
00:30:00,895 --> 00:30:07,105
it- it's just an approximation but it's another test.

611
00:30:07,105 --> 00:30:08,240
Okay. So I think that [OVERLAPPING]

612
00:30:08,240 --> 00:30:10,810
If you assume WH could be the same,

613
00:30:10,810 --> 00:30:14,725
it's good because you used a number of parameters,

614
00:30:14,725 --> 00:30:20,560
but this is just an, this is just an approximation.

615
00:30:20,560 --> 00:30:23,410
The underlying transfer, uh,

616
00:30:23,410 --> 00:30:25,660
probability, it shouldn't be the same. Especially [OVERLAPPING]

617
00:30:25,660 --> 00:30:28,835
Okay. Um, so I think the question is saying that given the- these

618
00:30:28,835 --> 00:30:30,540
words the students opened their

619
00:30:30,540 --> 00:30:32,490
are all different and they're happening in different context,

620
00:30:32,490 --> 00:30:35,850
then why should we be applying the same transformation each time?

621
00:30:35,850 --> 00:30:37,440
So that's a- that's a good question.

622
00:30:37,440 --> 00:30:41,670
I think, uh, the idea is that you are learning a general function, not just, you know,

623
00:30:41,670 --> 00:30:43,535
how to deal with students,

624
00:30:43,535 --> 00:30:46,090
the one-word students in this one context.

625
00:30:46,090 --> 00:30:48,520
We're trying to learn a general function of how you

626
00:30:48,520 --> 00:30:51,070
should deal with a word given the word so far.

627
00:30:51,070 --> 00:30:55,090
You're trying to learn a general representation of language and context so far,

628
00:30:55,090 --> 00:30:57,055
which is indeed a very difficult problem.

629
00:30:57,055 --> 00:31:00,175
Um, I think you also mentioned that something about an approximation.

630
00:31:00,175 --> 00:31:01,780
Uh, another thing to note is that all of

631
00:31:01,780 --> 00:31:04,570
the hidden states are vectors, they're not just single numbers, right?

632
00:31:04,570 --> 00:31:06,670
They are vectors of lengths, I don't know, 500 or something?

633
00:31:06,670 --> 00:31:09,610
So they have quite a large capacity to hold lots of information about

634
00:31:09,610 --> 00:31:13,530
different things in all of their different, um, positions.

635
00:31:13,530 --> 00:31:15,630
So, I think the idea is that you can

636
00:31:15,630 --> 00:31:18,255
store a lot of different information in different contexts,

637
00:31:18,255 --> 00:31:19,830
in different parts of the hidden state,

638
00:31:19,830 --> 00:31:21,960
but it is indeed an approximation and there is

639
00:31:21,960 --> 00:31:24,575
some kind of limit to how much information you can store.

640
00:31:24,575 --> 00:31:26,845
Okay, any other questions? Yes.

641
00:31:26,845 --> 00:31:29,410
Since you kinda process any single length frame,

642
00:31:29,410 --> 00:31:31,135
what length do you use during your training?

643
00:31:31,135 --> 00:31:35,035
And does the length you use for training affect WH?

644
00:31:35,035 --> 00:31:39,355
Okay, so, the question is, given that you can have any length input,

645
00:31:39,355 --> 00:31:41,950
what length is the input during training?

646
00:31:41,950 --> 00:31:44,185
So, I suppose in practice,

647
00:31:44,185 --> 00:31:46,510
you choose how long the inputs are in

648
00:31:46,510 --> 00:31:49,630
training either based on what your data is or maybe based on,

649
00:31:49,630 --> 00:31:52,615
uh, your efficiency concerns so maybe you make it artificially

650
00:31:52,615 --> 00:31:55,900
shorter by chopping it up. Um, what was the other question?

651
00:31:55,900 --> 00:31:58,360
Uh, does WH depend on that?

652
00:31:58,360 --> 00:32:01,255
Okay. So the question was, does WH depend on the length you used?

653
00:32:01,255 --> 00:32:04,075
So, no, and that's one of the good things in the advantages list.

654
00:32:04,075 --> 00:32:07,165
Is that the model size doesn't increase for longer input,

655
00:32:07,165 --> 00:32:09,040
because we just unroll the RNN

656
00:32:09,040 --> 00:32:11,245
applying the same weights again and again for as long as we'd like.

657
00:32:11,245 --> 00:32:13,930
There's no need to have more weights just because you have a longer input.

658
00:32:13,930 --> 00:32:16,795
[NOISE] Yeah.

659
00:32:16,795 --> 00:32:24,235
So how the ratios that you mentioned are [inaudible] the number of words.

660
00:32:24,235 --> 00:32:28,405
[NOISE] Are you asking about capital E or the lowercase E?

661
00:32:28,405 --> 00:32:29,485
Uh, lowercase E.

662
00:32:29,485 --> 00:32:30,790
Okay. So, the question is,

663
00:32:30,790 --> 00:32:32,890
how do we choose the dimension of the lowercase Es?

664
00:32:32,890 --> 00:32:34,300
Uh, so, you could, for example,

665
00:32:34,300 --> 00:32:37,120
assume that those are just pre-trained word vectors like the ones that you,

666
00:32:37,120 --> 00:32:38,815
uh, used in assignment one.

667
00:32:38,815 --> 00:32:39,715
More like word2vec.

668
00:32:39,715 --> 00:32:41,140
Yeah. For example, word2vec,

669
00:32:41,140 --> 00:32:42,610
and you just download them and use them,

670
00:32:42,610 --> 00:32:44,380
or maybe you learn them from scratch, in which case,

671
00:32:44,380 --> 00:32:46,930
you decide at the beginning of training how big you want those vectors to be.

672
00:32:46,930 --> 00:32:49,210
[NOISE] Okay. I'm gonna move on for now.

673
00:32:49,210 --> 00:32:54,895
[NOISE] So, we've learned what an RNN language model is and we've learned how you would,

674
00:32:54,895 --> 00:32:56,845
uh, run one forward, but the question remains,

675
00:32:56,845 --> 00:32:59,080
how would you train an RNN language model?

676
00:32:59,080 --> 00:33:02,230
How would you learn it? [NOISE]

677
00:33:02,230 --> 00:33:03,850
So, as always, in machine learning,

678
00:33:03,850 --> 00:33:06,670
our answer starts with, you're going to get a big corpus of text,

679
00:33:06,670 --> 00:33:11,230
and we're gonna call that just a sequence of words X1 up to X capital T. So,

680
00:33:11,230 --> 00:33:15,115
you feed the sequence of words into the RNN language model, and then,

681
00:33:15,115 --> 00:33:19,615
the idea is that you compute the output distribution Y-hat T for every step T. So,

682
00:33:19,615 --> 00:33:21,700
I know that the picture I showed on the previous, uh,

683
00:33:21,700 --> 00:33:23,560
slide [NOISE] only showed us doing on the last step,

684
00:33:23,560 --> 00:33:26,140
but the idea is, you would actually compute this on every step.

685
00:33:26,140 --> 00:33:28,420
So, this means that you're actually predicting

686
00:33:28,420 --> 00:33:31,000
the probability of the next word on every step.

687
00:33:31,000 --> 00:33:33,130
[NOISE] Okay.

688
00:33:33,130 --> 00:33:35,515
So, once you've done that, then you can define the loss function,

689
00:33:35,515 --> 00:33:37,120
and this should be familiar to you by now.

690
00:33:37,120 --> 00:33:39,190
Uh, this is the cross-entropy between [NOISE]

691
00:33:39,190 --> 00:33:43,915
our predicted probability distribution Y-hat T and the true, uh,

692
00:33:43,915 --> 00:33:47,260
distribution, which is Y-hat- sorry, just YT,

693
00:33:47,260 --> 00:33:49,570
which is a one-hot vector, uh,

694
00:33:49,570 --> 00:33:51,055
representing the true next [NOISE] words,

695
00:33:51,055 --> 00:33:52,495
which is XT plus one.

696
00:33:52,495 --> 00:33:54,490
So, as you've seen before, this, uh,

697
00:33:54,490 --> 00:33:57,100
cross-entropy [NOISE] between those two vectors can be written

698
00:33:57,100 --> 00:34:00,640
also as a negative log probability.

699
00:34:00,640 --> 00:34:05,635
And then, lastly, if you average this cross-entropy loss across every step, uh,

700
00:34:05,635 --> 00:34:08,740
every T in the corpus time step T, then,

701
00:34:08,740 --> 00:34:11,800
uh, this gives you your overall loss for the entire training set.

702
00:34:11,800 --> 00:34:16,360
[NOISE] Okay.

703
00:34:16,360 --> 00:34:18,475
So, just to make that even more clear with a picture,

704
00:34:18,475 --> 00:34:20,080
uh, suppose that our corpus is,

705
00:34:20,080 --> 00:34:21,370
the students open their exams,

706
00:34:21,370 --> 00:34:23,020
et cetera, and it goes on for a long time.

707
00:34:23,020 --> 00:34:24,550
Then, what we'd be doing is,

708
00:34:24,550 --> 00:34:26,980
we'd be running our RNN over this text, and then,

709
00:34:26,980 --> 00:34:30,535
on every step, we would be predicting the probability [NOISE] distribution Y-hats,

710
00:34:30,535 --> 00:34:31,780
and then, from each of those,

711
00:34:31,780 --> 00:34:33,310
you can calculate what your loss is,

712
00:34:33,310 --> 00:34:36,400
which is the JT, and then, uh, on the first step,

713
00:34:36,400 --> 00:34:38,965
the loss would be the negative log probability of the next word,

714
00:34:38,965 --> 00:34:40,060
which is, in this example,

715
00:34:40,060 --> 00:34:42,040
students, [NOISE] and so on.

716
00:34:42,040 --> 00:34:45,070
Each of those is the negative log probability of the next word.

717
00:34:45,070 --> 00:34:47,515
[NOISE] And then, once you've computed all of those,

718
00:34:47,515 --> 00:34:49,585
you can add them [NOISE] all up and average them,

719
00:34:49,585 --> 00:34:51,160
and then, this gives you your final loss.

720
00:34:51,160 --> 00:34:56,260
[NOISE] Okay. So, there's a caveat here.

721
00:34:56,260 --> 00:34:59,935
Um, computing the loss and gradients across the entire corpus,

722
00:34:59,935 --> 00:35:02,350
all of those words X1 up to X capital T is too

723
00:35:02,350 --> 00:35:04,840
expensive [NOISE] because your corpus is probably really big.

724
00:35:04,840 --> 00:35:07,810
[NOISE] So, um, as a student asked earlier,

725
00:35:07,810 --> 00:35:10,555
uh, in practice, what do you actually regard as your sequence?

726
00:35:10,555 --> 00:35:12,580
So, in practice, you might regard your sequence as, uh,

727
00:35:12,580 --> 00:35:14,590
something like a sentence or a document,

728
00:35:14,590 --> 00:35:17,270
some shorter unit of text.

729
00:35:17,430 --> 00:35:20,890
So, uh, another thing you'll do [NOISE] is, if you remember,

730
00:35:20,890 --> 00:35:23,785
stochastic gradient descent allows you to compute gradients

731
00:35:23,785 --> 00:35:26,980
for small chunks of data rather than the whole corpus at a time.

732
00:35:26,980 --> 00:35:29,275
So, in practice, if you're training a language model,

733
00:35:29,275 --> 00:35:32,830
what you're actually likely to be doing is computing the loss for a sentence,

734
00:35:32,830 --> 00:35:35,290
but that's actually a batch of sentences, and then,

735
00:35:35,290 --> 00:35:37,945
you compute the gradients with respect to that batch of sentences,

736
00:35:37,945 --> 00:35:39,760
update your weights, and repeat.

737
00:35:39,760 --> 00:35:46,405
Any questions at this point? [NOISE] Okay.

738
00:35:46,405 --> 00:35:48,040
So, uh, moving onto backprop.

739
00:35:48,040 --> 00:35:51,055
Don't worry, there won't be as much backprop as there was last week,

740
00:35:51,055 --> 00:35:53,230
but, uh, there's an interesting question here, right?

741
00:35:53,230 --> 00:35:55,899
So, the, uh, characteristic thing about RNNs

742
00:35:55,899 --> 00:35:58,975
is that they apply the same weight matrix repeatedly.

743
00:35:58,975 --> 00:36:00,280
So, the question is,

744
00:36:00,280 --> 00:36:02,215
[NOISE] what's the derivative of our loss function,

745
00:36:02,215 --> 00:36:03,610
let's say, on step T?

746
00:36:03,610 --> 00:36:08,635
What's the derivative of that loss with respect to the repeated weight matrix WH?

747
00:36:08,635 --> 00:36:13,570
So, the answer is that the derivative of the loss, uh,

748
00:36:13,570 --> 00:36:16,390
the gradient with respect to the repeated weight is

749
00:36:16,390 --> 00:36:19,780
the sum of the gradient with respect to each time it appears,

750
00:36:19,780 --> 00:36:21,355
and that's what that equation says.

751
00:36:21,355 --> 00:36:25,615
So, on the right, the notation with the vertical line and the I is saying, uh,

752
00:36:25,615 --> 00:36:30,670
the derivative of the loss with respect to WH when it appears on the Ith step.

753
00:36:30,670 --> 00:36:32,770
Okay. So, so, why is that true?

754
00:36:32,770 --> 00:36:35,260
[NOISE] Uh, to sketch why this is true,

755
00:36:35,260 --> 00:36:37,840
uh, [NOISE] I'm gonna remind you of the multivariable chain rule.

756
00:36:37,840 --> 00:36:42,535
So, uh, this is a screenshot from a Khan Academy article on the multivariable chain rule,

757
00:36:42,535 --> 00:36:44,440
and, uh, I advise you check it out if you

758
00:36:44,440 --> 00:36:46,630
want to learn more because it's very easy to understand.

759
00:36:46,630 --> 00:36:48,220
Uh, and what it says is,

760
00:36:48,220 --> 00:36:52,045
given a function F [NOISE] which depends on X and Y,

761
00:36:52,045 --> 00:36:56,140
which are both themselves functions of some variable T, then,

762
00:36:56,140 --> 00:36:59,430
if you want to get the derivative of F with respect to T,

763
00:36:59,430 --> 00:37:04,380
then you need to do the chain ru- rule across X and Y separately and then add them up.

764
00:37:04,380 --> 00:37:07,020
[NOISE] So, that's the multivariable chain rule,

765
00:37:07,020 --> 00:37:10,510
[NOISE] and if we apply this to our scenario with trying to take

766
00:37:10,510 --> 00:37:14,889
the derivative of the loss JT with respect to our weight matrix WH,

767
00:37:14,889 --> 00:37:19,300
then you could view it as this kind of diagram [NOISE] where WH has, uh,

768
00:37:19,300 --> 00:37:22,810
a relationship with all of these individual appearances of WH,

769
00:37:22,810 --> 00:37:23,860
but it's a [NOISE] simple relationship,

770
00:37:23,860 --> 00:37:25,495
it's just equality, and then,

771
00:37:25,495 --> 00:37:29,690
each of those appearances of WH affect the loss in different ways.

772
00:37:29,690 --> 00:37:34,080
So, then, if we apply the multivariable chain rule,

773
00:37:34,080 --> 00:37:37,470
then it says that the derivative of the loss with respect to

774
00:37:37,470 --> 00:37:41,190
WH is the sum of those chain rule things,

775
00:37:41,190 --> 00:37:45,600
but the expression on the right is just one because it's an equality relation,

776
00:37:45,600 --> 00:37:50,480
[NOISE] and then, that gives us the equation that I wrote on the previous slide.

777
00:37:50,480 --> 00:37:55,240
So, this is a proof sketch for why the derivative of the loss with

778
00:37:55,240 --> 00:38:00,565
respect to our recurrent matrix is the sum of the derivatives each time it appears.

779
00:38:00,565 --> 00:38:03,190
Okay. So, suppose you believe me on that, that is,

780
00:38:03,190 --> 00:38:04,555
how you compute the, uh,

781
00:38:04,555 --> 00:38:06,475
gradient with respect to the recurrent weight.

782
00:38:06,475 --> 00:38:08,440
So, a remaining question is, well,

783
00:38:08,440 --> 00:38:10,720
how [NOISE] do we actually calculate this in practice?

784
00:38:10,720 --> 00:38:16,660
[NOISE] So, the answer is that you're going to calculate this sum by doing backprop,

785
00:38:16,660 --> 00:38:19,390
uh, backwards, kind of right to left, um,

786
00:38:19,390 --> 00:38:23,590
through the RNN, and you're going to accumulate this sum as you go.

787
00:38:23,590 --> 00:38:24,940
So, the important thing is,

788
00:38:24,940 --> 00:38:28,435
you shouldn't compute each of those things separately, uh,

789
00:38:28,435 --> 00:38:30,880
you should compute them by accumulating, like,

790
00:38:30,880 --> 00:38:34,360
each one can be computed in form- in terms of the previous one.

791
00:38:34,360 --> 00:38:39,130
[NOISE] So, this algorithm of computing each of these,

792
00:38:39,130 --> 00:38:41,320
uh, each of these gradients with respect to

793
00:38:41,320 --> 00:38:44,305
the previous one is called backpropagation through time.

794
00:38:44,305 --> 00:38:47,650
And, um, I always think that this sounds way more sci-fi than it is.

795
00:38:47,650 --> 00:38:49,030
It sounds like it's time travel or something,

796
00:38:49,030 --> 00:38:50,560
but it's actually pretty simple.

797
00:38:50,560 --> 00:38:53,290
Uh, it's just the name you give to

798
00:38:53,290 --> 00:38:57,290
applying the backprop algorithm to a recurrent neural network.

799
00:38:57,960 --> 00:39:02,350
Any questions at this point? Yep. [NOISE]

800
00:39:02,350 --> 00:39:07,240
So, it seems that how you break up the batches matter your end result.

801
00:39:07,240 --> 00:39:15,700
[inaudible].

802
00:39:15,700 --> 00:39:21,460
So, if you break it into much more [inaudible].

803
00:39:21,460 --> 00:39:23,605
Okay. So the question is, um, surely,

804
00:39:23,605 --> 00:39:27,865
how you decide to break up your batches affects how you learn, right?

805
00:39:27,865 --> 00:39:29,560
Because if you choose, uh,

806
00:39:29,560 --> 00:39:31,660
one set of data to be your batch, right, then,

807
00:39:31,660 --> 00:39:33,880
you will make your update based on that, and then,

808
00:39:33,880 --> 00:39:36,760
you only update the next one based on [NOISE] where you go from there.

809
00:39:36,760 --> 00:39:38,950
So, if you decided to put different data in the batch,

810
00:39:38,950 --> 00:39:40,495
then you would have made a different step.

811
00:39:40,495 --> 00:39:42,910
So, that's true, [NOISE] and that is why

812
00:39:42,910 --> 00:39:45,910
stochastic gradient descent is only an approximation of

813
00:39:45,910 --> 00:39:49,660
true gradient descent because the gradient that you compute with

814
00:39:49,660 --> 00:39:53,950
respect to one batch is just an approximation of the true gradient with respect to the,

815
00:39:53,950 --> 00:39:56,095
uh, the loss over the whole corpus.

816
00:39:56,095 --> 00:39:58,165
So, yes, it's true that it's an approximation

817
00:39:58,165 --> 00:40:00,580
and how [NOISE] you choose to batch up your data can matter,

818
00:40:00,580 --> 00:40:03,040
and that's why, for example, shuffling your data is a good idea,

819
00:40:03,040 --> 00:40:05,575
and shuffling it differently, each epoch, is a good idea.

820
00:40:05,575 --> 00:40:09,130
Uh, but the, the core idea of SGD is [NOISE] that, um,

821
00:40:09,130 --> 00:40:12,085
it should be a good enough approximation that over many steps,

822
00:40:12,085 --> 00:40:14,740
you will, uh, minimize your loss.

823
00:40:14,740 --> 00:40:33,010
[NOISE] Any other questions? [NOISE] Yeah.

824
00:40:33,010 --> 00:40:35,410
[NOISE] So, is, uh, is the question,

825
00:40:35,410 --> 00:40:37,180
as you compute forward prop,

826
00:40:37,180 --> 00:40:40,345
do you start computing backprop before you've even, like, got to the loss?

827
00:40:40,345 --> 00:40:41,620
Is that the question? [NOISE]

828
00:40:41,620 --> 00:40:42,325
Yes.

829
00:40:42,325 --> 00:40:45,640
I didn't think so, right? Because you need to know what the loss is in

830
00:40:45,640 --> 00:40:49,030
order to compute the derivative of the loss with respect to something.

831
00:40:49,030 --> 00:40:50,560
So, I think you need to get to the end.

832
00:40:50,560 --> 00:40:51,760
So, if we assume simplicity,

833
00:40:51,760 --> 00:40:54,490
that there is only one loss which you get at the end of several steps,

834
00:40:54,490 --> 00:40:55,585
then you need to get to the end,

835
00:40:55,585 --> 00:40:59,365
compute the loss before you can compute the derivatives.

836
00:40:59,365 --> 00:41:02,200
But I suppose you, you, you could compute the derivative of two,

837
00:41:02,200 --> 00:41:04,240
kind of, adjacent things of one with respect to the other.

838
00:41:04,240 --> 00:41:05,470
[OVERLAPPING] But, yeah. [NOISE]

839
00:41:05,470 --> 00:41:07,780
As you're going forward, do- you need to sort of keep a track of what,

840
00:41:07,780 --> 00:41:13,720
what you would have [inaudible] the one you eventually get the loss. [inaudible]

841
00:41:13,720 --> 00:41:15,865
Yes. So, when you forward prop,

842
00:41:15,865 --> 00:41:19,660
you certainly have to hang on to all of the intervening factors.

843
00:41:19,660 --> 00:41:20,680
[NOISE] Okay. I'm gonna move on for now.

844
00:41:20,680 --> 00:41:24,790
Uh, so, that was a maths-heavy bit but,

845
00:41:24,790 --> 00:41:27,130
um, now, we're getting on to text generation,

846
00:41:27,130 --> 00:41:28,675
which someone asked about earlier.

847
00:41:28,675 --> 00:41:32,965
So, um, just as we use the n-gram language model to generate text,

848
00:41:32,965 --> 00:41:36,115
you can also use an RNN language model to generate text,

849
00:41:36,115 --> 00:41:38,650
uh, via the same repeated sampling technique.

850
00:41:38,650 --> 00:41:41,050
Um, so, here's a picture of how that would work.

851
00:41:41,050 --> 00:41:43,990
How you start off with your initial hidden state H0, uh,

852
00:41:43,990 --> 00:41:46,330
which, uh, we have either as a parameter of

853
00:41:46,330 --> 00:41:49,060
the model or we initialize it to zero, or something like that.

854
00:41:49,060 --> 00:41:51,340
So, let's suppose that we have the first word my,

855
00:41:51,340 --> 00:41:54,235
and Iet's suppose I, um, supply that to the model.

856
00:41:54,235 --> 00:41:57,235
So, then, using the inputs and the initial hidden state,

857
00:41:57,235 --> 00:41:59,200
you can get our first hidden state H1.

858
00:41:59,200 --> 00:42:01,555
And then from there, we can compute the, er,

859
00:42:01,555 --> 00:42:04,765
probability distribution Y hat one of what's coming next,

860
00:42:04,765 --> 00:42:07,435
and then we can use that distribution to sample some word.

861
00:42:07,435 --> 00:42:09,385
So let's suppose that we sampled the word favorite.

862
00:42:09,385 --> 00:42:14,200
So, the idea is that we use the outputted word as the input on the next step.

863
00:42:14,200 --> 00:42:16,960
So, we feed favorite into the second step of the RNN,

864
00:42:16,960 --> 00:42:18,220
we get a new hidden state,

865
00:42:18,220 --> 00:42:20,784
and again we get a new probability distribution,

866
00:42:20,784 --> 00:42:22,885
and from that we can sample a new word.

867
00:42:22,885 --> 00:42:25,675
So, we can just continue doing this process again and again,

868
00:42:25,675 --> 00:42:27,685
and in this way we can generate some text.

869
00:42:27,685 --> 00:42:29,500
So, uh, here we've generated the text,

870
00:42:29,500 --> 00:42:30,760
My favorite season is Spring,

871
00:42:30,760 --> 00:42:34,070
and we can keep going for as long as we'd like.

872
00:42:36,060 --> 00:42:39,130
Okay, so, uh, let's have some fun with this.

873
00:42:39,130 --> 00:42:41,395
Uh, you can generate,

874
00:42:41,395 --> 00:42:43,885
uh, text using an RNN language model.

875
00:42:43,885 --> 00:42:48,070
If you train the RNN language model on any kind of text,

876
00:42:48,070 --> 00:42:51,340
then you can use it to generate text in that style.

877
00:42:51,340 --> 00:42:53,380
And in fact, this has become a whole kind of

878
00:42:53,380 --> 00:42:55,780
genre of internet humor that you might've seen.

879
00:42:55,780 --> 00:42:57,595
So, uh, for example,

880
00:42:57,595 --> 00:43:00,925
here is an RNN language model trained on Obama speeches,

881
00:43:00,925 --> 00:43:03,100
and I found this in a blog post online.

882
00:43:03,100 --> 00:43:07,120
So, here's the text that the RNN language model generated.

883
00:43:07,120 --> 00:43:11,350
"The United States will step up to the cost of a new challenges of

884
00:43:11,350 --> 00:43:15,520
the American people that will share the fact that we created the problem.

885
00:43:15,520 --> 00:43:19,630
They were attacked and so that they have to say that

886
00:43:19,630 --> 00:43:24,190
all the task of the final days of war that I will not be able to get this done."

887
00:43:24,190 --> 00:43:27,130
[LAUGHTER] Okay.

888
00:43:27,130 --> 00:43:30,205
So, if we look at this and

889
00:43:30,205 --> 00:43:32,230
especially think about what did

890
00:43:32,230 --> 00:43:34,570
that text look like that we got from the n-gram language model,

891
00:43:34,570 --> 00:43:36,160
the one about the, the price of gold.

892
00:43:36,160 --> 00:43:39,715
Um, I'd say that this is kind of recognizably better than that.

893
00:43:39,715 --> 00:43:41,620
It seems more fluent overall.

894
00:43:41,620 --> 00:43:43,690
Uh, I'd say it has a more of

895
00:43:43,690 --> 00:43:48,535
a sustained context in that it kind of makes sense for longer stretches at a time,

896
00:43:48,535 --> 00:43:51,666
and I'd say it does sound totally like Obama as well.

897
00:43:51,666 --> 00:43:53,035
So, all of that's pretty good,

898
00:43:53,035 --> 00:43:55,735
but you can see that it's still pretty incoherent overall,

899
00:43:55,735 --> 00:43:58,930
like i- it was quite difficult to read it because it didn't really make sense, right?

900
00:43:58,930 --> 00:44:00,130
So I had to read the words carefully.

901
00:44:00,130 --> 00:44:02,890
Um, so, yeah, I think this shows

902
00:44:02,890 --> 00:44:06,310
some of the progress you can get from using RNNs to generate text but still,

903
00:44:06,310 --> 00:44:09,610
um, very far from human level. Here are some more examples.

904
00:44:09,610 --> 00:44:13,285
Uh, here's an RNN language model that was trained on the Harry Potter books.

905
00:44:13,285 --> 00:44:17,095
And here's what it said. "Sorry." Harry shouted, panicking.

906
00:44:17,095 --> 00:44:19,600
"I'll leave those brooms in London." Are they?

907
00:44:19,600 --> 00:44:21,880
"No idea." said Nearly Headless Nick,

908
00:44:21,880 --> 00:44:23,740
casting low close by Cedric,

909
00:44:23,740 --> 00:44:26,980
carrying the last bit of treacle Charms from Harry's shoulder.

910
00:44:26,980 --> 00:44:29,290
And to answer him the common room perched upon it,

911
00:44:29,290 --> 00:44:33,025
four arms held a shining knob from when the Spider hadn't felt it seemed.

912
00:44:33,025 --> 00:44:34,855
He reached the teams too."

913
00:44:34,855 --> 00:44:38,065
So, again, I'd say that this is fairly fluent.

914
00:44:38,065 --> 00:44:40,000
It sounds totally like the Harry Potter books.

915
00:44:40,000 --> 00:44:41,710
In fact, I'm pretty impressed by how much it does

916
00:44:41,710 --> 00:44:44,170
sound like in the voice of the Harry Potter books.

917
00:44:44,170 --> 00:44:46,510
You even got some character attributes,

918
00:44:46,510 --> 00:44:50,395
I'd say that Harry the character does often panic in the book so that seems right.

919
00:44:50,395 --> 00:44:54,520
Um, [LAUGHTER] but some bad things are that we have,

920
00:44:54,520 --> 00:44:58,660
for example, a pretty long run-on sentence in the second paragraph that's hard to read.

921
00:44:58,660 --> 00:45:01,490
Uh, you have some nonsensical things that really make no sense.

922
00:45:01,490 --> 00:45:03,195
Like, I don't know what a treacle charm is.

923
00:45:03,195 --> 00:45:04,890
It sounds delicious but I don't think it's real,

924
00:45:04,890 --> 00:45:07,790
uh, and overall it's just pretty nonsensical.

925
00:45:07,790 --> 00:45:12,865
Here's another example. Here is an RNN language model that was trained on recipes.

926
00:45:12,865 --> 00:45:16,000
So, uh, [LAUGHTER] this one's pretty bizarre,

927
00:45:16,000 --> 00:45:18,565
the title is 'chocolate ranch barbecue',

928
00:45:18,565 --> 00:45:20,950
It contains Parmesan cheese,

929
00:45:20,950 --> 00:45:25,555
coconut milk, eggs, and the recipe says place each pasta over layers of lumps,

930
00:45:25,555 --> 00:45:29,500
shape mixture into the moderate oven and simmer until firm.

931
00:45:29,500 --> 00:45:31,210
Serve hot in bodied fresh,

932
00:45:31,210 --> 00:45:32,575
mustard orange and cheese.

933
00:45:32,575 --> 00:45:35,815
Combine the cheese and salt together the dough in a large skillet;

934
00:45:35,815 --> 00:45:38,140
add the ingredients and stir in the chocolate and pepper.

935
00:45:38,140 --> 00:45:41,635
[LAUGHTER] Um, so, one thing that I think is

936
00:45:41,635 --> 00:45:45,340
even more clear here in the recipes example than the prose example,

937
00:45:45,340 --> 00:45:49,405
is the inability to remember what's [NOISE] what's happening overall, right?

938
00:45:49,405 --> 00:45:53,020
Cuz a recipe you could say is pretty challenging because you need to remember

939
00:45:53,020 --> 00:45:57,100
the title of what you're trying to make which in this case is chocolate ranch barbecue,

940
00:45:57,100 --> 00:45:59,470
and you need to actually, you know, make that thing by the end.

941
00:45:59,470 --> 00:46:01,060
Uh, you also need to remember what were the ingredients

942
00:46:01,060 --> 00:46:02,500
in the beginning and did you use them.

943
00:46:02,500 --> 00:46:05,230
And in a recipe, if you make something and put it in the oven,

944
00:46:05,230 --> 00:46:07,720
you need to take it out later, a- and stuff like that, right?

945
00:46:07,720 --> 00:46:09,400
So, clearly it's not really

946
00:46:09,400 --> 00:46:11,890
remembering what's happening overall or what it's trying to do,

947
00:46:11,890 --> 00:46:13,915
it seems to be just generating kind of

948
00:46:13,915 --> 00:46:17,785
generic recipe sentences and putting them in a random order.

949
00:46:17,785 --> 00:46:20,635
Uh, but again, I mean, we can see that it's fairly fluent,

950
00:46:20,635 --> 00:46:23,350
it's grammatically right, it kind of sounds like a recipe.

951
00:46:23,350 --> 00:46:25,855
Uh, but the problem is it's just nonsensical.

952
00:46:25,855 --> 00:46:28,300
Like for example, shape mixture into

953
00:46:28,300 --> 00:46:31,345
the moderate oven is grammatical but it doesn't make any sense.

954
00:46:31,345 --> 00:46:33,295
Okay, last example.

955
00:46:33,295 --> 00:46:37,510
So, here's an RNN language model that's trained on paint-color names.

956
00:46:37,510 --> 00:46:41,200
And this is an example of a character-level language model because

957
00:46:41,200 --> 00:46:44,845
it's predicting what character comes next not what word comes next.

958
00:46:44,845 --> 00:46:47,650
And this is why it's able to come up with new words.

959
00:46:47,650 --> 00:46:49,840
Another thing to note is that this language model was

960
00:46:49,840 --> 00:46:52,090
trained to be conditioned on some kind of input.

961
00:46:52,090 --> 00:46:55,780
So here, the input is the color itself I think represented by the three numbers,

962
00:46:55,780 --> 00:46:57,145
that's probably RGB numbers.

963
00:46:57,145 --> 00:47:00,925
And it generated some names for the colors.

964
00:47:00,925 --> 00:47:02,140
And I think these are pretty funny.

965
00:47:02,140 --> 00:47:04,060
My favorite one is Stanky Bean,

966
00:47:04,060 --> 00:47:05,140
which is in the bottom right.

967
00:47:05,140 --> 00:47:07,930
[LAUGHTER] Um, so, it's pretty creative,

968
00:47:07,930 --> 00:47:10,210
[LAUGHTER] and I think these do sound kind of

969
00:47:10,210 --> 00:47:13,360
like paint colors but often they're quite bizarre.

970
00:47:13,360 --> 00:47:20,570
[LAUGHTER] Light of Blast is pretty good too.

971
00:47:20,910 --> 00:47:23,500
So, uh, you're gonna learn more about

972
00:47:23,500 --> 00:47:25,765
character-level language models in a future lecture,

973
00:47:25,765 --> 00:47:28,870
and you're also going to learn more about how to condition a language model

974
00:47:28,870 --> 00:47:32,440
based on some kind of input such as the color, um, code.

975
00:47:32,440 --> 00:47:34,330
So, these are pretty funny,

976
00:47:34,330 --> 00:47:35,890
uh, but I do want to say a warning.

977
00:47:35,890 --> 00:47:38,920
Um, you'll find a lot of these kinds of articles online,

978
00:47:38,920 --> 00:47:40,585
uh, often with headlines like,

979
00:47:40,585 --> 00:47:43,000
"We forced a bot to watch, you know,

980
00:47:43,000 --> 00:47:46,705
1000 hours of sci-fi movies and it wrote a script," something like that.

981
00:47:46,705 --> 00:47:50,800
Um, so, my advice is you have to take these with a big pinch of salt, because often,

982
00:47:50,800 --> 00:47:53,080
uh, the examples that people put online were

983
00:47:53,080 --> 00:47:55,375
hand selected by humans to be the funniest examples.

984
00:47:55,375 --> 00:47:58,660
Like I think all of the examples I've shown today were definitely hand selected

985
00:47:58,660 --> 00:48:02,200
by humans as the funniest examples that the RNN came up with.

986
00:48:02,200 --> 00:48:05,455
And in some cases they might even have been edited by a human.

987
00:48:05,455 --> 00:48:08,560
So, uh, yeah, you do need to be a little bit skeptical when you look at these examples.

988
00:48:08,560 --> 00:48:10,195
[OVERLAPPING] Yep.

989
00:48:10,195 --> 00:48:12,925
So, uh, in the Harry Potter one,

990
00:48:12,925 --> 00:48:16,630
there was a opening quote and then there was a closing quote.

991
00:48:16,630 --> 00:48:18,745
So, like do you expect the RNN,

992
00:48:18,745 --> 00:48:22,000
like when it puts that opening quote and keeps putting more words,

993
00:48:22,000 --> 00:48:28,825
do you expect the probability of a closing quote to like increase as you're going or decrease?

994
00:48:28,825 --> 00:48:31,150
That's a great question. So, uh,

995
00:48:31,150 --> 00:48:32,515
the question was, uh,

996
00:48:32,515 --> 00:48:34,450
we noticed that in the Harry Potter example,

997
00:48:34,450 --> 00:48:36,295
there was some open quotes and some closed quotes.

998
00:48:36,295 --> 00:48:38,410
And it looks like the model didn't screw up, right?

999
00:48:38,410 --> 00:48:40,075
All of these open quotes and closed quotes,

1000
00:48:40,075 --> 00:48:41,815
uh, are in the correct places.

1001
00:48:41,815 --> 00:48:44,455
So, the question is, do we expect the model to put

1002
00:48:44,455 --> 00:48:48,775
a higher probability on closing the quote given that is inside a quo- quote passage?

1003
00:48:48,775 --> 00:48:51,115
So, I should say definitely yes and

1004
00:48:51,115 --> 00:48:54,220
that's most- mostly the explanation for why this works.

1005
00:48:54,220 --> 00:48:56,500
Um, there's been some really interesting work in trying

1006
00:48:56,500 --> 00:48:58,540
to look inside the hidden states of, uh,

1007
00:48:58,540 --> 00:49:01,345
language models to see whether it's tracking things like,

1008
00:49:01,345 --> 00:49:03,610
are we inside an open quote or a close quote?

1009
00:49:03,610 --> 00:49:06,430
And there has been some limited evidence to show that

1010
00:49:06,430 --> 00:49:09,370
maybe there are certain neuron or neurons inside the hidden state,

1011
00:49:09,370 --> 00:49:10,900
which are tracking things like,

1012
00:49:10,900 --> 00:49:12,550
are we currently inside a quote or not?

1013
00:49:12,550 --> 00:49:13,855
[NOISE]. Yeah.

1014
00:49:13,855 --> 00:49:18,370
So, so, like do you think the probability would increase  as you go more to the right [OVERLAPPING]?

1015
00:49:18,370 --> 00:49:22,270
So, the question is as the quote passage goes on for longer,

1016
00:49:22,270 --> 00:49:23,740
do you think the priority or

1017
00:49:23,740 --> 00:49:26,770
the probability of outputting a closed quote should increase?

1018
00:49:26,770 --> 00:49:28,045
Um, I don't know.

1019
00:49:28,045 --> 00:49:31,420
Maybe. Um, that would be good, I suppose,

1020
00:49:31,420 --> 00:49:32,980
because you don't want an infinite quote,

1021
00:49:32,980 --> 00:49:35,650
uh, but I wouldn't be surprised if that didn't happen.

1022
00:49:35,650 --> 00:49:39,400
Like I wouldn't be surprised if maybe some other worse-trained language models,

1023
00:49:39,400 --> 00:49:41,395
just opened quotes and never closed them.

1024
00:49:41,395 --> 00:49:44,815
Uh, any other questions? Yeah.

1025
00:49:44,815 --> 00:49:47,605
What are the dimensions of the W metric?

1026
00:49:47,605 --> 00:49:50,710
Okay. So, the question is what are the dimensions of the W metric?

1027
00:49:50,710 --> 00:49:52,480
So we're going back to the online stuff.

1028
00:49:52,480 --> 00:49:55,900
Uh, okay. You're asking me about W_h or W_e or something else?

1029
00:49:55,900 --> 00:49:56,610
Yeah.

1030
00:49:56,610 --> 00:49:58,960
So, W_h will be,

1031
00:49:58,960 --> 00:50:01,435
uh, if we say that the hidden size has size n,

1032
00:50:01,435 --> 00:50:07,240
then W_h will be n by n. And if we suppose that the embeddings have size d,

1033
00:50:07,240 --> 00:50:08,635
then W_e will be, uh,

1034
00:50:08,635 --> 00:50:12,550
d by n, n by d, maybe.

1035
00:50:12,550 --> 00:50:19,990
Does that answer your question? [NOISE] Uh,

1036
00:50:19,990 --> 00:50:23,380
any other questions about generating or anything? Yep.

1037
00:50:23,380 --> 00:50:28,030
So, you said that there was a long sentence in the Harry Potter-related text?

1038
00:50:28,030 --> 00:50:28,425
Yeah.

1039
00:50:28,425 --> 00:50:33,640
Is it ever sort of practical to combine RNNs with like in this hand written rules?

1040
00:50:33,640 --> 00:50:35,395
Sorry. Is it ever practical to combine-

1041
00:50:35,395 --> 00:50:37,810
RNNs with a written list of hand-written rules.

1042
00:50:37,810 --> 00:50:38,830
[OVERLAPPING]

1043
00:50:38,830 --> 00:50:39,880
Okay. Yeah. That's a great question.

1044
00:50:39,880 --> 00:50:42,220
So the question was, is it ever practical to

1045
00:50:42,220 --> 00:50:44,980
combine RNNs with a list of hand-written rules?

1046
00:50:44,980 --> 00:50:49,285
For example, don't let your sentence be longer than this many words.

1047
00:50:49,285 --> 00:50:50,530
Um, so yeah.

1048
00:50:50,530 --> 00:50:54,070
I'd say it probably is practical maybe especially if you're interested in, uh,

1049
00:50:54,070 --> 00:50:56,260
making sure that certain bad things don't happen,

1050
00:50:56,260 --> 00:51:01,900
you might apply some hacky rules like yeah forcing it to end, uh, early.

1051
00:51:01,900 --> 00:51:03,580
I mean, okay. So there's this thing called Beam Search

1052
00:51:03,580 --> 00:51:05,335
which we're going to learn about in a later lecture,

1053
00:51:05,335 --> 00:51:06,640
which essentially doesn't just,

1054
00:51:06,640 --> 00:51:09,340
um, choose one word in each step and continue.

1055
00:51:09,340 --> 00:51:12,325
It explores many different options for words you could generate.

1056
00:51:12,325 --> 00:51:14,410
And you can apply some kinds of rules on that

1057
00:51:14,410 --> 00:51:16,540
where if you have lots of different things to choose from,

1058
00:51:16,540 --> 00:51:18,250
then you can maybe get rid of

1059
00:51:18,250 --> 00:51:21,265
some options if you don't like them because they break some of your rules.

1060
00:51:21,265 --> 00:51:28,340
But, um, it can be difficult to do. Any other questions?

1061
00:51:29,490 --> 00:51:38,380
Okay. Um, so we've talked about generating from language models.

1062
00:51:38,380 --> 00:51:40,630
Uh, so unfortunately, you can't just use

1063
00:51:40,630 --> 00:51:44,140
generation as your evaluation metric for the language models.

1064
00:51:44,140 --> 00:51:47,245
You do need some kind of, um, measurable metric.

1065
00:51:47,245 --> 00:51:52,015
So, the standard evaluation metric for language models is called perplexity.

1066
00:51:52,015 --> 00:51:54,250
And, uh, perplexity is defined as

1067
00:51:54,250 --> 00:51:58,480
the inverse probability of the corpus according to the language model.

1068
00:51:58,480 --> 00:52:02,200
So, if you look at it you can see that that's what this formula is saying.

1069
00:52:02,200 --> 00:52:04,075
It's saying that for every, uh,

1070
00:52:04,075 --> 00:52:07,555
word xt, lowercase t, in the corpus, uh,

1071
00:52:07,555 --> 00:52:10,420
we're computing the probability of that word given

1072
00:52:10,420 --> 00:52:13,630
everything that came so far but its inverse is one over that.

1073
00:52:13,630 --> 00:52:16,600
And then lastly, when normalizing this big,

1074
00:52:16,600 --> 00:52:19,960
uh, product by the number of words,

1075
00:52:19,960 --> 00:52:23,995
which is capital T. And the reason why we're doing that is because if we didn't do that,

1076
00:52:23,995 --> 00:52:28,195
then perplexity would just get smaller and smaller as your corpus got bigger.

1077
00:52:28,195 --> 00:52:31,070
So we need to normalize by that factor.

1078
00:52:31,140 --> 00:52:33,910
So, you can actually show you that this, uh,

1079
00:52:33,910 --> 00:52:38,470
perplexity is equal to the exponential of the cross-entropy loss J Theta.

1080
00:52:38,470 --> 00:52:41,470
So if you remember, cross-entropy loss J Theta is, uh,

1081
00:52:41,470 --> 00:52:44,305
the training objective that we're using to train the language model.

1082
00:52:44,305 --> 00:52:46,555
And, uh, by rearranging things a little bit,

1083
00:52:46,555 --> 00:52:50,890
you can see that perplexity is actually the exponential of the cross-entropy.

1084
00:52:50,890 --> 00:52:52,750
And this is a good thing, uh,

1085
00:52:52,750 --> 00:52:55,750
because if we're training the language model to, uh,

1086
00:52:55,750 --> 00:52:58,900
minimize the cross-entropy loss,

1087
00:52:58,900 --> 00:53:04,070
then you are training it to optimize the perplexity as well.

1088
00:53:04,800 --> 00:53:08,860
So you should remember that the lower perplexity is better,

1089
00:53:08,860 --> 00:53:12,640
uh, because perplexity is the inverse probability of the corpus.

1090
00:53:12,640 --> 00:53:17,965
So, uh, if you want your language model to assign high probability to the corpus, right?

1091
00:53:17,965 --> 00:53:21,470
Then that means you want to get low perplexity.

1092
00:53:21,600 --> 00:53:28,480
Uh, any questions? [NOISE] Okay.

1093
00:53:28,480 --> 00:53:36,220
Uh, so RNNs have been pretty successful in recent years in improving perplexity.

1094
00:53:36,220 --> 00:53:39,880
So, uh, this is a results table from a recent,

1095
00:53:39,880 --> 00:53:43,630
uh, Facebook research paper about RNN language models.

1096
00:53:43,630 --> 00:53:46,600
And, uh, you don't have to understand all of the details of this table,

1097
00:53:46,600 --> 00:53:48,055
but what it's telling you is that,

1098
00:53:48,055 --> 00:53:50,785
on the, uh, top where we have n gram language model.

1099
00:53:50,785 --> 00:53:52,240
And thessssssssssssn in the subsequent various,

1100
00:53:52,240 --> 00:53:55,735
we have some increasingly complex and large RNNs.

1101
00:53:55,735 --> 00:53:58,945
And you can see that the perplexity numbers are decreasing,

1102
00:53:58,945 --> 00:54:00,475
because lower is better.

1103
00:54:00,475 --> 00:54:02,770
So RNNs have been really great for

1104
00:54:02,770 --> 00:54:06,320
making more effective language models in the last few years.

1105
00:54:08,910 --> 00:54:11,695
Okay. So to zoom out a little bit,

1106
00:54:11,695 --> 00:54:13,120
you might be thinking, uh,

1107
00:54:13,120 --> 00:54:15,460
why should I care about Language Modelling?

1108
00:54:15,460 --> 00:54:17,350
Why is it important? I'd say there are

1109
00:54:17,350 --> 00:54:19,735
two main reasons why Language Modelling is important.

1110
00:54:19,735 --> 00:54:21,160
Uh, so the first one is,

1111
00:54:21,160 --> 00:54:23,620
that language modelling is a benchmark task that

1112
00:54:23,620 --> 00:54:26,770
helps us measure our progress on understanding language.

1113
00:54:26,770 --> 00:54:28,540
So, you could view language modeling as

1114
00:54:28,540 --> 00:54:31,990
a pretty general language understanding task, right?

1115
00:54:31,990 --> 00:54:35,425
Because predicting what word comes next to given any,

1116
00:54:35,425 --> 00:54:37,795
any kind of, uh, generic text.

1117
00:54:37,795 --> 00:54:40,975
Um, that's quite a difficult and general problem.

1118
00:54:40,975 --> 00:54:43,330
And in order to be good at language modelling,

1119
00:54:43,330 --> 00:54:45,340
you have to understand a lot of things, right?

1120
00:54:45,340 --> 00:54:46,780
You have to understand grammar,

1121
00:54:46,780 --> 00:54:48,115
you have to understand syntax,

1122
00:54:48,115 --> 00:54:49,615
and you have to understand,

1123
00:54:49,615 --> 00:54:51,115
uh, logic and reasoning.

1124
00:54:51,115 --> 00:54:52,570
And you have to understand something about,

1125
00:54:52,570 --> 00:54:53,845
you know, real-world knowledge.

1126
00:54:53,845 --> 00:54:55,720
You have to understand a lot of things in order to be

1127
00:54:55,720 --> 00:54:57,970
able to do language modelling properly.

1128
00:54:57,970 --> 00:54:59,530
So, the reason why we care about it as

1129
00:54:59,530 --> 00:55:02,350
a benchmark task is because if you're able to build a model,

1130
00:55:02,350 --> 00:55:05,050
which is a better language model than the ones that came before it,

1131
00:55:05,050 --> 00:55:07,930
then you must have made some kind of progress on at

1132
00:55:07,930 --> 00:55:11,620
least some of those sub-components of natural language understanding.

1133
00:55:11,620 --> 00:55:14,470
So, another more tangible reason why you might

1134
00:55:14,470 --> 00:55:16,930
care about language modelling is that it's a sub-component of

1135
00:55:16,930 --> 00:55:19,990
many many NLP tasks especially those which involve

1136
00:55:19,990 --> 00:55:23,560
generating text or estimating the probability of text.

1137
00:55:23,560 --> 00:55:25,675
So, here's a bunch of examples.

1138
00:55:25,675 --> 00:55:27,220
Uh, one is predictive typing.

1139
00:55:27,220 --> 00:55:29,170
That's the example that we showed at the beginning of the lecture

1140
00:55:29,170 --> 00:55:31,450
with typing on your phone or searching on Google.

1141
00:55:31,450 --> 00:55:35,185
Uh, this is also very useful for people who have movement disabilities, uh,

1142
00:55:35,185 --> 00:55:39,595
because they are these systems that help people communicate using fewer movements.

1143
00:55:39,595 --> 00:55:41,920
Uh, another example is speech recognition.

1144
00:55:41,920 --> 00:55:43,600
So, in speech recognition you have

1145
00:55:43,600 --> 00:55:45,820
some kind of audio recording of a person saying something

1146
00:55:45,820 --> 00:55:49,975
and often it's kind of noisy and hard to make out what they're saying and you need to,

1147
00:55:49,975 --> 00:55:51,700
uh, figure out what words did they say.

1148
00:55:51,700 --> 00:55:55,300
So this an example where you have to estimate the probability of different,

1149
00:55:55,300 --> 00:55:58,210
uh, different options of what, what it is they could have said.

1150
00:55:58,210 --> 00:56:00,445
And in the same way, handwriting recognition,

1151
00:56:00,445 --> 00:56:02,410
is an example where there's a lot of noise

1152
00:56:02,410 --> 00:56:05,470
and you have to figure out what the person intended to say.

1153
00:56:05,470 --> 00:56:07,810
Uh, spelling and grammar correction is yet

1154
00:56:07,810 --> 00:56:10,705
another example where it's all about trying to figure out what someone meant.

1155
00:56:10,705 --> 00:56:12,340
And that means you actually understand how

1156
00:56:12,340 --> 00:56:14,695
likely it is that they were saying different things.

1157
00:56:14,695 --> 00:56:19,555
Uh, an interesting, an interesting application is authorship identification.

1158
00:56:19,555 --> 00:56:22,480
So suppose that you have a piece of text and you're trying to

1159
00:56:22,480 --> 00:56:25,495
figure out who likely wrote it and maybe you have,

1160
00:56:25,495 --> 00:56:29,830
uh, several different authors and you have text written by those different authors.

1161
00:56:29,830 --> 00:56:31,285
So you could, for example,

1162
00:56:31,285 --> 00:56:34,720
train a separate language model on each of the different authors' texts.

1163
00:56:34,720 --> 00:56:36,160
And then, because, remember,

1164
00:56:36,160 --> 00:56:39,805
a language model can tell you the probability of a given piece of text.

1165
00:56:39,805 --> 00:56:42,430
Then you could ask all the different language models,

1166
00:56:42,430 --> 00:56:45,790
um, how likely the texts and the question is,

1167
00:56:45,790 --> 00:56:49,720
and then if a certain author's language model says that it's likely then that

1168
00:56:49,720 --> 00:56:55,000
means that text the texts and the question is more likely to be written by that author.

1169
00:56:55,000 --> 00:56:57,820
Um, other examples include machine translation.

1170
00:56:57,820 --> 00:56:59,200
This is a huge, uh,

1171
00:56:59,200 --> 00:57:01,390
application of language models,

1172
00:57:01,390 --> 00:57:03,565
uh, because it's all about generating text.

1173
00:57:03,565 --> 00:57:05,740
Uh, similarly, summarization is

1174
00:57:05,740 --> 00:57:09,280
a task where we need to generate some text given some input text.

1175
00:57:09,280 --> 00:57:11,185
Uh, dialogue as well,

1176
00:57:11,185 --> 00:57:14,980
not all dialogue agents necessarily are RNN language models but you can

1177
00:57:14,980 --> 00:57:19,285
build a dialogue agent that generates the text using an RNN language model.

1178
00:57:19,285 --> 00:57:21,560
And there are more examples as well.

1179
00:57:21,560 --> 00:57:25,360
Any questions on this? [LAUGHTER] Yep.

1180
00:57:25,360 --> 00:57:47,875
So, I know that [inaudible]

1181
00:57:47,875 --> 00:57:49,945
Great question. So, the question was,

1182
00:57:49,945 --> 00:57:51,475
uh, for some of these examples, uh,

1183
00:57:51,475 --> 00:57:55,315
such as speech recognition or maybe [NOISE] image captioning,

1184
00:57:55,315 --> 00:57:59,290
the input is audio or image or something that is not text, right?

1185
00:57:59,290 --> 00:58:01,780
So, you can't represent it in the way that we've talked about so far.

1186
00:58:01,780 --> 00:58:04,180
Um, so, [NOISE] in those examples,

1187
00:58:04,180 --> 00:58:06,460
you will have some way of representing the input,

1188
00:58:06,460 --> 00:58:08,725
some way of encoding the audio or the image or whatever.

1189
00:58:08,725 --> 00:58:13,315
Uh, the reason I brought it up now in terms of language models is that that's the input,

1190
00:58:13,315 --> 00:58:15,685
but you use the language model to get the output, right?

1191
00:58:15,685 --> 00:58:17,170
So, the language model, [NOISE] uh, generates

1192
00:58:17,170 --> 00:58:19,345
the output in the way that we saw earlier, uh,

1193
00:58:19,345 --> 00:58:22,120
but we're gonna learn more about those conditional language [NOISE] models later.

1194
00:58:22,120 --> 00:58:25,090
[NOISE] Anyone else?

1195
00:58:25,090 --> 00:58:29,020
[NOISE] Okay.

1196
00:58:29,020 --> 00:58:32,965
[NOISE] So, uh, here's a recap.

1197
00:58:32,965 --> 00:58:36,730
If I've lost you somewhere in this lecture, uh, or you got tired,

1198
00:58:36,730 --> 00:58:38,770
um, now's a great time to jump back in

1199
00:58:38,770 --> 00:58:41,050
because things are gonna get a little bit more accessible.

1200
00:58:41,050 --> 00:58:43,045
Okay. So, here's a recap of what we've done today.

1201
00:58:43,045 --> 00:58:46,210
Uh, a language model is a system that predicts the next word,

1202
00:58:46,210 --> 00:58:48,460
[NOISE] and a recurrent neural network,

1203
00:58:48,460 --> 00:58:50,590
is a new family, oh, new to us,

1204
00:58:50,590 --> 00:58:53,710
a family of neural networks that takes sequential input

1205
00:58:53,710 --> 00:58:57,175
of any length and it applies the same weights on every step,

1206
00:58:57,175 --> 00:58:59,620
and it can optionally produce some kind of output on

1207
00:58:59,620 --> 00:59:02,020
each step or some of the steps or none of the steps.

1208
00:59:02,020 --> 00:59:04,945
[NOISE] So, don't be confused.

1209
00:59:04,945 --> 00:59:08,305
A recurrent neural network is not [NOISE] the same thing as a language model.

1210
00:59:08,305 --> 00:59:12,970
Uh, we've seen today that an RNN is a great way to build a language model, but actually,

1211
00:59:12,970 --> 00:59:15,010
it turns out that you can use RNNs for,

1212
00:59:15,010 --> 00:59:17,710
uh, a lot of other different things that are not language modeling.

1213
00:59:17,710 --> 00:59:19,840
[NOISE] So, here's a few examples of that.

1214
00:59:19,840 --> 00:59:24,085
[NOISE] Uh, you can use an RNN to do a tagging task.

1215
00:59:24,085 --> 00:59:26,320
So, some examples of tagging tasks are

1216
00:59:26,320 --> 00:59:29,260
part-of-speech tagging and named entity recognition.

1217
00:59:29,260 --> 00:59:32,590
So, pictured here is part-of-speech tagging, and this is the task.

1218
00:59:32,590 --> 00:59:35,245
We have some kind of input text such as, uh,

1219
00:59:35,245 --> 00:59:37,645
the startled cat knocked over the vase,

1220
00:59:37,645 --> 00:59:39,385
and your job is to, uh,

1221
00:59:39,385 --> 00:59:42,085
label or tag each word with its part of speech.

1222
00:59:42,085 --> 00:59:45,160
So, for example, cat is a noun and knocked is a verb.

1223
00:59:45,160 --> 00:59:48,205
So, you can use an RNN to do this task in,

1224
00:59:48,205 --> 00:59:50,350
in the way that we've pictured, which is that you, uh,

1225
00:59:50,350 --> 00:59:52,720
feed the text into the RNN, [NOISE] and then,

1226
00:59:52,720 --> 00:59:53,905
on each step of the RNN,

1227
00:59:53,905 --> 00:59:55,705
you, uh, have an output,

1228
00:59:55,705 --> 00:59:57,790
probably a distribution over what, uh,

1229
00:59:57,790 --> 01:00:01,775
tag you think it is, and then, uh, you can tag it in that way.

1230
01:00:01,775 --> 01:00:04,050
And then, also for named entity recognition,

1231
01:00:04,050 --> 01:00:05,190
that's all about, um,

1232
01:00:05,190 --> 01:00:08,085
tagging each of the words with what named entity type they are.

1233
01:00:08,085 --> 01:00:11,820
So, you do it in the same way. [NOISE] Okay.

1234
01:00:11,820 --> 01:00:13,470
Here's another thing you can use RNNs for,

1235
01:00:13,470 --> 01:00:16,200
uh, you can use them for sentence classification.

1236
01:00:16,200 --> 01:00:19,080
So, sentence classification is just a general term to mean

1237
01:00:19,080 --> 01:00:22,170
any kind of task where you want to take sentence or other piece of text,

1238
01:00:22,170 --> 01:00:24,945
and then, you want to classify it into one of several classes.

1239
01:00:24,945 --> 01:00:28,120
So, an example of that is sentiment classification.

1240
01:00:28,120 --> 01:00:30,400
Uh, sentiment classification is when you have some kind

1241
01:00:30,400 --> 01:00:32,680
of input text such as, let's say, overall,

1242
01:00:32,680 --> 01:00:34,510
I enjoyed the movie a lot, and then,

1243
01:00:34,510 --> 01:00:35,770
you're trying to classify that as being

1244
01:00:35,770 --> 01:00:38,095
positive or negative or [NOISE] neutral sentiment.

1245
01:00:38,095 --> 01:00:40,090
So, in this example, this is positive sentiment.

1246
01:00:40,090 --> 01:00:45,400
[NOISE] So, one way you might use an RNN to tackle this task is, uh,

1247
01:00:45,400 --> 01:00:49,450
you might encode the text using the RNN, and then,

1248
01:00:49,450 --> 01:00:53,350
really what you want is some kind of sentence encoding so that you

1249
01:00:53,350 --> 01:00:57,265
can output your label for the sentence, right?

1250
01:00:57,265 --> 01:00:59,680
And it'll be useful if you would have a single vector to

1251
01:00:59,680 --> 01:01:02,965
represent the sentence rather than all of these separate vectors.

1252
01:01:02,965 --> 01:01:04,870
So, how would you do this?

1253
01:01:04,870 --> 01:01:07,000
How would you get the sentence encoding from the RNN?

1254
01:01:07,000 --> 01:01:10,540
[NOISE] Uh, one thing you could do [NOISE] is,

1255
01:01:10,540 --> 01:01:14,290
you could use the final hidden state as your sentence encoding.

1256
01:01:14,290 --> 01:01:18,460
So, um, the reason why you might think this is a good idea is because,

1257
01:01:18,460 --> 01:01:19,810
for example, in the RNN,

1258
01:01:19,810 --> 01:01:22,675
we regard the, the final hidden state as,

1259
01:01:22,675 --> 01:01:25,735
um, this is the thing you use to predict what's coming next, right?

1260
01:01:25,735 --> 01:01:28,300
So, we're assuming that the final hidden state contains

1261
01:01:28,300 --> 01:01:31,465
information about all of the text that has come so far, right?

1262
01:01:31,465 --> 01:01:34,990
So, for that reason, you might suppose that this is a good sentence encoding,

1263
01:01:34,990 --> 01:01:36,460
and we could use that [NOISE] to predict, you know,

1264
01:01:36,460 --> 01:01:39,040
what, uh, what sentiment is this sentence.

1265
01:01:39,040 --> 01:01:41,350
And it turns out that usually, a better way to do this,

1266
01:01:41,350 --> 01:01:42,595
usually a more effective way,

1267
01:01:42,595 --> 01:01:46,240
is to do something like maybe take an element-wise max or

1268
01:01:46,240 --> 01:01:50,080
an element-wise mean of all these hidden states to get your sentence encoding,

1269
01:01:50,080 --> 01:01:52,345
um, [NOISE] and, uh,

1270
01:01:52,345 --> 01:01:54,640
this tends to work better than just using the final hidden state.

1271
01:01:54,640 --> 01:01:58,490
[NOISE] Uh, there are some other more advanced things you can do as well.

1272
01:01:59,310 --> 01:02:02,215
Okay. [NOISE] Another thing that you can use RNNs for

1273
01:02:02,215 --> 01:02:05,335
is as a general purpose encoder module.

1274
01:02:05,335 --> 01:02:08,470
Uh, so, here's an example that's question answering,

1275
01:02:08,470 --> 01:02:10,480
but really this idea of RNNs as

1276
01:02:10,480 --> 01:02:15,085
a general purpose encoder module is very common [NOISE] and use it in lots of different,

1277
01:02:15,085 --> 01:02:17,590
um, deep learning [NOISE] architectures for NLP.

1278
01:02:17,590 --> 01:02:21,175
[NOISE] So, here's an example which is question answering.

1279
01:02:21,175 --> 01:02:23,410
Uh, so, let's suppose that the, the task is,

1280
01:02:23,410 --> 01:02:24,670
you've got some kind of context,

1281
01:02:24,670 --> 01:02:26,110
which, in this, uh, situation,

1282
01:02:26,110 --> 01:02:29,365
is the Wikipedia article on Beethoven, and then,

1283
01:02:29,365 --> 01:02:31,210
you have a question which is asking,

1284
01:02:31,210 --> 01:02:33,070
what nationality was Beethoven?

1285
01:02:33,070 --> 01:02:36,400
Uh, and this is actually taken from the SQuAD Challenge,

1286
01:02:36,400 --> 01:02:38,680
which is the subject of the Default Final Project.

1287
01:02:38,680 --> 01:02:41,770
So, um, if you choose to do- to do the Default Final Project,

1288
01:02:41,770 --> 01:02:44,950
you're going to be building systems that solve this problem.

1289
01:02:44,950 --> 01:02:49,930
So, what you might do is, you might use an RNN to process the question,

1290
01:02:49,930 --> 01:02:51,970
what nationality was [NOISE] Beethoven?

1291
01:02:51,970 --> 01:02:56,215
And then, you might use those hidden states that you get from this, uh,

1292
01:02:56,215 --> 01:03:00,280
RNN of the question as a representation of the question.

1293
01:03:00,280 --> 01:03:03,580
And I'm being intentionally vague here [NOISE] about what might happen next, uh,

1294
01:03:03,580 --> 01:03:05,200
but the idea is that you have [NOISE]

1295
01:03:05,200 --> 01:03:08,500
both the context and the question are going to be fed some way,

1296
01:03:08,500 --> 01:03:10,900
and maybe you'll use an RNN on context as well,

1297
01:03:10,900 --> 01:03:14,485
and you're going to have lots more neural architecture in order to get your answer,

1298
01:03:14,485 --> 01:03:15,895
which is, uh, German.

1299
01:03:15,895 --> 01:03:21,355
So, the point here is that the RNN is acting as an encoder for the question,

1300
01:03:21,355 --> 01:03:23,920
that is, the hidden states that you get from running

1301
01:03:23,920 --> 01:03:26,650
the RNN over the question, represent the question.

1302
01:03:26,650 --> 01:03:31,810
[NOISE] Uh, so, the encoder is part of a larger neural system,

1303
01:03:31,810 --> 01:03:33,940
[NOISE] and it's the, the hidden states themselves

1304
01:03:33,940 --> 01:03:36,295
that you're interested in because they contain the information.

1305
01:03:36,295 --> 01:03:38,140
So, you could have, um, taken,

1306
01:03:38,140 --> 01:03:39,700
uh, element-wise max or mean,

1307
01:03:39,700 --> 01:03:41,005
like we showed in the previous slide,

1308
01:03:41,005 --> 01:03:44,170
to get a single vector for the question, but often, you don't do that.

1309
01:03:44,170 --> 01:03:48,160
Often, you'll, uh, do something else which uses the hidden states directly.

1310
01:03:48,160 --> 01:03:53,440
So, the general point here is that RNNs are quite powerful as a way to represent,

1311
01:03:53,440 --> 01:03:54,925
uh, a sequence of text,

1312
01:03:54,925 --> 01:03:57,710
uh, for further computation.

1313
01:03:58,170 --> 01:04:02,935
Okay. Last example. So, going back to RNN language models again, [NOISE] uh,

1314
01:04:02,935 --> 01:04:04,570
they can be used to generate text,

1315
01:04:04,570 --> 01:04:07,300
and there are lots of different, uh, applications for this.

1316
01:04:07,300 --> 01:04:11,020
So, for example, speech recognition, uh, you will have your input,

1317
01:04:11,020 --> 01:04:13,345
which is the audio, and as a student asked earlier,

1318
01:04:13,345 --> 01:04:15,865
this will be, uh, represented in some way,

1319
01:04:15,865 --> 01:04:19,480
and then, uh, maybe you'll do a neural encoding of that, [NOISE] and then,

1320
01:04:19,480 --> 01:04:22,615
you use your RNN language model to generate the output,

1321
01:04:22,615 --> 01:04:24,354
which, in this case, is going to be a transcription

1322
01:04:24,354 --> 01:04:26,275
of what the audio recording is saying.

1323
01:04:26,275 --> 01:04:28,030
So, you will have some way of conditioning,

1324
01:04:28,030 --> 01:04:29,830
and we're gonna talk more about how this works, uh,

1325
01:04:29,830 --> 01:04:31,780
in a later lecture, but you have some way of

1326
01:04:31,780 --> 01:04:35,230
conditioning your RNN language model on the input.

1327
01:04:35,230 --> 01:04:38,920
So, you'll use that to generate your text, [NOISE] and in this case,

1328
01:04:38,920 --> 01:04:41,335
the utterance might be something like, what's the weather,

1329
01:04:41,335 --> 01:04:44,590
question mark. [OVERLAPPING] [NOISE]

1330
01:04:44,590 --> 01:04:54,220
Yeah. [NOISE]

1331
01:04:54,220 --> 01:04:58,120
In speech recognition, [inaudible].

1332
01:04:58,120 --> 01:05:00,100
Okay. So, the question is, in speech recognition,

1333
01:05:00,100 --> 01:05:02,755
we often use word error rates to evaluate,

1334
01:05:02,755 --> 01:05:04,690
but would you use perplexity to evaluate?

1335
01:05:04,690 --> 01:05:07,690
[NOISE] Um, I don't actually know much about that. Do you know, Chris,

1336
01:05:07,690 --> 01:05:09,250
what they use in, uh,

1337
01:05:09,250 --> 01:05:15,010
speech recognition as an eval metric? [NOISE]

1338
01:05:15,010 --> 01:05:23,590
[inaudible] word error rate [inaudible].

1339
01:05:23,590 --> 01:05:25,375
The answer is, you often use WER,

1340
01:05:25,375 --> 01:05:27,550
uh, for eval, but you might also use perplexity.

1341
01:05:27,550 --> 01:05:29,500
Yeah. Any other questions?

1342
01:05:29,500 --> 01:05:35,575
[NOISE] Okay. So, um,

1343
01:05:35,575 --> 01:05:38,350
this is an example of a conditional language model,

1344
01:05:38,350 --> 01:05:39,970
and it's called a conditional language model

1345
01:05:39,970 --> 01:05:41,725
because we have the language model component,

1346
01:05:41,725 --> 01:05:44,740
but crucially, we're conditioning it on some kind of input.

1347
01:05:44,740 --> 01:05:48,580
So, unlike the, uh, fun examples like with the Harry Potter text where we were just, uh,

1348
01:05:48,580 --> 01:05:51,460
generating text basically unconditionally, you know,

1349
01:05:51,460 --> 01:05:52,750
we trained it on the training data, and then,

1350
01:05:52,750 --> 01:05:54,820
we just started [NOISE] with some kind of random seed,

1351
01:05:54,820 --> 01:05:56,305
and then, it generates unconditionally.

1352
01:05:56,305 --> 01:05:58,540
This is called a conditional language model

1353
01:05:58,540 --> 01:06:01,615
because there's some kind of input that we need to condition on.

1354
01:06:01,615 --> 01:06:05,980
Uh, machine translation is an example [NOISE] also of a conditional language model,

1355
01:06:05,980 --> 01:06:07,780
and we're going to see that in much more detail in

1356
01:06:07,780 --> 01:06:09,520
the lecture next week on machine translation.

1357
01:06:09,520 --> 01:06:12,895
[NOISE] All right. Are there any more questions?

1358
01:06:12,895 --> 01:06:14,320
You have a bit of extra time, I think.

1359
01:06:14,320 --> 01:06:17,665
[NOISE] Yeah.

1360
01:06:17,665 --> 01:06:20,350
I have a question about RNNs in general.

1361
01:06:20,350 --> 01:06:25,345
[NOISE] Do people ever combine the RNN,

1362
01:06:25,345 --> 01:06:27,220
uh, patterns of architecture,

1363
01:06:27,220 --> 01:06:29,965
um, with other neural networks?

1364
01:06:29,965 --> 01:06:31,885
Say, [NOISE] you have, um, you know,

1365
01:06:31,885 --> 01:06:34,285
N previous layers that could be doing anything,

1366
01:06:34,285 --> 01:06:35,410
and at the end of your network,

1367
01:06:35,410 --> 01:06:36,880
you wanna run them through,

1368
01:06:36,880 --> 01:06:39,160
uh, five recurrent layers.

1369
01:06:39,160 --> 01:06:40,810
Do people mix and match like that,

1370
01:06:40,810 --> 01:06:42,190
or these, uh, [inaudible]. [NOISE]

1371
01:06:42,190 --> 01:06:46,090
Uh, the question is,

1372
01:06:46,090 --> 01:06:48,580
do you ever combine RNN for the other types of architecture?

1373
01:06:48,580 --> 01:06:49,870
So, I think the answer is yes.

1374
01:06:49,870 --> 01:06:51,595
[NOISE] Uh, you might, [NOISE] you know, uh,

1375
01:06:51,595 --> 01:06:55,210
have- you might have other types of architectures, uh,

1376
01:06:55,210 --> 01:06:58,540
to produce the vectors that are going to be the input to RNN,

1377
01:06:58,540 --> 01:07:00,280
or you might use the output of your RNN

1378
01:07:00,280 --> 01:07:03,320
[NOISE] and feed that into a different type of neural network.

1379
01:07:06,390 --> 01:07:08,620
So, yes. [NOISE] Any other questions?

1380
01:07:08,620 --> 01:07:11,820
[NOISE] Okay.

1381
01:07:11,820 --> 01:07:15,510
Uh, so, before we finish, uh, I have a note on terminology.

1382
01:07:15,510 --> 01:07:17,490
Uh, when you're reading papers,

1383
01:07:17,490 --> 01:07:20,915
you might find often this phrase vanilla RNN,

1384
01:07:20,915 --> 01:07:23,065
and when you see the phrase vanilla RNN,

1385
01:07:23,065 --> 01:07:24,535
that usually means, uh,

1386
01:07:24,535 --> 01:07:26,905
the RNNs that are described in this lecture.

1387
01:07:26,905 --> 01:07:30,460
So, the reason why those are called vanilla RNNs is

1388
01:07:30,460 --> 01:07:34,765
because there are actually other more complex kinds of RNN flavors.

1389
01:07:34,765 --> 01:07:38,005
So, for example, there's GRU and LSTM,

1390
01:07:38,005 --> 01:07:40,330
and we're gonna learn about both of those next week.

1391
01:07:40,330 --> 01:07:42,610
And another thing we're going to learn about next week

1392
01:07:42,610 --> 01:07:45,085
[NOISE] is that you can actually get some multi-layer RNNs,

1393
01:07:45,085 --> 01:07:48,250
which is when you stack multiple RNNs on top of each other.

1394
01:07:48,250 --> 01:07:50,935
[NOISE] So, uh, you're gonna learn about those,

1395
01:07:50,935 --> 01:07:53,875
but we hope that by the time you reach the end of this course,

1396
01:07:53,875 --> 01:07:56,905
you're going to be able to read a research paper and see a phrase like

1397
01:07:56,905 --> 01:08:01,150
stacked bidirectional LSTM with residual connections and self-attention,

1398
01:08:01,150 --> 01:08:02,680
and you'll know exactly what that is.

1399
01:08:02,680 --> 01:08:04,840
[NOISE] That's just an RNN with all of the toppings.

1400
01:08:04,840 --> 01:08:07,840
[LAUGHTER] All right. Thank you. That's it for today.

1401
01:08:07,840 --> 01:08:15,910
[NOISE] Uh, next time- [APPLAUSE] next time,

1402
01:08:15,910 --> 01:08:18,340
we're learning about problems [NOISE] and fancy RNNs.

1403
01:08:18,340 --> 01:08:24,770
[NOISE]

