1
00:00:04,460 --> 00:00:07,800
Okay. Hi everyone, uh, let's get started.

2
00:00:07,800 --> 00:00:10,935
Um, so Chris is traveling this week so he's not here.

3
00:00:10,935 --> 00:00:13,320
But I'm very excited to say that today we've got

4
00:00:13,320 --> 00:00:17,640
Margaret Mitchell who is a Senior Research Scientist at Google AI.

5
00:00:17,640 --> 00:00:20,435
She's going to tell us about, uh, the latest

6
00:00:20,435 --> 00:00:23,625
work defining and understanding and improving

7
00:00:23,625 --> 00:00:27,255
the situation with bias in artificial intelligence.

8
00:00:27,255 --> 00:00:30,395
Uh, Margaret has a background working in NLP and deep learning,

9
00:00:30,395 --> 00:00:33,440
so I'm really interested to hear what she has to say today. Take it away.

10
00:00:33,440 --> 00:00:36,620
Great, thank you. Um, can you guys hear me okay?

11
00:00:36,620 --> 00:00:39,230
I'm not sure if this mic is exactly picking up my voice,

12
00:00:39,230 --> 00:00:41,300
everything's cool? Okay, cool.

13
00:00:41,300 --> 00:00:44,285
Um, so this work is, uh,

14
00:00:44,285 --> 00:00:46,250
the product of a ton of different people and

15
00:00:46,250 --> 00:00:48,520
collaborators that I've tried to put up here.

16
00:00:48,520 --> 00:00:53,330
Um, some students at Stanford also Johns Hopkins, Google,

17
00:00:53,330 --> 00:00:58,820
Facebook and Microsoft are all represented, cool.

18
00:00:58,820 --> 00:01:05,915
So, um, for those of you who haven't seen the set of slides before,

19
00:01:05,915 --> 00:01:08,480
what do you see here? Just shout it out.

20
00:01:08,480 --> 00:01:10,130
Bananas.

21
00:01:10,130 --> 00:01:12,130
Bananas. Okay what else?

22
00:01:12,130 --> 00:01:13,345
Stickers.

23
00:01:13,345 --> 00:01:14,570
Stickers. What else?

24
00:01:14,570 --> 00:01:20,810
[NOISE] Shelves. What else?

25
00:01:20,810 --> 00:01:21,995
Bunches of bananas.

26
00:01:21,995 --> 00:01:24,540
Bunches of bananas. What else?

27
00:01:24,580 --> 00:01:28,025
Yellow, ripe bananas.

28
00:01:28,025 --> 00:01:30,500
You said ripe bananas, good.

29
00:01:30,500 --> 00:01:34,730
So you can add [LAUGHTER] bananas with stickers on them.

30
00:01:34,730 --> 00:01:37,400
You can start doing, like, embedded clauses, you know,

31
00:01:37,400 --> 00:01:41,180
bunches of bananas with stickers on them on shelves in the store to get, kinda, crazy.

32
00:01:41,180 --> 00:01:44,875
But we don't tend to say yellow bananas, right?

33
00:01:44,875 --> 00:01:47,160
So given something like this,

34
00:01:47,160 --> 00:01:51,455
we might say green bananas or we might say unripe bananas.

35
00:01:51,455 --> 00:01:55,130
Given an image like this we might say ripe bananas or,

36
00:01:55,130 --> 00:01:57,340
uh, bananas with spots on them.

37
00:01:57,340 --> 00:02:00,935
Uh, if you're me, you might say bananas that are good for banana bread.

38
00:02:00,935 --> 00:02:05,240
Um, but given an image like this or something like this in the real world,

39
00:02:05,240 --> 00:02:07,955
we tend not to mention the yellowness.

40
00:02:07,955 --> 00:02:12,710
And the reason for this is because yellow is prototypical for bananas.

41
00:02:12,710 --> 00:02:15,545
So the idea of prototypes, uh,

42
00:02:15,545 --> 00:02:18,620
stems from prototype theory which goes back to the early '70s,

43
00:02:18,620 --> 00:02:21,795
uh, coming out of the work of Eleanor Rosch and colleagues.

44
00:02:21,795 --> 00:02:23,750
Um, and it's this idea that there are

45
00:02:23,750 --> 00:02:28,335
some stored central prototypical notions of objects,

46
00:02:28,335 --> 00:02:31,559
um, that we access as we're operating,

47
00:02:31,559 --> 00:02:33,065
uh, throughout the world.

48
00:02:33,065 --> 00:02:36,950
There's some disagreement about whether these prototypes are

49
00:02:36,950 --> 00:02:42,320
actual exemplars of objects or something like a distribution over what's likely,

50
00:02:42,320 --> 00:02:45,440
but there is general agreement that we do have some, sort of,

51
00:02:45,440 --> 00:02:49,340
sense of what's typical and what's a typical of the things in

52
00:02:49,340 --> 00:02:55,200
the world and we tend to notice and talk about the things that are atypical.

53
00:02:55,400 --> 00:02:59,330
Um, so this is a riddle that I

54
00:02:59,330 --> 00:03:02,710
heard in middle school that worked a little bit more at that time,

55
00:03:02,710 --> 00:03:04,895
um, some of you might have heard it before.

56
00:03:04,895 --> 00:03:06,350
A man and his son are in

57
00:03:06,350 --> 00:03:09,875
a terrible accident and are rushed to the hospital in critical care.

58
00:03:09,875 --> 00:03:12,245
The doctor looks at the boy and exclaims,

59
00:03:12,245 --> 00:03:13,775
"I can't operate on this boy,

60
00:03:13,775 --> 00:03:17,070
he's my son," How could this be? [NOISE].

61
00:03:17,070 --> 00:03:19,190
Two dads?

62
00:03:19,190 --> 00:03:22,905
Two dads or he has a mum who's a doctor, right.

63
00:03:22,905 --> 00:03:25,920
Otherwise known as a female doctor,

64
00:03:25,920 --> 00:03:29,090
which might be contracted- contrasted with doctor.

65
00:03:29,090 --> 00:03:31,920
Um, in a study they did,

66
00:03:31,920 --> 00:03:33,120
uh, when they first, sort of,

67
00:03:33,120 --> 00:03:36,450
put forward this riddle at Boston University,

68
00:03:36,450 --> 00:03:38,375
they found that the majority of test subjects

69
00:03:38,375 --> 00:03:41,510
overlooked the possibility that the doctor could be a she.

70
00:03:41,510 --> 00:03:45,695
And that included men, women and self-described feminists.

71
00:03:45,695 --> 00:03:46,940
So the point is that,

72
00:03:46,940 --> 00:03:49,100
these, kinds of, uh,

73
00:03:49,100 --> 00:03:52,880
ways of talking about things and assumptions that we make,

74
00:03:52,880 --> 00:03:58,070
aren't necessarily something that speaks to a negative intent,

75
00:03:58,070 --> 00:04:01,670
but something that speaks to how we actually store representations in

76
00:04:01,670 --> 00:04:05,600
our minds and how we access those representations as we interact,

77
00:04:05,600 --> 00:04:07,285
uh, in the world.

78
00:04:07,285 --> 00:04:11,825
So this, uh, this affects what we can learn when we're learning from text.

79
00:04:11,825 --> 00:04:15,395
So, um, this is work from 2013,

80
00:04:15,395 --> 00:04:17,300
where they took a look at what was, sort of,

81
00:04:17,300 --> 00:04:21,355
most likely, what would you learn if you were just learning from raw text,

82
00:04:21,355 --> 00:04:24,440
um, what were some things that were common in the world?

83
00:04:24,440 --> 00:04:28,070
Um, they found that in this setup

84
00:04:28,070 --> 00:04:32,090
something like murdering was ten times more likely than blinking.

85
00:04:32,090 --> 00:04:34,850
And the reason for this is because people tend

86
00:04:34,850 --> 00:04:38,150
not to mention these typical things that go without saying.

87
00:04:38,150 --> 00:04:42,500
We don't tend to mention things like blinking and breathing,

88
00:04:42,500 --> 00:04:47,250
but we do mention atypical events like murder and that affects the, kind of,

89
00:04:47,250 --> 00:04:50,960
things a machine can learn from texts that we put out in the world,

90
00:04:50,960 --> 00:04:52,490
because it's been subject to all of

91
00:04:52,490 --> 00:04:57,135
these filtering processes that we have as humans before we, uh, communicate.

92
00:04:57,135 --> 00:05:01,690
Um, this issue in particular is known as Human Reporting Bias.

93
00:05:01,690 --> 00:05:04,010
Which is that the frequency with which people write

94
00:05:04,010 --> 00:05:06,650
about actions, outcomes or properties,

95
00:05:06,650 --> 00:05:09,470
is not a reflection of real-world frequencies or

96
00:05:09,470 --> 00:05:13,010
the degree to which a property is characteristic of a class of individuals,

97
00:05:13,010 --> 00:05:14,990
but says a lot more about how we're actually

98
00:05:14,990 --> 00:05:18,205
processing the world and what we think is remarkable.

99
00:05:18,205 --> 00:05:22,350
So this affects everything a system can learn.

100
00:05:22,350 --> 00:05:24,620
Um, in a typical machine learning paradigm,

101
00:05:24,620 --> 00:05:29,210
one of the first steps is to collect and potentially annotate training data.

102
00:05:29,210 --> 00:05:32,230
From there a model can be trained,

103
00:05:32,230 --> 00:05:34,965
uh, from there, uh,

104
00:05:34,965 --> 00:05:38,340
media can be filtered rank- ranked, aggregated,

105
00:05:38,340 --> 00:05:39,900
generated in some way,

106
00:05:39,900 --> 00:05:42,985
um, and from there people see the output.

107
00:05:42,985 --> 00:05:46,699
And we like to think of this as a relatively straightforward pipeline,

108
00:05:46,699 --> 00:05:49,100
um, but at the very start, uh,

109
00:05:49,100 --> 00:05:51,080
even before we're collecting the data,

110
00:05:51,080 --> 00:05:52,895
actually within the data itself,

111
00:05:52,895 --> 00:05:55,940
are a host of different kinds of human biases.

112
00:05:55,940 --> 00:05:58,580
So things like stereotyping, things like prejudice,

113
00:05:58,580 --> 00:06:03,080
things like racism and that's embedded within the data before we collect it.

114
00:06:03,080 --> 00:06:05,600
Then as we collect and annotate data,

115
00:06:05,600 --> 00:06:07,745
further biases become introduced.

116
00:06:07,745 --> 00:06:11,815
So things like sampling errors, confirmation bias, um,

117
00:06:11,815 --> 00:06:15,050
uh, in-group bias and out-group bias and I'll talk about these,

118
00:06:15,050 --> 00:06:16,070
um, a little bit.

119
00:06:16,070 --> 00:06:19,625
Oh, and I should mention feel free to ask questions as I go,

120
00:06:19,625 --> 00:06:21,380
um, totally fine to just,

121
00:06:21,380 --> 00:06:23,825
kind of, interact, uh, throughout.

122
00:06:23,825 --> 00:06:27,290
So here are some of the biases that I think are

123
00:06:27,290 --> 00:06:30,320
relatively important for work in AI and machine learning.

124
00:06:30,320 --> 00:06:32,570
There's hundreds you can go into,

125
00:06:32,570 --> 00:06:34,370
um, but some of the ones that I've, sort of,

126
00:06:34,370 --> 00:06:36,750
become the most aware of working in this space,

127
00:06:36,750 --> 00:06:39,680
um, are these sets and I'll go through each of these a bit.

128
00:06:39,680 --> 00:06:42,140
Um, so I talked about reporting bias earlier,

129
00:06:42,140 --> 00:06:45,470
which is, uh, which affects what we can learn from text.

130
00:06:45,470 --> 00:06:48,890
Um, another example of a kind

131
00:06:48,890 --> 00:06:52,135
of bias that really affects what we can learn from text is selection bias.

132
00:06:52,135 --> 00:06:54,835
So, uh, a lot of times that we,

133
00:06:54,835 --> 00:06:57,440
a lot of times when we get data annotated we'd use something

134
00:06:57,440 --> 00:07:00,230
like Amazon's Mechanical Turk, um,

135
00:07:00,230 --> 00:07:03,890
and the distribution of workers across the world is not even, sort of,

136
00:07:03,890 --> 00:07:06,485
uniform distribution, it's actually, um,

137
00:07:06,485 --> 00:07:09,920
concentrated in India, the US and then some in Europe.

138
00:07:09,920 --> 00:07:12,065
So this leaves out South America,

139
00:07:12,065 --> 00:07:13,250
this leaves out Africa,

140
00:07:13,250 --> 00:07:16,220
this leaves out a lot of China and that affects the, kind of,

141
00:07:16,220 --> 00:07:20,640
things that we'll be able to learn about the world when we have things annotated.

142
00:07:20,920 --> 00:07:25,250
Um, another kind of bias is Out-group Homogeneity Bias,

143
00:07:25,250 --> 00:07:29,405
which is the tendency to see out-group members as more alike than in-group members.

144
00:07:29,405 --> 00:07:32,420
And this is gonna affect what people are able to describe

145
00:07:32,420 --> 00:07:35,575
and talk about when they're annotating things such as emotion.

146
00:07:35,575 --> 00:07:38,565
So, uh, so for example we have these two, like,

147
00:07:38,565 --> 00:07:42,265
adorable puppies on the left here and they're looking at these four cats.

148
00:07:42,265 --> 00:07:44,715
Um, these are all different black cats,

149
00:07:44,715 --> 00:07:46,200
very different in different ways,

150
00:07:46,200 --> 00:07:50,825
but the two puppies look at the cats and they see four cats basically the same.

151
00:07:50,825 --> 00:07:53,720
And it's kind of trivial to understand how that also extends to

152
00:07:53,720 --> 00:07:56,795
human cognition and how we also process people.

153
00:07:56,795 --> 00:07:59,869
Um, it's this- it's the sense we have that the,

154
00:07:59,869 --> 00:08:01,915
the cohort that we're in,

155
00:08:01,915 --> 00:08:03,560
the people that we interact with,

156
00:08:03,560 --> 00:08:06,230
those are the kinds of people that are nuanced and

157
00:08:06,230 --> 00:08:08,930
everybody else is somehow less nuanced,

158
00:08:08,930 --> 00:08:10,715
has less detail to them.

159
00:08:10,715 --> 00:08:14,870
It's a trick our minds play on us in order to help us process the world,

160
00:08:14,870 --> 00:08:19,380
but it affects how we talk about it and it affects further how we annotate it.

161
00:08:19,930 --> 00:08:24,055
Um, this leads to stuff like biased data representations.

162
00:08:24,055 --> 00:08:27,170
So it's possible that you have an appropriate amount of data for

163
00:08:27,170 --> 00:08:30,710
every possible human group you can think of in your data,

164
00:08:30,710 --> 00:08:32,945
um, but it might be the case that some groups

165
00:08:32,945 --> 00:08:35,090
are represented less positively than others.

166
00:08:35,090 --> 00:08:37,190
And if we have time I'll go into, uh,

167
00:08:37,190 --> 00:08:39,970
a long- a longer example of that.

168
00:08:39,970 --> 00:08:43,045
Um, it also leads to things like biased labels.

169
00:08:43,045 --> 00:08:46,160
So, um, this is an issue that came up when we were

170
00:08:46,160 --> 00:08:49,580
getting some annotations for Inclusive Images competition,

171
00:08:49,580 --> 00:08:54,605
asking people to annotate things like bride and wedding and groom.

172
00:08:54,605 --> 00:08:57,100
And we found that given three different kinds of bride,

173
00:08:57,100 --> 00:08:58,685
wedding and groom images,

174
00:08:58,685 --> 00:09:03,465
um, ones that were more Western, European American, uh,

175
00:09:03,465 --> 00:09:06,985
got the appropriate labels and ones that weren't,

176
00:09:06,985 --> 00:09:09,190
just got, sort of, more generic person,

177
00:09:09,190 --> 00:09:11,235
kinds of, labels, uh,

178
00:09:11,235 --> 00:09:17,960
not able to actually tease out what's actually happening in these images.

179
00:09:17,960 --> 00:09:21,160
Compounding this issue are biases in interpretation

180
00:09:21,160 --> 00:09:24,145
when the model outputs, uh, its decisions.

181
00:09:24,145 --> 00:09:27,820
So, um, one, one issue is confirmation bias,

182
00:09:27,820 --> 00:09:30,820
which is the tendency to search for, interpret, favor,

183
00:09:30,820 --> 00:09:34,180
recall information in a way that confirms preexisting beliefs.

184
00:09:34,180 --> 00:09:36,520
And so a lot of times when we, uh,

185
00:09:36,520 --> 00:09:40,120
build end-to-end systems and try and test our hypotheses,

186
00:09:40,120 --> 00:09:42,325
we're kind of just testing it towards, uh,

187
00:09:42,325 --> 00:09:46,525
things that we want to be true and analyzing the results in a way that will,

188
00:09:46,525 --> 00:09:49,000
uh, help confirm what we want to be true.

189
00:09:49,000 --> 00:09:52,120
Um, overgeneralization, which is coming to

190
00:09:52,120 --> 00:09:55,765
a conclusion based on information that's too general or not specific enough.

191
00:09:55,765 --> 00:09:58,165
Um, this is an issue that happens a lot of times

192
00:09:58,165 --> 00:10:01,555
in the analysis of deep learning model results um,

193
00:10:01,555 --> 00:10:03,760
where it's assumed that there's,

194
00:10:03,760 --> 00:10:05,470
there's some kind of general, uh,

195
00:10:05,470 --> 00:10:08,515
conclusion that can be taken away when really it's actually just,

196
00:10:08,515 --> 00:10:10,615
uh, an effect of really skewed data.

197
00:10:10,615 --> 00:10:13,900
Um, this is also closely related to overfitting which

198
00:10:13,900 --> 00:10:17,275
is kind of the machine learning version of overgeneralization,

199
00:10:17,275 --> 00:10:20,050
which is where you're still making predictions and outcomes,

200
00:10:20,050 --> 00:10:23,965
but it's based on a small set of possible features, um,

201
00:10:23,965 --> 00:10:28,600
so it's not actually capturing the space of the correct features for the outcome,

202
00:10:28,600 --> 00:10:31,795
uh, the desired output prediction correctly.

203
00:10:31,795 --> 00:10:35,320
Um, there's also a correlation fallacy,

204
00:10:35,320 --> 00:10:37,975
which is confusing correlation with causation.

205
00:10:37,975 --> 00:10:40,510
And this happens a lot again in talking about what

206
00:10:40,510 --> 00:10:41,920
machine learning models are learning and

207
00:10:41,920 --> 00:10:44,575
deep learning models are learning in particular, um,

208
00:10:44,575 --> 00:10:47,095
where just because things happen together,

209
00:10:47,095 --> 00:10:49,240
doesn't mean that one is causing the other,

210
00:10:49,240 --> 00:10:50,770
but, uh, models don't tell you

211
00:10:50,770 --> 00:10:52,900
anything- deep learning models directly don't

212
00:10:52,900 --> 00:10:55,150
tell you anything about the causal relations.

213
00:10:55,150 --> 00:10:58,030
And so it's easy to think that some output that is predicted

214
00:10:58,030 --> 00:11:01,195
based on a correlation is actually something that's causal,

215
00:11:01,195 --> 00:11:03,310
and I'll talk about some examples of this too.

216
00:11:03,310 --> 00:11:07,120
Um, a further issue is automation bias,

217
00:11:07,120 --> 00:11:10,870
and this really affects the machine learning models we put out there in the world that

218
00:11:10,870 --> 00:11:15,010
then get used by people in systems like justice systems.

219
00:11:15,010 --> 00:11:17,215
Um, so that's the tendency to, um,

220
00:11:17,215 --> 00:11:19,750
favor the suggestions of

221
00:11:19,750 --> 00:11:24,535
automatic predictions of models that output predictions over the,

222
00:11:24,535 --> 00:11:27,880
um, uh, over the different kinds of um,

223
00:11:27,880 --> 00:11:29,740
suggestions of another human.

224
00:11:29,740 --> 00:11:33,070
Um, and this happens even in the face of contradictory evidence.

225
00:11:33,070 --> 00:11:36,085
So, if a system is telling you, you know, "This,

226
00:11:36,085 --> 00:11:40,810
this is the score or this is the risk of this individual",

227
00:11:40,810 --> 00:11:45,970
then we're more likely to think it's true because it came out of a mathematical system,

228
00:11:45,970 --> 00:11:48,850
and we automatically sort of see this as something more objective,

229
00:11:48,850 --> 00:11:50,860
something more mathematical, that something's going to

230
00:11:50,860 --> 00:11:53,440
be more true than humans some- somehow.

231
00:11:53,440 --> 00:11:56,155
Um, and that's automation bias.

232
00:11:56,155 --> 00:11:58,915
So, um, rather than this kind of

233
00:11:58,915 --> 00:12:02,215
clean straightforward pipeline that we have in machine learning,

234
00:12:02,215 --> 00:12:06,430
um, we have human bias coming in at the very start in the data, um,

235
00:12:06,430 --> 00:12:10,570
and then human bias coming in in data collection, annotation,

236
00:12:10,570 --> 00:12:14,290
and then further getting propagated through the system as we train on that data,

237
00:12:14,290 --> 00:12:17,605
um, as we start putting outputs based on that data,

238
00:12:17,605 --> 00:12:19,195
as people act on that data.

239
00:12:19,195 --> 00:12:21,385
And this creates a feedback loop where

240
00:12:21,385 --> 00:12:25,555
the kinds of things that we output for people to act on,

241
00:12:25,555 --> 00:12:27,520
um, are then, are then,

242
00:12:27,520 --> 00:12:31,345
then serves as further training data for input into your system,

243
00:12:31,345 --> 00:12:36,640
so you end up amplifying even further these different kinds of implicit biases.

244
00:12:36,640 --> 00:12:41,935
This is known as a Bias Network Effect or Bias "Laundering", I like to call it.

245
00:12:41,935 --> 00:12:47,530
And so, the message is that human data perpetuates human biases.

246
00:12:47,530 --> 00:12:51,400
And then as as machine learning or deep learning learns from human data,

247
00:12:51,400 --> 00:12:54,025
the results is a bias network effect.

248
00:12:54,025 --> 00:13:01,210
So, I want to steer clear of the idea that if I say bias or someone says bias that equals bad,

249
00:13:01,210 --> 00:13:03,130
it's a little bit more nuanced than that.

250
00:13:03,130 --> 00:13:05,650
Um, so there are all kinds of things that people

251
00:13:05,650 --> 00:13:08,155
mean when they're talking about bias, um,

252
00:13:08,155 --> 00:13:12,415
and even the same bias can be good in some situations and bad in some situations,

253
00:13:12,415 --> 00:13:14,590
so bias in statistics and ML.

254
00:13:14,590 --> 00:13:17,410
Um, we, we talk about the bias of an estimator which is

255
00:13:17,410 --> 00:13:20,980
the difference between the predictions and the truth, the ground truth.

256
00:13:20,980 --> 00:13:24,085
Uh, we talk about the bias term in linear regression.

257
00:13:24,085 --> 00:13:26,649
Um, we also have cognitive biases,

258
00:13:26,649 --> 00:13:28,210
and I talked about that in the beginning,

259
00:13:28,210 --> 00:13:30,220
and not all of those are negative or,

260
00:13:30,220 --> 00:13:31,675
or have to be, uh,

261
00:13:31,675 --> 00:13:33,400
or have to be seen as negative.

262
00:13:33,400 --> 00:13:36,430
So optimism is another kind of bias that we can

263
00:13:36,430 --> 00:13:39,640
have that affects our worldview and the way we sort of process things.

264
00:13:39,640 --> 00:13:42,250
Um, and even things like recency bias and

265
00:13:42,250 --> 00:13:45,760
confirmation bias are just ways that our minds can like, um,

266
00:13:45,760 --> 00:13:50,080
handle the combinatorial explosion of all the different things that can be

267
00:13:50,080 --> 00:13:52,030
true in the world and put it down to something

268
00:13:52,030 --> 00:13:55,060
tractable that we can sort of operate with in the real world.

269
00:13:55,060 --> 00:13:58,345
Um, so algorithmic bias is what a lot

270
00:13:58,345 --> 00:14:01,645
of people mean in headlines and whatnot when we're talking about bias,

271
00:14:01,645 --> 00:14:03,790
which is, uh, more about unjust,

272
00:14:03,790 --> 00:14:07,510
unfair or prejudicial treatment of people that's an output of,

273
00:14:07,510 --> 00:14:09,610
a automated decision system.

274
00:14:09,610 --> 00:14:12,505
Um, and the focus here is really on, uh,

275
00:14:12,505 --> 00:14:15,460
unjust, unfair or prejudicial treatment of people.

276
00:14:15,460 --> 00:14:19,315
So, a lot of the work in this space right now is focusing on trying to understand,

277
00:14:19,315 --> 00:14:22,480
what does it mean to be unjust from an algorithm,

278
00:14:22,480 --> 00:14:25,570
what does it mean to be unfair from an algorithm,

279
00:14:25,570 --> 00:14:27,190
and how can we handle this,

280
00:14:27,190 --> 00:14:30,490
how can we sort of mitigate these issues in order to be able to keep

281
00:14:30,490 --> 00:14:34,675
developing technology that's useful for people without worsening social divides.

282
00:14:34,675 --> 00:14:41,170
Um, and I felt the Guardian put it really well a few years ago.

283
00:14:41,170 --> 00:14:45,295
Um, they said, "Although neural networks might be said to write their own programs,

284
00:14:45,295 --> 00:14:50,125
they do so towards goals set by humans using data collected for human purposes.

285
00:14:50,125 --> 00:14:52,420
If the data is skewed, even by accident,

286
00:14:52,420 --> 00:14:54,865
the computers will amplify injustice."

287
00:14:54,865 --> 00:14:58,540
And it really keyed in on this amplify injustice idea.

288
00:14:58,540 --> 00:15:01,555
Um, and let's talk about what that can mean.

289
00:15:01,555 --> 00:15:05,680
So, one of the avenues of deep learning research that's

290
00:15:05,680 --> 00:15:09,415
taken off in the past few years is predicting criminal behavior.

291
00:15:09,415 --> 00:15:14,020
Um, so, um, how many of you are familiar with Predictive Policing?

292
00:15:14,020 --> 00:15:17,995
[NOISE] Okay, like, half of the class.

293
00:15:17,995 --> 00:15:22,345
Okay. So, in predictive policing, algorithms, um,

294
00:15:22,345 --> 00:15:28,675
predict areas to deploy officers where crime is considered to be likely to occur.

295
00:15:28,675 --> 00:15:33,520
But the data that the- the- the models are trained off

296
00:15:33,520 --> 00:15:38,935
of is based on where police officers have already gone and made arrests.

297
00:15:38,935 --> 00:15:42,520
So, the systems are simply learning the patterns of bias that

298
00:15:42,520 --> 00:15:46,900
humans have and where do they go and where they are trying to decide to def- uh,

299
00:15:46,900 --> 00:15:48,820
to find crime, um,

300
00:15:48,820 --> 00:15:50,425
and then reflecting them back.

301
00:15:50,425 --> 00:15:52,930
So, because this system hones in on some of

302
00:15:52,930 --> 00:15:56,125
the top spots where people have been arrested,

303
00:15:56,125 --> 00:15:57,940
notice that's not the same of- that's

304
00:15:57,940 --> 00:16:00,490
the same thing as where crimes have been committed, right?

305
00:16:00,490 --> 00:16:02,440
It's where arrests have been made.

306
00:16:02,440 --> 00:16:04,720
Um, it means that the other areas that

307
00:16:04,720 --> 00:16:07,240
might be explored for crime don't get explored at all.

308
00:16:07,240 --> 00:16:09,175
That worsens the situation.

309
00:16:09,175 --> 00:16:10,795
Um, some neighborhoods, uh,

310
00:16:10,795 --> 00:16:14,080
get really acutely focused attention on them,

311
00:16:14,080 --> 00:16:16,810
and that heightens the chances of serious repercussions for

312
00:16:16,810 --> 00:16:19,735
even minor infractions, that means arrests.

313
00:16:19,735 --> 00:16:22,060
And that means a feedback loop of data that

314
00:16:22,060 --> 00:16:25,400
you will get an arrest in this place if you go there.

315
00:16:25,470 --> 00:16:33,085
Um, another, uh, sort of related issue in this space is, uh, predictive sentencing.

316
00:16:33,085 --> 00:16:36,100
Um, so there was a really nice article that came out

317
00:16:36,100 --> 00:16:39,280
from Pro- ProPublica a few years ago discussing this.

318
00:16:39,280 --> 00:16:41,950
Um, but when most defendants are booked in jail,

319
00:16:41,950 --> 00:16:44,320
they respond to a questionnaire called COMPAS.

320
00:16:44,320 --> 00:16:47,920
Um, and their answers are fed into this software system that

321
00:16:47,920 --> 00:16:51,640
generates scores that correspond to the risk of recidivism,

322
00:16:51,640 --> 00:16:53,500
that's the risk of um,

323
00:16:53,500 --> 00:16:55,330
er, making a crime again.

324
00:16:55,330 --> 00:16:57,970
Um, and the questions are used to gather data

325
00:16:57,970 --> 00:17:00,565
on the defendant's socio-economic status,

326
00:17:00,565 --> 00:17:03,070
family background, neighborhood crime,

327
00:17:03,070 --> 00:17:05,830
employment status, and other factors in order to reach

328
00:17:05,830 --> 00:17:11,785
some predictim- prediction of an individual's crime or criminal risk.

329
00:17:11,785 --> 00:17:17,470
Um, but what ends up happening is that it ends up focusing on the key bias issues

330
00:17:17,470 --> 00:17:19,600
that humans have and propagating it

331
00:17:19,600 --> 00:17:23,215
back with something that looks like an objective score.

332
00:17:23,215 --> 00:17:26,065
So, you're a lot more likely um,

333
00:17:26,065 --> 00:17:28,315
to be convicted of a crime, um,

334
00:17:28,315 --> 00:17:30,055
if you're black than if you're white,

335
00:17:30,055 --> 00:17:31,975
even if you've made the exact same crime.

336
00:17:31,975 --> 00:17:34,330
And the system will pick up on this,

337
00:17:34,330 --> 00:17:36,790
and will reflect this back to say that people who are

338
00:17:36,790 --> 00:17:39,580
black are more likely to have reci- like recidivism,

339
00:17:39,580 --> 00:17:41,020
more likely to convict a,

340
00:17:41,020 --> 00:17:43,165
uh, to make a crime again.

341
00:17:43,165 --> 00:17:49,885
Um, so this is an example of automation bias, preferring the output of a system, uh,

342
00:17:49,885 --> 00:17:54,085
in the face of overgeneralization, feedback loops,

343
00:17:54,085 --> 00:17:55,795
and correlation fallacy,

344
00:17:55,795 --> 00:18:00,620
confusing things that are occurring together as being somehow causal.

345
00:18:02,520 --> 00:18:06,985
There's another, uh, sort of area of research and, uh,

346
00:18:06,985 --> 00:18:12,010
startups looking at predicting criminality in particular from things like face images.

347
00:18:12,010 --> 00:18:14,890
So there's a company out there, uh, called Faception.

348
00:18:14,890 --> 00:18:18,520
They are based in Israel and they claim to be able to,

349
00:18:18,520 --> 00:18:21,625
um, use individual images, uh,

350
00:18:21,625 --> 00:18:25,150
with computer vision and machine learning technology for profiling people

351
00:18:25,150 --> 00:18:28,855
and revealing their personality based only on their facial image,

352
00:18:28,855 --> 00:18:32,395
um, recognizing things like high IQ,

353
00:18:32,395 --> 00:18:35,575
white-collar offender, pedophile, and terrorist.

354
00:18:35,575 --> 00:18:38,350
Um, and their main clients are Homeland Security,

355
00:18:38,350 --> 00:18:39,550
lots of other, uh,

356
00:18:39,550 --> 00:18:42,985
lots of other countries dealing with sort of public safety issues.

357
00:18:42,985 --> 00:18:45,760
They've not published any details about their methods,

358
00:18:45,760 --> 00:18:47,425
their sources of training data,

359
00:18:47,425 --> 00:18:49,090
or their quantitative results.

360
00:18:49,090 --> 00:18:50,980
We know that in light of automation bias,

361
00:18:50,980 --> 00:18:54,535
people will tend to think it just works even when it doesn't work well.

362
00:18:54,535 --> 00:18:57,700
Um, but there was a paper that came out wi- in

363
00:18:57,700 --> 00:19:01,765
a similar line predicting criminal, criminality,

364
00:19:01,765 --> 00:19:05,890
or purporting to predict criminality from individual face images,

365
00:19:05,890 --> 00:19:08,110
and that one had some results and,

366
00:19:08,110 --> 00:19:11,350
uh, some more details about the data that we could kinda dig

367
00:19:11,350 --> 00:19:15,100
into to understand where are these kinds of claims coming from.

368
00:19:15,100 --> 00:19:19,880
Um, so this was an article that was posted on Archive near the end of 2016.

369
00:19:19,880 --> 00:19:26,230
Um, and they said they were using less than 2,000 closely cropped images of faces, um,

370
00:19:26,230 --> 00:19:30,415
including wanted suspect ID pictures from specific regions,

371
00:19:30,415 --> 00:19:35,035
and they claimed that even based on this very small training dataset, um,

372
00:19:35,035 --> 00:19:37,540
that they were able to predict, uh,

373
00:19:37,540 --> 00:19:40,240
whether or not someone was likely to be a criminal,

374
00:19:40,240 --> 00:19:42,640
uh, greater than 90 percent accuracy.

375
00:19:42,640 --> 00:19:45,235
Um, and they got so lost in this,

376
00:19:45,235 --> 00:19:47,020
this idea that, uh,

377
00:19:47,020 --> 00:19:49,150
it's sort of funny to read to just take

378
00:19:49,150 --> 00:19:51,640
a step back and realize what's actually happening.

379
00:19:51,640 --> 00:19:52,840
So for example, one of

380
00:19:52,840 --> 00:19:57,640
their really great exciting claims was that the angle Theta from nose tip to

381
00:19:57,640 --> 00:19:59,770
two mouth corners is on average

382
00:19:59,770 --> 00:20:03,640
19,6 percent smaller for criminals than for non-criminals.

383
00:20:03,640 --> 00:20:05,980
This is otherwise known as smiling. [LAUGHTER]

384
00:20:05,980 --> 00:20:09,070
Uh, and [LAUGHTER] you know,

385
00:20:09,070 --> 00:20:11,170
exactly the kind of images people would

386
00:20:11,170 --> 00:20:13,780
use when trying to put out wanted criminal pictures,

387
00:20:13,780 --> 00:20:15,655
probably not really happy pictures.

388
00:20:15,655 --> 00:20:18,265
But you get so lost in the confirmation bias.

389
00:20:18,265 --> 00:20:20,230
You get so lost in the correlation and the

390
00:20:20,230 --> 00:20:22,930
feedback loops that you end up overlooking these

391
00:20:22,930 --> 00:20:25,945
really obvious kinds of things.

392
00:20:25,945 --> 00:20:29,010
Um, so that's an example of selection bias,

393
00:20:29,010 --> 00:20:32,880
experimenter's bias, confirmation bias, correlation fallacy,

394
00:20:32,880 --> 00:20:36,090
and feedback loops all coming together to create

395
00:20:36,090 --> 00:20:37,950
a deep learning system that people think is

396
00:20:37,950 --> 00:20:40,730
scary and can do things that it can't actually do.

397
00:20:40,730 --> 00:20:44,485
Um, one of the issues with this was that the media loved it.

398
00:20:44,485 --> 00:20:46,180
Like it was all over the news,

399
00:20:46,180 --> 00:20:49,060
and there's been similar kinds of things happening again and again.

400
00:20:49,060 --> 00:20:51,175
Media wants to sell the story,

401
00:20:51,175 --> 00:20:53,830
and so it's part of our job as researchers,

402
00:20:53,830 --> 00:20:55,315
that people who work on this stuff,

403
00:20:55,315 --> 00:20:58,615
to be very clear about what the technology is actually doing,

404
00:20:58,615 --> 00:21:00,760
uh, and make a distinction between what you

405
00:21:00,760 --> 00:21:03,520
might think it's doing and what it's actually doing.

406
00:21:03,520 --> 00:21:07,930
Um, so another issue that has come up recently, um,

407
00:21:07,930 --> 00:21:09,490
it's claiming to be able to predict

408
00:21:09,490 --> 00:21:13,660
internal qualities but specifically ones that are subject to discrimination,

409
00:21:13,660 --> 00:21:15,535
um, and loss of opportunity.

410
00:21:15,535 --> 00:21:18,610
So in particular, there was this work that came out that claimed

411
00:21:18,610 --> 00:21:21,565
to be able to predict whether or not someone was homosexual,

412
00:21:21,565 --> 00:21:23,905
just based on single face images.

413
00:21:23,905 --> 00:21:28,390
Um, now, it's important to know that the images that they used in the study included

414
00:21:28,390 --> 00:21:33,400
images that were from dating websites where people self-identified as straight or gay,

415
00:21:33,400 --> 00:21:37,045
and identified as whether they were looking for a partner who was straight or gay,

416
00:21:37,045 --> 00:21:40,015
and these became the sources of the training data,

417
00:21:40,015 --> 00:21:41,890
and still from this, uh.

418
00:21:41,890 --> 00:21:44,470
Oh! Before I go on, can you guys just understand

419
00:21:44,470 --> 00:21:47,500
just from that what the issue might have been?

420
00:21:47,500 --> 00:21:48,640
Rainbows.

421
00:21:48,640 --> 00:21:54,250
[LAUGHTER] I don't think that there was actually anything about rainbows,

422
00:21:54,250 --> 00:21:55,390
but that's really unfortunate.

423
00:21:55,390 --> 00:21:59,980
[LAUGHTER].

424
00:21:59,980 --> 00:22:00,280
[inaudible]

425
00:22:00,280 --> 00:22:03,475
Right. Yeah. So this has more to do with the presentation of the self,

426
00:22:03,475 --> 00:22:07,030
the presentation of the social self when you're trying to for example,

427
00:22:07,030 --> 00:22:08,784
attract a partner on a website,

428
00:22:08,784 --> 00:22:11,695
and less to do with how you look day to day.

429
00:22:11,695 --> 00:22:14,800
Um, and yet they kind of went to

430
00:22:14,800 --> 00:22:19,660
these large conclusions that aren't supported at all by the data or by their study,

431
00:22:19,660 --> 00:22:24,595
um, but things like consistent with a prenatal hormone theory of sexual orientation.

432
00:22:24,595 --> 00:22:28,630
Gay men and women tended to have gender atypical facial morphology.

433
00:22:28,630 --> 00:22:34,420
Now, none of the authors actually were prenatal hormone theory specialists, you know.

434
00:22:34,420 --> 00:22:37,120
They have doctor in their name so maybe that's a thing.

435
00:22:37,120 --> 00:22:39,865
Um, this was a Stanford professor and like I've,

436
00:22:39,865 --> 00:22:42,160
I've presented this a few times at Stanford and gotten into

437
00:22:42,160 --> 00:22:44,680
some like pretty harsh fights about this.

438
00:22:44,680 --> 00:22:46,570
So I'm ready if anyone wants to take me on.

439
00:22:46,570 --> 00:22:50,410
[LAUGHTER] But uh, me and my uh,

440
00:22:50,410 --> 00:22:52,495
some of my colleagues decided we'd,

441
00:22:52,495 --> 00:22:54,025
we'd play around with this a bit,

442
00:22:54,025 --> 00:22:56,755
and what we found was that a simple decision tree.

443
00:22:56,755 --> 00:22:59,580
Um, so I'm kind of assuming you guys know what a decision tree is.

444
00:22:59,580 --> 00:23:01,090
So, okay.

445
00:23:01,090 --> 00:23:04,270
Cool. So based on wearing makeup or wearing glasses,

446
00:23:04,270 --> 00:23:06,790
got us pretty close to the accuracy reported in

447
00:23:06,790 --> 00:23:09,670
the paper. That says nothing about internal hormones,

448
00:23:09,670 --> 00:23:11,200
that says nothing about any of that,

449
00:23:11,200 --> 00:23:13,795
and it says a lot about the physical presentation,

450
00:23:13,795 --> 00:23:15,535
the things that are on the surface.

451
00:23:15,535 --> 00:23:17,830
Um, it says a lot more about how people are

452
00:23:17,830 --> 00:23:20,515
presenting themselves than what is happening internally.

453
00:23:20,515 --> 00:23:23,530
Um, so the key thing that's recently kind of

454
00:23:23,530 --> 00:23:26,695
been overlooked is that deep learning is somehow,

455
00:23:26,695 --> 00:23:30,940
i- it's sort of considered that it's somehow magically going beyond surface level.

456
00:23:30,940 --> 00:23:34,420
But the point is that it's working on the surface level and working well.

457
00:23:34,420 --> 00:23:38,260
And in the face of confirmation bias and other kinds of bias factors,

458
00:23:38,260 --> 00:23:41,455
it's easy to assume that something else is happening that's not.

459
00:23:41,455 --> 00:23:43,720
Without critical examination, uh,

460
00:23:43,720 --> 00:23:46,660
for example simple baselines, uh,

461
00:23:46,660 --> 00:23:50,635
simple sanity checks, these kinds of things can just be ignored and,

462
00:23:50,635 --> 00:23:52,705
and not noticed at all.

463
00:23:52,705 --> 00:23:56,620
Um, so that's example of selection bias,

464
00:23:56,620 --> 00:24:00,530
um, experimenter's bias, and correlation fallacy.

465
00:24:00,780 --> 00:24:03,655
Okay. So now I'm going to talk to,

466
00:24:03,655 --> 00:24:05,680
talk about measuring algorithmic bias.

467
00:24:05,680 --> 00:24:10,690
So I just said a lot about different kinds of biases that come in in the data,

468
00:24:10,690 --> 00:24:13,870
in the collection, in the interpretation of the results.

469
00:24:13,870 --> 00:24:18,265
[NOISE] Let's talk about actually quantitatively measuring different kinds of biases.

470
00:24:18,265 --> 00:24:22,045
Um, so one of the key things that's, uh,

471
00:24:22,045 --> 00:24:25,750
emerged in a few different works and really ties nicely to a lot

472
00:24:25,750 --> 00:24:29,650
of fairness work is this idea of disaggregated evaluation.

473
00:24:29,650 --> 00:24:31,855
So in disaggregated evaluation,

474
00:24:31,855 --> 00:24:35,230
you evaluate across different subgroups as opposed to

475
00:24:35,230 --> 00:24:39,280
looking at one single score for your overall testing data set.

476
00:24:39,280 --> 00:24:41,620
Um, so, okay.

477
00:24:41,620 --> 00:24:44,200
You guys are probably familiar with the training testing data split.

478
00:24:44,200 --> 00:24:45,505
You kind of train on there,

479
00:24:45,505 --> 00:24:47,320
on your given training data,

480
00:24:47,320 --> 00:24:51,565
you test on your given testing data and then you repo- you report like precision,

481
00:24:51,565 --> 00:24:53,875
recall, F-score, things like that.

482
00:24:53,875 --> 00:24:57,280
Um, but what that masks is how well the system is actually

483
00:24:57,280 --> 00:25:01,315
working across different kinds of individuals and across different, different subgroups.

484
00:25:01,315 --> 00:25:04,660
Um, and so one just straightforward way to handle

485
00:25:04,660 --> 00:25:08,320
this is to actually evaluate with respect to those different subgroups.

486
00:25:08,320 --> 00:25:11,650
So creating for each sort of subgroup prediction pair.

487
00:25:11,650 --> 00:25:13,450
Um, so for an example,

488
00:25:13,450 --> 00:25:15,535
you might look at women face detection,

489
00:25:15,535 --> 00:25:18,205
men face detection, and look at how the,

490
00:25:18,205 --> 00:25:19,600
the error rates are,

491
00:25:19,600 --> 00:25:22,375
are different or are um, similar.

492
00:25:22,375 --> 00:25:27,145
Um, another important part of this is to look at things intersectionally,

493
00:25:27,145 --> 00:25:29,530
um, combining things, um,

494
00:25:29,530 --> 00:25:34,000
like gender and race at the same time and seeing how those, uh,

495
00:25:34,000 --> 00:25:36,370
how the error rates on those sorts of things

496
00:25:36,370 --> 00:25:39,700
change and how they're different across uh, different intersections.

497
00:25:39,700 --> 00:25:42,445
Um, and this is inspired by Kimberle Crenshaw.

498
00:25:42,445 --> 00:25:45,879
Um, who she, she pioneered intersectional research,

499
00:25:45,879 --> 00:25:48,175
uh, in critical race theory.

500
00:25:48,175 --> 00:25:51,805
Um, and she discussed the story of Emma DeGraffenreid, uh,

501
00:25:51,805 --> 00:25:55,585
who was a woman at General Motors, um,

502
00:25:55,585 --> 00:25:59,770
and she claimed that the company's hiring practices discriminated against black women.

503
00:25:59,770 --> 00:26:01,780
Um, but in her court opinion,

504
00:26:01,780 --> 00:26:04,795
the judges ruled that General Motors hired, um,

505
00:26:04,795 --> 00:26:10,255
many women for secretarial positions and many black people for factory roles,

506
00:26:10,255 --> 00:26:14,050
and thus they could not have discriminated against black women.

507
00:26:14,050 --> 00:26:16,690
What they failed to do was look at the intersection of

508
00:26:16,690 --> 00:26:19,270
the two and understand that the experience there might be

509
00:26:19,270 --> 00:26:21,160
fundamentally different than any of

510
00:26:21,160 --> 00:26:25,840
the experiences of either of these sort of subgroups in isolation.

511
00:26:25,840 --> 00:26:29,290
Um, and the same becomes true when you start looking

512
00:26:29,290 --> 00:26:32,590
at errors that are regularly made in deep learning systems.

513
00:26:32,590 --> 00:26:34,630
Um, so we've been able to uncover a lot of

514
00:26:34,630 --> 00:26:37,360
different kinds of unintended errors by looking not only at

515
00:26:37,360 --> 00:26:43,475
the disaggregated evaluation but also at intersectional disaggregated evaluation.

516
00:26:43,475 --> 00:26:46,645
Um, so I'm going to walk through a bit how this works.

517
00:26:46,645 --> 00:26:49,660
This is probably going to be review for most of you,

518
00:26:49,660 --> 00:26:52,330
but I think it's really important to understand this because it also

519
00:26:52,330 --> 00:26:55,645
ties to how we measure fairness and when we say like,

520
00:26:55,645 --> 00:26:58,690
uh, algorithmic fairness, what we're talking about.

521
00:26:58,690 --> 00:27:02,740
So um, the confusion matrix is a way, you guys.

522
00:27:02,740 --> 00:27:04,750
Okay. Are you guys familiar with the confusion matrix?

523
00:27:04,750 --> 00:27:06,250
[LAUGHTER]. I just want want to know where.

524
00:27:06,250 --> 00:27:09,085
Okay. Awesome. Cool. So you're familiar with the confusion matrix, right.

525
00:27:09,085 --> 00:27:11,425
So you have model predictions and references.

526
00:27:11,425 --> 00:27:14,470
Um, and you can kind of look at these as negative and positive,

527
00:27:14,470 --> 00:27:17,110
uh, binary classification, uh,

528
00:27:17,110 --> 00:27:19,495
kind of approach here where if

529
00:27:19,495 --> 00:27:23,200
the ground truth says something is true and the model predicts it's true,

530
00:27:23,200 --> 00:27:24,385
it's a true positive.

531
00:27:24,385 --> 00:27:25,780
If the ground truth says, uh,

532
00:27:25,780 --> 00:27:27,520
it's, it's, it's false,

533
00:27:27,520 --> 00:27:31,045
um, and the model predicts it's false, it's true negative.

534
00:27:31,045 --> 00:27:33,670
Um, and the errors that the kind of different issues that

535
00:27:33,670 --> 00:27:36,400
arise are false negatives and false positives.

536
00:27:36,400 --> 00:27:39,670
Um, so in false positives the, um,

537
00:27:39,670 --> 00:27:44,290
the ground truth says something is negative but the model predicts that it's positive.

538
00:27:44,290 --> 00:27:47,560
Uh, and then in false negatives, vice versa.

539
00:27:47,560 --> 00:27:49,765
Um, from these, you know,

540
00:27:49,765 --> 00:27:51,250
uh, basic kind of, uh,

541
00:27:51,250 --> 00:27:53,290
these basic breakdown of errors,

542
00:27:53,290 --> 00:27:55,330
you can get a few different metrics.

543
00:27:55,330 --> 00:28:01,105
Um, these metrics actually trivially map to a lot of different fairness criteria.

544
00:28:01,105 --> 00:28:02,965
So um, for example,

545
00:28:02,965 --> 00:28:04,720
if we're looking at something like

546
00:28:04,720 --> 00:28:09,880
a female versus male patient results and figuring out things like precision and recall,

547
00:28:09,880 --> 00:28:12,670
which is relatively common in NLP, um,

548
00:28:12,670 --> 00:28:16,405
if you have equal recall across your subgroups

549
00:28:16,405 --> 00:28:20,875
that's the same as the fairness criteria of equality of opportunity,

550
00:28:20,875 --> 00:28:23,440
um, I could work through the math.

551
00:28:23,440 --> 00:28:24,805
But I mean, this is basically just,

552
00:28:24,805 --> 00:28:27,475
just the main point that, that, uh,

553
00:28:27,475 --> 00:28:32,200
it says that given that something is true in the ground truth,

554
00:28:32,200 --> 00:28:35,365
the model should predict that it's true,

555
00:28:35,365 --> 00:28:37,660
uh, at equal rates across different subgroups.

556
00:28:37,660 --> 00:28:41,965
So this ends up being equivalent to having the same recall across different subgroups.

557
00:28:41,965 --> 00:28:45,730
Similarly, um, having the same precision across

558
00:28:45,730 --> 00:28:50,800
different subgroups is equivalent to a fairness criterion called predictive parity.

559
00:28:50,800 --> 00:28:55,285
And so as fairness has been defined again and again, um,

560
00:28:55,285 --> 00:28:58,960
it was originally some of these definitions came in

561
00:28:58,960 --> 00:29:02,980
1966 following the Civil Rights Act of 1964.

562
00:29:02,980 --> 00:29:05,875
Um, they were reinvented a few times, uh,

563
00:29:05,875 --> 00:29:09,895
and most recently reinvented in, uh, 2016.

564
00:29:09,895 --> 00:29:12,790
Um, but they all sort of boiled down to

565
00:29:12,790 --> 00:29:16,540
this disaggregated comparison across subgroups and the math,

566
00:29:16,540 --> 00:29:20,770
the metrics end being roughly equivalent to what we get from the confusion matrix,

567
00:29:20,770 --> 00:29:24,560
specifically in classification systems.

568
00:29:25,980 --> 00:29:29,530
So which kind of fairness metric do you use,

569
00:29:29,530 --> 00:29:31,870
what are the different criteria you want

570
00:29:31,870 --> 00:29:35,020
to use to look at the differences across different subgroups,

571
00:29:35,020 --> 00:29:37,780
that really comes down to the trade-offs

572
00:29:37,780 --> 00:29:39,925
between false positives and false negatives.

573
00:29:39,925 --> 00:29:42,025
So this is the same problem that you are dealing with

574
00:29:42,025 --> 00:29:44,515
when you're just figuring out how to evaluate generally.

575
00:29:44,515 --> 00:29:46,900
Um, there's no one fairness criteria and that is

576
00:29:46,900 --> 00:29:49,960
the fairness criteria and to rule them all, um,

577
00:29:49,960 --> 00:29:52,630
deciding which one is better than the other is the same as

578
00:29:52,630 --> 00:29:55,480
kind of trying to decide which is better, precision or recall, right?

579
00:29:55,480 --> 00:29:58,630
It depends on what the problem is and what you're interested in measuring.

580
00:29:58,630 --> 00:30:03,100
Um, so a case where false positives might be better than

581
00:30:03,100 --> 00:30:07,780
false negatives and so you want to prioritize something like a false positive rate,

582
00:30:07,780 --> 00:30:10,630
ah, across subgroups is privacy and images.

583
00:30:10,630 --> 00:30:15,370
So here a false positive is something that doesn't need to be blurred gets blurred.

584
00:30:15,370 --> 00:30:16,780
That's just kind of a bummer.

585
00:30:16,780 --> 00:30:19,540
Um, but a false negative would be something that needs to be

586
00:30:19,540 --> 00:30:22,405
blurred is not blurred and that can be identity theft.

587
00:30:22,405 --> 00:30:24,295
It's a much more serious issue.

588
00:30:24,295 --> 00:30:26,110
And so it's important to prioritize

589
00:30:26,110 --> 00:30:29,440
the evaluation metrics that stress the false negative rates.

590
00:30:29,440 --> 00:30:32,650
Um, an example where false negatives

591
00:30:32,650 --> 00:30:35,335
might be better than false positives is in spam filtering.

592
00:30:35,335 --> 00:30:41,635
So a false-negative could be an e-mail that's spam not caught so you see it in your inbox,

593
00:30:41,635 --> 00:30:44,350
that's usually just annoying, it's not a big deal.

594
00:30:44,350 --> 00:30:47,170
Um, but a false positive here would be e-mail flagged as

595
00:30:47,170 --> 00:30:50,245
spam and then removed from your inbox, which,

596
00:30:50,245 --> 00:30:52,780
you know, if its from a friend or a loved one,

597
00:30:52,780 --> 00:30:54,535
it can be, it can be a loss,

598
00:30:54,535 --> 00:30:56,620
maybe a job offer something like that.

599
00:30:56,620 --> 00:30:58,360
All right.

600
00:30:58,360 --> 00:31:02,560
So, um, I just kind of covered how AI can unintentionally lead to

601
00:31:02,560 --> 00:31:05,050
unjust outcomes and some of the things to do

602
00:31:05,050 --> 00:31:07,045
or some of the things to be aware of here,

603
00:31:07,045 --> 00:31:11,590
are the lack of insight into sources of bias in the data, in the model,

604
00:31:11,590 --> 00:31:16,690
lack of insight into the feedback loops from the original data that's collected

605
00:31:16,690 --> 00:31:21,519
as an example of what humans do to the data that's then repurposed,

606
00:31:21,519 --> 00:31:24,475
re-used, acted on, and then further fed in.

607
00:31:24,475 --> 00:31:28,060
Um, a lack of careful disaggregated evaluation,

608
00:31:28,060 --> 00:31:29,560
looking at the disparities,

609
00:31:29,560 --> 00:31:33,730
the differences between different subgroups in order to understand this bias,

610
00:31:33,730 --> 00:31:35,605
this difference across the subgroups.

611
00:31:35,605 --> 00:31:38,949
Um, and then human biases in interpreting, and accepting,

612
00:31:38,949 --> 00:31:40,525
and talking about the results,

613
00:31:40,525 --> 00:31:45,950
which then kind of further the media cycles and the hype around AI right now.

614
00:31:46,080 --> 00:31:50,875
Um, but it's up to us to influence how AI evolves.

615
00:31:50,875 --> 00:31:55,330
So I like to think of this in terms of short term,

616
00:31:55,330 --> 00:31:57,295
middle term, and long-term objectives.

617
00:31:57,295 --> 00:32:00,355
So short term today,

618
00:32:00,355 --> 00:32:05,170
we might be working on some specific model where we're trying to find some local optimum,

619
00:32:05,170 --> 00:32:07,705
we have a task, we have data, something like that.

620
00:32:07,705 --> 00:32:09,880
And that's sort of short-term objectives.

621
00:32:09,880 --> 00:32:14,590
Um, we might have a slightly longer-term objective of getting a paper published,

622
00:32:14,590 --> 00:32:16,990
or if you're an industry like getting a product launched,

623
00:32:16,990 --> 00:32:18,505
whatever it might be.

624
00:32:18,505 --> 00:32:23,440
Um, from there we might see our next endpoint is getting an award or,

625
00:32:23,440 --> 00:32:25,870
you know, maybe become sort of famous for something for

626
00:32:25,870 --> 00:32:28,600
a few minutes, something like that and that's cool.

627
00:32:28,600 --> 00:32:31,300
Um, but there's a longer-term objective that we

628
00:32:31,300 --> 00:32:34,165
can work towards as well at the same time.

629
00:32:34,165 --> 00:32:38,110
And that's something like a positive outcome for humans in their environment.

630
00:32:38,110 --> 00:32:42,130
So instead of just kind of focusing on these local decisions,

631
00:32:42,130 --> 00:32:43,555
these local optima and these

632
00:32:43,555 --> 00:32:48,370
sort of local paper by paper-based approaches to solving problems,

633
00:32:48,370 --> 00:32:51,400
you can also kind of think about what's the long-term objective.

634
00:32:51,400 --> 00:32:56,739
Where does this get me as I trace out an evolutionary path for artificial intelligence,

635
00:32:56,739 --> 00:32:58,555
down the line in 10 years,

636
00:32:58,555 --> 00:33:01,210
15 years, 20 years.

637
00:33:01,210 --> 00:33:06,220
Um, and one of the ways you can address this is by thinking,

638
00:33:06,220 --> 00:33:10,405
now how can the work I'm interested in now be best focused to help others?

639
00:33:10,405 --> 00:33:12,235
And that involves talking to experts,

640
00:33:12,235 --> 00:33:14,125
um, and kind of going outside your bubble,

641
00:33:14,125 --> 00:33:16,540
speaking across interdisciplinary fields like

642
00:33:16,540 --> 00:33:19,660
cognitive science which I've just talked a bit about.

643
00:33:19,660 --> 00:33:23,410
Um, so let's talk about some things we can do.

644
00:33:23,410 --> 00:33:25,825
So first off is data.

645
00:33:25,825 --> 00:33:32,095
Um, so a lot of the issues of bias and fairness,

646
00:33:32,095 --> 00:33:36,220
ah, in machine learning models really come down to the data.

647
00:33:36,220 --> 00:33:39,580
Unfortunately in machine learning and deep learning,

648
00:33:39,580 --> 00:33:42,670
working on data is really not seen as sexy.

649
00:33:42,670 --> 00:33:45,085
Ah, there's a few datasets, ah,

650
00:33:45,085 --> 00:33:47,350
that people use that are out there,

651
00:33:47,350 --> 00:33:48,580
that's what people use,

652
00:33:48,580 --> 00:33:50,710
and there's not a lot of analysis done on,

653
00:33:50,710 --> 00:33:55,000
on how well these datasets capture different truths about the world,

654
00:33:55,000 --> 00:33:56,740
how problematic they might be,

655
00:33:56,740 --> 00:34:01,945
[NOISE] um, but it's a pretty wide area that needs a lot of future,

656
00:34:01,945 --> 00:34:04,885
like lea- needs a lot of future additional work.

657
00:34:04,885 --> 00:34:08,740
Um, [NOISE] so we're going to understanding the data skews and the correlations.

658
00:34:08,740 --> 00:34:10,900
If you understand your data skews and the,

659
00:34:10,900 --> 00:34:13,960
ah, correlations that might be problematic in your data,

660
00:34:13,960 --> 00:34:17,305
then you can start working on either models that address those,

661
00:34:17,305 --> 00:34:20,980
or data augmentation approaches in order to sort of make

662
00:34:20,980 --> 00:34:24,115
the dataset a little bit better or a little bit more representative

663
00:34:24,115 --> 00:34:25,990
of how you want the world to be.

664
00:34:25,990 --> 00:34:30,280
Um, it's also important to abandon the single training set- testing

665
00:34:30,280 --> 00:34:34,420
set from similar distribution approach to advancing deep learning.

666
00:34:34,420 --> 00:34:37,975
So um, when we do projects in deep learning,

667
00:34:37,975 --> 00:34:39,610
you know, we tend to have the training set,

668
00:34:39,610 --> 00:34:43,330
and the testing set and then that's what we sort of benchmark on and prioritize,

669
00:34:43,330 --> 00:34:46,060
but the point is, as you move around different testing sets,

670
00:34:46,060 --> 00:34:48,160
you're gonna get vastly different results.

671
00:34:48,160 --> 00:34:50,440
Um, and so by keeping in

672
00:34:50,440 --> 00:34:55,810
this just sort of one training testing dat- training testing dataset paradigm,

673
00:34:55,810 --> 00:35:00,265
you're really likely to not notice issues that might otherwise be there.

674
00:35:00,265 --> 00:35:02,140
And one way to really focus in on them,

675
00:35:02,140 --> 00:35:05,155
is having a hard set of,

676
00:35:05,155 --> 00:35:09,070
of test cases, that you really wanna make sure the model does well on.

677
00:35:09,070 --> 00:35:12,055
So these are things that are particularly problematic.

678
00:35:12,055 --> 00:35:14,905
Things that would be really harmful to individuals,

679
00:35:14,905 --> 00:35:17,545
um, If they were to experience the output.

680
00:35:17,545 --> 00:35:21,730
Um, and you kinda collect those in a small test set and then it's really easy

681
00:35:21,730 --> 00:35:26,140
to evaluate on that test set as you benchmark improvements on your model,

682
00:35:26,140 --> 00:35:28,330
as you add different kinds of things to your model,

683
00:35:28,330 --> 00:35:30,025
in order to see, um,

684
00:35:30,025 --> 00:35:32,215
not just how your model is doing overall,

685
00:35:32,215 --> 00:35:33,940
in terms of your testing dataset,

686
00:35:33,940 --> 00:35:36,700
but how well you're doing in terms of these examples,

687
00:35:36,700 --> 00:35:38,590
you really want it to do well on.

688
00:35:38,590 --> 00:35:41,950
That you know that is going to be a problem if it doesn't do well on,

689
00:35:41,950 --> 00:35:43,900
and any sort of degradation in that,

690
00:35:43,900 --> 00:35:46,210
you might want to prioritize, um,

691
00:35:46,210 --> 00:35:50,815
to fix above degragaish- degradation and overall accuracy.

692
00:35:50,815 --> 00:35:53,530
Um, and it's also important to talk to experts

693
00:35:53,530 --> 00:35:56,335
about the additional signals that you can incorporate.

694
00:35:56,335 --> 00:36:00,490
Um, so we've put out a tool to help with this,

695
00:36:00,490 --> 00:36:03,220
ah, understanding data skews called facets,

696
00:36:03,220 --> 00:36:04,855
um, it's just available there.

697
00:36:04,855 --> 00:36:10,075
Um, and it's a really handy kinda visualizer for slicing, ah, understanding,

698
00:36:10,075 --> 00:36:13,330
um, you know, what some of the differences are between different subgroups

699
00:36:13,330 --> 00:36:16,630
and different representations and you can sort of dig in and explore a bit more.

700
00:36:16,630 --> 00:36:18,685
So this is just to sort of help people, ah,

701
00:36:18,685 --> 00:36:21,430
come to terms with the data that they're actually using and,

702
00:36:21,430 --> 00:36:23,050
and where there might be, um,

703
00:36:23,050 --> 00:36:25,795
unwanted associations or, or missing,

704
00:36:25,795 --> 00:36:26,920
missing kind of features.

705
00:36:26,920 --> 00:36:33,220
[NOISE] Um, another approach that's been put forward recently,

706
00:36:33,220 --> 00:36:36,565
ah, specifically on the data side is this data,

707
00:36:36,565 --> 00:36:38,725
datasheets for datasets approach.

708
00:36:38,725 --> 00:36:42,040
Um, so this is this idea that when you release a dataset,

709
00:36:42,040 --> 00:36:44,770
it's not enough to just release the dataset with like

710
00:36:44,770 --> 00:36:48,880
some pretty graphs and like talking about basic distributional information,

711
00:36:48,880 --> 00:36:52,330
you need to talk about who the annotators were, where they were,

712
00:36:52,330 --> 00:36:54,400
what the inter-annotator agreement was,

713
00:36:54,400 --> 00:36:56,500
what their background information was,

714
00:36:56,500 --> 00:36:59,005
um, motivation for the dataset.

715
00:36:59,005 --> 00:37:00,550
All these other kinds of details.

716
00:37:00,550 --> 00:37:03,625
So now you actually know that this isn't just a dataset,

717
00:37:03,625 --> 00:37:06,700
this is a dataset that has these specific biases.

718
00:37:06,700 --> 00:37:10,300
There's no such thing as a dataset that isn't biased in some way.

719
00:37:10,300 --> 00:37:14,905
A dataset by virtue of the fact that it's collected from the world as a subset,

720
00:37:14,905 --> 00:37:18,415
is a, is a biased set of the world in some way.

721
00:37:18,415 --> 00:37:20,440
The point is to make it clear what it is,

722
00:37:20,440 --> 00:37:22,015
how it is biased, what are the,

723
00:37:22,015 --> 00:37:23,545
what are the various biases,

724
00:37:23,545 --> 00:37:25,675
ah, that are important to know about in the dataset.

725
00:37:25,675 --> 00:37:29,410
So that's one of these ideas between- behind datasheets for datasets,

726
00:37:29,410 --> 00:37:32,185
releasing its datasets publicly.

727
00:37:32,185 --> 00:37:35,815
All right. Now let's switch a little bit to machine learning.

728
00:37:35,815 --> 00:37:40,375
Um, so there are a couple of techniques that I like to use. Um, I'll talk about two.

729
00:37:40,375 --> 00:37:42,835
One, ah, is bias mitigation,

730
00:37:42,835 --> 00:37:45,850
which is removing the signal for a problematic output.

731
00:37:45,850 --> 00:37:48,760
Um, so removing, ah, stereotyping,

732
00:37:48,760 --> 00:37:52,390
sexism, racism, trying to remove these kind of effects from the model.

733
00:37:52,390 --> 00:37:56,530
Um, this is also sometimes called de-biasing or unbiasing,

734
00:37:56,530 --> 00:38:00,520
but that's a little bit of a misnomer because you're- you're generally just kind of

735
00:38:00,520 --> 00:38:04,690
moving around bias based on a specific set of words for example,

736
00:38:04,690 --> 00:38:07,795
um, so to say it's unbiased is is not true.

737
00:38:07,795 --> 00:38:10,330
Um, but you are kind of mitigating bias with respect to

738
00:38:10,330 --> 00:38:13,675
some certain kinds of information that you provide it with.

739
00:38:13,675 --> 00:38:18,910
Um, and there's inclusion which is then adding signal for desired variables.

740
00:38:18,910 --> 00:38:22,105
So that's kind of the opposite side of bias mitigation.

741
00:38:22,105 --> 00:38:24,610
So increasing model performance with attention to

742
00:38:24,610 --> 00:38:28,075
subgroups or data slices with the worst performance.

743
00:38:28,075 --> 00:38:31,825
Um, so, ah, in order to,

744
00:38:31,825 --> 00:38:33,475
er, address inclusion, ah,

745
00:38:33,475 --> 00:38:36,865
kind of adding signal for under-represented sub-groups,

746
00:38:36,865 --> 00:38:39,910
one technique that's worked relatively well is multi-task learning.

747
00:38:39,910 --> 00:38:43,990
Um, so I've heard that you guys have studied multi-task learning which is great,

748
00:38:43,990 --> 00:38:46,390
um, so I'll tell you a bit about a case study here.

749
00:38:46,390 --> 00:38:48,235
Um, so this is work I did, ah,

750
00:38:48,235 --> 00:38:52,390
in collaboration with a UPenn World Well-being Project, ah,

751
00:38:52,390 --> 00:38:54,190
working directly with clinicians,

752
00:38:54,190 --> 00:38:56,530
and the goal was to create a system that could alert

753
00:38:56,530 --> 00:38:59,560
clinicians if there was a suicide attempt that was imminent.

754
00:38:59,560 --> 00:39:02,320
Um, and they wanted to understand the feasibility of

755
00:39:02,320 --> 00:39:05,650
these kinds of diagnoses when there were very few training,

756
00:39:05,650 --> 00:39:07,690
ah, training instances available.

757
00:39:07,690 --> 00:39:11,230
So that's similar to kind of the minority problem in datasets.

758
00:39:11,230 --> 00:39:14,125
Um, [NOISE]

759
00:39:14,125 --> 00:39:17,290
And, uh, in this work,

760
00:39:17,290 --> 00:39:19,015
we had two kinds of data.

761
00:39:19,015 --> 00:39:22,960
One was the internal data which was the electronic health records, um,

762
00:39:22,960 --> 00:39:27,190
with the- that was either provided by the patient or from the family.

763
00:39:27,190 --> 00:39:30,070
Um, it included mental health diagnoses,

764
00:39:30,070 --> 00:39:32,380
uh, suicide attempts or completions, um,

765
00:39:32,380 --> 00:39:34,750
if, if, if that were the case along with,

766
00:39:34,750 --> 00:39:36,175
uh, the user's, uh,

767
00:39:36,175 --> 00:39:37,915
the person's social media data.

768
00:39:37,915 --> 00:39:40,690
And that was the internal data that we did not publish on,

769
00:39:40,690 --> 00:39:43,000
but that we were able to work with clinicians on in

770
00:39:43,000 --> 00:39:45,685
order to understand if our methods were actually working.

771
00:39:45,685 --> 00:39:48,385
Um, the external data, the proxy data,

772
00:39:48,385 --> 00:39:50,845
the stuff that we could kinda publish on and talk about,

773
00:39:50,845 --> 00:39:52,045
was based on Twitter.

774
00:39:52,045 --> 00:39:53,910
Um, and this was, uh,

775
00:39:53,910 --> 00:39:57,810
using regular expressions in order to extract, uh,

776
00:39:57,810 --> 00:40:02,315
phases in Twitter feeds that had something that was kind of like diagnoses.

777
00:40:02,315 --> 00:40:04,960
So something like, I've been diagnosed with X,

778
00:40:04,960 --> 00:40:06,820
or I've tried to commit suicide.

779
00:40:06,820 --> 00:40:08,425
And that became kind of the,

780
00:40:08,425 --> 00:40:12,325
the proxy dataset and the corresponding social media feeds for,

781
00:40:12,325 --> 00:40:13,735
for those individuals, uh,

782
00:40:13,735 --> 00:40:16,270
for the actual diagnoses.

783
00:40:16,270 --> 00:40:22,840
Um, and the state-of-the-art in clinical medicine, uh,

784
00:40:22,840 --> 00:40:24,100
kind of until this work,

785
00:40:24,100 --> 00:40:26,425
there's been more recently but, uh, it's,

786
00:40:26,425 --> 00:40:30,595
it's sort of this single task logistic regress- lo- lo- logistic regression setup.

787
00:40:30,595 --> 00:40:32,080
Where you have some input features,

788
00:40:32,080 --> 00:40:35,185
and then you're making some output predictions like true or false.

789
00:40:35,185 --> 00:40:41,260
Um, you can add some layers and start making it deep learning which is much fancier.

790
00:40:41,260 --> 00:40:45,340
Um, you can have a bunch of tasks in order to

791
00:40:45,340 --> 00:40:49,420
do a bunch of logistic regression tasks for a clinical environment.

792
00:40:49,420 --> 00:40:51,850
Um, or you can use multitask learning, uh,

793
00:40:51,850 --> 00:40:55,900
which is taking the basic deep learning model and adding a bunch of heads to it,

794
00:40:55,900 --> 00:40:58,150
uh, predicted jointly at the same time.

795
00:40:58,150 --> 00:41:01,660
Um, and here we had a bunch of diagnosis data.

796
00:41:01,660 --> 00:41:04,420
So, um, we predicted things like depression,

797
00:41:04,420 --> 00:41:07,585
anxiety, uh, post-traumatic stress disorder.

798
00:41:07,585 --> 00:41:10,900
Um, we also added in gender because this is

799
00:41:10,900 --> 00:41:13,900
something that the clinicians told us actually, uh,

800
00:41:13,900 --> 00:41:16,180
had some correlation with some of these conditions,

801
00:41:16,180 --> 00:41:18,970
and that they actually used it in making decisions themselves,

802
00:41:18,970 --> 00:41:21,535
for whether or not someone was likely to,

803
00:41:21,535 --> 00:41:24,010
uh, attempt, uh, suicide or not.

804
00:41:24,010 --> 00:41:27,310
Um, and this also used this idea of comorbidity.

805
00:41:27,310 --> 00:41:32,935
So multi-task learning is actually kind of perfect for comorbidity in clinical domains.

806
00:41:32,935 --> 00:41:34,870
So comorbidity is, um,

807
00:41:34,870 --> 00:41:36,220
when you have one condition,

808
00:41:36,220 --> 00:41:38,185
you're a lot more likely to have another.

809
00:41:38,185 --> 00:41:39,820
Um, so people who have

810
00:41:39,820 --> 00:41:43,630
post-traumatic stress disorder are much more likely to have depression and anxiety.

811
00:41:43,630 --> 00:41:46,359
Um, and depression and anxiety tend to be cormorbid,

812
00:41:46,359 --> 00:41:48,695
so people who have one often have the other.

813
00:41:48,695 --> 00:41:52,320
So this points to the fact- this points to the idea that perhaps there's

814
00:41:52,320 --> 00:41:55,680
some underlying representation that is similar across them,

815
00:41:55,680 --> 00:41:57,990
that can be leveraged in a deep learning model,

816
00:41:57,990 --> 00:42:00,885
with individual heads further specifying,

817
00:42:00,885 --> 00:42:04,170
uh, each of the different kinds of conditions.

818
00:42:04,170 --> 00:42:07,960
Um, and so what we found was that as we moved from

819
00:42:07,960 --> 00:42:12,160
logistic regression to the single task deep learning to the multi-task deep learning,

820
00:42:12,160 --> 00:42:14,605
we were able to get significantly better results.

821
00:42:14,605 --> 00:42:17,725
And this was true both in the suicide risk case where we had a,

822
00:42:17,725 --> 00:42:19,510
a lot of data, as well as

823
00:42:19,510 --> 00:42:22,960
the post-traumatic stress disorder case where we had very little data.

824
00:42:22,960 --> 00:42:25,270
Um, the behavior here was a little bit different.

825
00:42:25,270 --> 00:42:28,660
So going from logistic regression to,

826
00:42:28,660 --> 00:42:30,490
um, single task deep learning,

827
00:42:30,490 --> 00:42:32,065
when we had, um,

828
00:42:32,065 --> 00:42:33,685
a lot of data, uh,

829
00:42:33,685 --> 00:42:36,295
as we did with the suicide risk, um,

830
00:42:36,295 --> 00:42:38,230
had the single task deep learning model

831
00:42:38,230 --> 00:42:40,690
working better than the logistic regression model.

832
00:42:40,690 --> 00:42:43,105
Um, but when we have very few instances, uh,

833
00:42:43,105 --> 00:42:46,270
this is where the deep learning models really struggled a lot more.

834
00:42:46,270 --> 00:42:50,320
Um, and so the logistic regression models were actually much better.

835
00:42:50,320 --> 00:42:54,130
But once we started adding heads for the cormorbid different kinds of conditions,

836
00:42:54,130 --> 00:42:55,780
the different kinds of tasks, um,

837
00:42:55,780 --> 00:42:57,400
that related to, you know,

838
00:42:57,400 --> 00:42:59,845
whether or not the person might be committing suicide, um,

839
00:42:59,845 --> 00:43:01,210
we were able to, uh,

840
00:43:01,210 --> 00:43:03,145
bump the accuracy way back up again.

841
00:43:03,145 --> 00:43:05,485
Um, and, it, you know,

842
00:43:05,485 --> 00:43:09,235
it's roughly 120 at-risk individuals that we were able to collect, uh,

843
00:43:09,235 --> 00:43:12,250
in the suicide case that we wouldn't have otherwise been able to,

844
00:43:12,250 --> 00:43:15,110
to notice as being at risk.

845
00:43:16,110 --> 00:43:20,050
Um, one of the approaches we took in this was to

846
00:43:20,050 --> 00:43:24,670
contextualize and consider the ethical dimensions of releasing this kind of technology.

847
00:43:24,670 --> 00:43:28,900
So, um, it's really common in NLP papers to give examples.

848
00:43:28,900 --> 00:43:31,030
Um, but this was an area where we decided that

849
00:43:31,030 --> 00:43:33,475
giving examples of like depressed language,

850
00:43:33,475 --> 00:43:35,860
could be used to discriminate against people,

851
00:43:35,860 --> 00:43:37,480
like at, you know, job,

852
00:43:37,480 --> 00:43:39,565
interviews, or something like that, you know,

853
00:43:39,565 --> 00:43:42,085
the sort of armchair psychology approach.

854
00:43:42,085 --> 00:43:45,240
So we decided that while it was important to talk about the technique,

855
00:43:45,240 --> 00:43:47,190
and the utility of multitask learning in

856
00:43:47,190 --> 00:43:51,840
a clinical domain and for bringing in inclusion of underrepresented subgroups,

857
00:43:51,840 --> 00:43:54,210
it had to be balanced with the fact that there was a lot of

858
00:43:54,210 --> 00:43:57,030
risk in talking about depression,

859
00:43:57,030 --> 00:44:00,270
and anxiety, and how those kinds of things could be predicted.

860
00:44:00,270 --> 00:44:03,435
Um, so we tried to take a more balanced approach here, um,

861
00:44:03,435 --> 00:44:07,020
and since then I've been putting ethical considerations in all of my papers.

862
00:44:07,020 --> 00:44:10,420
Um, it's becoming more and more common actually.

863
00:44:10,790 --> 00:44:15,565
Um, so another kind of approach that's now turning this on its head,

864
00:44:15,565 --> 00:44:18,535
where you're trying to remove some effect, um,

865
00:44:18,535 --> 00:44:20,230
mitigate bias in some way,

866
00:44:20,230 --> 00:44:22,390
is adversarial multi-task learning.

867
00:44:22,390 --> 00:44:24,010
So I just talked about multi-task learning,

868
00:44:24,010 --> 00:44:26,065
and I'll talk about the adversarial case.

869
00:44:26,065 --> 00:44:30,205
Um, and the idea in the adversarial case is that you have a few heads.

870
00:44:30,205 --> 00:44:32,470
Um, one is predicting the main task,

871
00:44:32,470 --> 00:44:35,200
and the other one is predicting the thing that you don't

872
00:44:35,200 --> 00:44:38,290
want to be affecting your model's predictions.

873
00:44:38,290 --> 00:44:42,670
So for example, something like whether or not someone should be promoted based on,

874
00:44:42,670 --> 00:44:44,650
uh, you know, their performance reviews,

875
00:44:44,650 --> 00:44:46,285
and things like that.

876
00:44:46,285 --> 00:44:49,555
Um, you don't want that to be affected by their gender.

877
00:44:49,555 --> 00:44:53,260
Ideally, gender is independent of a promotion decision.

878
00:44:53,260 --> 00:44:54,805
And so you can, uh,

879
00:44:54,805 --> 00:44:57,385
you can create a model for this that actually,

880
00:44:57,385 --> 00:44:59,620
uh, puts that independence, um,

881
00:44:59,620 --> 00:45:02,365
criteria in place by saying, uh,

882
00:45:02,365 --> 00:45:05,485
I want to minimize my loss on the promotion,

883
00:45:05,485 --> 00:45:07,885
while maximizing my loss on the gender.

884
00:45:07,885 --> 00:45:10,390
And so how we're doing that is just predicting gender,

885
00:45:10,390 --> 00:45:12,250
and then negating the gradient.

886
00:45:12,250 --> 00:45:15,010
So removing the effect of that signal.

887
00:45:15,010 --> 00:45:17,920
Um, this is another adversarial approach.

888
00:45:17,920 --> 00:45:20,950
So you might have been familiar with like generative adversarial networks.

889
00:45:20,950 --> 00:45:23,470
So this is like two discriminators, uh,

890
00:45:23,470 --> 00:45:25,435
two different task heads, uh,

891
00:45:25,435 --> 00:45:28,270
where one is trying to do the task that we care about,

892
00:45:28,270 --> 00:45:30,745
and the other one is removing the signal, uh,

893
00:45:30,745 --> 00:45:32,320
that we really don't want to,

894
00:45:32,320 --> 00:45:35,890
um, uh, be coming into play in our downstream predictions.

895
00:45:35,890 --> 00:45:37,900
Um, so this is a way of,

896
00:45:37,900 --> 00:45:39,670
uh, kind of putting this into practice.

897
00:45:39,670 --> 00:45:41,410
So the probability of your output,

898
00:45:41,410 --> 00:45:43,030
uh, predicted output given the,

899
00:45:43,030 --> 00:45:46,540
the ground truth and your sensitive attribute like gender, um,

900
00:45:46,540 --> 00:45:48,835
is equal across all the different, uh,

901
00:45:48,835 --> 00:45:52,060
sensitive attributes or equal across all the different genders.

902
00:45:52,060 --> 00:45:56,410
Um, and that's an example of equality of opportunity in supervised learning,

903
00:45:56,410 --> 00:45:57,775
being put into practice.

904
00:45:57,775 --> 00:46:00,220
So this is one of the key fairness definitions.

905
00:46:00,220 --> 00:46:01,750
It's equivalent to, uh,

906
00:46:01,750 --> 00:46:05,320
equal recall across different subgroups as I mentioned earlier.

907
00:46:05,320 --> 00:46:07,495
Um, and that's a model that will actually,

908
00:46:07,495 --> 00:46:10,270
uh, implement that or help you achieve that.

909
00:46:10,270 --> 00:46:13,375
Um, where you're saying that a classifier's output decisions should be the same

910
00:46:13,375 --> 00:46:16,480
across sensitive characteristics given what the,

911
00:46:16,480 --> 00:46:19,070
what the correct decision should be.

912
00:46:20,190 --> 00:46:23,365
Okay, so how are we on time?

913
00:46:23,365 --> 00:46:30,265
Cool. Are there any questions so far? Are we good?

914
00:46:30,265 --> 00:46:35,350
Okay, cool. So I'm gonna go into a little bit of a case study now, an end-to-end, uh,

915
00:46:35,350 --> 00:46:37,780
system that Google has been working on, uh,

916
00:46:37,780 --> 00:46:39,325
my colleagues have been working on, uh,

917
00:46:39,325 --> 00:46:42,790
that is in NLP domain and deals with some of these bias issues.

918
00:46:42,790 --> 00:46:46,675
Um, so you can find out more about this work, um,

919
00:46:46,675 --> 00:46:51,745
in papers at AIES in 2018 and FAT* tutorial 2019,

920
00:46:51,745 --> 00:46:56,410
um, called Measuring and Mitigating Unintended Bias in Text Classification.

921
00:46:56,410 --> 00:47:01,345
Um, and this came out of Conversation-AI which is a, uh,

922
00:47:01,345 --> 00:47:03,505
which is a product that's, um,

923
00:47:03,505 --> 00:47:07,720
like it's part of this- it's called a bet at Google.

924
00:47:07,720 --> 00:47:10,810
It's a kind of spin-off company called Jigsaw that

925
00:47:10,810 --> 00:47:14,245
focuses on trying to like combat abuse online.

926
00:47:14,245 --> 00:47:16,225
Um, and the Conversation-AI, uh,

927
00:47:16,225 --> 00:47:20,065
team is trying to use deep learning to improve online conversations.

928
00:47:20,065 --> 00:47:23,320
Um, and collaborate with a ton of different,

929
00:47:23,320 --> 00:47:25,375
uh, different people to do that.

930
00:47:25,375 --> 00:47:27,805
Um, so how this works is,

931
00:47:27,805 --> 00:47:30,460
oh you can try it out too, on perspectiveapi.com.

932
00:47:30,460 --> 00:47:33,970
So given some phrase like you're a dork, uh,

933
00:47:33,970 --> 00:47:41,358
it puts out a toxicity score associated to that like 0,91. [NOISE]

934
00:47:41,358 --> 00:47:44,140
Um, and the model starts sort of falsely associating

935
00:47:44,140 --> 00:47:47,305
frequently attacked identities with toxicity.

936
00:47:47,305 --> 00:47:50,245
So this is a kind of false positive bias.

937
00:47:50,245 --> 00:47:53,080
So I'm a proud tall person gets a model,

938
00:47:53,080 --> 00:47:55,360
uh, toxicity score of 0,18.

939
00:47:55,360 --> 00:47:56,995
I'm a proud, uh,

940
00:47:56,995 --> 00:48:00,925
gay person gets a toxicity model score of 0,69.

941
00:48:00,925 --> 00:48:06,805
And this is because these- the term gay tends to be used in really toxic situations.

942
00:48:06,805 --> 00:48:10,960
And so the model starts to learn that gay itself is toxic.

943
00:48:10,960 --> 00:48:12,730
But that's not actually what we want,

944
00:48:12,730 --> 00:48:15,865
and we don't want these kinds of predictions coming out of the model.

945
00:48:15,865 --> 00:48:22,705
Um, so, uh, the bias is largely caused here by the dataset imbalance.

946
00:48:22,705 --> 00:48:25,765
Again, this is data kinda coming and wearing its hat again.

947
00:48:25,765 --> 00:48:28,000
Um, so frequently attacked, uh,

948
00:48:28,000 --> 00:48:31,180
identities are really overrepresented in toxic comments.

949
00:48:31,180 --> 00:48:35,155
There's a lot of toxicity towards LGBTQ identities, um,

950
00:48:35,155 --> 00:48:37,240
it's really horrible to work on this stuff that like

951
00:48:37,240 --> 00:48:39,985
really [LAUGHTER] it can really affect you personally.

952
00:48:39,985 --> 00:48:42,790
Um, uh, and, uh,

953
00:48:42,790 --> 00:48:48,100
one of the approaches that the team took was just to add nontoxic data from Wikipedia.

954
00:48:48,100 --> 00:48:54,025
So helping to- helping the model to understand that these kinds of terms can be used in,

955
00:48:54,025 --> 00:48:58,040
you know, more positive sorts of contexts.

956
00:49:00,720 --> 00:49:03,940
One of the challenges with measuring, uh,

957
00:49:03,940 --> 00:49:06,295
how well the system was doing is that there's not

958
00:49:06,295 --> 00:49:11,725
a really nice way to have controlled toxicity evaluation.

959
00:49:11,725 --> 00:49:13,750
Um, so in real-world conversation,

960
00:49:13,750 --> 00:49:18,805
it can be kind of anyone's guess what the toxicity is of a specific sentence.

961
00:49:18,805 --> 00:49:21,070
Um, if you really wanna control for a different kind of

962
00:49:21,070 --> 00:49:24,040
subgroups or intersectional subgroups,

963
00:49:24,040 --> 00:49:25,810
and it can be even harder to get, uh,

964
00:49:25,810 --> 00:49:28,765
a real good data to evaluate properly.

965
00:49:28,765 --> 00:49:32,395
So what the team ended up doing was developing a synthetic data approach.

966
00:49:32,395 --> 00:49:35,185
Um, so this is kind of like a bias Mad Libs.

967
00:49:35,185 --> 00:49:37,465
Um, where you take template sentences [NOISE], um,

968
00:49:37,465 --> 00:49:41,380
and you use those for evaluation. This is the kind of, um,

969
00:49:41,380 --> 00:49:45,280
evaluation you'd want to use in addition to your target downstream

970
00:49:45,280 --> 00:49:47,650
ah, kind of dataset.

971
00:49:47,650 --> 00:49:51,685
But this helps you get at the biases specifically.

972
00:49:51,685 --> 00:49:55,840
So, um, some template phrase like I am a proud blank person,

973
00:49:55,840 --> 00:49:58,480
and then filling in different subgroup identities.

974
00:49:58,480 --> 00:50:01,660
And you don't want to release the model unless you see that

975
00:50:01,660 --> 00:50:04,990
the scores across these different kinds of, uh,

976
00:50:04,990 --> 00:50:08,860
these different kinds of template sentences with synthetic, uh,

977
00:50:08,860 --> 00:50:10,795
the synthetic template sentences, um,

978
00:50:10,795 --> 00:50:14,320
are relatively kind of the same across, ah, yeah.

979
00:50:14,320 --> 00:50:17,210
All of the different model runs.

980
00:50:17,460 --> 00:50:24,995
Cool. Um, so some assumptions that they made in this was that the dataset, um, uh,

981
00:50:24,995 --> 00:50:28,090
didn't have annotated bias and they didn't do

982
00:50:28,090 --> 00:50:31,525
any causal analysis because they were just trying to focus in particular,

983
00:50:31,525 --> 00:50:33,745
um, on this toxicity problem.

984
00:50:33,745 --> 00:50:36,880
Um, they used the CNN,

985
00:50:36,880 --> 00:50:39,610
ah, convolutional, yeah you guys know, blah, blah, blah.

986
00:50:39,610 --> 00:50:41,650
Uh, with pretrained chain GloVe embeddings.

987
00:50:41,650 --> 00:50:43,240
This is probably like your bread and butter.

988
00:50:43,240 --> 00:50:44,170
Pretrained GloVe embeddings.

989
00:50:44,170 --> 00:50:45,940
I'm sure you know all about this in Word2vec.

990
00:50:45,940 --> 00:50:48,250
Cool, uh, Keras implementation of this.

991
00:50:48,250 --> 00:50:53,470
Um, and, uh, and using these kind of data augmentation approaches, um,

992
00:50:53,470 --> 00:50:55,420
both a Wikipedia, uh,

993
00:50:55,420 --> 00:51:00,955
kind of approach as well as actually collecting positive statements about LGBTQ identity.

994
00:51:00,955 --> 00:51:03,730
So there's this project called Project Respect at Google,

995
00:51:03,730 --> 00:51:04,990
where we go out and,

996
00:51:04,990 --> 00:51:08,890
and talk to people who identify as queer or people who have friends who do,

997
00:51:08,890 --> 00:51:11,290
and like talk about this in a positive way,

998
00:51:11,290 --> 00:51:13,120
and we add this as data.

999
00:51:13,120 --> 00:51:17,035
Um, so we can actually know that this is can be a positive thing.

1000
00:51:17,035 --> 00:51:21,385
Um, and in order to measure the model performance here, um,

1001
00:51:21,385 --> 00:51:25,480
again it's looking at the differences across different subgroups and trying to

1002
00:51:25,480 --> 00:51:29,695
compare also the subgroup performance to some sort of general distribution.

1003
00:51:29,695 --> 00:51:31,930
So here they use AUC, um,

1004
00:51:31,930 --> 00:51:34,630
where AUC is essentially the probability that a model will

1005
00:51:34,630 --> 00:51:37,975
give a randomly sel- selected positive example,

1006
00:51:37,975 --> 00:51:42,070
a higher score than a randomly selected, uh, negative example.

1007
00:51:42,070 --> 00:51:44,890
So, um, here you can see some toxic comments and

1008
00:51:44,890 --> 00:51:49,240
nontoxic comments with a example sort of low AUC.

1009
00:51:49,240 --> 00:51:52,225
Um, here, ah, this is an

1010
00:51:52,225 --> 00:51:53,770
example with a high AUC,

1011
00:51:53,770 --> 00:51:58,000
so the model is doing a relatively good job of separating these two kinds of comments.

1012
00:51:58,000 --> 00:52:02,350
Um, and there are different kinds of biases that they've defined in this work.

1013
00:52:02,350 --> 00:52:05,080
So, uh, low subgroup performance means that

1014
00:52:05,080 --> 00:52:08,005
the model performs worse on subgroup comments than it does,

1015
00:52:08,005 --> 00:52:09,625
ah, on comments overall.

1016
00:52:09,625 --> 00:52:14,150
And the metric they've introduced to measure this is called subgroup AUC.

1017
00:52:14,220 --> 00:52:17,350
Um, another one is subgroup shift.

1018
00:52:17,350 --> 00:52:19,960
And that's when the model systematically scores comments,

1019
00:52:19,960 --> 00:52:22,120
um, from some subgroup higher.

1020
00:52:22,120 --> 00:52:24,280
Um, so this is sort of like to the right.

1021
00:52:24,280 --> 00:52:26,620
Um, and then there's also, uh,

1022
00:52:26,620 --> 00:52:31,220
this Background Positive Subgroup Negative shifting to the left.

1023
00:52:31,620 --> 00:52:36,775
Yeah. Um, yeah that's sort of saying what I said.

1024
00:52:36,775 --> 00:52:38,980
It can go either way to the right or the left and there's just

1025
00:52:38,980 --> 00:52:42,740
kind of different metrics that can define each of these.

1026
00:52:42,780 --> 00:52:46,000
Cool. Um, and the results in this,

1027
00:52:46,000 --> 00:52:49,765
ah, sort of going through not only just looking at, you know,

1028
00:52:49,765 --> 00:52:53,380
qualitative examples, um, and general evaluation metrics,

1029
00:52:53,380 --> 00:52:56,470
but also focusing in on some of the key metrics defined for this work,

1030
00:52:56,470 --> 00:52:58,465
these sort of AUC-based approaches.

1031
00:52:58,465 --> 00:53:01,450
And they were able to see significant differences in

1032
00:53:01,450 --> 00:53:05,184
the original release which didn't account for any of these unintended biases,

1033
00:53:05,184 --> 00:53:07,510
and downstream releases, uh, which did,

1034
00:53:07,510 --> 00:53:10,195
which incorporated this kind of normative data

1035
00:53:10,195 --> 00:53:14,690
that said the sort of things that we thought the model should be learning.

1036
00:53:15,390 --> 00:53:18,250
Cool. Um, so, um,

1037
00:53:18,250 --> 00:53:20,980
the last thing to keep in mind as you sort of develop and,

1038
00:53:20,980 --> 00:53:25,840
and work towards, uh, creating deeper better models is to release responsibly.

1039
00:53:25,840 --> 00:53:28,450
Um, so this is a project I've been working on with

1040
00:53:28,450 --> 00:53:31,510
a ton of different people called Model Cards for Model Reporting.

1041
00:53:31,510 --> 00:53:36,280
It's, uh, it's a little bit of like the next step after Datasheets for Datasets,

1042
00:53:36,280 --> 00:53:41,335
um, where, um, Datasheets for Datasets focuses on information about the data.

1043
00:53:41,335 --> 00:53:45,490
Ah, Model Cards for Model Reporting focuses on information about the model.

1044
00:53:45,490 --> 00:53:47,935
Um, so it captures what it does,

1045
00:53:47,935 --> 00:53:50,005
how it works, why it matters.

1046
00:53:50,005 --> 00:53:55,600
Um, and one of the key ideas here is disaggregated in intersectional evaluation.

1047
00:53:55,600 --> 00:53:57,055
So it's not enough, uh,

1048
00:53:57,055 --> 00:54:00,430
any more to put out human-centered technology that just

1049
00:54:00,430 --> 00:54:04,105
has some vague overall score associated to it.

1050
00:54:04,105 --> 00:54:07,855
You actually need to understand how it works across different subpopulations.

1051
00:54:07,855 --> 00:54:11,620
And you have to understand what the data is telling you that.

1052
00:54:11,620 --> 00:54:14,530
Um, so here's some example details that a

1053
00:54:14,530 --> 00:54:16,000
model card would have,

1054
00:54:16,000 --> 00:54:17,410
um, who it's developed by,

1055
00:54:17,410 --> 00:54:18,970
what the intended use is,

1056
00:54:18,970 --> 00:54:22,715
so that it doesn't start being used in ways that it's not intended to be used.

1057
00:54:22,715 --> 00:54:25,350
Um, the factors that are likely to be

1058
00:54:25,350 --> 00:54:28,125
affected by disproportionate performance of the model.

1059
00:54:28,125 --> 00:54:31,430
Um, so different kinds of identity groups, things like that.

1060
00:54:31,430 --> 00:54:33,580
Um, the metrics that, ah,

1061
00:54:33,580 --> 00:54:37,150
that you're deciding to use in order to understand the fairness of the model or

1062
00:54:37,150 --> 00:54:42,025
the different performance of the model across different kinds of subgroups and factors,

1063
00:54:42,025 --> 00:54:45,970
information about the evaluation data and training data.

1064
00:54:45,970 --> 00:54:49,210
Um, as well as ethical considerations, um,

1065
00:54:49,210 --> 00:54:51,010
so what were some of the things you took into

1066
00:54:51,010 --> 00:54:53,800
account or what are some of the risks and benefits,

1067
00:54:53,800 --> 00:54:56,935
um, that, uh, that are relevant to this model?

1068
00:54:56,935 --> 00:54:59,455
Um, and additional caveats and recommendations.

1069
00:54:59,455 --> 00:55:02,740
So for example, in the conversation AI case,

1070
00:55:02,740 --> 00:55:04,270
they're working with synthetic data.

1071
00:55:04,270 --> 00:55:08,395
So this is the sort of limitation of the evaluation that's important to understand, uh,

1072
00:55:08,395 --> 00:55:11,020
because it can tell you a lot about the biases,

1073
00:55:11,020 --> 00:55:13,660
but doesn't tell you a lot about how it works generally.

1074
00:55:13,660 --> 00:55:19,570
[NOISE] And then the key component in the quantitative,

1075
00:55:19,570 --> 00:55:21,730
uh, section of the model card is to have

1076
00:55:21,730 --> 00:55:24,640
this both intersectional and disaggregated evaluation.

1077
00:55:24,640 --> 00:55:28,300
And from here, you trivially get to different kinds of fairness definitions.

1078
00:55:28,300 --> 00:55:30,925
The closer you get to parity across subgroups,

1079
00:55:30,925 --> 00:55:34,840
the closer you're getting to something that is mathematically fair.

1080
00:55:34,840 --> 00:55:39,175
Okay. So hopefully by paying attention to these kinds of approaches,

1081
00:55:39,175 --> 00:55:41,080
taking into account all these kinds of things,

1082
00:55:41,080 --> 00:55:44,080
we can move from majority representation of data in

1083
00:55:44,080 --> 00:55:47,620
our models to something more like diverse representation,

1084
00:55:47,620 --> 00:55:49,435
uh, from our ethical AI.

1085
00:55:49,435 --> 00:55:50,905
Okay. That's it.

1086
00:55:50,905 --> 00:56:02,540
Thanks. [APPLAUSE]

