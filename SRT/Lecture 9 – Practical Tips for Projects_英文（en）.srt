1
00:00:00,000 --> 00:00:10,650
[NOISE] Okay everyone, let's get started for today.

2
00:00:10,650 --> 00:00:15,345
Okay. So, we're into week five of CS224n.

3
00:00:15,345 --> 00:00:18,450
And so, this is the plan for today.

4
00:00:18,450 --> 00:00:22,080
Um, in some sense a lot of this class is gonna be

5
00:00:22,080 --> 00:00:26,955
an easy class because I'm gonna talk about things like,

6
00:00:26,955 --> 00:00:30,780
um, final projects and tips for what you're meant to do,

7
00:00:30,780 --> 00:00:32,100
and finding a topic,

8
00:00:32,100 --> 00:00:33,510
and writing up your work,

9
00:00:33,510 --> 00:00:34,680
and things like that.

10
00:00:34,680 --> 00:00:36,240
Um, so for, um, so,

11
00:00:36,240 --> 00:00:39,135
two-thirds of the class there isn't a lot of,

12
00:00:39,135 --> 00:00:41,010
um, deep technical content.

13
00:00:41,010 --> 00:00:42,110
But I hope they're actually

14
00:00:42,110 --> 00:00:46,610
just some useful stuff and stuff that would be good to know about.

15
00:00:46,610 --> 00:00:49,820
One way you can think about this is until,

16
00:00:49,820 --> 00:00:53,090
until this year we had a midterm in this class.

17
00:00:53,090 --> 00:00:56,540
So, you know, if we weren't doing this class should instead be doing the

18
00:00:56,540 --> 00:01:00,740
the mid-term based on all the material that we've covered, um, so far.

19
00:01:00,740 --> 00:01:03,380
So, this should be really pleasant by comparison.

20
00:01:03,380 --> 00:01:06,650
Um, but that isn't gonna be quite the entire class.

21
00:01:06,650 --> 00:01:09,615
So, for this piece here in the middle I'm gonna

22
00:01:09,615 --> 00:01:13,910
spend a while back on some of the topics of last week.

23
00:01:13,910 --> 00:01:19,415
So, I wanted to have one more look at some of these gated recurrent models,

24
00:01:19,415 --> 00:01:21,905
um, that Abby introduced last week.

25
00:01:21,905 --> 00:01:24,380
And I guess my hope is that now that you've

26
00:01:24,380 --> 00:01:26,780
had a bit more time to look and read about things,

27
00:01:26,780 --> 00:01:31,670
and hopefully even have started working on homework for that.

28
00:01:31,670 --> 00:01:36,800
Maybe it starts to make a bit more sense or else even if it's more confusing then before,

29
00:01:36,800 --> 00:01:40,100
you've got some idea of what your confusions are and questions.

30
00:01:40,100 --> 00:01:41,840
And so, hopefully it's, um,

31
00:01:41,840 --> 00:01:47,715
good to think about those one more time because I think they are quite a complex notion,

32
00:01:47,715 --> 00:01:51,860
and it's not so obvious what they're doing and why they're doing anything useful,

33
00:01:51,860 --> 00:01:55,085
or whether they're just this big complex blob of mystery.

34
00:01:55,085 --> 00:01:59,210
And then also to touch on a couple of machine translation topics that have um, come up

35
00:01:59,210 --> 00:02:03,300
in the final project that we didn't really get m- time to say much about last week.

36
00:02:03,300 --> 00:02:04,620
[NOISE] Okay.

37
00:02:04,620 --> 00:02:06,480
So, let's get started.

38
00:02:06,480 --> 00:02:12,515
Um, so, this is our coursework in grading that we showed at the beginning.

39
00:02:12,515 --> 00:02:17,000
And so, the main thing I wanna do today is talk about this final project.

40
00:02:17,000 --> 00:02:18,620
Um, but before tha- I do that,

41
00:02:18,620 --> 00:02:22,145
let's just save one minute on participation.

42
00:02:22,145 --> 00:02:27,350
Um, so, I guess we started into one aspect of the participation policy, um,

43
00:02:27,350 --> 00:02:29,780
last Thursday when we took attendance,

44
00:02:29,780 --> 00:02:31,730
and that makes it sound draconian,

45
00:02:31,730 --> 00:02:33,215
but I wanted to say, um,

46
00:02:33,215 --> 00:02:34,820
the positive viewpoint of,

47
00:02:34,820 --> 00:02:36,970
um, the participation points.

48
00:02:36,970 --> 00:02:38,840
I mean, obviously this is a big class.

49
00:02:38,840 --> 00:02:40,640
There are lots of people.

50
00:02:40,640 --> 00:02:44,090
Um, our hope is just that people will variously,

51
00:02:44,090 --> 00:02:47,480
they're sort of engaged and involved in the class,

52
00:02:47,480 --> 00:02:49,580
and the participation points,

53
00:02:49,580 --> 00:02:51,335
ah, are our way of doing that.

54
00:02:51,335 --> 00:02:53,945
I mean, basically the way this is set up.

55
00:02:53,945 --> 00:02:57,005
I mean, if you do much of anything

56
00:02:57,005 --> 00:03:00,230
you should just get three percent for the participation points.

57
00:03:00,230 --> 00:03:01,610
It shouldn't be hard.

58
00:03:01,610 --> 00:03:05,730
I mean, I will bet you that there will be some people who at the end,

59
00:03:05,730 --> 00:03:09,005
will have gotten seven points in the participation category.

60
00:03:09,005 --> 00:03:11,420
And unfortunately we cap you, we'll only give you

61
00:03:11,420 --> 00:03:14,450
three percent for the participation category, but you know,

62
00:03:14,450 --> 00:03:17,450
providing you usually come to class,

63
00:03:17,450 --> 00:03:19,400
or usually write the,

64
00:03:19,400 --> 00:03:22,160
um, what we've got to [NOISE] the invited speakers

65
00:03:22,160 --> 00:03:25,220
the reaction paragraphs if you are an SCPD student.

66
00:03:25,220 --> 00:03:29,210
Sometimes, um, write a helpful answer on Piazza, right.

67
00:03:29,210 --> 00:03:31,895
You're already gonna be there on three percent.

68
00:03:31,895 --> 00:03:33,450
Um, yeah.

69
00:03:33,450 --> 00:03:36,060
And so, one, but one other thing, um,

70
00:03:36,060 --> 00:03:39,905
that's a way to get some parti- participation points that's out today.

71
00:03:39,905 --> 00:03:44,120
So, um, today we're putting up our Mid-quarter feedback survey.

72
00:03:44,120 --> 00:03:46,280
And we'd love to have you fill that in.

73
00:03:46,280 --> 00:03:49,565
I mean, we'd like to get your thoughts on the course so far.

74
00:03:49,565 --> 00:03:51,720
And, you know, for you guys,

75
00:03:51,720 --> 00:03:53,370
there are two ways that you can win.

76
00:03:53,370 --> 00:03:57,500
First if you give us some feedback that can help the rest of your quarter be better,

77
00:03:57,500 --> 00:04:00,860
but we've also got a simple bribe built into this, um,

78
00:04:00,860 --> 00:04:04,640
which is you get half a participation point simply for filling in,

79
00:04:04,640 --> 00:04:06,890
um, the, um, Mid-quarter survey,

80
00:04:06,890 --> 00:04:09,265
but it'd be really good to get your feedback on that.

81
00:04:09,265 --> 00:04:11,780
Okay. So, then the main thing I want to get to

82
00:04:11,780 --> 00:04:15,925
today is to talk about [NOISE] the final project.

83
00:04:15,925 --> 00:04:19,410
Okay. And so, I'll jump right ahead, um, into that.

84
00:04:19,410 --> 00:04:23,240
So, for the final project there are two choices.

85
00:04:23,240 --> 00:04:26,599
Um, you, you can either do our default final project,

86
00:04:26,599 --> 00:04:30,560
which I'll say a little bit about, it's doing SQuAD question answering,

87
00:04:30,560 --> 00:04:32,675
or you can propose a final,

88
00:04:32,675 --> 00:04:34,310
a custom final project,

89
00:04:34,310 --> 00:04:36,110
which we then have to approve.

90
00:04:36,110 --> 00:04:37,835
And in the course of that,

91
00:04:37,835 --> 00:04:40,910
um, if you have some outside mentor, um,

92
00:04:40,910 --> 00:04:43,840
you can say who they are and your project proposal,

93
00:04:43,840 --> 00:04:49,150
but otherwise, um, we'll attempt to assign you a mentor somewhere out of the course staff.

94
00:04:49,150 --> 00:04:51,295
Um, so, for all the assignments,

95
00:04:51,295 --> 00:04:53,215
through assignments one through five,

96
00:04:53,215 --> 00:04:55,495
you have to do them by yourself.

97
00:04:55,495 --> 00:04:59,135
Um, for the final project in either form of that,

98
00:04:59,135 --> 00:05:00,905
you can do it as a team.

99
00:05:00,905 --> 00:05:02,210
So, you can do it as one,

100
00:05:02,210 --> 00:05:04,310
two, or three people.

101
00:05:04,310 --> 00:05:06,515
And how does that work?

102
00:05:06,515 --> 00:05:10,350
Um, well, it works like this, um,

103
00:05:10,350 --> 00:05:12,410
if you're a bigger team,

104
00:05:12,410 --> 00:05:14,569
we do expect you to do more,

105
00:05:14,569 --> 00:05:17,825
and there are actually two ways you can be a bigger team that I'll point out.

106
00:05:17,825 --> 00:05:20,900
One way is having more people being two or three people.

107
00:05:20,900 --> 00:05:23,750
And the other thing that comes up is, um,

108
00:05:23,750 --> 00:05:27,970
sometimes people wanna do a final project for more than one class at the same time.

109
00:05:27,970 --> 00:05:30,050
In particular for this quarter I know there are

110
00:05:30,050 --> 00:05:32,480
at least a couple of people who are hoping to do,

111
00:05:32,480 --> 00:05:37,065
um, a joint project with Emma's reinforcement learning class.

112
00:05:37,065 --> 00:05:38,675
And we allow that as well.

113
00:05:38,675 --> 00:05:43,490
But we sort of do multiplication because if you're two people using it for two classes,

114
00:05:43,490 --> 00:05:46,910
that means it should be four times as great as

115
00:05:46,910 --> 00:05:50,390
what one person is doing for one class, right?

116
00:05:50,390 --> 00:05:54,470
So, how, how it works with larger teams, you know,

117
00:05:54,470 --> 00:05:59,510
in all honesty it's a little bit subtle because, you know,

118
00:05:59,510 --> 00:06:02,975
the truth is if something is just bad, um,

119
00:06:02,975 --> 00:06:05,495
your model was broken, um,

120
00:06:05,495 --> 00:06:08,540
or you, your experiment failed,

121
00:06:08,540 --> 00:06:10,390
um, and you don't know why.

122
00:06:10,390 --> 00:06:16,040
Um, you know. If, if there's just obvious ways in what you've done as bad as it's sort of,

123
00:06:16,040 --> 00:06:19,325
it's sort of bad whether you're one person or four person.

124
00:06:19,325 --> 00:06:21,860
Um, and if you've written it up beautifully,

125
00:06:21,860 --> 00:06:23,840
you've written up beautifully regardless of whether

126
00:06:23,840 --> 00:06:26,240
you're one person or four per- people,

127
00:06:26,240 --> 00:06:32,000
that you know nevertheless the expectation is that if you're one person will be pleased,

128
00:06:32,000 --> 00:06:35,960
that if you put together one model and gotten it to work well, um,

129
00:06:35,960 --> 00:06:38,790
but if you're three people will say, "Well,

130
00:06:38,790 --> 00:06:40,905
that wasn't such a big effort, um,

131
00:06:40,905 --> 00:06:43,620
running this one model against this task."

132
00:06:43,620 --> 00:06:45,315
Surely if there are three people,

133
00:06:45,315 --> 00:06:46,700
they could have investigated

134
00:06:46,700 --> 00:06:51,515
some other model classes and seeing whether they perform better or worse on this task.

135
00:06:51,515 --> 00:06:53,450
And we'll feel a sense of lightweight.

136
00:06:53,450 --> 00:06:58,175
So, we are expecting that sort of both more ambitious projects,

137
00:06:58,175 --> 00:07:01,190
and more thorough exploration of them if you're

138
00:07:01,190 --> 00:07:04,735
being a bigger team or you're using it for multiple classes.

139
00:07:04,735 --> 00:07:06,410
Um, for the final project,

140
00:07:06,410 --> 00:07:09,545
you are allowed to use any language or deep learning,

141
00:07:09,545 --> 00:07:11,960
um, framework that you choose to.

142
00:07:11,960 --> 00:07:13,910
We don't insist on what you use,

143
00:07:13,910 --> 00:07:16,025
though in practice in past years.

144
00:07:16,025 --> 00:07:18,725
Basically everyone keeps on using what they've learned,

145
00:07:18,725 --> 00:07:19,880
um, in the assignments.

146
00:07:19,880 --> 00:07:21,695
I expect that will be true, um,

147
00:07:21,695 --> 00:07:24,030
this time as well. [NOISE]

148
00:07:24,030 --> 00:07:29,880
Okay. So, um, let me just mention quickly the default final project,

149
00:07:29,880 --> 00:07:31,320
so that you've got, um,

150
00:07:31,320 --> 00:07:33,120
some sense of context.

151
00:07:33,120 --> 00:07:36,375
So, the materials of that will be released this Thursday.

152
00:07:36,375 --> 00:07:38,460
And so, for the tasks for it is,

153
00:07:38,460 --> 00:07:42,090
a textural question-answering task which is done over the,

154
00:07:42,090 --> 00:07:45,240
the Stanford Question Answering Dataset, SQuAD,

155
00:07:45,240 --> 00:07:47,475
which was a dataset put together, um,

156
00:07:47,475 --> 00:07:51,870
by Percy Liang and the department and the student .

157
00:07:51,870 --> 00:07:55,380
Um, so, we've used this as a default final project,

158
00:07:55,380 --> 00:07:58,680
um, before but we're mixing up a couple of things this year.

159
00:07:58,680 --> 00:08:03,840
I mean, firstly, the starter code we're providing this year is in pytorch,

160
00:08:03,840 --> 00:08:06,465
to fit in with what we've done to the rest of the class.

161
00:08:06,465 --> 00:08:09,765
But secondly, the SQuAD team,

162
00:08:09,765 --> 00:08:11,700
released a new version of SQuAD,

163
00:08:11,700 --> 00:08:15,840
SQuAD 2,0 and we're going to use that for the class this year.

164
00:08:15,840 --> 00:08:18,630
And the essential difference in SQuAD 2,0,

165
00:08:18,630 --> 00:08:21,975
is in SQuAD 1,1 or 1,0,

166
00:08:21,975 --> 00:08:28,065
every question had an answer in the passage of text whereas in SQuAD 2,0,

167
00:08:28,065 --> 00:08:30,210
a lot of questions don't have answers.

168
00:08:30,210 --> 00:08:34,770
So, there's this extra significant thing that you need to do which is working out,

169
00:08:34,770 --> 00:08:36,960
um, whether a question has an answer.

170
00:08:36,960 --> 00:08:39,510
So, th- this is just one example,

171
00:08:39,510 --> 00:08:43,430
um, which just gives you a sense of the SQuAD,  what SQuAD is like.

172
00:08:43,430 --> 00:08:45,680
So, there's a paragraph of text.

173
00:08:45,680 --> 00:08:48,680
I've just put a subset of it here, um, Bill Aken,

174
00:08:48,680 --> 00:08:52,460
adopted by Mexican movie actress, Lupe Mayorga, um,

175
00:08:52,460 --> 00:08:55,290
grew up in the neighborhood town, neighboring, sorry,

176
00:08:55,290 --> 00:08:57,990
neighboring town of Madeira and his song chronicled

177
00:08:57,990 --> 00:09:01,650
the hardships faced by the migrant farm workers he saw as a child.

178
00:09:01,650 --> 00:09:04,035
Right, there's then a question, um,

179
00:09:04,035 --> 00:09:05,760
in what town did Bill,

180
00:09:05,760 --> 00:09:07,650
right, actually I misspelled that sorry,

181
00:09:07,650 --> 00:09:13,230
it should have been Aken without an I. I got confused with our former department chair,

182
00:09:13,230 --> 00:09:15,315
Alex Aiken, I guess when I was typing.

183
00:09:15,315 --> 00:09:17,175
Um, Bill Aken grow up?

184
00:09:17,175 --> 00:09:19,920
And the answer you are meant to give is Madeira.

185
00:09:19,920 --> 00:09:22,320
Um, so, just incidentally,

186
00:09:22,320 --> 00:09:24,180
it's a random fact.

187
00:09:24,180 --> 00:09:28,500
Um, so, quite a few of you know about something that was

188
00:09:28,500 --> 00:09:30,450
recently in the kind of tech news, tech

189
00:09:30,450 --> 00:09:33,285
news and we're going to talk about later in the class.

190
00:09:33,285 --> 00:09:34,860
Um, that people, um,

191
00:09:34,860 --> 00:09:39,015
from Google produced this very strong New Natural Language

192
00:09:39,015 --> 00:09:42,090
Understanding representation model called BERT.

193
00:09:42,090 --> 00:09:46,695
And which is one of several kind of models that are in a class of,

194
00:09:46,695 --> 00:09:52,650
models that contextually model words that have come into prominence in 2017 and 18.

195
00:09:52,650 --> 00:09:58,770
And in general, BERT has sort of produced very good performance for very many tasks.

196
00:09:58,770 --> 00:10:03,900
Indeed, if you look at the SQuAD 2,0 leader board online, um,

197
00:10:03,900 --> 00:10:06,975
at this URL, what you'll find is that

198
00:10:06,975 --> 00:10:11,625
all of the leading systems use BERT in some way or another, these days.

199
00:10:11,625 --> 00:10:14,910
Um, but nevertheless, this was actually a question that BERT got wrong.

200
00:10:14,910 --> 00:10:16,290
Um, that BERT said,

201
00:10:16,290 --> 00:10:18,000
"No answer to this question,

202
00:10:18,000 --> 00:10:19,680
" rather than getting the correct answer.

203
00:10:19,680 --> 00:10:23,070
Even though it looks kind of straightforward reading it as a human being.

204
00:10:23,070 --> 00:10:27,315
It doesn't really look a human tricky reading comprehension question.

205
00:10:27,315 --> 00:10:30,390
Um, so, that's the default final project.

206
00:10:30,390 --> 00:10:35,355
So, on Thursday, I'm going to talk more about the default final project.

207
00:10:35,355 --> 00:10:39,255
I'm going to talk about how people build textual question answering systems.

208
00:10:39,255 --> 00:10:43,740
And the details on the default final project should all be posted by then,

209
00:10:43,740 --> 00:10:47,220
but that's just to give you a bit of context of what the other choice is.

210
00:10:47,220 --> 00:10:51,165
And today, I'm sort of more going to be aiming at people,

211
00:10:51,165 --> 00:10:54,180
um, doing the custom final project.

212
00:10:54,180 --> 00:10:58,590
But let me just sort of say a bit first about the choice between the two of them.

213
00:10:58,590 --> 00:11:02,940
So, um, why might you want to choose the default final project?

214
00:11:02,940 --> 00:11:07,320
So, if you have limited experience with research,

215
00:11:07,320 --> 00:11:12,180
you don't have any clear idea of a research project you want to do this quarter,

216
00:11:12,180 --> 00:11:14,850
you're just really busy with other classes that, uh,

217
00:11:14,850 --> 00:11:17,700
you're enrolled in CS140 and you're just really loade- loaded

218
00:11:17,700 --> 00:11:21,195
[LAUGHTER] now with other classes you're doing this quarter.

219
00:11:21,195 --> 00:11:25,890
Um, you'd be happy to have just a clear goal towards, to work towards.

220
00:11:25,890 --> 00:11:29,550
A leaderboard of your fellow students that you can compete against.

221
00:11:29,550 --> 00:11:31,890
Um, do the default final project.

222
00:11:31,890 --> 00:11:36,510
Um, I think for many people it's actually the good right choice.

223
00:11:36,510 --> 00:11:38,670
And I mean, for what it's worth, I mean,

224
00:11:38,670 --> 00:11:43,160
typically, slightly over half of people have done the default final project.

225
00:11:43,160 --> 00:11:45,170
It's normally that, so 55 percent have done

226
00:11:45,170 --> 00:11:48,680
the default final project and the rest the custom final project.

227
00:11:48,680 --> 00:11:51,140
So, if you do the default final project,

228
00:11:51,140 --> 00:11:52,715
you'll get lots of guidance.

229
00:11:52,715 --> 00:11:54,545
You get lots of scaffolding.

230
00:11:54,545 --> 00:11:58,355
There are clear things to aim at in what you do.

231
00:11:58,355 --> 00:12:04,015
Um, the course staff are in general most prepared and most able to help you.

232
00:12:04,015 --> 00:12:05,990
Um, and in particular,

233
00:12:05,990 --> 00:12:09,005
I mean, the, for the bottom bullet here.

234
00:12:09,005 --> 00:12:11,510
I mean, you know, something to think about in making

235
00:12:11,510 --> 00:12:16,040
the choices that some of it comes down to how committed,

236
00:12:16,040 --> 00:12:21,320
organized, and keen are you to be wanting to do your own custom final project.

237
00:12:21,320 --> 00:12:24,890
If you've got a, something you really want to do for a custom final project, great.

238
00:12:24,890 --> 00:12:28,270
We love to see interesting custom final projects.

239
00:12:28,270 --> 00:12:32,760
But, you know, if you're going to end up doing something that just looks

240
00:12:32,760 --> 00:12:39,150
worse like [LAUGHTER] not done as well [LAUGHTER] as you would've done a, done a project.

241
00:12:39,150 --> 00:12:42,090
If you'd just done the fin-, default final project,

242
00:12:42,090 --> 00:12:45,090
then you should probably choose the default final project [LAUGHTER].

243
00:12:45,090 --> 00:12:47,180
Um, okay.

244
00:12:47,180 --> 00:12:48,785
But even if you are doing,

245
00:12:48,785 --> 00:12:51,125
think you'll do the default final project.

246
00:12:51,125 --> 00:12:54,620
I hope that some of this lecture will still, um, be useful.

247
00:12:54,620 --> 00:12:56,660
While the part in the middle, when I talk back about

248
00:12:56,660 --> 00:12:59,525
MT and Gater or current networks are definitely useful.

249
00:12:59,525 --> 00:13:01,670
But, you know, beyond that, um,

250
00:13:01,670 --> 00:13:05,345
some of the tips on doing research and discussions of,

251
00:13:05,345 --> 00:13:10,230
sort of looking at how to make neural networks work and error analysis, paper writing.

252
00:13:10,230 --> 00:13:14,715
These are all good topics that apply to the default final project as well.

253
00:13:14,715 --> 00:13:16,770
So, in the other direction, um,

254
00:13:16,770 --> 00:13:19,680
if you have some research project that you're excited about.

255
00:13:19,680 --> 00:13:22,590
Possibly, it's one you are already working on or possibly,

256
00:13:22,590 --> 00:13:24,615
that you've just always wished to do.

257
00:13:24,615 --> 00:13:27,690
Something exciting with neural networks and rap music.

258
00:13:27,690 --> 00:13:32,340
Um, well, you know, that custom final project is an opportunity to do that.

259
00:13:32,340 --> 00:13:35,550
Um, so, it's a chance for you to do something on your own.

260
00:13:35,550 --> 00:13:37,980
Um, it, you know, obviously,

261
00:13:37,980 --> 00:13:40,200
if you're not interested in textural question-answering

262
00:13:40,200 --> 00:13:42,150
but do you think you might like machine translation.

263
00:13:42,150 --> 00:13:43,740
Well, it's an opportunity, um,

264
00:13:43,740 --> 00:13:45,765
to choose any topic of your own.

265
00:13:45,765 --> 00:13:52,590
It's also a way to sort of experience much more of the research pro- process because,

266
00:13:52,590 --> 00:13:55,290
you know, for the default final project, it's a bigger,

267
00:13:55,290 --> 00:13:58,545
more open-ended thing than any of our assignments.

268
00:13:58,545 --> 00:13:59,895
But, you know, nevertheless,

269
00:13:59,895 --> 00:14:01,800
the default final project is still

270
00:14:01,800 --> 00:14:05,790
sort of a pre-setup thing that you don't have to find your own problem,

271
00:14:05,790 --> 00:14:07,185
find your own data,

272
00:14:07,185 --> 00:14:08,940
work out a good approach to it.

273
00:14:08,940 --> 00:14:10,980
A lot of that's sort of been done for you.

274
00:14:10,980 --> 00:14:14,505
So, that, for a custom final project it's much more

275
00:14:14,505 --> 00:14:18,900
your own job to sort of define and execute a mini research project.

276
00:14:18,900 --> 00:14:22,440
And so, if all of that stuff seems appealing or some of it seems appealing,

277
00:14:22,440 --> 00:14:24,975
um, then aim at the custom final project.

278
00:14:24,975 --> 00:14:30,045
Um, doing this just reminded me about a fact about assignments one to five.

279
00:14:30,045 --> 00:14:32,309
You know, for assignments one to five,

280
00:14:32,309 --> 00:14:36,090
we are hoping that they can be a set of stepping

281
00:14:36,090 --> 00:14:39,885
stones for learning how to build deep learning systems.

282
00:14:39,885 --> 00:14:47,310
But, you know, one of our goals in that is to give you less hand holds as time goes by.

283
00:14:47,310 --> 00:14:51,650
So, you know, assignment one was really easy and assignment three,

284
00:14:51,650 --> 00:14:53,880
we tried to make it really handholdy,

285
00:14:53,880 --> 00:14:56,700
so people could start to learn PyTorch.

286
00:14:56,700 --> 00:14:59,280
But, you know, we're actually hoping for assignments

287
00:14:59,280 --> 00:15:02,280
four and five that they're actually harder,

288
00:15:02,280 --> 00:15:04,850
so that you're getting more experience of working

289
00:15:04,850 --> 00:15:07,430
out how to build and do things by yourself

290
00:15:07,430 --> 00:15:12,830
because if the only thing you ever see is completely scaffolded assignments.

291
00:15:12,830 --> 00:15:17,300
It's sort of like when you do CS106A that you have to do a great job on

292
00:15:17,300 --> 00:15:21,980
the CS106A assignments but you don't really know how to write a program by yourselves.

293
00:15:21,980 --> 00:15:23,555
And that's sort of what we want to, um,

294
00:15:23,555 --> 00:15:25,310
sort of get you beyond,

295
00:15:25,310 --> 00:15:27,050
um, in the latter two assignments.

296
00:15:27,050 --> 00:15:29,860
So, I hope you have started on assignment four.

297
00:15:29,860 --> 00:15:34,885
If not, you really should start and get underway soon as Abby was emphasizing.

298
00:15:34,885 --> 00:15:37,575
Okay. So, this year for the,

299
00:15:37,575 --> 00:15:40,935
um, final project, whichever one you're doing.

300
00:15:40,935 --> 00:15:43,770
Um, we're actually putting more structure in than we have

301
00:15:43,770 --> 00:15:46,730
in previous years to encourage people to get going.

302
00:15:46,730 --> 00:15:48,035
And so, in particular,

303
00:15:48,035 --> 00:15:52,190
there are early on components which are worth points in the grading.

304
00:15:52,190 --> 00:15:55,505
So, the first part of that is a project proposal,

305
00:15:55,505 --> 00:15:57,410
um, which is, um,

306
00:15:57,410 --> 00:15:59,020
we want from each team.

307
00:15:59,020 --> 00:16:00,915
So, one per team, um,

308
00:16:00,915 --> 00:16:02,670
you can just do a joint one,

309
00:16:02,670 --> 00:16:04,620
um, which is worth five percent.

310
00:16:04,620 --> 00:16:08,300
Um, so, it's, we're releasing the details on Thursday which is when

311
00:16:08,300 --> 00:16:12,650
assignment four is due and it'll be due the following Thursday.

312
00:16:12,650 --> 00:16:16,430
So, we're actually having an interruption in the sequence of current assignments, right.

313
00:16:16,430 --> 00:16:19,250
So, for the next week, um,

314
00:16:19,250 --> 00:16:22,805
what the thing to do is project proposal.

315
00:16:22,805 --> 00:16:24,770
And then the week after that, um,

316
00:16:24,770 --> 00:16:29,080
we're back to assignment five and then we go full time into final project.

317
00:16:29,080 --> 00:16:30,590
So, what we're wanting for

318
00:16:30,590 --> 00:16:34,250
the project proposal is we're actually wanting you to do a little bit

319
00:16:34,250 --> 00:16:39,470
of starting off research and the fine ter- terms of reading some paper.

320
00:16:39,470 --> 00:16:41,750
So, find some paper that's, um,

321
00:16:41,750 --> 00:16:43,520
relevant to your research,

322
00:16:43,520 --> 00:16:45,545
um, that you are going to do.

323
00:16:45,545 --> 00:16:49,215
Um, read it, write a summary of what it does.

324
00:16:49,215 --> 00:16:54,275
Um, write down some thoughts on how you could adapt or extend ideas in it,

325
00:16:54,275 --> 00:16:56,450
in your own final project.

326
00:16:56,450 --> 00:16:59,810
Um, and then say something about what your plan is for

327
00:16:59,810 --> 00:17:03,080
what you're goi- hoping to do for your final project.

328
00:17:03,080 --> 00:17:05,660
And especially, if you're doing a custom final project

329
00:17:05,660 --> 00:17:08,210
there's more to write there because we'll want to make

330
00:17:08,210 --> 00:17:10,445
sure that you have some idea as to

331
00:17:10,445 --> 00:17:13,360
what data you can use and how are you going to evaluate it.

332
00:17:13,360 --> 00:17:16,130
Whereas a couple of those things are actually sort of

333
00:17:16,130 --> 00:17:20,250
determined for you if you're doing the default final project.

334
00:17:20,430 --> 00:17:25,540
Um, and so then after that we're going to have a project milestone, um,

335
00:17:25,540 --> 00:17:28,390
which is the progress report where we're hoping that you can

336
00:17:28,390 --> 00:17:31,300
report that you're well along in your final project.

337
00:17:31,300 --> 00:17:33,850
That you've run at least some experiment and have

338
00:17:33,850 --> 00:17:37,075
some results on some data that you can talk about.

339
00:17:37,075 --> 00:17:39,820
So the default- the project milestone is due on,

340
00:17:39,820 --> 00:17:41,785
um, Thursday, March seven.

341
00:17:41,785 --> 00:17:45,010
So it's actually more than halfway through

342
00:17:45,010 --> 00:17:48,130
the period that's sort of dedicated to the final project.

343
00:17:48,130 --> 00:17:51,220
So, if you are not- we sort of put it past

344
00:17:51,220 --> 00:17:54,970
halfway because the fact of the matter is it always takes people time to get going,

345
00:17:54,970 --> 00:17:56,635
um, but nevertheless, you know,

346
00:17:56,635 --> 00:17:59,350
what you should have in your head is unless you're halfway

347
00:17:59,350 --> 00:18:02,440
through by the time you're handing in your,

348
00:18:02,440 --> 00:18:06,040
um, project milestone, then you're definitely behind.

349
00:18:06,040 --> 00:18:09,910
And you'll be doing that typical Stanford thing of having a lot of late nights

350
00:18:09,910 --> 00:18:14,750
and lack of sleep in the last week [LAUGHTER] of class trying to catch up for that.

351
00:18:14,760 --> 00:18:17,485
Um, okay. So, um,

352
00:18:17,485 --> 00:18:19,015
so now I've sort of, um,

353
00:18:19,015 --> 00:18:22,900
want to sort of just start saying a bit of- for

354
00:18:22,900 --> 00:18:25,270
custom final projects of some of the sort of

355
00:18:25,270 --> 00:18:28,195
thinking and types of things that you could do about that.

356
00:18:28,195 --> 00:18:31,570
Um, so you have to determine some project,

357
00:18:31,570 --> 00:18:35,140
um, for- if you're doing a custom final project.

358
00:18:35,140 --> 00:18:37,330
So, in philosophy of science, you know,

359
00:18:37,330 --> 00:18:40,810
there are basically two ways for any field you can have a project.

360
00:18:40,810 --> 00:18:44,515
You either start with some domain problem of interest.

361
00:18:44,515 --> 00:18:48,460
You're [NOISE] just got something you're interested in or say,

362
00:18:48,460 --> 00:18:51,895
"Gee, I'd like to do better machine translation."

363
00:18:51,895 --> 00:18:55,225
And then you work out some ways to address it with technology,

364
00:18:55,225 --> 00:18:56,560
or you start with some, um,

365
00:18:56,560 --> 00:18:58,705
technical approach of interest.

366
00:18:58,705 --> 00:19:00,550
And you say, "Oh well,

367
00:19:00,550 --> 00:19:02,500
those LSTMs seemed kind of neat,

368
00:19:02,500 --> 00:19:04,360
but I didn't understand why there's

369
00:19:04,360 --> 00:19:08,035
that extra 10H and I think it'd be better if it changed in this other way.

370
00:19:08,035 --> 00:19:13,570
And you start exploring from a technical direction to try and come up with a better idea.

371
00:19:13,570 --> 00:19:15,970
And then you're wanting to prove that it works.

372
00:19:15,970 --> 00:19:20,589
So in kinds of the projects that people do for this class,

373
00:19:20,589 --> 00:19:22,510
this isn't quite an exhaustive list,

374
00:19:22,510 --> 00:19:24,970
but this is sort of in general what people do.

375
00:19:24,970 --> 00:19:28,510
So, the first category and really I think this

376
00:19:28,510 --> 00:19:32,080
is the bulk of projects over half is people find

377
00:19:32,080 --> 00:19:35,650
some task replication of interest and they build

378
00:19:35,650 --> 00:19:39,745
some neural network models to try and do it as effectively as possible.

379
00:19:39,745 --> 00:19:47,020
Um, there's a second category where people sort of concentrate on implementing,

380
00:19:47,020 --> 00:19:53,575
so re-implementing some complex neural architecture and getting it to work on some data.

381
00:19:53,575 --> 00:19:57,115
And so let me just say a couple of sentences on this.

382
00:19:57,115 --> 00:20:01,525
Um, so, it's certainly okay for you to,

383
00:20:01,525 --> 00:20:05,395
um, start by re-implementing some existing model.

384
00:20:05,395 --> 00:20:10,975
Um, and some people that's as far as they get.

385
00:20:10,975 --> 00:20:14,635
And then the question is, um, is that okay?

386
00:20:14,635 --> 00:20:17,650
And the answer to whether that's okay sort

387
00:20:17,650 --> 00:20:20,920
of largely depends on how complex your neural model is.

388
00:20:20,920 --> 00:20:28,060
Um, so if what you think is okay I'm going to, um,

389
00:20:28,060 --> 00:20:31,270
re-implement something like we've seen already,

390
00:20:31,270 --> 00:20:34,600
like a window-based classification model and you

391
00:20:34,600 --> 00:20:38,110
just re-implement that and run it on some data and get some results and stop.

392
00:20:38,110 --> 00:20:40,360
That's definitely a bad project.

393
00:20:40,360 --> 00:20:45,100
Um, but there are lots of very complicated and sophisticated neural,

394
00:20:45,100 --> 00:20:47,065
um, architectures out there.

395
00:20:47,065 --> 00:20:51,790
And if you're trying to do something complicated well then that can be a fine project.

396
00:20:51,790 --> 00:20:55,840
Um, so, I actually sort of stuck in a few examples of projects.

397
00:20:55,840 --> 00:21:00,490
So, I mean, here's one that was actually from a couple of years ago.

398
00:21:00,490 --> 00:21:03,535
Um, so this was in the 2017 class.

399
00:21:03,535 --> 00:21:07,150
And so, shortly before the 2017 class,

400
00:21:07,150 --> 00:21:11,230
"Deep Mind" who's one of the um, organizations producing

401
00:21:11,230 --> 00:21:14,380
the most complicated neural models had just released

402
00:21:14,380 --> 00:21:17,890
a paper about the differentiable neural computer model,

403
00:21:17,890 --> 00:21:20,170
which was a model of how to have something like

404
00:21:20,170 --> 00:21:23,109
a differentiate- differentiable Turing machine-like

405
00:21:23,109 --> 00:21:26,665
architecture inside a neural network, um,

406
00:21:26,665 --> 00:21:29,050
and  thought, um,

407
00:21:29,050 --> 00:21:32,230
this would be a great challenge to try and, um,

408
00:21:32,230 --> 00:21:36,970
re-implement the differentiable neural computer which Deep Mind hadn't released

409
00:21:36,970 --> 00:21:39,100
any source code for because they're not the kind of

410
00:21:39,100 --> 00:21:41,860
place that generally releases their source code.

411
00:21:41,860 --> 00:21:46,420
Um, and, you know, this was actually an extremely ambitious project because it

412
00:21:46,420 --> 00:21:51,835
was, it's a very complex architecture which is hard to get to train.

413
00:21:51,835 --> 00:21:54,265
And so, you know, at the end,

414
00:21:54,265 --> 00:21:58,180
at the end she hadn't been able to sort of train as

415
00:21:58,180 --> 00:22:02,230
big a model or get as good results as they report in the paper that,

416
00:22:02,230 --> 00:22:04,030
you know, frankly we thought it was pretty

417
00:22:04,030 --> 00:22:07,120
miraculous that she managed to get it working at all.

418
00:22:07,120 --> 00:22:11,920
In the period of time we had in the class and she did successfully do an open-source

419
00:22:11,920 --> 00:22:16,600
re-implementation of this model which basically worked the same as in their paper.

420
00:22:16,600 --> 00:22:17,770
Though not quite as well.

421
00:22:17,770 --> 00:22:19,810
So, you know, that seemed a huge achievement.

422
00:22:19,810 --> 00:22:23,905
So, you certainly can do something of that sort.

423
00:22:23,905 --> 00:22:28,210
Right. So, um, so you- you can sort of from

424
00:22:28,210 --> 00:22:32,845
a technical direction have some ideas for variant model and explore,

425
00:22:32,845 --> 00:22:35,650
um, how to make a different kind of model class and then look

426
00:22:35,650 --> 00:22:39,065
at how it works on some problem that works well.

427
00:22:39,065 --> 00:22:43,200
Another kind of project you can do is an analysis project,

428
00:22:43,200 --> 00:22:45,690
so that you might be interested in something in

429
00:22:45,690 --> 00:22:49,515
natural language or something on the behavior of neural networks,

430
00:22:49,515 --> 00:22:52,740
and just think that you want to analyze them more closely.

431
00:22:52,740 --> 00:22:54,705
So, you might think, "Oh,

432
00:22:54,705 --> 00:22:58,230
maybe these neural machine translation systems work great

433
00:22:58,230 --> 00:23:02,530
providing the word order is the same in the source and target language,

434
00:23:02,530 --> 00:23:07,180
but can they really do a good job of reordering phrases for different language types?

435
00:23:07,180 --> 00:23:09,670
How much does their performance vary based on

436
00:23:09,670 --> 00:23:12,625
the amount of reordering between the source and target language?"

437
00:23:12,625 --> 00:23:14,755
And you could do some experiments to try and

438
00:23:14,755 --> 00:23:18,610
investigate that as an analysis problem that looks at a model,

439
00:23:18,610 --> 00:23:21,010
and we sometimes get projects like that.

440
00:23:21,010 --> 00:23:24,040
Down at the bottom is the rarest kind of project,

441
00:23:24,040 --> 00:23:26,860
which is when some people try to do something

442
00:23:26,860 --> 00:23:30,625
theoretical which is to prove some properties of a system.

443
00:23:30,625 --> 00:23:35,410
So if- this is easiest to do in simple systems for something like word vectors,

444
00:23:35,410 --> 00:23:39,130
that if you might want to prove something about

445
00:23:39,130 --> 00:23:43,165
the kind of spaces that are induced by word vectors,

446
00:23:43,165 --> 00:23:45,490
and what properties you need to have in

447
00:23:45,490 --> 00:23:49,375
models for word analogies to work or something like that.

448
00:23:49,375 --> 00:23:53,995
Um here are just another couple of examples that so- shows some of the other classes.

449
00:23:53,995 --> 00:23:57,940
So, this one is an example of find a problem and build some models.

450
00:23:57,940 --> 00:24:04,150
So, these three people um, looked at Shakespearean Sonnet generation and then they considered

451
00:24:04,150 --> 00:24:07,780
several different models for Shakespearean Sonnet generation and

452
00:24:07,780 --> 00:24:11,770
got the best results from this sort of- you'd probably can't really see all the details,

453
00:24:11,770 --> 00:24:15,070
but they have a sort of a mixture of word level and

454
00:24:15,070 --> 00:24:18,400
character level gated model that feeds into

455
00:24:18,400 --> 00:24:23,125
a word level LSTM and produces sonnets and the output wasn't totally bad.

456
00:24:23,125 --> 00:24:26,305
"Thy youth's time and face his form shall cover.

457
00:24:26,305 --> 00:24:28,870
Now all fresh beauty my love there.

458
00:24:28,870 --> 00:24:32,290
Will ever time to greet forget each like ever decease,

459
00:24:32,290 --> 00:24:35,815
but in a- in a best at worship his glory die."

460
00:24:35,815 --> 00:24:37,780
Okay. It's maybe not perfect,

461
00:24:37,780 --> 00:24:41,965
[LAUGHTER] but it sort of sounds like a Shakespearean sonnet.

462
00:24:41,965 --> 00:24:44,160
Um, okay.

463
00:24:44,160 --> 00:24:46,880
Yeah. So, I showed you that one already.

464
00:24:46,880 --> 00:24:54,215
Um, here's, um, an example of someone who designed a different kind of network,

465
00:24:54,215 --> 00:24:58,760
and this was a project that came out of this class that was then continued with,

466
00:24:58,760 --> 00:25:01,310
and the- they got a conference paper out of it,

467
00:25:01,310 --> 00:25:03,860
the ICLR 2017 paper.

468
00:25:03,860 --> 00:25:09,440
So, this was looking at doing a better job at building a neural language model.

469
00:25:09,440 --> 00:25:12,110
And essentially, they had two ideas,

470
00:25:12,110 --> 00:25:16,430
both of which seem useful for building better neural language models.

471
00:25:16,430 --> 00:25:20,750
And so, one is that in the stuff that we've presented so far,

472
00:25:20,750 --> 00:25:22,790
whether it was the early word vectors,

473
00:25:22,790 --> 00:25:25,610
or what Abby presented last week in the neural language model,

474
00:25:25,610 --> 00:25:30,440
there are effectively two vectors for each word: there's one for the word encoding

475
00:25:30,440 --> 00:25:35,420
on the input and then when you have the softmax on the other side effectively,

476
00:25:35,420 --> 00:25:39,500
the rows of that matrix that go into the softmax are also

477
00:25:39,500 --> 00:25:44,195
word vectors for determining how likely you are to produce different words.

478
00:25:44,195 --> 00:25:48,710
And so, um, these two people had the idea that maybe if we actually in the model

479
00:25:48,710 --> 00:25:54,950
tied those two word ve- vectors together that would help and produce a better model and,

480
00:25:54,950 --> 00:25:57,230
um, and so this was actually done

481
00:25:57,230 --> 00:26:00,860
several years ago when that was a novel idea which hadn't actually been done.

482
00:26:00,860 --> 00:26:04,085
So, this was done in the 2016 class,

483
00:26:04,085 --> 00:26:06,875
and then they had this second idea which was,

484
00:26:06,875 --> 00:26:09,080
well maybe doing the kind of,

485
00:26:09,080 --> 00:26:11,660
cross entropy one, zero,

486
00:26:11,660 --> 00:26:14,600
sort of you look at the correct word that you are meant to

487
00:26:14,600 --> 00:26:18,620
produce and sort of work out a loss based on that.

488
00:26:18,620 --> 00:26:21,140
Maybe that's not very good because you don't get

489
00:26:21,140 --> 00:26:25,520
partial points if you produce a different word that's semantically similar.

490
00:26:25,520 --> 00:26:28,100
And so, that they had this idea that they could use

491
00:26:28,100 --> 00:26:33,350
word vector similarity and then you'd be giving a score for any word that was

492
00:26:33,350 --> 00:26:36,305
produced next based on how similar it was

493
00:26:36,305 --> 00:26:39,470
according to word vector similarity to the word that you are

494
00:26:39,470 --> 00:26:41,720
meant to produce next and that was also

495
00:26:41,720 --> 00:26:45,875
a useful idea that they're able to produce improved language models with.

496
00:26:45,875 --> 00:26:47,420
So, that was a cool project.

497
00:26:47,420 --> 00:26:50,180
Um, here's an example of, um,

498
00:26:50,180 --> 00:26:52,010
somebody from last year,

499
00:26:52,010 --> 00:26:54,560
um, who did an analysis project.

500
00:26:54,560 --> 00:26:57,135
So, their idea was,

501
00:26:57,135 --> 00:26:59,660
um, that they- well,

502
00:26:59,660 --> 00:27:00,680
they were going to, um,

503
00:27:00,680 --> 00:27:02,345
evaluate on some task,

504
00:27:02,345 --> 00:27:04,160
they actually did several tasks, um,

505
00:27:04,160 --> 00:27:07,130
word similarity, analogy, and the SQuAD,

506
00:27:07,130 --> 00:27:09,215
um, question answering system.

507
00:27:09,215 --> 00:27:11,180
But the question was, okay,

508
00:27:11,180 --> 00:27:16,235
a lot of neural network models are big and so aren't very suitable for phones, um,

509
00:27:16,235 --> 00:27:21,950
could we get away with compressing the models a lot so that rather than having doubles,

510
00:27:21,950 --> 00:27:25,580
or 32-bit floats, or even 16-bit floats,

511
00:27:25,580 --> 00:27:28,595
that are now used quite a bit in neural networks, could we,

512
00:27:28,595 --> 00:27:32,900
um, compress a lot more and quantize, um,

513
00:27:32,900 --> 00:27:35,450
numeric values so that we can only be, say,

514
00:27:35,450 --> 00:27:40,385
using two bits fo- per parameter so they'll literally need four bits per parameter?

515
00:27:40,385 --> 00:27:42,890
And if you do that naively, it doesn't work.

516
00:27:42,890 --> 00:27:48,500
But if you explore some cleverer ways of doing it and see how to make things work,

517
00:27:48,500 --> 00:27:51,455
you can actually get it to work, um, really well.

518
00:27:51,455 --> 00:27:54,680
Um, in fact, it actually seems like sometimes you can improve

519
00:27:54,680 --> 00:27:59,390
your performance doing this because the quantization acts as a form of regularizer.

520
00:27:59,390 --> 00:28:03,290
Um, you can find lots of other projects, um, online,

521
00:28:03,290 --> 00:28:07,145
if you look at the CS224n pages and you should.

522
00:28:07,145 --> 00:28:08,990
Um, okay.

523
00:28:08,990 --> 00:28:12,830
So, if you want to do a final project you have to find someplace to start.

524
00:28:12,830 --> 00:28:15,950
You know, one place is to start looking at papers there's

525
00:28:15,950 --> 00:28:19,760
online anthology of most of the NLP conference papers.

526
00:28:19,760 --> 00:28:23,690
You can look at M- ML conferences have lots of relevant papers as well.

527
00:28:23,690 --> 00:28:28,715
You can look at past CS224n papers that cover lots of topics.

528
00:28:28,715 --> 00:28:33,200
Um, though, you know, I- I sugge- don't also forget, um,

529
00:28:33,200 --> 00:28:36,185
the advice down the bottom, um,

530
00:28:36,185 --> 00:28:39,980
which is look for an interesting problem in the world.

531
00:28:39,980 --> 00:28:43,670
Um, so, our Stanford's CS emeritus professor

532
00:28:43,670 --> 00:28:47,240
Ed Feigenbaum likes to quote the advice of his,

533
00:28:47,240 --> 00:28:50,465
um, advisor, Herb Simon, um,

534
00:28:50,465 --> 00:28:55,655
of "If you see a research area where many people are working, go somewhere else."

535
00:28:55,655 --> 00:28:56,870
Um, well, you know,

536
00:28:56,870 --> 00:29:01,115
in the context of this class don't go so far away that you're not using

537
00:29:01,115 --> 00:29:05,825
neural networks or NLP because that won't work for project for this class.

538
00:29:05,825 --> 00:29:08,090
But, you know, nevertheless, I mean,

539
00:29:08,090 --> 00:29:10,250
in some sense it's a bad strategy of

540
00:29:10,250 --> 00:29:12,920
saying let's look at all the papers that were published last year,

541
00:29:12,920 --> 00:29:15,485
and let's wo- start working on one of their problems,

542
00:29:15,485 --> 00:29:18,605
or lots of people are working on question-answering, I'll do it too.

543
00:29:18,605 --> 00:29:21,695
You know, there are lots of interesting different problems

544
00:29:21,695 --> 00:29:24,245
in the world and if you know of some, you know,

545
00:29:24,245 --> 00:29:28,340
cool website that somehow does something interesting related to language,

546
00:29:28,340 --> 00:29:31,505
you know, maybe you can make a final project out of that.

547
00:29:31,505 --> 00:29:34,685
Um, other ways to find final projects.

548
00:29:34,685 --> 00:29:38,090
Um, so the person who's first put together most of

549
00:29:38,090 --> 00:29:43,220
the CS231n content was And- Andrej Karpathy, um,

550
00:29:43,220 --> 00:29:46,760
who now works at Tesla and among his other- things

551
00:29:46,760 --> 00:29:50,735
he did for the world he put together this site Arxiv Sanity Preserver, um,

552
00:29:50,735 --> 00:29:54,560
which is a way to find online archive papers which is

553
00:29:54,560 --> 00:29:59,000
a major pre-print server and if you say a few papers you're interested in,

554
00:29:59,000 --> 00:30:01,430
it'll show you other papers that you're interested in.

555
00:30:01,430 --> 00:30:03,755
It'll show you papers that are currently trending.

556
00:30:03,755 --> 00:30:05,705
So, that can be a good way to look.

557
00:30:05,705 --> 00:30:08,150
Um, if you think it'd be just good to be in

558
00:30:08,150 --> 00:30:10,610
some competition where you're wanting to

559
00:30:10,610 --> 00:30:13,205
build a system that's better than other people's,

560
00:30:13,205 --> 00:30:16,415
um, you can look at leaderboards for various tasks.

561
00:30:16,415 --> 00:30:19,160
So, there's this brand new site which is pretty good though

562
00:30:19,160 --> 00:30:21,950
not completely error free and correct, of

563
00:30:21,950 --> 00:30:26,120
paperswithcode.com, and it collects a whole lot of

564
00:30:26,120 --> 00:30:31,190
leaderboards for a whole lot of machine learning tasks including tons of language ones.

565
00:30:31,190 --> 00:30:33,860
So, it gives leaderboards for question answering,

566
00:30:33,860 --> 00:30:35,990
machine translation, named entity recognition,

567
00:30:35,990 --> 00:30:38,090
language modeling, part of speech tagging.

568
00:30:38,090 --> 00:30:40,115
All sorts of tasks you can find there,

569
00:30:40,115 --> 00:30:44,670
and find out what the current states of the art and datasets are.

570
00:30:44,920 --> 00:30:48,470
Okay. Um, so, you know,

571
00:30:48,470 --> 00:30:50,300
different projects are different,

572
00:30:50,300 --> 00:30:54,680
but often for a lot of projects the things you need to be making sure of is

573
00:30:54,680 --> 00:30:59,210
that something that you can get a decent amount of data about so you can train a model.

574
00:30:59,210 --> 00:31:00,800
It's a feasible task,

575
00:31:00,800 --> 00:31:04,100
it's not so enormous you can't possibly do it in four weeks.

576
00:31:04,100 --> 00:31:08,420
Um, you'll want to have some evaluation metric and

577
00:31:08,420 --> 00:31:10,760
normally for deep learning you have to have-

578
00:31:10,760 --> 00:31:13,220
even if you hope to do some human evaluation,

579
00:31:13,220 --> 00:31:17,105
as well, you have to have some automatic evaluation metric.

580
00:31:17,105 --> 00:31:19,655
Because unless there's just some code that you can run

581
00:31:19,655 --> 00:31:22,415
that gives you a score for how well you're doing,

582
00:31:22,415 --> 00:31:24,020
then unless you have that,

583
00:31:24,020 --> 00:31:27,920
you just sort of can't do the deep learning trick of saying, "Okay,

584
00:31:27,920 --> 00:31:34,040
let's, um, do backpropagation to optimize our scores according to this metric."

585
00:31:34,040 --> 00:31:39,050
And pretty much you'll want to do that to be able to do neural network optimization.

586
00:31:39,050 --> 00:31:45,020
Um, and we do require that there is an important part of NLP in your class project.

587
00:31:45,020 --> 00:31:46,400
I mean, it doesn't have to be only thing,

588
00:31:46,400 --> 00:31:48,665
you can be doing reinforcement learning as well,

589
00:31:48,665 --> 00:31:51,380
or you could do images to caption, say you're

590
00:31:51,380 --> 00:31:53,300
doing joint vision and NLP,

591
00:31:53,300 --> 00:31:55,650
but there has to be NLP in it.

592
00:31:55,650 --> 00:32:02,345
Okay. Ah, last bit before I get back onto the content from last week.

593
00:32:02,345 --> 00:32:07,605
Ah, so, something that you'll need to do is have data for your project.

594
00:32:07,605 --> 00:32:12,365
Um, so some people collect their own data for a project and, you know,

595
00:32:12,365 --> 00:32:14,700
it's not impossible to collect your own data

596
00:32:14,700 --> 00:32:17,950
especially if there's something you can do with unsupervised data.

597
00:32:17,950 --> 00:32:21,455
You might be able to get it by just sort of crawling an interesting website.

598
00:32:21,455 --> 00:32:25,170
You can annotate a small amount of data yourself.

599
00:32:25,170 --> 00:32:28,665
If you have any site that has some kind of, you know,

600
00:32:28,665 --> 00:32:31,325
ratings annotation stars on it,

601
00:32:31,325 --> 00:32:36,210
you can treat those as a form of, ah, annotation.

602
00:32:36,210 --> 00:32:41,985
Right? So, if you want to predict something like, um, you know,

603
00:32:41,985 --> 00:32:46,670
which descriptions on product review websites

604
00:32:46,670 --> 00:32:50,230
or which reviews on product review websites do people like?

605
00:32:50,230 --> 00:32:53,290
Well, they get star ratings at the bottom from people and

606
00:32:53,290 --> 00:32:56,605
then you can try and fit to that as your supervision.

607
00:32:56,605 --> 00:33:01,030
Um, sometimes people have data from an existing project for a company.

608
00:33:01,030 --> 00:33:02,630
You can use that.

609
00:33:02,630 --> 00:33:05,330
But nevertheless for most people, um,

610
00:33:05,330 --> 00:33:08,130
given that classes are short and things like that,

611
00:33:08,130 --> 00:33:10,530
the practical thing to do is use

612
00:33:10,530 --> 00:33:15,190
an existing curated dataset that's been built by previous researchers.

613
00:33:15,190 --> 00:33:20,120
That normally gives you a fast start and lets you get to work building models, um,

614
00:33:20,120 --> 00:33:21,935
there's obvious prior work,

615
00:33:21,935 --> 00:33:24,630
there are baselines and previous systems

616
00:33:24,630 --> 00:33:28,250
that you can compare your performance on, et cetera.

617
00:33:28,250 --> 00:33:32,040
Okay. Um, so, where can you find data?

618
00:33:32,040 --> 00:33:35,145
I'll just mention a couple of places here and there are lots more.

619
00:33:35,145 --> 00:33:37,470
So, traditionally the biggest source of

620
00:33:37,470 --> 00:33:40,540
linguistic data used by academics was this place called

621
00:33:40,540 --> 00:33:43,420
the Linguistic Data Consortium and they have lots of

622
00:33:43,420 --> 00:33:46,960
datasets for treebanks and named entities and coreference,

623
00:33:46,960 --> 00:33:48,980
parallel machine, translation data,

624
00:33:48,980 --> 00:33:50,405
et cetera, et cetera.

625
00:33:50,405 --> 00:33:55,310
And so, um, the Linguistic Data Consortium licenses their data,

626
00:33:55,310 --> 00:33:59,110
Stanford pays that license so you can use any of it.

627
00:33:59,110 --> 00:34:01,500
Um, but if you want to use it, um,

628
00:34:01,500 --> 00:34:05,355
you go to that, um, linguistics.stanford.edu page.

629
00:34:05,355 --> 00:34:08,315
And there's a sign-up, um, ah,

630
00:34:08,315 --> 00:34:12,490
piece on how to sign up where you basically, um, say,

631
00:34:12,490 --> 00:34:14,200
"I will use this data only for

632
00:34:14,200 --> 00:34:17,940
good Stanford purposes and not as the basis of my startup."

633
00:34:17,940 --> 00:34:21,075
And, um, then you can have access to that data

634
00:34:21,075 --> 00:34:24,785
and it can be made available by NFS or otherwise.

635
00:34:24,785 --> 00:34:27,340
Um, but as time has gone by,

636
00:34:27,340 --> 00:34:32,280
there's a ton of curated NLP data that's available on various websites.

637
00:34:32,280 --> 00:34:34,610
In fact, if anything the problem is it's just sort of

638
00:34:34,610 --> 00:34:37,985
spread over the web and that's sort of hard to find different things.

639
00:34:37,985 --> 00:34:42,310
But there are some, some sites that have a lot of data for various purposes.

640
00:34:42,310 --> 00:34:45,975
So, anything related to machine translation or just parallel,

641
00:34:45,975 --> 00:34:47,975
um, data across different languages.

642
00:34:47,975 --> 00:34:52,680
The statistical MT statmt.org site has a great amount of

643
00:34:52,680 --> 00:34:57,425
data and that organization runs shared tasks every year,

644
00:34:57,425 --> 00:34:59,315
the Workshop on Machine Translation,

645
00:34:59,315 --> 00:35:03,365
WMT which Abby already mentioned in her class.

646
00:35:03,365 --> 00:35:05,280
And they've got datasets that we use for

647
00:35:05,280 --> 00:35:08,210
those tasks and then there are leaderboards for those tasks.

648
00:35:08,210 --> 00:35:10,410
And you can find data for that.

649
00:35:10,410 --> 00:35:13,900
Um, if you thought dependency parsing was cool, um,

650
00:35:13,900 --> 00:35:18,695
there's the Universal Dependencies site which has parallel, not parallel site,

651
00:35:18,695 --> 00:35:21,720
which has treebanks in the same annotation scheme for

652
00:35:21,720 --> 00:35:24,300
about 60 different languages and you can work on

653
00:35:24,300 --> 00:35:27,800
parsers for different languages and things like that.

654
00:35:27,800 --> 00:35:31,330
Um, I'm not gonna bore you with going through all of them but, you know,

655
00:35:31,330 --> 00:35:33,840
there are just tons and tons of other datasets that

656
00:35:33,840 --> 00:35:37,675
Facebook has released datasets, Google's released datasets,

657
00:35:37,675 --> 00:35:41,375
I said Stanford have released several other datasets including

658
00:35:41,375 --> 00:35:45,230
the Stanford Sentiment Treebank and the Stanford Na- Natural Language, um,

659
00:35:45,230 --> 00:35:48,780
Inference corpus, uh, new question-answering datasets and

660
00:35:48,780 --> 00:35:52,980
including HotPotQA and conversational question answering.

661
00:35:52,980 --> 00:35:56,180
Other groups at different universities have released datasets.

662
00:35:56,180 --> 00:35:57,660
There are just tons of them.

663
00:35:57,660 --> 00:36:02,955
You can find data on sites like Kaggle where it has machine-learning competitions.

664
00:36:02,955 --> 00:36:06,020
There are sites with lists of datasets.

665
00:36:06,020 --> 00:36:09,860
You can look at research papers and see what datasets they used.

666
00:36:09,860 --> 00:36:12,695
And of course, you can ask the course staff or on Piazza

667
00:36:12,695 --> 00:36:16,305
to try and find suitable datasets for a project.

668
00:36:16,305 --> 00:36:19,570
Okay. Um, so that's a fair bit about

669
00:36:19,570 --> 00:36:23,180
the projects that I've got a bit more to say later about doing projects.

670
00:36:23,180 --> 00:36:27,230
Does anyone have any questions up until now on projects?

671
00:36:28,640 --> 00:36:34,180
Okay. Um, well, so now we're gonna sort of, um,

672
00:36:34,180 --> 00:36:39,205
flip a switch in our brains and go back and have one more look,

673
00:36:39,205 --> 00:36:42,105
um, at gated recurrent units,

674
00:36:42,105 --> 00:36:45,490
um, and what happens and what they mean.

675
00:36:45,490 --> 00:36:47,245
Um, and, you know,

676
00:36:47,245 --> 00:36:48,725
this is sort of,

677
00:36:48,725 --> 00:36:51,570
sort of the same material that Abby presented,

678
00:36:51,570 --> 00:36:54,065
presented a little bit differently but, you know,

679
00:36:54,065 --> 00:36:57,130
I hope it might just sort of give one more way of

680
00:36:57,130 --> 00:37:00,520
sort of thinking a bit about what's happening about

681
00:37:00,520 --> 00:37:03,795
these gated recurrent units and why they might be doing

682
00:37:03,795 --> 00:37:07,445
something useful and what are the alternatives to them.

683
00:37:07,445 --> 00:37:11,640
So, if you remember the problem we started with is that we

684
00:37:11,640 --> 00:37:16,525
wanted to understand sort of derivatives backward in time.

685
00:37:16,525 --> 00:37:18,270
And so, the idea of that is well,

686
00:37:18,270 --> 00:37:22,064
if we twiddle this a little bit at time T,

687
00:37:22,064 --> 00:37:27,240
how much effect is that going to have so we make some adjustment here.

688
00:37:27,240 --> 00:37:32,045
How much effect is that going to have n time steps later?

689
00:37:32,045 --> 00:37:38,210
Um, and well, we sort of looked at the derivatives and we sort of saw we got these,

690
00:37:38,210 --> 00:37:41,900
um, terms for each successive time step.

691
00:37:41,900 --> 00:37:48,700
And so as Abby discussed the problem is that for the derivatives that we got,

692
00:37:48,700 --> 00:37:52,225
we kind of got this matrix form for each time step.

693
00:37:52,225 --> 00:37:55,165
And so that if we're going through a lot of time steps,

694
00:37:55,165 --> 00:38:00,585
we got a lot of matrix multiplies and as the result of those matrix multiplies,

695
00:38:00,585 --> 00:38:03,280
pretty much either things disappeared down to

696
00:38:03,280 --> 00:38:07,280
zero or exploded upward depending on what was in the matrix.

697
00:38:07,280 --> 00:38:10,235
And so that- and so that's sort of means we,

698
00:38:10,235 --> 00:38:11,590
When the gradient goes to zero,

699
00:38:11,590 --> 00:38:14,870
we kind of can't know what's happening there.

700
00:38:14,870 --> 00:38:18,630
Whether there isn't any conditioning or just we can't measure it.

701
00:38:18,630 --> 00:38:23,030
And so that's sort of made people think that maybe this naive, um,

702
00:38:23,030 --> 00:38:29,350
recurrent neural network transition function just isn't a good one to use.

703
00:38:29,350 --> 00:38:33,755
And that sort of leads into these ideas of gated recurrent units.

704
00:38:33,755 --> 00:38:35,930
Right? Because if we have

705
00:38:35,930 --> 00:38:39,245
the simple recurrent neural network where we're

706
00:38:39,245 --> 00:38:42,800
sort of feeding forward for each step in time.

707
00:38:42,800 --> 00:38:45,520
Well, what happens is when we backpropagate.

708
00:38:45,520 --> 00:38:46,950
We have to backpropagate through

709
00:38:46,950 --> 00:38:52,230
every intermediate node and that's where we sort of have our gradients disappear.

710
00:38:52,230 --> 00:38:57,190
And so an idea of how you could fix that is to say well,

711
00:38:57,190 --> 00:39:03,130
suppose we just put in direct connections that were longer distance, um,

712
00:39:03,130 --> 00:39:07,215
then we'd also get direct backpropagation signal

713
00:39:07,215 --> 00:39:11,855
and so then we wouldn't have this same problem of vanishing gradients.

714
00:39:11,855 --> 00:39:17,130
And effectively, we've sort of looked at two ways in which you can achieve that effect.

715
00:39:17,130 --> 00:39:21,240
Because one way of you can achieve that effect which Abby looked at

716
00:39:21,240 --> 00:39:25,450
in the end part of the last lecture was this idea of attention.

717
00:39:25,450 --> 00:39:27,455
So, when you've got attention,

718
00:39:27,455 --> 00:39:31,890
you're actually are creating these shortcut connections,

719
00:39:31,890 --> 00:39:33,770
oops, they're the blue ones, um,

720
00:39:33,770 --> 00:39:38,870
from every time step and using it to calculate an attention distribution.

721
00:39:38,870 --> 00:39:41,320
But the way the attention was done that we looked at,

722
00:39:41,320 --> 00:39:46,125
it was sort of mushing together all previous time steps into some kind of an average.

723
00:39:46,125 --> 00:39:50,550
But the idea of the gated recurrent units is in some sense we want to

724
00:39:50,550 --> 00:39:55,760
achieve this same kind of ability to have shortcut connections.

725
00:39:55,760 --> 00:39:57,955
But we want to do it in

726
00:39:57,955 --> 00:40:04,415
a more controlled and adaptive fashion where we still do remember the position of things.

727
00:40:04,415 --> 00:40:08,975
So, how can we create an adaptive shortcut connection?

728
00:40:08,975 --> 00:40:10,770
And so that's, um,

729
00:40:10,770 --> 00:40:17,580
what we start to do with the gates that are put into a gated recurrent network.

730
00:40:17,580 --> 00:40:22,355
So, if- so first off we sort of say let's have

731
00:40:22,355 --> 00:40:26,219
a candidate update which is exactly the same

732
00:40:26,219 --> 00:40:30,390
as the one that's used in a simple recurrent neural network.

733
00:40:30,390 --> 00:40:34,285
But what we can do is add a gate.

734
00:40:34,285 --> 00:40:37,895
And so, the gate will calculate a value from zero to one.

735
00:40:37,895 --> 00:40:41,595
And so what we're going to do here is mix together

736
00:40:41,595 --> 00:40:46,210
using our candidate update which is just like

737
00:40:46,210 --> 00:40:51,720
a simple recurrent neural network which will be then mixed together with simply

738
00:40:51,720 --> 00:40:57,845
directly carrying forward the hidden state from the previous time step.

739
00:40:57,845 --> 00:41:02,710
So, once we're doing that we are sort of then adaptively-

740
00:41:02,780 --> 00:41:09,990
we're adaptively partly using a computation from one time step back,

741
00:41:09,990 --> 00:41:13,080
um, done as a recurrent neural network.

742
00:41:13,080 --> 00:41:16,980
And we're partly just inheriting the,

743
00:41:16,980 --> 00:41:19,540
we're just part- sorry, we're partly inheriting

744
00:41:19,540 --> 00:41:22,265
the hidden state from the previous time step.

745
00:41:22,265 --> 00:41:26,240
So, it's sort of like a shortcut connection but we're waiting as to

746
00:41:26,240 --> 00:41:30,840
how much we're short cutting and how much we're doing our computation.

747
00:41:30,840 --> 00:41:38,750
And we control that adaptive choice by using a calculation to set the gate.

748
00:41:38,750 --> 00:41:41,070
And we do that with a sigmoid, um,

749
00:41:41,070 --> 00:41:46,540
computed over the import and the hidden- previous hidden state and using it again,

750
00:41:46,540 --> 00:41:50,775
an equation kind of like a simple recurrent neural network.

751
00:41:50,775 --> 00:41:53,930
Okay. Um, but, you know,

752
00:41:53,930 --> 00:41:57,725
if you wanted to go a bit further than that,

753
00:41:57,725 --> 00:42:00,380
um, you could think well,

754
00:42:00,380 --> 00:42:05,825
maybe sometimes we sort of might actually

755
00:42:05,825 --> 00:42:11,435
just want to get rid of the stuff that was in the past.

756
00:42:11,435 --> 00:42:15,470
That maybe the stuff in the past sometimes becomes irrelevant, like,

757
00:42:15,470 --> 00:42:18,290
maybe sometimes we start a new sentence or a new

758
00:42:18,290 --> 00:42:21,905
thought and we just want to get rid of the stuff that's in the past.

759
00:42:21,905 --> 00:42:25,700
And so, that can lead into this idea of having a second gate,

760
00:42:25,700 --> 00:42:31,355
a reset gate and so the reset gate calculates a value from 0 to 1, um,

761
00:42:31,355 --> 00:42:33,065
just like the other gates,

762
00:42:33,065 --> 00:42:38,660
and then we're doing this element wise dot-product between

763
00:42:38,660 --> 00:42:44,435
the reset gate and the previous hidden state and that's then sort of saying well,

764
00:42:44,435 --> 00:42:47,900
maybe we want to keep some parts of what was stored

765
00:42:47,900 --> 00:42:52,355
previously and some parts that we now want to throw away.

766
00:42:52,355 --> 00:42:56,150
And so we put that into the model as a second gate.

767
00:42:56,150 --> 00:43:01,010
Um, and so an interesting way to think about that is to sort of think

768
00:43:01,010 --> 00:43:05,540
about this as if this recurrent neural network is like

769
00:43:05,540 --> 00:43:10,130
a little tiny computer as the kind of little tiny computers you

770
00:43:10,130 --> 00:43:15,035
might do in a sort of simple architecture class and if you think about it that way,

771
00:43:15,035 --> 00:43:20,300
um, for the basic simple recurrent neural network

772
00:43:20,300 --> 00:43:25,475
the way the tiny computer works is that you've got a bank of registers h,

773
00:43:25,475 --> 00:43:30,035
your hidden state, and at each time step you have to

774
00:43:30,035 --> 00:43:37,910
read- whoops, at each time step you have to read the entirety of your bank of registers,

775
00:43:37,910 --> 00:43:41,000
you do some computation and then you write

776
00:43:41,000 --> 00:43:44,600
the entirety of your bank of registers and, you know,

777
00:43:44,600 --> 00:43:47,960
if in terms of thinking about computer architecture,

778
00:43:47,960 --> 00:43:52,190
that sounds like a pretty bad way to implement a simple computer.

779
00:43:52,190 --> 00:43:57,545
Um, so precisely what a gated recurrent unit is doing is saying,

780
00:43:57,545 --> 00:44:01,960
"Well, maybe we can have a slightly more sophisticated little baby computer."

781
00:44:01,960 --> 00:44:08,095
Instead of that, we could select a subset of the registers that we want to read.

782
00:44:08,095 --> 00:44:11,170
And so, the reset gate can control that because it can say,

783
00:44:11,170 --> 00:44:13,720
"We'll just ignore a bunch of the other registers."

784
00:44:13,720 --> 00:44:20,785
Um, it then will compute a new value based on just these, um,

785
00:44:20,785 --> 00:44:27,220
stored registers and then the update gate which is also adaptive can say, "Well,

786
00:44:27,220 --> 00:44:29,305
I want you to write

787
00:44:29,305 --> 00:44:34,580
some registers but the rest of the registers will just keep their previous value."

788
00:44:34,580 --> 00:44:37,490
That seems a useful idea to have in a computer.

789
00:44:37,490 --> 00:44:39,680
And so, that's what we're doing here.

790
00:44:39,680 --> 00:44:42,710
And so, this model here is, um,

791
00:44:42,710 --> 00:44:49,115
what was- Abby presented second as the gated recurrent unit.

792
00:44:49,115 --> 00:44:53,390
So, this is sort of a much more realistic model

793
00:44:53,390 --> 00:44:57,515
and it sort of in some sense overlaps with the ideas of attention.

794
00:44:57,515 --> 00:45:03,245
Okay. Um, so gated recurrent units are actually a quite new model.

795
00:45:03,245 --> 00:45:07,970
Um, the model that was done way earlier and has had huge impact

796
00:45:07,970 --> 00:45:13,340
is these LSTM long short-term memory units and they are a bit more complex.

797
00:45:13,340 --> 00:45:15,035
Um, but, you know,

798
00:45:15,035 --> 00:45:17,690
a lot of it is sort of the same, right?

799
00:45:17,690 --> 00:45:20,210
So, the hidden state of

800
00:45:20,210 --> 00:45:25,040
a gated recurrent unit is kind of equivalent to the cell of the LSTM.

801
00:45:25,040 --> 00:45:29,990
So, both of them are using the same idea of summing together,

802
00:45:29,990 --> 00:45:34,460
a mixture of just directly interpret- directly inheriting

803
00:45:34,460 --> 00:45:39,140
what you had from the previous time step together with, um,

804
00:45:39,140 --> 00:45:43,790
something that you've calculated for the current time step and the way you count-

805
00:45:43,790 --> 00:45:49,550
calculate it for the current time step is exactly the same in both cases.

806
00:45:49,550 --> 00:45:53,375
Whoops, sorry. Both cases again that you're calculating

807
00:45:53,375 --> 00:45:58,130
the current update using this sort of simple RNN equation.

808
00:45:58,130 --> 00:46:00,560
So, those parts are exactly the same.

809
00:46:00,560 --> 00:46:04,310
Um, but the LSTM is a little bit more complicated.

810
00:46:04,310 --> 00:46:07,310
It now has three gates, um,

811
00:46:07,310 --> 00:46:08,795
and it's got this extra, um,

812
00:46:08,795 --> 00:46:12,500
hidden state that's then worked out with a bit more complexity.

813
00:46:12,500 --> 00:46:17,170
So, in terms of my LSTM picture, you know,

814
00:46:17,170 --> 00:46:22,360
the LSTM picture looks as if you sort of pull apart all of its math pretty

815
00:46:22,360 --> 00:46:29,990
complex but so there are three gates so that you can forget or ignore everything.

816
00:46:29,990 --> 00:46:32,030
So, you can forget or ignore the input,

817
00:46:32,030 --> 00:46:33,890
you can forget or ignore parts of

818
00:46:33,890 --> 00:46:38,750
your previous hidden state and you can forget or ignore parts of the cell

819
00:46:38,750 --> 00:46:42,065
when calculating the output and each of these

820
00:46:42,065 --> 00:46:46,145
is produce- when I say forget or ignore parts of,

821
00:46:46,145 --> 00:46:50,630
what that's meaning is you're calculating a vector which is then going to be element-wise

822
00:46:50,630 --> 00:46:56,075
multiplied by the import of the previous hidden state or the cell.

823
00:46:56,075 --> 00:46:59,270
And so, that's why you have this effective now an addressable bank of

824
00:46:59,270 --> 00:47:03,335
registers where you can use some of them but not others of them.

825
00:47:03,335 --> 00:47:06,785
Okay. So, the bottom part of the LSTM is just

826
00:47:06,785 --> 00:47:10,400
like a simpler simple recurrent neural network,

827
00:47:10,400 --> 00:47:12,815
um, which then calculates,

828
00:47:12,815 --> 00:47:15,125
um, a candidate update.

829
00:47:15,125 --> 00:47:21,290
And so, for both of the GRU and the LSTM the real secret is

830
00:47:21,290 --> 00:47:24,140
that rather than just keeping on multiplying

831
00:47:24,140 --> 00:47:28,025
stuff what you do is you add two things together.

832
00:47:28,025 --> 00:47:32,120
Um, and so this adding is why you don't

833
00:47:32,120 --> 00:47:36,050
get the same vanishing gradient evil effects because you're calculating a

834
00:47:36,050 --> 00:47:39,320
new candidate update and you're adding it to stuff that was

835
00:47:39,320 --> 00:47:42,665
previously in the cell and that gives you

836
00:47:42,665 --> 00:47:46,190
a simple gradient when you backpropagate that- that you have

837
00:47:46,190 --> 00:47:52,745
direct linear connection between the cell at time t and the cell at time t minus one.

838
00:47:52,745 --> 00:47:56,240
And so, really that simple addition there is sort of

839
00:47:56,240 --> 00:48:00,350
the secret of most of the power of LSTMs and

840
00:48:00,350 --> 00:48:04,010
this same idea of adding two things together has also been a

841
00:48:04,010 --> 00:48:08,105
secret of many of the other advances in deep learning recently.

842
00:48:08,105 --> 00:48:12,455
So, envision in the last couple of years the sort of standard model

843
00:48:12,455 --> 00:48:17,060
that everybody uses as ResNets, residual networks and they use

844
00:48:17,060 --> 00:48:23,000
exactly the same secret of allowing these adaptive updates where you add

845
00:48:23,000 --> 00:48:30,680
together a current layer's value with directly inheriting a value from the layer below.

846
00:48:30,680 --> 00:48:35,060
Um, other things that use similar ideas are things like highway networks and so on.

847
00:48:35,060 --> 00:48:39,050
So, that's proven to be an extremely powerful idea.

848
00:48:39,050 --> 00:48:42,440
Um, the LSTM is slightly different from

849
00:48:42,440 --> 00:48:46,505
the GRU because when we look back at its equations

850
00:48:46,505 --> 00:48:53,989
that the- the GRU kind of does a linear mixture where you have one gate value,

851
00:48:53,989 --> 00:48:57,545
UT, and one minus UT,

852
00:48:57,545 --> 00:49:02,870
where the LSTM adds values controlled by two different gates,

853
00:49:02,870 --> 00:49:05,615
a forget gate, and an input gate.

854
00:49:05,615 --> 00:49:09,290
Theoretically, having the adding of

855
00:49:09,290 --> 00:49:13,940
two separate gates rather than than a mixture is theoretically more powerful.

856
00:49:13,940 --> 00:49:16,550
Um, depending on the application,

857
00:49:16,550 --> 00:49:19,370
sometimes it doesn't seem to make much difference, um,

858
00:49:19,370 --> 00:49:23,480
but there's definitely a theoretical advantage to the LSTM there.

859
00:49:23,480 --> 00:49:31,070
Okay. Um, just, I hope that's maybe a little bit more helpful to have seen those again,

860
00:49:31,070 --> 00:49:35,550
um, any questions on gated recurrent units?

861
00:49:37,970 --> 00:49:41,350
Still look confusing?

862
00:49:42,650 --> 00:49:48,450
I think it's useful to have some kind of idea as to why the people come up with

863
00:49:48,450 --> 00:49:53,670
these things and why do they make sense but,

864
00:49:53,670 --> 00:49:58,680
you know, nevertheless, the reality is in the sort of era of

865
00:49:58,680 --> 00:50:03,750
2015 plus any deep learning package you use whether it's PyTorch,

866
00:50:03,750 --> 00:50:05,940
TensorFlow, MXNet whatever, you know,

867
00:50:05,940 --> 00:50:11,250
it just comes with LSTM and GRUs and you don't have to program your own.

868
00:50:11,250 --> 00:50:13,170
In fact, you're at disadvantage if you

869
00:50:13,170 --> 00:50:16,020
program your own because if you are using the built-in one,

870
00:50:16,020 --> 00:50:19,070
it's using an efficient CUDA kernel from

871
00:50:19,070 --> 00:50:23,915
Nvidia whereas your custom built one won't and/or run three times slower.

872
00:50:23,915 --> 00:50:26,915
Um, so, you know, essentially don't have to know how to do it,

873
00:50:26,915 --> 00:50:30,530
you can just take the attitude that an LSTM is just like

874
00:50:30,530 --> 00:50:35,345
a fancy recurrent network which will be easier to train and that's true.

875
00:50:35,345 --> 00:50:39,620
Um, but you know, these kind of architectural ideas have actually been

876
00:50:39,620 --> 00:50:45,420
central to most of the big advances that have come in deep learning in the last couple of years,

877
00:50:45,420 --> 00:50:47,640
so there's actually good to have an ID,

878
00:50:47,640 --> 00:50:49,920
to have some sense of what were

879
00:50:49,920 --> 00:50:53,685
these important ideas that made everything so much better because they had

880
00:50:53,685 --> 00:50:56,850
the same kind of component building blocks you might also want

881
00:50:56,850 --> 00:51:00,970
to use in custom models that you design for yourself.

882
00:51:02,120 --> 00:51:06,840
Okay, two bits of machine translation.

883
00:51:06,840 --> 00:51:11,250
Um, so a bit of machine translation that we

884
00:51:11,250 --> 00:51:15,720
sort of didn't cover next week but lots of people have been seeing

885
00:51:15,720 --> 00:51:19,920
and getting confused by in the assignments so I thought I'd explain

886
00:51:19,920 --> 00:51:24,210
a bit about is UNKs and explain where do UNKs

887
00:51:24,210 --> 00:51:28,410
come from and why are there UNKs and the reason why

888
00:51:28,410 --> 00:51:33,075
there are UNKs is effectively kind of for efficiency reasons.

889
00:51:33,075 --> 00:51:39,705
So, if you sort of think about producing output in a neural machine translation system

890
00:51:39,705 --> 00:51:43,170
and really this is the same as producing output

891
00:51:43,170 --> 00:51:46,680
in any natural, neural natural language generation system,

892
00:51:46,680 --> 00:51:49,785
so that's really the same for neural language model, um,

893
00:51:49,785 --> 00:51:56,970
that if you have a very large output vocabulary is just a expensive operation.

894
00:51:56,970 --> 00:52:04,850
So you have a big matrix of softmax parameters where you have a row for every word, um,

895
00:52:04,850 --> 00:52:12,420
and then you have what,

896
00:52:12,420 --> 00:52:15,330
[NOISE] then we have an animation that is not working for me.

897
00:52:15,330 --> 00:52:18,210
Oh, all right there, there we go.

898
00:52:18,210 --> 00:52:21,030
Um, so then we have some hidden state that we've

899
00:52:21,030 --> 00:52:25,335
calculated in our recurrent neural network.

900
00:52:25,335 --> 00:52:29,985
And so, what we gonna do is sort of multiply, um,

901
00:52:29,985 --> 00:52:33,105
that vector by every row of the matrix,

902
00:52:33,105 --> 00:52:39,030
put it through a softmax and then get probabilities without putting every word.

903
00:52:39,030 --> 00:52:40,770
Um, and you know,

904
00:52:40,770 --> 00:52:44,040
this seems pretty simple but the problem is that

905
00:52:44,040 --> 00:52:47,400
to the extent that you have a humongous vocabulary here,

906
00:52:47,400 --> 00:52:51,240
you just have to do a humongous number of rows

907
00:52:51,240 --> 00:52:55,185
of this multiplication and it actually turns out that

908
00:52:55,185 --> 00:52:59,025
doing this is the expensive part of

909
00:52:59,025 --> 00:53:03,600
having a neural machine translation or neural language model system, right?

910
00:53:03,600 --> 00:53:07,380
The LSTM might look complicated and hard to understand, but you know,

911
00:53:07,380 --> 00:53:11,939
it's relatively small vectors that you multiply or dot-product once,

912
00:53:11,939 --> 00:53:16,020
and it's not that much work whereas if you have a huge number of words,

913
00:53:16,020 --> 00:53:17,430
this is a huge amount of work.

914
00:53:17,430 --> 00:53:22,560
So, just for instance sort of for the pion- pioneering sequence to sequence,

915
00:53:22,560 --> 00:53:26,355
um, neural machine translation system that Google first did,

916
00:53:26,355 --> 00:53:30,840
they ran it on an eight GPU machine because they have lots of GPUs but

917
00:53:30,840 --> 00:53:36,075
the way they set it up to maximize performance was of those eight GPUs,

918
00:53:36,075 --> 00:53:38,490
three of them were running

919
00:53:38,490 --> 00:53:44,070
a deep multi-layer neural sequence model and the other five GPUs,

920
00:53:44,070 --> 00:53:47,970
the only thing that they were doing was calculating softmaxes because that's

921
00:53:47,970 --> 00:53:52,770
actually the bulk of the computation that you need to be able to do.

922
00:53:52,770 --> 00:53:56,850
Um, so the simplest way to make this, um,

923
00:53:56,850 --> 00:54:01,560
computation not completely excessive is to say,

924
00:54:01,560 --> 00:54:03,930
"Hey, I'll just limit the vocabulary."

925
00:54:03,930 --> 00:54:07,365
Yeah I know that you can make

926
00:54:07,365 --> 00:54:13,230
a million different words in English and if you look at Spanish inflections of verbs,

927
00:54:13,230 --> 00:54:16,245
there are a lot of them and there's gonna be huge number of words, um,

928
00:54:16,245 --> 00:54:20,220
but maybe I can just make do with a modest vocabulary and it'll be near enough.

929
00:54:20,220 --> 00:54:22,305
Surely 50,000 common words,

930
00:54:22,305 --> 00:54:25,245
I can cover a lot of stuff and so,

931
00:54:25,245 --> 00:54:29,580
that was sort of the starting off point of neural machine translation that you,

932
00:54:29,580 --> 00:54:34,515
people use the modest vocabulary like around 50,000 words.

933
00:54:34,515 --> 00:54:36,915
And well, if you do that, um,

934
00:54:36,915 --> 00:54:40,980
well, then what happens is you have UNKs.

935
00:54:40,980 --> 00:54:43,260
So UNK means, this is an unknown word,

936
00:54:43,260 --> 00:54:47,325
that's not in my vocabulary and so there are two kinds of UNKs,

937
00:54:47,325 --> 00:54:51,315
they can be UNKs in the source language and you know,

938
00:54:51,315 --> 00:54:55,710
they're sort of optional because, you know,

939
00:54:55,710 --> 00:54:59,475
it's not actually a problem having a large source language vocabulary,

940
00:54:59,475 --> 00:55:02,070
but the fact of the matter is if you've sort of trained

941
00:55:02,070 --> 00:55:04,620
a model on a certain amount of data,

942
00:55:04,620 --> 00:55:06,720
there are some words you aren't going to have seen,

943
00:55:06,720 --> 00:55:09,000
so you are going to have words that you just didn't

944
00:55:09,000 --> 00:55:11,520
see in your training data and you won't have

945
00:55:11,520 --> 00:55:14,430
any pre-trained or trained word vector

946
00:55:14,430 --> 00:55:17,760
for them and you can deal with that by either just treating them as UNK,

947
00:55:17,760 --> 00:55:20,595
so giving them a new word vector when you encounter them.

948
00:55:20,595 --> 00:55:24,570
But the tricky part is on the translation that you're wanting to

949
00:55:24,570 --> 00:55:28,725
produce these rare words but they're not in your output vocabulary,

950
00:55:28,725 --> 00:55:35,550
so your system is producing UNK, UNK to UNK, which is not a very good translation really.

951
00:55:35,550 --> 00:55:39,720
Um, yeah, and so that was sort of what the first,

952
00:55:39,720 --> 00:55:44,220
um, machine, neural machine translation systems, um, did.

953
00:55:44,220 --> 00:55:46,260
And so, you know, obviously that's not

954
00:55:46,260 --> 00:55:51,555
a very satisfactory state of affairs and so there's been a whole bunch of work,

955
00:55:51,555 --> 00:55:53,220
um, as to how to deal with this,

956
00:55:53,220 --> 00:56:00,465
so you can use methods that allow you to deal with a larger output vocabulary,

957
00:56:00,465 --> 00:56:03,780
um, without the computation being excessive.

958
00:56:03,780 --> 00:56:07,785
So one method of doing that is to have what's called a hierarchical softmax,

959
00:56:07,785 --> 00:56:11,505
so that rather than just having a huge matrix of words,

960
00:56:11,505 --> 00:56:14,910
you sort of have a tree structure in your vocabulary

961
00:56:14,910 --> 00:56:18,480
so you can do calculations with hierarchical,

962
00:56:18,480 --> 00:56:22,815
um, multiple small softmaxes and you can do that more quickly.

963
00:56:22,815 --> 00:56:25,620
Um, I'm not gonna go through all these exam,

964
00:56:25,620 --> 00:56:27,270
all these things in detail now,

965
00:56:27,270 --> 00:56:31,575
I'm just sort of very quickly mentioning them and if anyone's interested, they can look.

966
00:56:31,575 --> 00:56:34,830
People have used the noise-contrastive estimation idea that we

967
00:56:34,830 --> 00:56:38,235
saw with Word2vec in this context as well.

968
00:56:38,235 --> 00:56:42,660
So this is a way to get much faster training which is important,

969
00:56:42,660 --> 00:56:45,315
it's not really a way to solve, um,

970
00:56:45,315 --> 00:56:47,790
speed at translation time but, you know,

971
00:56:47,790 --> 00:56:50,580
if this means you can train your system in six hours instead of

972
00:56:50,580 --> 00:56:55,155
six days that's a big win and so that's a good technique to use.

973
00:56:55,155 --> 00:57:00,330
Um, people have done much smarter things, so really, um,

974
00:57:00,330 --> 00:57:03,750
the large vocabulary problem is basically solved

975
00:57:03,750 --> 00:57:07,650
now and so the kind of things that you can do is you can produce

976
00:57:07,650 --> 00:57:11,970
subsets of your vocabulary and train on particular subsets of

977
00:57:11,970 --> 00:57:16,380
vocabulary at a time and then when you're testing,

978
00:57:16,380 --> 00:57:20,820
you adaptively choose kind of a likely list of words that might

979
00:57:20,820 --> 00:57:25,290
appear in the translation of particular sentences or passages and then

980
00:57:25,290 --> 00:57:28,200
you can effectively work with sort of an appropriate subset of

981
00:57:28,200 --> 00:57:32,850
a vocabulary and that's sort of an efficient technique by which you can

982
00:57:32,850 --> 00:57:36,330
deal with an unlimited vocabulary but only be using

983
00:57:36,330 --> 00:57:41,955
a moderate sized softmax for any particular paragraph that you're translating,

984
00:57:41,955 --> 00:57:44,790
there's a paper that talks about that method.

985
00:57:44,790 --> 00:57:49,425
Um, another idea is you can use attention when you do translation,

986
00:57:49,425 --> 00:57:51,930
the idea talked about at the end of last time.

987
00:57:51,930 --> 00:57:55,095
So if you have attention, that sort of means that you can,

988
00:57:55,095 --> 00:57:57,660
you're pointing somewhere in the source and you

989
00:57:57,660 --> 00:58:00,660
know what you're translating at any point in time.

990
00:58:00,660 --> 00:58:05,070
So, if that word is a rare word that's not in your vocabulary,

991
00:58:05,070 --> 00:58:07,560
there are things that you could do to deal with that.

992
00:58:07,560 --> 00:58:09,930
I mean, firstly, if it's a rare word,

993
00:58:09,930 --> 00:58:13,140
its translation is much more likely to be constant,

994
00:58:13,140 --> 00:58:17,475
so you might just look it up in a dictionary or word list, um, and,

995
00:58:17,475 --> 00:58:19,830
um, stick in its translation,

996
00:58:19,830 --> 00:58:22,485
sometimes it's appropriate to do other things.

997
00:58:22,485 --> 00:58:24,450
I mean, turns out that, you know,

998
00:58:24,450 --> 00:58:29,685
quite a lot of things that unknown words turn out to be other things like, you know,

999
00:58:29,685 --> 00:58:32,970
hexadecimal numbers, or FedEx tracking IDs,

1000
00:58:32,970 --> 00:58:35,685
or GitHub shards, or things like that.

1001
00:58:35,685 --> 00:58:37,020
So for a lot of things like that,

1002
00:58:37,020 --> 00:58:39,390
the right thing to do is just to copy them across.

1003
00:58:39,390 --> 00:58:42,660
And so, another thing that people have looked at is copying models,

1004
00:58:42,660 --> 00:58:45,220
um, in machine translation.

1005
00:58:45,220 --> 00:58:48,215
Okay, um, there are more ideas that you can,

1006
00:58:48,215 --> 00:58:50,840
we can get into to solve this and actually, um,

1007
00:58:50,840 --> 00:58:52,790
next week we're gonna start dealing with

1008
00:58:52,790 --> 00:58:55,085
some of the other ways that you could solve this, um,

1009
00:58:55,085 --> 00:58:59,405
but I hope there to have given you sort of a sense of,

1010
00:58:59,405 --> 00:59:01,805
um, sort of what these UNKs are about,

1011
00:59:01,805 --> 00:59:03,635
why you see them and, uh,

1012
00:59:03,635 --> 00:59:06,140
that there are sort of some ways that you might

1013
00:59:06,140 --> 00:59:08,600
deal with them but you're not expected to be doing that,

1014
00:59:08,600 --> 00:59:10,900
um, for assignment four.

1015
00:59:10,900 --> 00:59:16,680
Okay, then I just wanted to give a teeny bit more on evaluation.

1016
00:59:16,680 --> 00:59:19,515
Um, so Abby said a little bit about

1017
00:59:19,515 --> 00:59:23,370
evaluation with blue and that then comes up in the assignment,

1018
00:59:23,370 --> 00:59:26,130
so I just thought I'd give you a little bit more context on

1019
00:59:26,130 --> 00:59:29,095
that since they're being quite a few questions about it.

1020
00:59:29,095 --> 00:59:33,045
So, um, so the general context here is, you know,

1021
00:59:33,045 --> 00:59:38,895
how do you evaluate machine translation quality and sort of to this day,

1022
00:59:38,895 --> 00:59:43,980
if you wanted to do a first rate bang up evaluation of machine translation quality,

1023
00:59:43,980 --> 00:59:47,670
the way you do it is you get human beings to assess quality,

1024
00:59:47,670 --> 00:59:50,835
you take translations and you send them to

1025
00:59:50,835 --> 00:59:54,870
human beings with good bilingual skills and get them to score things.

1026
00:59:54,870 --> 00:59:57,255
And there are two ways that are commonly used.

1027
00:59:57,255 --> 00:59:59,550
One is sort of rating on

1028
00:59:59,550 --> 01:00:04,290
Likert scales for things like adequacy and fluency of translations,

1029
01:00:04,290 --> 01:00:09,030
um, but another way that often works better is asking for comparative judgments.

1030
01:00:09,030 --> 01:00:14,035
So here are two translations of this sentence which is better, um.

1031
01:00:14,035 --> 01:00:16,940
And so that's, you know,

1032
01:00:16,940 --> 01:00:20,075
sort of still our gold standard of translation.

1033
01:00:20,075 --> 01:00:22,880
Um, another way you can evaluate translation is

1034
01:00:22,880 --> 01:00:25,925
use your translations in the downstream task.

1035
01:00:25,925 --> 01:00:28,640
So, you could say "I'm gonna build

1036
01:00:28,640 --> 01:00:33,500
a cross-lingual question answering system and inside that system I'm,

1037
01:00:33,500 --> 01:00:35,780
gonna use machine translation.

1038
01:00:35,780 --> 01:00:37,970
I'm gonna translate the questions um,

1039
01:00:37,970 --> 01:00:40,625
and then try and match them against the documents.

1040
01:00:40,625 --> 01:00:45,830
Um, and then my score will be how good my question answering system is,

1041
01:00:45,830 --> 01:00:48,800
and so the machine translation system is better

1042
01:00:48,800 --> 01:00:52,190
if my question-answering score um, goes up."

1043
01:00:52,190 --> 01:00:57,245
I mean, that's kind of a nice way to do things because you're kinda then taking them in, run around needing,

1044
01:00:57,245 --> 01:01:00,110
needing human beings, and yet you do have

1045
01:01:00,110 --> 01:01:03,485
a clear numerical measure that's coming out the back end.

1046
01:01:03,485 --> 01:01:06,545
But it sort of has some catches because, you know,

1047
01:01:06,545 --> 01:01:09,980
often there will be a fairly indirect connection between

1048
01:01:09,980 --> 01:01:14,090
your end task and the quality of the machine translation,

1049
01:01:14,090 --> 01:01:16,640
and it might turn out that there certain aspects of

1050
01:01:16,640 --> 01:01:20,510
the machine translation like whether you get agreement endings,

1051
01:01:20,510 --> 01:01:22,970
right on nouns and verbs or something.

1052
01:01:22,970 --> 01:01:26,120
They are actually just irrelevant to your performance in the task and say you're

1053
01:01:26,120 --> 01:01:29,645
not assessing all aspects of um, quality.

1054
01:01:29,645 --> 01:01:32,810
Um, and so then the third way to do it is to come up with

1055
01:01:32,810 --> 01:01:35,840
some way to score the direct tasks.

1056
01:01:35,840 --> 01:01:40,415
So, here, um, the direct task is machine translation,

1057
01:01:40,415 --> 01:01:44,450
and this has been a valuable tool.

1058
01:01:44,450 --> 01:01:47,300
For, you know, really the last so

1059
01:01:47,300 --> 01:01:51,289
25 years when people are doing machine learning models,

1060
01:01:51,289 --> 01:01:55,100
because as soon as you have an automatic way to score things,

1061
01:01:55,100 --> 01:02:02,060
you can then run automated experiments to say "Let me try out these 50 different options.

1062
01:02:02,060 --> 01:02:07,250
Let me start varying these hyper-parameters and work out which way to do things is best."

1063
01:02:07,250 --> 01:02:10,760
And that importance has only grown in the deep learning era,

1064
01:02:10,760 --> 01:02:15,200
when all the time what we want you to do is as Abby discussed, um,

1065
01:02:15,200 --> 01:02:18,140
build end-to-end systems and then back

1066
01:02:18,140 --> 01:02:21,200
propagate throughout the entire system to improve them,

1067
01:02:21,200 --> 01:02:22,910
and we're doing that based on having

1068
01:02:22,910 --> 01:02:26,465
some objective measure which is our automatic metric.

1069
01:02:26,465 --> 01:02:29,405
And so, that led into the development of

1070
01:02:29,405 --> 01:02:33,364
automatic metrics to try and assess machine translation quality,

1071
01:02:33,364 --> 01:02:38,135
and the most famous and still most used one is this one called BLEU.

1072
01:02:38,135 --> 01:02:41,375
And so, as Abby briefly mentioned,

1073
01:02:41,375 --> 01:02:44,900
we have a reference translation done by human beings.

1074
01:02:44,900 --> 01:02:49,790
At some time a human being has to translate each piece of source material once,

1075
01:02:49,790 --> 01:02:53,180
but then you take a machine translation and you

1076
01:02:53,180 --> 01:02:57,320
score it based on the extent to which there

1077
01:02:57,320 --> 01:03:00,920
are one or more word sequences that appear in

1078
01:03:00,920 --> 01:03:06,065
the reference translation and also appear in the machine translation.

1079
01:03:06,065 --> 01:03:12,530
And so you are working out n-gram preci-precision scores for different values of n. So,

1080
01:03:12,530 --> 01:03:16,010
the standard way of doing it is you do it for one grams,

1081
01:03:16,010 --> 01:03:18,560
bigrams, trigrams, and four-grams.

1082
01:03:18,560 --> 01:03:21,395
So, word sequences of size one to four,

1083
01:03:21,395 --> 01:03:26,270
and you try and find for ones of those in the machine translation,

1084
01:03:26,270 --> 01:03:31,760
whether they also appear in the reference translation,

1085
01:03:31,760 --> 01:03:34,415
and there are two tricks at work here.

1086
01:03:34,415 --> 01:03:39,515
Um, one trick is you have to do a kind of a bipartite matching um,

1087
01:03:39,515 --> 01:03:42,665
because it just can't be that um,

1088
01:03:42,665 --> 01:03:45,185
there's a word um,

1089
01:03:45,185 --> 01:03:49,550
in the, in the reference translation somewhere.

1090
01:03:49,550 --> 01:03:51,230
Um, [NOISE] I don't know if there's.

1091
01:03:51,230 --> 01:03:53,510
I've got a good example here [NOISE].

1092
01:03:53,510 --> 01:03:57,770
Um, maybe I can only do a silly example,

1093
01:03:57,770 --> 01:03:59,555
but I'll do a silly example.

1094
01:03:59,555 --> 01:04:03,320
Um, that it's- it doesn't seem like you wanna say "Okay.

1095
01:04:03,320 --> 01:04:05,420
Because there's a "the" in the reference,

1096
01:04:05,420 --> 01:04:08,975
that means that this "the" is right and this "the" is right,

1097
01:04:08,975 --> 01:04:12,800
and this "the" is right and every other "the" is also right."

1098
01:04:12,800 --> 01:04:14,495
That sort of seems unfair.

1099
01:04:14,495 --> 01:04:20,825
So, you're only allowed to use each thing in the reference once in matching n-grams,

1100
01:04:20,825 --> 01:04:24,140
but you are allowed to use it multiple times for different order n-grams.

1101
01:04:24,140 --> 01:04:26,570
So, you can use it both in the uh unigram,

1102
01:04:26,570 --> 01:04:28,985
bigram, trigram and 4-gram.

1103
01:04:28,985 --> 01:04:32,270
The other idea is that although you're measuring

1104
01:04:32,270 --> 01:04:37,205
the precision of n-grams that are in the machine translation,

1105
01:04:37,205 --> 01:04:39,860
you wouldn't want people to be able to cheat by

1106
01:04:39,860 --> 01:04:42,710
putting almost nothing into the machine translation.

1107
01:04:42,710 --> 01:04:47,450
So, you might wanna game it by no matter what the source document is.

1108
01:04:47,450 --> 01:04:49,520
If the target language is English,

1109
01:04:49,520 --> 01:04:51,110
you could just um say,

1110
01:04:51,110 --> 01:04:52,790
"My translation is the,

1111
01:04:52,790 --> 01:04:55,490
because I'm pretty sure that will be in

1112
01:04:55,490 --> 01:04:59,315
the reference translation somewhere and I'll get 0,3 unigram,

1113
01:04:59,315 --> 01:05:02,840
and that's not great but I'll get something for that and I am done."

1114
01:05:02,840 --> 01:05:04,880
And so you wouldn't want that and so,

1115
01:05:04,880 --> 01:05:08,870
you're then being penalized by something called the brevity penalty if

1116
01:05:08,870 --> 01:05:14,040
your translation is shorter than the reference translation,

1117
01:05:14,040 --> 01:05:18,370
and so this BLEU metric is um forming

1118
01:05:18,370 --> 01:05:24,280
a geometric average of n-gram precision up to some n. Normally,

1119
01:05:24,280 --> 01:05:25,300
it's sort of up to four,

1120
01:05:25,300 --> 01:05:26,485
is how it's done.

1121
01:05:26,485 --> 01:05:29,004
Where it's a weighted geometric average,

1122
01:05:29,004 --> 01:05:32,415
where you're putting weights on the different n-grams.

1123
01:05:32,415 --> 01:05:35,870
Um, for the assignment, we're only using unigrams and bigrams.

1124
01:05:35,870 --> 01:05:39,455
So, you could say that means we're putting a weight of zero on um,

1125
01:05:39,455 --> 01:05:42,650
the trigrams and 4-grams.

1126
01:05:42,650 --> 01:05:46,235
Okay. Um, and so that's basically what we're doing.

1127
01:05:46,235 --> 01:05:49,280
I-I've just mentioned um couple of other things.

1128
01:05:49,280 --> 01:05:51,845
You might think that this is kind of random,

1129
01:05:51,845 --> 01:05:53,780
and so people have um,

1130
01:05:53,780 --> 01:05:57,530
used this idea of rather than just having one reference translation,

1131
01:05:57,530 --> 01:06:00,080
we could have multiple reference translations,

1132
01:06:00,080 --> 01:06:02,720
because that way we can allow for there being

1133
01:06:02,720 --> 01:06:05,540
variation and good ways of translating things,

1134
01:06:05,540 --> 01:06:09,740
because in language there's always lots of good ways that you can translate one sentence.

1135
01:06:09,740 --> 01:06:12,425
Um, people have done that quite a bit,

1136
01:06:12,425 --> 01:06:16,820
but people have also decided that even if you have one translation,

1137
01:06:16,820 --> 01:06:20,990
provided it's independent and on a kind of statistical basis,

1138
01:06:20,990 --> 01:06:25,340
you're still more likely to match it if your translation is a good translation.

1139
01:06:25,340 --> 01:06:27,560
So, it's probably okay.

1140
01:06:27,560 --> 01:06:32,930
Um, so when BLEU was originally um, introduced,

1141
01:06:32,930 --> 01:06:37,370
BLEU seemed marvelous and people drew graphs like this showing how

1142
01:06:37,370 --> 01:06:41,915
closely BLEU scores correlated um,

1143
01:06:41,915 --> 01:06:45,605
with human judgments of translation quality.

1144
01:06:45,605 --> 01:06:48,710
However, um, like a lot of things in life,

1145
01:06:48,710 --> 01:06:50,900
there are a lot of things that are great measures,

1146
01:06:50,900 --> 01:06:53,870
providing people aren't directly trying to optimize it,

1147
01:06:53,870 --> 01:06:56,720
and so what's happened since then um,

1148
01:06:56,720 --> 01:07:00,620
is that everybody has been trying to optimize BLEU scores,

1149
01:07:00,620 --> 01:07:06,380
and the result of that is that BLEU scores have gone up massively but the correlation

1150
01:07:06,380 --> 01:07:08,540
between BLEU scores and human judgments of

1151
01:07:08,540 --> 01:07:12,185
translation in quality have gone down massively,

1152
01:07:12,185 --> 01:07:16,550
and so we're in this current state that um, the BLEU scores,

1153
01:07:16,550 --> 01:07:22,640
the machines, um are pretty near the scores of human translations.

1154
01:07:22,640 --> 01:07:24,800
So, you know, according to BLEU scores,

1155
01:07:24,800 --> 01:07:28,565
we're producing almost human quality machine translation,

1156
01:07:28,565 --> 01:07:32,690
but if you actually look at the real quality of the translations,

1157
01:07:32,690 --> 01:07:34,100
they're still well behind

1158
01:07:34,100 --> 01:07:39,560
human beings um and because you could say the metric is being gamed.

1159
01:07:39,560 --> 01:07:45,950
Okay. I'll hope those things help for giving more sense um for assignment four.

1160
01:07:45,950 --> 01:07:48,260
Um, so now for the last um,

1161
01:07:48,260 --> 01:07:50,135
about 12 minutes, um,

1162
01:07:50,135 --> 01:07:51,500
I just now wanna um,

1163
01:07:51,500 --> 01:07:58,160
return to um final projects and say a little bit more um about final projects.

1164
01:07:58,160 --> 01:08:01,205
Um so, there many,

1165
01:08:01,205 --> 01:08:03,710
many different ways you can do final projects,

1166
01:08:03,710 --> 01:08:06,290
but just to sort of go through the steps.

1167
01:08:06,290 --> 01:08:09,170
I mean, you know, for a simple straightforward project,

1168
01:08:09,170 --> 01:08:11,510
this is kind of the steps that you want to go through.

1169
01:08:11,510 --> 01:08:13,295
So, you choose some tasks,

1170
01:08:13,295 --> 01:08:17,375
summarizing text um, producing a shorter version of a text.

1171
01:08:17,375 --> 01:08:20,180
You work out some dataset that you can use.

1172
01:08:20,180 --> 01:08:22,970
So, this is an example of the kind of tasks that there

1173
01:08:22,970 --> 01:08:26,015
are academic data sets for that other people have used,

1174
01:08:26,015 --> 01:08:28,250
and so you could just use one of those,

1175
01:08:28,250 --> 01:08:31,730
and that's it, you're already done or you could think "Oh no!

1176
01:08:31,730 --> 01:08:33,350
I'm much too creative for that.

1177
01:08:33,350 --> 01:08:38,780
I'm gonna come up with my own dataset [NOISE] um and get some online source and do it."

1178
01:08:38,780 --> 01:08:40,370
Um, and you know,

1179
01:08:40,370 --> 01:08:45,800
summaries of the kind of things you can find online and produce your own dataset.

1180
01:08:45,800 --> 01:08:48,805
Um [NOISE] I wanna say a bit in,

1181
01:08:48,805 --> 01:08:50,395
in just after this,

1182
01:08:50,395 --> 01:08:53,380
about separating off um data sets for

1183
01:08:53,380 --> 01:08:56,860
training and test data, so I'll delay that, but that's important.

1184
01:08:56,860 --> 01:09:01,445
Then, you want to work out a way to evaluate your um,

1185
01:09:01,445 --> 01:09:04,940
system including an automatic evaluation.

1186
01:09:04,940 --> 01:09:06,530
Um, normally, for summarization,

1187
01:09:06,530 --> 01:09:08,510
people use a slightly different metric called

1188
01:09:08,510 --> 01:09:12,335
ROUGE but it's sort of related to BLEU hence its name.

1189
01:09:12,335 --> 01:09:14,960
Um, it's the same story that it sort of works,

1190
01:09:14,960 --> 01:09:17,165
but human evaluation is much better.

1191
01:09:17,165 --> 01:09:21,335
Um, but you need- so you need to work out some metrics you can use for the project.

1192
01:09:21,335 --> 01:09:25,535
Um, the next thing you should do is establish a baseline.

1193
01:09:25,535 --> 01:09:29,555
So, if it's a well-worked on problem there might already be one,

1194
01:09:29,555 --> 01:09:33,170
but it's not bad to try and calculate one for yourself anyway,

1195
01:09:33,170 --> 01:09:36,170
and in particular what you should first have is

1196
01:09:36,170 --> 01:09:39,440
a very simple model and see how well it works.

1197
01:09:39,440 --> 01:09:42,155
So, for human language material,

1198
01:09:42,155 --> 01:09:45,020
often doing things like bag of words models,

1199
01:09:45,020 --> 01:09:48,050
whether they're just a simple classifier over

1200
01:09:48,050 --> 01:09:52,540
words or a new bag of words, averaging word vectors.

1201
01:09:52,540 --> 01:09:56,995
It's just useful to try that on the task and see how it works,

1202
01:09:56,995 --> 01:09:59,680
see what kinds of things it already gets right,

1203
01:09:59,680 --> 01:10:01,825
what kind of things it gets wrong.

1204
01:10:01,825 --> 01:10:03,880
You know, one possibility is you will find that

1205
01:10:03,880 --> 01:10:07,135
a very simple model already does great on your task.

1206
01:10:07,135 --> 01:10:08,575
If that's the case, um,

1207
01:10:08,575 --> 01:10:10,270
you have too easy a task,

1208
01:10:10,270 --> 01:10:16,460
and you probably need to find a task that's more challenging to work on. Um, yes.

1209
01:10:16,460 --> 01:10:20,090
So after that, you'll then sort of think about what could be a good kind

1210
01:10:20,090 --> 01:10:23,930
of neural network model that might do well, implement it,

1211
01:10:23,930 --> 01:10:28,640
test it um, see what kind of errors that makes and you know,

1212
01:10:28,640 --> 01:10:30,545
that's sort of if you've gotten that far,

1213
01:10:30,545 --> 01:10:33,605
you're sort of in the right space for a class project.

1214
01:10:33,605 --> 01:10:37,400
But, you know, it's sort of hoped that you could do more than that.

1215
01:10:37,400 --> 01:10:39,935
But after you've seen the errors from the first version,

1216
01:10:39,935 --> 01:10:43,865
you could think about how to make it better and come up with a better project,

1217
01:10:43,865 --> 01:10:46,055
and so I would encourage everyone,

1218
01:10:46,055 --> 01:10:48,680
you know, you really do want to look at the data, right?

1219
01:10:48,680 --> 01:10:54,620
You don't just wanna be sort of having things and files and run and say "Okay, 0,71.

1220
01:10:54,620 --> 01:10:57,370
Let me make some random change 0,70.

1221
01:10:57,370 --> 01:11:00,235
Oh, that's not a good one," repeat over.

1222
01:11:00,235 --> 01:11:04,330
You actually want to be sort of looking at your dataset in any way you can.

1223
01:11:04,330 --> 01:11:06,760
It's good to visualize the dataset to understand what's

1224
01:11:06,760 --> 01:11:09,500
important in it that you might be able to take advantage of,

1225
01:11:09,500 --> 01:11:11,110
you want to be able to look at what kind of

1226
01:11:11,110 --> 01:11:12,970
errors are being made because that might give you

1227
01:11:12,970 --> 01:11:16,865
ideas of how you could put more stuff into the model that would do better.

1228
01:11:16,865 --> 01:11:20,465
Um, you might wanna do some graphing of the effect of hyper-parameters,

1229
01:11:20,465 --> 01:11:22,460
so you can kind of understand that better.

1230
01:11:22,460 --> 01:11:24,370
And so, the hope is that you will try out

1231
01:11:24,370 --> 01:11:27,250
some other kinds of models and make things better.

1232
01:11:27,250 --> 01:11:29,525
And sort of one of the goals here is,

1233
01:11:29,525 --> 01:11:34,085
it's good if you've sort of got a well-setup experimental setup,

1234
01:11:34,085 --> 01:11:37,300
so you can easily turn around experiments because then you're just more

1235
01:11:37,300 --> 01:11:41,855
likely to be able to try several things in the time available.

1236
01:11:41,855 --> 01:11:45,395
Okay. Um, couple of other things I wanted to mention.

1237
01:11:45,395 --> 01:11:49,615
Um, one is sort of different amounts of data.

1238
01:11:49,615 --> 01:11:53,510
So, it's really, really important for all the stuff that we do,

1239
01:11:53,510 --> 01:11:56,870
that we have different sets of data.

1240
01:11:56,870 --> 01:11:58,635
So, we have trained data,

1241
01:11:58,635 --> 01:12:00,425
we have dev test data,

1242
01:12:00,425 --> 01:12:03,130
we have test data at least,

1243
01:12:03,130 --> 01:12:05,540
and sometimes it's useful to have even,

1244
01:12:05,540 --> 01:12:08,240
um, more data available.

1245
01:12:08,240 --> 01:12:14,080
So, for many of the public datasets, they're already split into different subsets like this,

1246
01:12:14,080 --> 01:12:15,095
but there are some that aren't.

1247
01:12:15,095 --> 01:12:17,285
There are some that might only have a training set,

1248
01:12:17,285 --> 01:12:19,000
and a test set.

1249
01:12:19,000 --> 01:12:21,260
And what you don't want to do is think,

1250
01:12:21,260 --> 01:12:23,500
"Oh, there's only a training set and a test set.

1251
01:12:23,500 --> 01:12:26,180
Therefore I'll just run every time on the test set."

1252
01:12:26,180 --> 01:12:29,890
That- that's a really invalid way to go about your research.

1253
01:12:29,890 --> 01:12:30,990
So, if there aren't

1254
01:12:30,990 --> 01:12:34,385
dev sets available or you need to do some more tuning,

1255
01:12:34,385 --> 01:12:36,375
and you need some separate tuning data,

1256
01:12:36,375 --> 01:12:39,460
you sort of have to, um,

1257
01:12:39,460 --> 01:12:43,400
make it for yourself by splitting off some of the training data,

1258
01:12:43,400 --> 01:12:47,775
and not using it for the basic training and using it for tuning,

1259
01:12:47,775 --> 01:12:50,435
and fo- as dev data.

1260
01:12:50,435 --> 01:12:52,530
Um, yes.

1261
01:12:52,530 --> 01:12:56,490
So, to go on about that, um, more, more.

1262
01:12:56,490 --> 01:13:02,675
So, the basic issue is this issue of fitting and overfitting to particular datasets.

1263
01:13:02,675 --> 01:13:05,610
So, when we train a model, um,

1264
01:13:05,610 --> 01:13:07,560
on some training data,

1265
01:13:07,560 --> 01:13:10,460
we train it and the error rate goes down.

1266
01:13:10,460 --> 01:13:15,900
And over time, we gradually overfit to the training data because we sort of

1267
01:13:15,900 --> 01:13:21,820
pick up on our neural network f- facts about the particular training data items,

1268
01:13:21,820 --> 01:13:24,030
and we just sort of start to learn them.

1269
01:13:24,030 --> 01:13:25,790
Now in the old days,

1270
01:13:25,790 --> 01:13:30,060
the fact that you overfit to the training data was seen as evil.

1271
01:13:30,060 --> 01:13:32,130
In modern neural network think,

1272
01:13:32,130 --> 01:13:35,630
we don't think it is evil what we overfit to the training data

1273
01:13:35,630 --> 01:13:40,115
because all neural nets that are any good overfit to the training data,

1274
01:13:40,115 --> 01:13:42,875
and we would be very sad if they didn't.

1275
01:13:42,875 --> 01:13:44,660
I'll come back to that in a moment.

1276
01:13:44,660 --> 01:13:47,565
But nevertheless, they're overfitting like crazy.

1277
01:13:47,565 --> 01:13:52,920
So, what we, but and what we want to build is something that generalizes well.

1278
01:13:52,920 --> 01:13:55,085
So, we have to have some separate data,

1279
01:13:55,085 --> 01:13:56,815
that's our validation data,

1280
01:13:56,815 --> 01:14:01,030
and say look at what performance looks like on the validation data.

1281
01:14:01,030 --> 01:14:04,875
And commonly we find that training up until some point,

1282
01:14:04,875 --> 01:14:08,500
improves our performance on separate validation data,

1283
01:14:08,500 --> 01:14:11,050
and then we start to overfit to

1284
01:14:11,050 --> 01:14:15,765
the training data in a way that our validation set performance gets worse.

1285
01:14:15,765 --> 01:14:17,595
Um, and so, then,

1286
01:14:17,595 --> 01:14:21,965
further training on the training data isn't useful because we're starting

1287
01:14:21,965 --> 01:14:26,705
to build a model that generalizes worse when run on other data.

1288
01:14:26,705 --> 01:14:28,810
But there's- the whole point here is,

1289
01:14:28,810 --> 01:14:34,835
we can only do this experiment if our validation data is separate from our training data.

1290
01:14:34,835 --> 01:14:37,910
If it's the same data or if it's overlapping data,

1291
01:14:37,910 --> 01:14:39,950
we can't draw this graph.

1292
01:14:39,950 --> 01:14:42,810
Um, and so, therefore, we can't do valid experiments.

1293
01:14:42,810 --> 01:14:47,085
Um, now you might think, "Oh, well,

1294
01:14:47,085 --> 01:14:49,040
maybe I can, um,

1295
01:14:49,040 --> 01:14:52,175
do this and just use the test set of data."

1296
01:14:52,175 --> 01:14:55,805
Um, but that's also invalid,

1297
01:14:55,805 --> 01:14:58,920
and the reason why that's invalid is,

1298
01:14:58,920 --> 01:15:00,835
as you do experiments,

1299
01:15:00,835 --> 01:15:05,495
you also start slowly over fitting to your development data.

1300
01:15:05,495 --> 01:15:11,560
So, the standard practice is you do a run and you get a score on the development data.

1301
01:15:11,560 --> 01:15:13,145
You do a second run.

1302
01:15:13,145 --> 01:15:15,040
You do worse on the development data,

1303
01:15:15,040 --> 01:15:17,770
and so you throw that second model away.

1304
01:15:17,770 --> 01:15:19,020
You do a third experiment.

1305
01:15:19,020 --> 01:15:20,950
You do better on the development data,

1306
01:15:20,950 --> 01:15:24,905
and so you keep that model and you repeat over 50 times.

1307
01:15:24,905 --> 01:15:28,520
And while some of those subsequent models you keep,

1308
01:15:28,520 --> 01:15:34,195
are genuinely better because you sort of worked out something good to do.

1309
01:15:34,195 --> 01:15:38,885
But it turns out that some of those subsequent models only sort of just happened.

1310
01:15:38,885 --> 01:15:42,985
You just got lucky and they happened to score better on the development data.

1311
01:15:42,985 --> 01:15:46,905
And so, if you kind of keep repeating that process 60 or 100 times,

1312
01:15:46,905 --> 01:15:50,570
you're also gradually [NOISE] overfitting on your development data,

1313
01:15:50,570 --> 01:15:53,575
and you get unrealistically good dev scores.

1314
01:15:53,575 --> 01:15:55,475
And so, that means two things.

1315
01:15:55,475 --> 01:15:59,820
You know, if you want to be rigorous and do a huge amount of hyper-parameter exploration,

1316
01:15:59,820 --> 01:16:02,830
it can be good to have a second development se- test set,

1317
01:16:02,830 --> 01:16:05,660
so that you have one, that you haven't overfit as much.

1318
01:16:05,660 --> 01:16:08,450
And if you want to have valid scores on te-

1319
01:16:08,450 --> 01:16:12,595
on as to what is my actual performance on independent data,

1320
01:16:12,595 --> 01:16:15,725
it's vital that you have separate test data that you are

1321
01:16:15,725 --> 01:16:19,265
not using at all in this process, right?

1322
01:16:19,265 --> 01:16:21,395
So, the ideal state is that,

1323
01:16:21,395 --> 01:16:24,860
for your real test data, um,

1324
01:16:24,860 --> 01:16:29,590
that you never used it at all until you've finished training your data, uh,

1325
01:16:29,590 --> 01:16:34,060
training your model, and then you run your final model once on the test data,

1326
01:16:34,060 --> 01:16:36,510
and you write up your paper and those are your results.

1327
01:16:36,510 --> 01:16:39,495
Now, I will be honest and say the world usually isn't

1328
01:16:39,495 --> 01:16:42,790
quite that perfect because after you've done that,

1329
01:16:42,790 --> 01:16:44,960
you then go to sleep [NOISE] and wake up thinking.

1330
01:16:44,960 --> 01:16:47,640
"I've got a fantastic idea of how to make my model better."

1331
01:16:47,640 --> 01:16:49,520
and you run off and implement that,

1332
01:16:49,520 --> 01:16:51,700
and it works great on the dev data,

1333
01:16:51,700 --> 01:16:55,385
and then for you, run it on the test data again and the numbers go up.

1334
01:16:55,385 --> 01:16:57,640
Um, sort of everybody does that.

1335
01:16:57,640 --> 01:16:59,035
Um, and you know,

1336
01:16:59,035 --> 01:17:01,295
in modicum it's okay,

1337
01:17:01,295 --> 01:17:06,325
you know, if that means you occasionally run on the test data it's not so bad, um,

1338
01:17:06,325 --> 01:17:10,550
but you really need to be aware of the slippery slope because,

1339
01:17:10,550 --> 01:17:13,560
if you then start falling into, "I've got a new model.

1340
01:17:13,560 --> 01:17:14,885
Let me try that one on the test data.

1341
01:17:14,885 --> 01:17:16,925
I've got a new model. Let me try this one on the test data."

1342
01:17:16,925 --> 01:17:20,130
Then you're just sort of overfitting to the test data,

1343
01:17:20,130 --> 01:17:23,100
and getting an unrealistically high score.

1344
01:17:23,100 --> 01:17:27,605
And that's precisely why a lot of the competitions like Kaggle competitions,

1345
01:17:27,605 --> 01:17:31,680
have a secret test dataset that you can't run on.

1346
01:17:31,680 --> 01:17:33,615
So, that they can do a genuine,

1347
01:17:33,615 --> 01:17:37,150
independent test on the actual test data.

1348
01:17:37,150 --> 01:17:42,550
Okay. Um, let's see, um, a couple more minutes.

1349
01:17:42,550 --> 01:17:46,515
So, yeah, getting your neural network to train.

1350
01:17:46,515 --> 01:17:49,135
Um, my two messages are, you know,

1351
01:17:49,135 --> 01:17:52,425
first of all, you should start with a positive attitude.

1352
01:17:52,425 --> 01:17:54,565
Neural networks want to learn.

1353
01:17:54,565 --> 01:17:55,955
If they're not learning,

1354
01:17:55,955 --> 01:17:58,500
you're doing something to stop them from learning.

1355
01:17:58,500 --> 01:18:00,070
And so, you should just stop that,

1356
01:18:00,070 --> 01:18:02,260
and they will learn because they want to learn.

1357
01:18:02,260 --> 01:18:03,935
They're just like little children.

1358
01:18:03,935 --> 01:18:09,790
Um, but, if the follow up to that is the grim reality that there are just tons

1359
01:18:09,790 --> 01:18:11,910
of things you can do that will cause

1360
01:18:11,910 --> 01:18:15,710
your neural networks not to learn very well or at all,

1361
01:18:15,710 --> 01:18:17,820
and this is the frustrating part of

1362
01:18:17,820 --> 01:18:21,605
this whole field because you know, it's not like a compile error.

1363
01:18:21,605 --> 01:18:25,335
It can just be hard to find and fix them.

1364
01:18:25,335 --> 01:18:27,715
And, you know, it is just really

1365
01:18:27,715 --> 01:18:32,015
standard that you spend more time dealing with trying to find,

1366
01:18:32,015 --> 01:18:35,225
and fix why it doesn't work well and getting it to work well than

1367
01:18:35,225 --> 01:18:39,265
you- than the time you spent writing the code for your model.

1368
01:18:39,265 --> 01:18:43,734
So, remember to budget for that when you're doing your final project,

1369
01:18:43,734 --> 01:18:48,470
it just won't work if you finish the code a day or two before the deadline.

1370
01:18:48,470 --> 01:18:51,990
Um, so, you need to work out what those things are,

1371
01:18:51,990 --> 01:18:54,970
"That can be hard," but you know experience,

1372
01:18:54,970 --> 01:18:57,260
experimental care, rules of thumb help.

1373
01:18:57,260 --> 01:18:59,750
So, there are just lots of things that are important.

1374
01:18:59,750 --> 01:19:02,480
So, you know, your learning rates are important.

1375
01:19:02,480 --> 01:19:05,775
If your learning rates are way too high, things won't learn.

1376
01:19:05,775 --> 01:19:07,960
If your learning rates are way too low,

1377
01:19:07,960 --> 01:19:10,655
they will learn very slowly and badly.

1378
01:19:10,655 --> 01:19:13,270
Um, initialization makes a difference.

1379
01:19:13,270 --> 01:19:19,040
Having good initialization often determines how well neural networks, um, learn.

1380
01:19:19,040 --> 01:19:23,435
Um, I have a separate slide here that I probably haven't got time to go

1381
01:19:23,435 --> 01:19:28,225
through all of on sort of for sequence [NOISE] models,

1382
01:19:28,225 --> 01:19:31,950
some of the tips of what people normally think are

1383
01:19:31,950 --> 01:19:35,735
good ways to get those models, um, working.

1384
01:19:35,735 --> 01:19:38,415
But I'll just say this one last thing.

1385
01:19:38,415 --> 01:19:41,850
Um, I think the strategy that you really want to

1386
01:19:41,850 --> 01:19:45,920
take is to work incrementally and build up slowly.

1387
01:19:45,920 --> 01:19:47,490
It just doesn't work to think,

1388
01:19:47,490 --> 01:19:49,534
"Oh I've got the mother of all models,

1389
01:19:49,534 --> 01:19:51,660
and build this enormously complex thing,

1390
01:19:51,660 --> 01:19:53,000
and then run it on the data,

1391
01:19:53,000 --> 01:19:54,784
and it crashes and burns."

1392
01:19:54,784 --> 01:19:57,455
You have no idea what to do at that point,

1393
01:19:57,455 --> 01:20:00,645
that the only good way is to sort of build up slowly.

1394
01:20:00,645 --> 01:20:02,935
So [NOISE] start with a very simple model,

1395
01:20:02,935 --> 01:20:04,300
get it to work,

1396
01:20:04,300 --> 01:20:05,820
add your bells and whistles,

1397
01:20:05,820 --> 01:20:07,490
extra layers and so on.

1398
01:20:07,490 --> 01:20:09,585
Get them to work or abandon them.

1399
01:20:09,585 --> 01:20:14,230
And so, try and proceed from one working model to another as much as possible.

1400
01:20:14,230 --> 01:20:18,975
One of- another way that you can start small and build up is with data.

1401
01:20:18,975 --> 01:20:22,580
The easiest way to see bugs and problems in your model,

1402
01:20:22,580 --> 01:20:25,610
is with the minutest possible amount of data.

1403
01:20:25,610 --> 01:20:29,035
So, start with a dataset of eight items.

1404
01:20:29,035 --> 01:20:32,660
Sometimes it's even best if those eight items are ones that are

1405
01:20:32,660 --> 01:20:34,925
artificial data that you designed yourself

1406
01:20:34,925 --> 01:20:37,565
because then you can often more easily see problems,

1407
01:20:37,565 --> 01:20:38,805
and what's going wrong.

1408
01:20:38,805 --> 01:20:40,560
So, you should train on that,

1409
01:20:40,560 --> 01:20:42,420
um, because it's only eight items,

1410
01:20:42,420 --> 01:20:44,119
training will only take seconds,

1411
01:20:44,119 --> 01:20:47,205
and that's really, really useful for being able to iterate quickly.

1412
01:20:47,205 --> 01:20:49,560
And you know, if you can't have your model get

1413
01:20:49,560 --> 01:20:55,055
100 percent accuracy on training and testing on those eight examples,

1414
01:20:55,055 --> 01:20:59,734
well, you know, either the model is woefully under powered or the model is broken,

1415
01:20:59,734 --> 01:21:02,900
and you've got clear things to do right there.

1416
01:21:02,900 --> 01:21:06,395
Um, when you go to a bigger model, um,

1417
01:21:06,395 --> 01:21:10,115
the standard practice with modern neural networks is,

1418
01:21:10,115 --> 01:21:12,330
you want to train your models.

1419
01:21:12,330 --> 01:21:16,240
You want models that can overfit massively on the training set.

1420
01:21:16,240 --> 01:21:19,565
So, in general, your models should still be getting

1421
01:21:19,565 --> 01:21:23,375
close to 100 percent accuracy on the training set after you've

1422
01:21:23,375 --> 01:21:27,160
trained it for a long time because powerful neural network models are

1423
01:21:27,160 --> 01:21:31,090
just really good at over-fitting to, and memorizing data.

1424
01:21:31,090 --> 01:21:33,455
Um, if that's not the case well, you know,

1425
01:21:33,455 --> 01:21:34,805
maybe you want a bigger model.

1426
01:21:34,805 --> 01:21:38,165
Maybe you want to have higher hidden dimensions or

1427
01:21:38,165 --> 01:21:41,910
add an extra layer to your neural network or something like that.

1428
01:21:41,910 --> 01:21:44,925
You shouldn't be scared of overfitting on the training data.

1429
01:21:44,925 --> 01:21:47,180
But once you've proved you can do that,

1430
01:21:47,180 --> 01:21:50,555
you then do want a model that also generalizes well.

1431
01:21:50,555 --> 01:21:55,415
And so, normally the way that you're addressing that is then by regularizing the model,

1432
01:21:55,415 --> 01:21:57,845
and there are different ways to regularize your model,

1433
01:21:57,845 --> 01:22:01,295
but we talked about in the assignment, doing dropout.

1434
01:22:01,295 --> 01:22:03,755
I mean, using generous dropout is

1435
01:22:03,755 --> 01:22:07,975
one very common and effective strategy for regularizing your models.

1436
01:22:07,975 --> 01:22:11,735
And so, then you've, what you want to be doing is regularizing

1437
01:22:11,735 --> 01:22:16,265
your model enough that the curve no longer looks like this,

1438
01:22:16,265 --> 01:22:21,095
but instead that your validation performance kind of levels out,

1439
01:22:21,095 --> 01:22:23,510
but doesn't start ramping back up again,

1440
01:22:23,510 --> 01:22:26,820
and that's then a sort of a sign of a well regularized model.

1441
01:22:26,820 --> 01:22:29,210
Okay. I will stop there,

1442
01:22:29,210 --> 01:22:33,310
and then we'll come back to the question-answering project on Thursday.

