1
00:00:04,280 --> 00:00:07,620
The plan for today is what I am gonna talk about

2
00:00:07,620 --> 00:00:10,710
is the topic of convolutional neural networks.

3
00:00:10,710 --> 00:00:13,920
So essentially, um, there's actually quite a lot of

4
00:00:13,920 --> 00:00:17,700
content in this lecture of different things that's good to know about,

5
00:00:17,700 --> 00:00:20,760
since essentially this is going to be learn about

6
00:00:20,760 --> 00:00:24,840
convolutional neural networks in one large bite for NLP.

7
00:00:24,840 --> 00:00:27,465
So, um, bit on announcements,

8
00:00:27,465 --> 00:00:30,945
explain the general idea of convolutional neural networks,

9
00:00:30,945 --> 00:00:33,270
and then for quite a bit of it,

10
00:00:33,270 --> 00:00:38,490
I want to go through in sort of some detail to particular papers that made

11
00:00:38,490 --> 00:00:40,950
use of convolutional neural networks for

12
00:00:40,950 --> 00:00:44,235
text classification, sentence classification tasks.

13
00:00:44,235 --> 00:00:47,040
Um, the first is a sort of a pretty simple,

14
00:00:47,040 --> 00:00:50,370
um, CNN that was done in 2014,

15
00:00:50,370 --> 00:00:52,365
and then the second one is a

16
00:00:52,365 --> 00:00:58,435
way more complex CNN that was done much more recently in 2017.

17
00:00:58,435 --> 00:01:01,275
Okay. But first, a couple of announcements.

18
00:01:01,275 --> 00:01:06,360
Um, firstly, the last reminder on the mid-quarter feedback survey.

19
00:01:06,360 --> 00:01:08,700
So tons of you have done the- this already.

20
00:01:08,700 --> 00:01:10,470
Thank you, thank you very much.

21
00:01:10,470 --> 00:01:14,340
Um, but if you'd still be putting it off till the very last minute, um,

22
00:01:14,340 --> 00:01:17,330
tonight at midnight is your last chance, um,

23
00:01:17,330 --> 00:01:20,045
to fill in the mid-quarter survey to get your,

24
00:01:20,045 --> 00:01:23,275
um, to give us feedback and to get your half-a-point.

25
00:01:23,275 --> 00:01:26,990
Um, okay. And then the other thing that you should be thinking about,

26
00:01:26,990 --> 00:01:29,510
and I know lots of you are thinking about

27
00:01:29,510 --> 00:01:32,495
since I spent three hours talking to people yesterday,

28
00:01:32,495 --> 00:01:35,195
um, is about final projects.

29
00:01:35,195 --> 00:01:39,060
Um, and so make sure you've got some plans from that, um,

30
00:01:39,060 --> 00:01:40,725
in place for, um,

31
00:01:40,725 --> 00:01:44,640
04:00 p.m, uh, 04:30 p.m. Thursday.

32
00:01:44,640 --> 00:01:47,535
I mean, in particular as we've discussed, um,

33
00:01:47,535 --> 00:01:52,745
your- part of what you're meant to do this year is to have found some research paper,

34
00:01:52,745 --> 00:01:57,730
have read it, and have a summary and thoughts as to how it can inform your work.

35
00:01:57,730 --> 00:02:01,550
Um, and then just make sure you have in your calendars, um,

36
00:02:01,550 --> 00:02:05,735
the final project poster session for CS224n,

37
00:02:05,735 --> 00:02:09,320
which is gonna be in the evening of Wednesday March 20th,

38
00:02:09,320 --> 00:02:12,480
and we're holding it at the Alumni Center.

39
00:02:12,640 --> 00:02:19,940
Okay. Um, one more sort of announcement or just general stuff to cogitate.

40
00:02:19,940 --> 00:02:23,060
Um, so we're now officially in the second half of the class.

41
00:02:23,060 --> 00:02:24,545
Congratulations.

42
00:02:24,545 --> 00:02:26,630
Um, and, you know,

43
00:02:26,630 --> 00:02:31,880
there's sort of still a few things that we want to teach you that are sort of basic,

44
00:02:31,880 --> 00:02:34,700
and actually convolutional neural networks is one of them.

45
00:02:34,700 --> 00:02:39,950
But, I mean, nevertheless in the second half of the class, I mean,

46
00:02:39,950 --> 00:02:44,480
things start to change and we're hoping to much more, um,

47
00:02:44,480 --> 00:02:49,970
prepare you for being real deep learning NLP researchers or practitioners.

48
00:02:49,970 --> 00:02:52,395
And so what does that mean concretely?

49
00:02:52,395 --> 00:02:55,745
Well, the lectures start to be less

50
00:02:55,745 --> 00:02:59,660
giving every detail of how to build a very basic thing,

51
00:02:59,660 --> 00:03:02,629
and more giving you some ideas

52
00:03:02,629 --> 00:03:05,880
to sort of some of the work that's been done in different areas.

53
00:03:05,880 --> 00:03:08,510
And so to the extent that there's something of interest or

54
00:03:08,510 --> 00:03:11,375
rele- relevant to a project or things like that.

55
00:03:11,375 --> 00:03:14,360
Um, the hope is that while you can take some initiative to

56
00:03:14,360 --> 00:03:17,915
find out more about some of the things that are being talked about.

57
00:03:17,915 --> 00:03:22,100
Um, also would really welcome any questions about things that people,

58
00:03:22,100 --> 00:03:24,440
um, would want to know more about.

59
00:03:24,440 --> 00:03:26,420
And the other thing that you should know about

60
00:03:26,420 --> 00:03:30,440
deep learning is that once we get past the fundamentals,

61
00:03:30,440 --> 00:03:33,350
a lot of the stuff we teach just isn't

62
00:03:33,350 --> 00:03:38,120
really known science or things that people are sure of that,

63
00:03:38,120 --> 00:03:41,870
you know, most of what I'm teaching in the second half of the course is pretty

64
00:03:41,870 --> 00:03:46,175
much what people think is good practice in 2019.

65
00:03:46,175 --> 00:03:49,370
But, you know, the fact of the matter is what people think is

66
00:03:49,370 --> 00:03:53,390
good practice in deep learning has been changing really rapidly.

67
00:03:53,390 --> 00:03:58,330
So if you go back even two years or definitely if you go back four years, right?

68
00:03:58,330 --> 00:04:01,640
There's just a lot of different things that people used to believe,

69
00:04:01,640 --> 00:04:04,850
and now people have some different ideas as to what works best.

70
00:04:04,850 --> 00:04:09,530
And it's perfectly clear that come 2021 or 2023,

71
00:04:09,530 --> 00:04:12,350
there will be some different ideas again as to what,

72
00:04:12,350 --> 00:04:14,090
um, people think is best.

73
00:04:14,090 --> 00:04:17,750
So you sort of just have to accept that this is, um,

74
00:04:17,750 --> 00:04:20,630
a nascent rapidly emerging field

75
00:04:20,630 --> 00:04:24,125
and it's good to understand the fundamentals and how things fit together.

76
00:04:24,125 --> 00:04:27,740
But after that, quite a bit of the knowledge is this is what people

77
00:04:27,740 --> 00:04:31,280
think is good at the moment and it keeps evolving over time.

78
00:04:31,280 --> 00:04:34,745
And if you want to stay in the field, or doing things with deep learning,

79
00:04:34,745 --> 00:04:37,505
you kind of still have to keep up with how it changes.

80
00:04:37,505 --> 00:04:39,710
It's called lifelong learning these days.

81
00:04:39,710 --> 00:04:41,810
It's a very trendy concept.

82
00:04:41,810 --> 00:04:45,200
Um, and so as well as the lectures,

83
00:04:45,200 --> 00:04:49,735
this is also true for the assignments.

84
00:04:49,735 --> 00:04:51,720
Um, and, you know,

85
00:04:51,720 --> 00:04:57,050
we've been trying to make the assignments so that they started off very introductory,

86
00:04:57,050 --> 00:05:01,340
and gradually started to use less scaffolding,

87
00:05:01,340 --> 00:05:03,395
and we're going to hope to, um,

88
00:05:03,395 --> 00:05:10,530
continue that, um, with the sort of less hand holding in assignment five.

89
00:05:10,530 --> 00:05:13,910
And, you know, I guess what we're hoping to do is prepare you

90
00:05:13,910 --> 00:05:17,495
both for the final project and for real life.

91
00:05:17,495 --> 00:05:21,005
I guess I was making an analogy this morning,

92
00:05:21,005 --> 00:05:25,370
um, comparing this to the sort of intro CS sequence,

93
00:05:25,370 --> 00:05:29,135
so when there's CS106A and B that have tons of scaffolding,

94
00:05:29,135 --> 00:05:31,025
and then in CS107,

95
00:05:31,025 --> 00:05:34,850
you're meant to learn how to diagnose and solve problems

96
00:05:34,850 --> 00:05:38,910
for yourself in a debugger that is kind of the same,

97
00:05:38,910 --> 00:05:41,010
um, for neural networks that, you know,

98
00:05:41,010 --> 00:05:43,770
for the early assignments, um, you know,

99
00:05:43,770 --> 00:05:46,610
we've given you every bit of handholding here, all of

100
00:05:46,610 --> 00:05:49,490
these tests to make sure every little bit of it is okay,

101
00:05:49,490 --> 00:05:51,815
and here's exactly how to structure things.

102
00:05:51,815 --> 00:05:54,305
But, you know, in the real world,

103
00:05:54,305 --> 00:05:57,695
um, you're only going to be able to build and use neural networks.

104
00:05:57,695 --> 00:06:00,259
If you can figure out why they're not working

105
00:06:00,259 --> 00:06:02,990
and what you have to change to make them work.

106
00:06:02,990 --> 00:06:06,785
And, you know, the truth is as I talked a bit about last week, you know,

107
00:06:06,785 --> 00:06:11,210
that's often well more than half of the job that it seems easy enough to stick down.

108
00:06:11,210 --> 00:06:14,270
Here's my neural net and the pieces that make sense to me,

109
00:06:14,270 --> 00:06:17,660
and then you can spend the remaining 80 percent of the time

110
00:06:17,660 --> 00:06:21,230
scratching your head wondering why it doesn't actually work well,

111
00:06:21,230 --> 00:06:24,335
and how you could change it to make it to work well.

112
00:06:24,335 --> 00:06:29,810
Um, so, um, I confess that debugging neural nets can often be hard, but, you know,

113
00:06:29,810 --> 00:06:34,190
the goal is that you should actually learn something about doing it,

114
00:06:34,190 --> 00:06:38,600
and that's kind of one of the learning goals of the course when it comes down to it.

115
00:06:38,600 --> 00:06:41,045
Um, final little advertisement.

116
00:06:41,045 --> 00:06:43,370
If you feel like you'd like to read a book,

117
00:06:43,370 --> 00:06:45,155
um, just out this week,

118
00:06:45,155 --> 00:06:48,305
there's a new book on natural language processing with PyTorch

119
00:06:48,305 --> 00:06:51,575
by Delip Rao and Brian McMahan.

120
00:06:51,575 --> 00:06:53,990
Delip actually lives in San Francisco.

121
00:06:53,990 --> 00:06:56,660
Um, so, um, if you want to,

122
00:06:56,660 --> 00:06:58,310
you can buy a copy of this, of course.

123
00:06:58,310 --> 00:07:00,230
But if you don't want to, um,

124
00:07:00,230 --> 00:07:03,230
buy it and you feel like having a bit of a look through it, um,

125
00:07:03,230 --> 00:07:09,110
the Stanford library is actually has a license to the O'Reilly's Safari Books collection.

126
00:07:09,110 --> 00:07:14,945
So you can start off at library.stanford.edu and read it for free.

127
00:07:14,945 --> 00:07:18,230
There's one catch to this which is the library only has

128
00:07:18,230 --> 00:07:21,710
16 simultaneous licenses to Safari Books.

129
00:07:21,710 --> 00:07:25,450
So if you'd also like your classmates to be able to read it for free,

130
00:07:25,450 --> 00:07:29,944
it really helps if you remember to log out of Safari Books Online,

131
00:07:29,944 --> 00:07:32,270
um, when you're done looking at it.

132
00:07:32,270 --> 00:07:34,785
Um, yes, so this is sort of a,

133
00:07:34,785 --> 00:07:36,420
I mean, in some sense,

134
00:07:36,420 --> 00:07:39,025
I hope you will feel if you look at this book,

135
00:07:39,025 --> 00:07:41,610
"Boy, I already know most of that stuff already.

136
00:07:41,610 --> 00:07:43,740
It's not a super advanced book.

137
00:07:43,740 --> 00:07:49,780
But it's a good well-written tutorial of how to do things with PyTorch and NLP."

138
00:07:49,780 --> 00:07:52,620
If you don't feel like you know most of the stuff in this book,

139
00:07:52,620 --> 00:07:56,250
you can let me know but I will be a little sad.

140
00:07:56,250 --> 00:08:01,030
Um, okay, um, yeah.

141
00:08:01,030 --> 00:08:03,760
So, let, so starting into today.

142
00:08:03,760 --> 00:08:06,430
Um, so, we spent a lot of time on

143
00:08:06,430 --> 00:08:10,630
recurrent neural networks and they are great for many things.

144
00:08:10,630 --> 00:08:15,670
Um, but there's sort of some things that they're not so good at.

145
00:08:15,670 --> 00:08:21,280
So, you know, we kind of might like to know about a phrase like my birth,

146
00:08:21,280 --> 00:08:23,800
or a bigger phrase like of my birth,

147
00:08:23,800 --> 00:08:27,550
and there's sort of no independent, um,

148
00:08:27,550 --> 00:08:31,480
representation of those spans in a recurrent neural network.

149
00:08:31,480 --> 00:08:35,365
We kind of get sort of prefixes of a whole sentence.

150
00:08:35,365 --> 00:08:38,815
And while we did, um, bidirectional, um,

151
00:08:38,815 --> 00:08:42,100
recurrent neural networks, and you could say, 'Well,

152
00:08:42,100 --> 00:08:45,670
wait a minute you could use it in both directions' and to some extent that's true.

153
00:08:45,670 --> 00:08:49,120
We can get stuff from this direction and stuff from this direction,

154
00:08:49,120 --> 00:08:51,265
but we still kind of have sort of

155
00:08:51,265 --> 00:08:54,730
whole sequences that go to one end of the sentence or another.

156
00:08:54,730 --> 00:08:57,790
We don't just have pieces of sentences.

157
00:08:57,790 --> 00:09:03,595
And often, we'd like to sort of work out meanings of pieces of sentences,

158
00:09:03,595 --> 00:09:06,280
and so, we sort of have two problems here.

159
00:09:06,280 --> 00:09:09,835
We only have sort of initial and final sub-sequences.

160
00:09:09,835 --> 00:09:14,230
And also, if you look at these representations, like if you say,

161
00:09:14,230 --> 00:09:18,820
take this last state as the representation of the meaning of this text.

162
00:09:18,820 --> 00:09:20,080
What you find out,

163
00:09:20,080 --> 00:09:22,360
is it's very dominated by the meaning of

164
00:09:22,360 --> 00:09:27,640
the most recent words and what they are trying to predict as to what comes after them,

165
00:09:27,640 --> 00:09:30,085
and that's part of the reason why I mentioned

166
00:09:30,085 --> 00:09:33,280
last time in the question answering, um, lecture,

167
00:09:33,280 --> 00:09:37,060
the idea that well you can do better by having a sentinel and training

168
00:09:37,060 --> 00:09:41,755
something that has attention over the whole, um, LSTM structure.

169
00:09:41,755 --> 00:09:44,560
Okay. But today we're going to look at

170
00:09:44,560 --> 00:09:48,565
a different alternative which is convolutional neural nets,

171
00:09:48,565 --> 00:09:53,485
which are often abbreviated as either CNN's or ConvNets.

172
00:09:53,485 --> 00:09:57,385
Um, and the idea of these is, well,

173
00:09:57,385 --> 00:09:59,920
look maybe we could just take

174
00:09:59,920 --> 00:10:06,910
every sub-sequence of a certain length and calculate a representation for it, um,

175
00:10:06,910 --> 00:10:10,090
so that, you know, if we have some piece of text like,

176
00:10:10,090 --> 00:10:12,684
tentative deal reached to keep government open,

177
00:10:12,684 --> 00:10:14,320
and we could sort of just say, well,

178
00:10:14,320 --> 00:10:17,110
let's just take all three words sequences,

179
00:10:17,110 --> 00:10:19,765
tentative deal reached, deal reached to,

180
00:10:19,765 --> 00:10:21,385
reached to keep et cetera,

181
00:10:21,385 --> 00:10:26,470
and we're going to calculate some kind of representation for each of those sequences.

182
00:10:26,470 --> 00:10:30,250
So, this is an- isn't a strongly linguistic idea.

183
00:10:30,250 --> 00:10:33,430
Right? We're not worrying about whether it's a coherent phrase,

184
00:10:33,430 --> 00:10:36,310
that's grammatical linguistically valid,

185
00:10:36,310 --> 00:10:41,125
cognitively plausible, we're just taking every sub-sequence of a certain length.

186
00:10:41,125 --> 00:10:45,370
And then, once we've calculated representations of those,

187
00:10:45,370 --> 00:10:48,025
we're going to look at how to group them.

188
00:10:48,025 --> 00:10:55,900
Okay. So, let's get into more detail as to what CNN's are and how they work.

189
00:10:55,900 --> 00:11:01,900
Um, yeah, so, there's this general idea of a convolution which you may or may

190
00:11:01,900 --> 00:11:07,855
not have seen in some math or electrical engineering class.

191
00:11:07,855 --> 00:11:12,010
And then, there's the particular version of convolutions,

192
00:11:12,010 --> 00:11:15,310
the discrete convolutions, which you can means that

193
00:11:15,310 --> 00:11:18,910
you can use the friendly summation symbol rather than an integral.

194
00:11:18,910 --> 00:11:20,905
Um, and that's a,

195
00:11:20,905 --> 00:11:22,480
that's a discrete convolution.

196
00:11:22,480 --> 00:11:25,495
I find that that notation as completely unhelpful.

197
00:11:25,495 --> 00:11:27,040
So, I won't even try and explain it.

198
00:11:27,040 --> 00:11:28,690
But I've got lots of examples,

199
00:11:28,690 --> 00:11:34,075
and convolutions are really easy for neural nets in terms of what they do for examples.

200
00:11:34,075 --> 00:11:38,590
All right, so the classic case of where convolutional neural networks are used,

201
00:11:38,590 --> 00:11:40,270
is in vision applications.

202
00:11:40,270 --> 00:11:44,605
So, if you do CS231N next quarter,

203
00:11:44,605 --> 00:11:47,770
essentially you know,  the first four weeks is just all doing

204
00:11:47,770 --> 00:11:51,715
convolutional neural networks in all their variants and glory.

205
00:11:51,715 --> 00:11:55,540
Um, and the sort of essential idea of, um,

206
00:11:55,540 --> 00:11:57,894
convolutions for a vision,

207
00:11:57,894 --> 00:12:02,410
is that you want to recognize things no matter where they appear in an image.

208
00:12:02,410 --> 00:12:05,620
So, you have a sort of property of translation and variance,

209
00:12:05,620 --> 00:12:08,230
and the idea of a convolution as a way

210
00:12:08,230 --> 00:12:10,810
of finding something in different places in the image,

211
00:12:10,810 --> 00:12:12,670
regardless of where it appears.

212
00:12:12,670 --> 00:12:19,360
Um, so this is the vision example which I stole from Andrew Ng's UFLDL website.

213
00:12:19,360 --> 00:12:21,925
And so, what a convolution is,

214
00:12:21,925 --> 00:12:24,130
is it's here a patch,

215
00:12:24,130 --> 00:12:26,815
but you can think of it as just as a vector,

216
00:12:26,815 --> 00:12:31,450
and the patch has weights which are these little numbers in red,

217
00:12:31,450 --> 00:12:32,995
and what you're gonna do,

218
00:12:32,995 --> 00:12:40,345
is slide that patch over the image as this as this animation does.

219
00:12:40,345 --> 00:12:43,075
Um, and so at each position,

220
00:12:43,075 --> 00:12:47,905
you're going to multiply each of the red numbers by the black number in that position,

221
00:12:47,905 --> 00:12:50,080
and then you're just going to sum them up.

222
00:12:50,080 --> 00:12:53,245
So, that's what a discrete convolution does,

223
00:12:53,245 --> 00:12:55,180
which is what that notation at the top is saying,

224
00:12:55,180 --> 00:12:58,495
right? You're multiplying things together and then you're summing them up,

225
00:12:58,495 --> 00:13:00,235
and so you're doing this,

226
00:13:00,235 --> 00:13:04,240
and then you're filling in the pink with the products,

227
00:13:04,240 --> 00:13:05,710
um, the sum products.

228
00:13:05,710 --> 00:13:07,855
So, it's sort of like, you're taking these sort of

229
00:13:07,855 --> 00:13:12,400
patch dot products and putting them into the pink matrix,

230
00:13:12,400 --> 00:13:14,815
and that's then your convolved feature.

231
00:13:14,815 --> 00:13:17,350
So, that's a 2D convolution,

232
00:13:17,350 --> 00:13:18,760
which for the rest of today,

233
00:13:18,760 --> 00:13:20,470
we're not going to look at anymore.

234
00:13:20,470 --> 00:13:23,215
So, this is all you're learning about vision.

235
00:13:23,215 --> 00:13:28,390
Um, and so we're now going to go back and look at 1D convolutions,

236
00:13:28,390 --> 00:13:32,995
which is what people use when they're using convolutional neural networks for text.

237
00:13:32,995 --> 00:13:36,610
So, the starting point of a convolutional neural network for text,

238
00:13:36,610 --> 00:13:38,410
is we have an input.

239
00:13:38,410 --> 00:13:42,190
So, here's my sentence and for each word

240
00:13:42,190 --> 00:13:45,970
in the sentence I have here got a dense word vector,

241
00:13:45,970 --> 00:13:51,325
I made it a 4D, want to keep it small in my example but usually as you know, it's more.

242
00:13:51,325 --> 00:13:54,580
So, our starting point is we have some input, you know,

243
00:13:54,580 --> 00:13:58,060
input could just be a one-hot encoding that's not forbidden here,

244
00:13:58,060 --> 00:14:01,795
but normally we'll have these kind of dense word vectors.

245
00:14:01,795 --> 00:14:06,310
And so, then it's sort of the same as the 3D as the 2D one,

246
00:14:06,310 --> 00:14:08,185
apart from we've only got one dimension.

247
00:14:08,185 --> 00:14:10,510
So, we have a filter.

248
00:14:10,510 --> 00:14:14,410
Um, so here is our filter,

249
00:14:14,410 --> 00:14:21,680
and so our filter is gonna do three steps and time, three words.

250
00:14:21,750 --> 00:14:25,930
And that's going to work across the dimensions.

251
00:14:25,930 --> 00:14:28,240
So, these different dimensions in

252
00:14:28,240 --> 00:14:32,500
the convolutional neural network often get referred to as channels.

253
00:14:32,500 --> 00:14:35,665
So, we're kind of working across the input channels,

254
00:14:35,665 --> 00:14:37,990
and so we have a patch like this.

255
00:14:37,990 --> 00:14:45,430
And we're going to take this patch and put it on top of the first three words.

256
00:14:45,430 --> 00:14:47,980
I don't have as good an animation as the previous slide.

257
00:14:47,980 --> 00:14:51,610
Sorry. And we're going to work out the dot product,

258
00:14:51,610 --> 00:14:56,410
um, between those, and I did that at home by putting this into Excel.

259
00:14:56,410 --> 00:14:58,015
And the answer [LAUGHTER] to that,

260
00:14:58,015 --> 00:15:01,255
is that the product is minus 1,0.

261
00:15:01,255 --> 00:15:05,495
And then at that point, we slide our,

262
00:15:05,495 --> 00:15:08,345
We slide this, um,

263
00:15:08,345 --> 00:15:11,410
matrix which gets referred to as a kernel or

264
00:15:11,410 --> 00:15:16,305
a filter which is the patch that we're using for our convolutional neural network.

265
00:15:16,305 --> 00:15:21,520
We slide it down one and do the dot product of those terms again.

266
00:15:21,520 --> 00:15:28,955
And that comes out as minus a half and we keep on sliding that down and we get what,

267
00:15:28,955 --> 00:15:33,095
um, gets what's shown on the right as our output.

268
00:15:33,095 --> 00:15:34,265
So at this point,

269
00:15:34,265 --> 00:15:36,690
we've just reduced the sentence,

270
00:15:36,690 --> 00:15:39,105
um, to a single vector.

271
00:15:39,105 --> 00:15:44,740
Um, and that seems like we might want to do more than that.

272
00:15:44,740 --> 00:15:48,455
Um, but the other thing that you will have noticed is that

273
00:15:48,455 --> 00:15:52,500
our sentence is sort of shrunk because before, you know,

274
00:15:52,500 --> 00:15:57,710
we had a seven word sentence but because I've just sort of slid this three word,

275
00:15:57,710 --> 00:15:59,615
um, kernel down here,

276
00:15:59,615 --> 00:16:03,015
I ended up with only five positions to put it in.

277
00:16:03,015 --> 00:16:05,825
So it's become a five word thing.

278
00:16:05,825 --> 00:16:08,960
Um, so to first of all address that problem,

279
00:16:08,960 --> 00:16:14,030
commonly when people do convolutional neural networks, they add padding.

280
00:16:14,030 --> 00:16:18,790
Um, so what I can do is I can add zero padding at

281
00:16:18,790 --> 00:16:25,805
both ends and then sort of do the same trick and say run a convolution on that.

282
00:16:25,805 --> 00:16:31,355
And now, I'll be able to put my size three filter in seven different places as I

283
00:16:31,355 --> 00:16:37,835
slide it down and so I'm getting out a vector that's the same length of my input.

284
00:16:37,835 --> 00:16:40,650
Um, that, you know, there are different way,

285
00:16:40,650 --> 00:16:43,200
so this is the most common way of doing things.

286
00:16:43,200 --> 00:16:46,765
And it's kind of seems logical because it maintains size.

287
00:16:46,765 --> 00:16:50,460
I mean, you know, there's always more than one way to do it.

288
00:16:50,460 --> 00:16:52,310
Um, if you really wanted to,

289
00:16:52,310 --> 00:16:54,390
you, oops, I don't want you, yeah,

290
00:16:54,390 --> 00:16:59,560
there, oops, I made, uh,

291
00:16:59,560 --> 00:17:05,855
I made a slight mistake on my slide because this

292
00:17:05,855 --> 00:17:08,405
turns out which I was about to get to in a minute

293
00:17:08,405 --> 00:17:12,790
but I'll just explain this bit here anyway [LAUGHTER].

294
00:17:12,790 --> 00:17:15,450
Um, you know, if you wanted to,

295
00:17:15,450 --> 00:17:19,740
you could have two steps of padding on both ends here.

296
00:17:19,740 --> 00:17:24,290
So that your first convolution we'll be looking at zero, zero,

297
00:17:24,290 --> 00:17:30,585
10 to the of and then the convolution would actually grow the size of your input.

298
00:17:30,585 --> 00:17:35,915
Yeah. But, yes. So I mean,

299
00:17:35,915 --> 00:17:38,565
so what we've done so far,

300
00:17:38,565 --> 00:17:41,380
we've started with these word vectors which in

301
00:17:41,380 --> 00:17:46,335
convolutional neural networks terms were of length four.

302
00:17:46,335 --> 00:17:49,475
So our kind of input had four channels.

303
00:17:49,475 --> 00:17:53,030
But when we were back here, um,

304
00:17:53,030 --> 00:17:56,515
we were just producing from this, um,

305
00:17:56,515 --> 00:17:59,690
kernel, one column of output.

306
00:17:59,690 --> 00:18:02,560
So our output has only a single channel.

307
00:18:02,560 --> 00:18:08,690
So we've sort of shrunk things in the columns direction from four to one.

308
00:18:08,690 --> 00:18:11,490
And that might seem bad.

309
00:18:11,490 --> 00:18:14,105
And for many purposes, it is bad.

310
00:18:14,105 --> 00:18:16,714
Um, and so, a lot of the time,

311
00:18:16,714 --> 00:18:21,164
what you want to do is to say,

312
00:18:21,164 --> 00:18:25,325
well, rather than have only one filter,

313
00:18:25,325 --> 00:18:29,260
instead of that, why don't I have several filters?

314
00:18:29,260 --> 00:18:32,680
So here I've got three different filters and each of

315
00:18:32,680 --> 00:18:36,620
these filters is just sort of the same size three,

316
00:18:36,620 --> 00:18:41,825
three the size, the kernel size times the input,

317
00:18:41,825 --> 00:18:46,145
number of channels for the matrix.

318
00:18:46,145 --> 00:18:49,550
So I have three different filters and I'm going to run

319
00:18:49,550 --> 00:18:53,380
each one down the text and get a column here.

320
00:18:53,380 --> 00:18:56,510
So now, I'm ending up with three columns of output.

321
00:18:56,510 --> 00:18:59,675
And so I have this sort of a three channel output.

322
00:18:59,675 --> 00:19:04,940
And the way to intuitively think of this is for these filters,

323
00:19:04,940 --> 00:19:07,505
well, you know, for what we do in neural networks,

324
00:19:07,505 --> 00:19:11,040
we're going to learn them by backpropagation like everything else.

325
00:19:11,040 --> 00:19:16,760
But our hope is that these filters could somehow specialize in different things.

326
00:19:16,760 --> 00:19:20,480
So maybe this filter could specialize on,

327
00:19:20,480 --> 00:19:22,355
is this language polite?

328
00:19:22,355 --> 00:19:26,725
And it will produce a high value whenever it sees polite words.

329
00:19:26,725 --> 00:19:29,850
And maybe, um, this, um,

330
00:19:29,850 --> 00:19:35,605
filter could specialize on, I don't know,

331
00:19:35,605 --> 00:19:38,795
eating and it will have a high value whenever it sees words

332
00:19:38,795 --> 00:19:42,430
about food and you know this filter will do a third thing.

333
00:19:42,430 --> 00:19:49,235
And so that's the sense in which people sometimes talk about, um, the, um,

334
00:19:49,235 --> 00:19:53,075
what you're getting is output of different features because your hope is that

335
00:19:53,075 --> 00:19:57,515
you'll kind of gain different latent features coming out of the text.

336
00:19:57,515 --> 00:20:02,555
Okay. So that gives us a representation and that's sort of

337
00:20:02,555 --> 00:20:07,540
a useful sort of having found learn features in our text.

338
00:20:07,540 --> 00:20:11,290
That quite often though, what we'll want to do is just

339
00:20:11,290 --> 00:20:15,610
summarize the text with re- with respect to those features.

340
00:20:15,610 --> 00:20:18,030
So you might just have the question of, well,

341
00:20:18,030 --> 00:20:20,045
in this piece of text, um,

342
00:20:20,045 --> 00:20:23,435
is it polite and does it talk about food?

343
00:20:23,435 --> 00:20:26,555
So another operation that we'll quite often

344
00:20:26,555 --> 00:20:30,410
do is wanna summarize the output of a convolutional network.

345
00:20:30,410 --> 00:20:32,750
And the simplest way to do that,

346
00:20:32,750 --> 00:20:35,110
is for 1D convolutions,

347
00:20:35,110 --> 00:20:37,635
is called max pooling over time.

348
00:20:37,635 --> 00:20:40,080
So if we max pool over time,

349
00:20:40,080 --> 00:20:43,935
that each of the channels or otherwise known as features,

350
00:20:43,935 --> 00:20:53,865
we're just simply going to look down and see what is its maximum value, 0,3, 1,6, 1,4.

351
00:20:53,865 --> 00:20:55,775
Um, and so, you know,

352
00:20:55,775 --> 00:20:58,730
if I use my story about the first two, um,

353
00:20:58,730 --> 00:21:00,700
filters, it's sort of saying, well,

354
00:21:00,700 --> 00:21:04,600
it's not very polite text but it's really about food, right?

355
00:21:04,600 --> 00:21:06,300
That we're sort of summarizing,

356
00:21:06,300 --> 00:21:08,460
um, what we've detected there.

357
00:21:08,460 --> 00:21:14,400
Um, so the concept of max pooling in some sense captures,

358
00:21:14,400 --> 00:21:18,640
does, is this thing being activated anywhere, right?

359
00:21:18,640 --> 00:21:22,180
So if we have things like politeness and about food,

360
00:21:22,180 --> 00:21:25,510
that the output of max pooling will have a high value.

361
00:21:25,510 --> 00:21:28,600
If somewhere in the sentence there was a clear marker of

362
00:21:28,600 --> 00:21:32,035
politeness or something clearly about food.

363
00:21:32,035 --> 00:21:37,210
And that's often a useful notion because often what you want to know is,

364
00:21:37,210 --> 00:21:42,260
you know, is there some discussion of food in this sentence or is there not?

365
00:21:42,260 --> 00:21:46,150
There's another thing, there are other things that you could do.

366
00:21:46,150 --> 00:21:48,635
Instead of, ah, max pooling,

367
00:21:48,635 --> 00:21:51,210
you can instead do average pooling.

368
00:21:51,210 --> 00:21:55,405
So here you just take these numbers and find the average of them.

369
00:21:55,405 --> 00:21:58,910
That then has the different semantics which is sort of

370
00:21:58,910 --> 00:22:02,595
what's the average amount of politeness of this, um,

371
00:22:02,595 --> 00:22:05,855
text or on average how much, you know, how,

372
00:22:05,855 --> 00:22:10,265
what percent of the sentence is about food or something like that.

373
00:22:10,265 --> 00:22:12,190
Um, for some purposes,

374
00:22:12,190 --> 00:22:13,680
this is better because, you know,

375
00:22:13,680 --> 00:22:16,960
it takes in all of the important builds to an average.

376
00:22:16,960 --> 00:22:18,900
I mean, a lot of the time,

377
00:22:18,900 --> 00:22:22,890
people have found that actually max pooling is better because,

378
00:22:22,890 --> 00:22:27,490
you know, a lot of signals in natural language are sparse.

379
00:22:27,490 --> 00:22:30,630
You know, no matter how polite you are trying to be,

380
00:22:30,630 --> 00:22:32,940
you're not going to be being polite in every word.

381
00:22:32,940 --> 00:22:37,430
You're going to say nouns and articles like that and a,

382
00:22:37,430 --> 00:22:40,390
and prepositions and conjunctions,

383
00:22:40,390 --> 00:22:42,635
none of which are inherently polite, right?

384
00:22:42,635 --> 00:22:46,325
Um, so that if there's some politeness showing up prominently,

385
00:22:46,325 --> 00:22:51,470
then the sentence becomes polite and max pooling is actually better for capturing that.

386
00:22:51,470 --> 00:22:54,430
Um, of course the one other kind of thing that you can do as

387
00:22:54,430 --> 00:22:58,115
min pooling and find the least [LAUGHTER] active thing.

388
00:22:58,115 --> 00:23:01,135
Um, it doesn't get used much but you could do that as well.

389
00:23:01,135 --> 00:23:04,380
Okay. So, um, so if you're in PyTorch,

390
00:23:04,380 --> 00:23:07,365
this is all pretty easy stuff to do.

391
00:23:07,365 --> 00:23:10,005
So there's a handy dandy Conv1d.

392
00:23:10,005 --> 00:23:13,025
There's also a Conv2d as you might guess for vision.

393
00:23:13,025 --> 00:23:15,005
But there's a Conv1d, um,

394
00:23:15,005 --> 00:23:18,790
where you're specifying how many input channels there are.

395
00:23:18,790 --> 00:23:20,725
That was our word embedding size.

396
00:23:20,725 --> 00:23:22,735
How many output channels there are?

397
00:23:22,735 --> 00:23:24,365
We have three.

398
00:23:24,365 --> 00:23:27,820
What the size of the convolutional kernel is?

399
00:23:27,820 --> 00:23:29,525
So the ones that we were showing were also

400
00:23:29,525 --> 00:23:32,380
three and then there are various other parameters you can have.

401
00:23:32,380 --> 00:23:35,990
Like you can say that you want a padding of one and things like that.

402
00:23:35,990 --> 00:23:38,075
And then once you've got one of those,

403
00:23:38,075 --> 00:23:39,695
you can just sort of run

404
00:23:39,695 --> 00:23:44,355
your convolutional filter on the input to get a new hidden state.

405
00:23:44,355 --> 00:23:46,220
And then if you wanna max pool,

406
00:23:46,220 --> 00:23:47,570
you can just max,

407
00:23:47,570 --> 00:23:51,750
um, through the output of that and then you've got a max pooled output.

408
00:23:51,750 --> 00:23:58,869
Okay. So that gives us the basics of building a kind of a convolutional neural network,

409
00:23:58,869 --> 00:24:01,150
um, for, um, NLP.

410
00:24:01,150 --> 00:24:04,280
Does that sort of makes sense up until there?

411
00:24:06,000 --> 00:24:10,570
Yeah. Okay. So next bit is to sort of show

412
00:24:10,570 --> 00:24:15,265
you three or four other things that you can do.

413
00:24:15,265 --> 00:24:18,325
Um, I started off typing these slides

414
00:24:18,325 --> 00:24:20,920
other less useful notions because I

415
00:24:20,920 --> 00:24:23,590
kinda thought, oh, at least they don't really come up much in NLP.

416
00:24:23,590 --> 00:24:28,090
But, you know, actually it turned out when I got on to that second paper,

417
00:24:28,090 --> 00:24:32,740
when I say the complex convolutional neural network, actually,

418
00:24:32,740 --> 00:24:37,750
in that paper they try out just about all of these things that I say no one uses.

419
00:24:37,750 --> 00:24:42,145
So it's sort of good to know what they are for looking at various papers.

420
00:24:42,145 --> 00:24:49,795
So here, when we did things so far then we were calculating these convolutions,

421
00:24:49,795 --> 00:24:52,660
that we're sort of trying them out at every position.

422
00:24:52,660 --> 00:24:55,285
So we had one for zero, tentative deal.

423
00:24:55,285 --> 00:24:58,420
Then for tentative deal reached then deal reached to.

424
00:24:58,420 --> 00:25:00,970
And so we were just walking down one step at

425
00:25:00,970 --> 00:25:04,765
a time which is referred to as a stride as, of one.

426
00:25:04,765 --> 00:25:08,095
And that's by far the most common thing to do.

427
00:25:08,095 --> 00:25:09,595
But you could observe,

428
00:25:09,595 --> 00:25:10,825
look wait a minute,

429
00:25:10,825 --> 00:25:15,655
since the first convolution concerns zero tentative deal.

430
00:25:15,655 --> 00:25:18,085
I've got all those three words in there.

431
00:25:18,085 --> 00:25:25,225
Even if I skip down to a next did, deal reach to and then I did to keep government,

432
00:25:25,225 --> 00:25:30,460
I'd still have in one or other of the convolutions every word of the sentence

433
00:25:30,460 --> 00:25:32,950
so I can do half as much computation and I've

434
00:25:32,950 --> 00:25:35,635
still got everything in there in some sense.

435
00:25:35,635 --> 00:25:38,425
And so that's referred to as using a stride of two.

436
00:25:38,425 --> 00:25:42,130
And so then I get something with half as many rows out.

437
00:25:42,130 --> 00:25:46,840
So it's one way to sort of compactify your representation and produce

438
00:25:46,840 --> 00:25:52,855
something shorter from a longer sentence and we'll see that use of it coming up later.

439
00:25:52,855 --> 00:25:59,890
There's other ways to compactify what cut representation that comes out of your sentence.

440
00:25:59,890 --> 00:26:05,710
And so there's a different notion of pooling which is local pooling.

441
00:26:05,710 --> 00:26:09,640
Now, if if you've seen any of

442
00:26:09,640 --> 00:26:13,510
the vision world when people talk about max pooling and vision,

443
00:26:13,510 --> 00:26:16,960
they normally mean local pooling as opposed to

444
00:26:16,960 --> 00:26:21,400
the max pooling through time that I showed you first.

445
00:26:21,400 --> 00:26:27,070
So here we're sort of back to where we started and we've done

446
00:26:27,070 --> 00:26:33,535
our size three stride one convolution which is producing output as before.

447
00:26:33,535 --> 00:26:39,310
But now, what I'm gonna do is local pool with a stride of two.

448
00:26:39,310 --> 00:26:44,650
Which means I'm gonna take each two rows and I'm gonna pool them together into

449
00:26:44,650 --> 00:26:47,110
one row and I could do that again by

450
00:26:47,110 --> 00:26:50,680
either maxing or averaging or whatever appeals to me.

451
00:26:50,680 --> 00:26:53,200
So I take the first two rows,

452
00:26:53,200 --> 00:26:54,970
I max pool them I get this.

453
00:26:54,970 --> 00:26:56,800
I take the next two rows,

454
00:26:56,800 --> 00:26:58,555
I max pool them I get this.

455
00:26:58,555 --> 00:27:01,420
Next two, next two and I sort of pad it

456
00:27:01,420 --> 00:27:04,285
on the bottom so I have two rows at the bottom.

457
00:27:04,285 --> 00:27:09,415
And so that's then give me a local max pooling of a stride of two.

458
00:27:09,415 --> 00:27:13,300
And that sort of had exactly the same effect in the sense but

459
00:27:13,300 --> 00:27:16,990
with a different result as using a stride of two in

460
00:27:16,990 --> 00:27:20,530
my convolution because I have again reduced it to

461
00:27:20,530 --> 00:27:26,090
something of four rows that used to be eight rows.

462
00:27:26,970 --> 00:27:29,935
Yeah, picture that.

463
00:27:29,935 --> 00:27:33,640
Okay so that's that one.

464
00:27:33,640 --> 00:27:35,410
What else can you do.

465
00:27:35,410 --> 00:27:38,080
There are more things you can do to make it complex.

466
00:27:38,080 --> 00:27:43,765
Another thing that people have sometimes done is k-max pooling.

467
00:27:43,765 --> 00:27:49,510
And so this is a more complex thing and it's sort of saying well,

468
00:27:49,510 --> 00:27:53,530
rather than just keeping the max over time,

469
00:27:53,530 --> 00:28:00,325
if a feature is being kind of activated two or three times in the sentence,

470
00:28:00,325 --> 00:28:03,640
maybe it'd be good to record all the times that it's

471
00:28:03,640 --> 00:28:07,375
activated in the sentence while throwing away the rest.

472
00:28:07,375 --> 00:28:09,070
So in k-max pooling,

473
00:28:09,070 --> 00:28:10,870
and I'm doing two max here,

474
00:28:10,870 --> 00:28:17,335
you look down this column and you find the two highest values for that column.

475
00:28:17,335 --> 00:28:23,665
But then you put the two highest values not in the order of highest to lowest,

476
00:28:23,665 --> 00:28:26,620
but in the order in which they are in these columns.

477
00:28:26,620 --> 00:28:28,840
So it's minus 0,2,

478
00:28:28,840 --> 00:28:32,230
0,3 for this one and it's 1,6,

479
00:28:32,230 --> 00:28:38,065
0,6 for this one because it reflects the orders of the columns up above.

480
00:28:38,065 --> 00:28:43,210
Okay. Almost done, one more concept.

481
00:28:43,210 --> 00:28:52,285
This is another way of compressing data which is a dilated convolution.

482
00:28:52,285 --> 00:28:55,315
So if you have a dilated convolution,

483
00:28:55,315 --> 00:29:01,870
so dilated convolution doing it over here doesn't really make sense but where you can use

484
00:29:01,870 --> 00:29:08,440
a dilated convolution is if I take this and put it through another convolutional layer,

485
00:29:08,440 --> 00:29:13,540
we can kind of have deep convolutional networks that have multiple convolutional layers.

486
00:29:13,540 --> 00:29:20,560
So the idea of a dilated convolution issue is you're gonna skip some of the rows.

487
00:29:20,560 --> 00:29:24,295
So if you use a dilation of two starting at the top,

488
00:29:24,295 --> 00:29:27,460
you're going to take the first, third,

489
00:29:27,460 --> 00:29:31,870
and the fifth row and multiply them by my fil- sorry,

490
00:29:31,870 --> 00:29:32,980
I have different filters.

491
00:29:32,980 --> 00:29:38,305
Multiply them by my filters and then get the values that appear here.

492
00:29:38,305 --> 00:29:40,480
And then if stride as one,

493
00:29:40,480 --> 00:29:46,900
you'd then use, you would go on and sort of do the next spread out rows.

494
00:29:46,900 --> 00:29:51,025
And so this allows you to have convolutions that see

495
00:29:51,025 --> 00:29:56,680
a bigger spread of the sentence without having many parameters.

496
00:29:56,680 --> 00:29:59,065
So you don't have to do things this way.

497
00:29:59,065 --> 00:30:00,670
You could have said, look,

498
00:30:00,670 --> 00:30:07,015
I could just instead have convolutions with a kernel size of five.

499
00:30:07,015 --> 00:30:08,470
And then they'd say five,

500
00:30:08,470 --> 00:30:11,500
see five words in a row but then I'd be having

501
00:30:11,500 --> 00:30:17,230
sort of bigger matrices to specify my feature.

502
00:30:17,230 --> 00:30:20,770
Whereas, this way I can keep the matrices small but still

503
00:30:20,770 --> 00:30:25,105
see a bigger range of the sentence in one operation.

504
00:30:25,105 --> 00:30:30,670
Yeah and that concept of how much of a sentence you

505
00:30:30,670 --> 00:30:36,490
see is kind of an important notion in convolutional neural networks.

506
00:30:36,490 --> 00:30:39,940
Because, you know, if you start at the beginning of a sentence

507
00:30:39,940 --> 00:30:43,780
and you're just running three-by-three convolutions, um,

508
00:30:43,780 --> 00:30:47,995
you're sort of seeing these three word patches of the sentence.

509
00:30:47,995 --> 00:30:50,350
And it turns out in natural language that's

510
00:30:50,350 --> 00:30:53,305
already actually quite a useful representation.

511
00:30:53,305 --> 00:30:56,920
Because sort of having those kind of n-grams as features is

512
00:30:56,920 --> 00:31:01,165
just good for many purposes including text classification.

513
00:31:01,165 --> 00:31:05,680
But if you want to sort of understand more of the semantics of a sentence,

514
00:31:05,680 --> 00:31:08,575
somehow you wanna see more of that at once.

515
00:31:08,575 --> 00:31:13,780
And you've sort of got several tools you can use to see more of it once,

516
00:31:13,780 --> 00:31:15,730
you can use bigger filters,

517
00:31:15,730 --> 00:31:16,870
you could use, uh,

518
00:31:16,870 --> 00:31:18,460
kernel size five, seven,

519
00:31:18,460 --> 00:31:20,650
nine or something convolution.

520
00:31:20,650 --> 00:31:25,585
You could do something like dilated convolution so you can see spread out pictures.

521
00:31:25,585 --> 00:31:28,120
And the third thing that you can do is you

522
00:31:28,120 --> 00:31:30,835
can have depth of a convolutional neural network.

523
00:31:30,835 --> 00:31:35,605
Because as you have greater depth of a convolutional neural network, you see more.

524
00:31:35,605 --> 00:31:37,690
So at this first layer,

525
00:31:37,690 --> 00:31:43,150
the rows now have sort of info about three words in them.

526
00:31:43,150 --> 00:31:46,630
And if you sort of just stuck a second layer of

527
00:31:46,630 --> 00:31:48,280
convolutional neural network with

528
00:31:48,280 --> 00:31:51,670
the same general nature on top of it and you sort of take

529
00:31:51,670 --> 00:31:55,450
the first three rows and convolve it again then and

530
00:31:55,450 --> 00:32:00,940
then the next ones that those then know about five words of your original input sentence.

531
00:32:00,940 --> 00:32:03,700
So as you kind of have a deeper ConvNet stack you

532
00:32:03,700 --> 00:32:07,495
start to know about bigger and bigger patches of the sentence.

533
00:32:07,495 --> 00:32:09,970
Okay. All good?

534
00:32:09,970 --> 00:32:12,530
Any questions?

535
00:32:14,760 --> 00:32:22,899
No, that's good, okay. So, um, the next piece is essentially shows you this stuff again,

536
00:32:22,899 --> 00:32:26,560
um, in the context of a particular paper.

537
00:32:26,560 --> 00:32:27,850
So this was, um,

538
00:32:27,850 --> 00:32:32,125
a paper by Yoon Kim who was a Harvard student,

539
00:32:32,125 --> 00:32:36,460
maybe still is a Harvard student, um, in 2014.

540
00:32:36,460 --> 00:32:39,790
So this was sort of a fairly early paper.

541
00:32:39,790 --> 00:32:45,520
Um, and he wanted to show that you could use convolutional neural networks to do

542
00:32:45,520 --> 00:32:47,500
a good job for doing

543
00:32:47,500 --> 00:32:52,240
text classification when what you want to classify is a single sentence.

544
00:32:52,240 --> 00:32:55,750
So, the kind of thing you might want to do is look at the kind of

545
00:32:55,750 --> 00:33:00,400
snippets of movie reviews that you see on the Rotten Tomatoes site and say,

546
00:33:00,400 --> 00:33:04,900
"Is this a positive or is this a negative sentence description?"

547
00:33:04,900 --> 00:33:08,155
And the model he built is actually kind of similar

548
00:33:08,155 --> 00:33:11,695
to the convolutional neural networks that Collobert and Weston,

549
00:33:11,695 --> 00:33:14,980
um, introduced in their 2011 paper that we

550
00:33:14,980 --> 00:33:18,100
mentioned before when we were talking about window-based classifiers.

551
00:33:18,100 --> 00:33:20,500
So, in their paper they actually use

552
00:33:20,500 --> 00:33:25,600
both window-based classifiers and the convolutional classifier.

553
00:33:25,600 --> 00:33:28,570
Okay. Um, so yeah,

554
00:33:28,570 --> 00:33:29,800
I sort of already said this.

555
00:33:29,800 --> 00:33:34,210
So their tasks are sentence classification, could be sentiment.

556
00:33:34,210 --> 00:33:35,875
It could be other things like,

557
00:33:35,875 --> 00:33:39,100
is this sentence subjective or objective?

558
00:33:39,100 --> 00:33:42,040
So objective is what the main news articles are meant

559
00:33:42,040 --> 00:33:45,295
to be and subjective is what the opinion pieces are meant to be.

560
00:33:45,295 --> 00:33:48,970
Um, and then other things like question classification.

561
00:33:48,970 --> 00:33:51,220
Is this a question asking about a person,

562
00:33:51,220 --> 00:33:53,200
location, number, or whatever?

563
00:33:53,200 --> 00:33:57,400
Okay, so here is what he did.

564
00:33:57,400 --> 00:34:01,495
And it's sort of the- these slides sort of, um,

565
00:34:01,495 --> 00:34:06,880
use the notation of his paper which is sort of a little bit different the

566
00:34:06,880 --> 00:34:09,310
way the math gets written down to what I just showed

567
00:34:09,310 --> 00:34:12,160
you, that it's really doing exactly the same thing.

568
00:34:12,160 --> 00:34:16,930
So we start with word vectors of length k. Um,

569
00:34:16,930 --> 00:34:24,610
the sentence is made by just concatenating all of those word vectors together and then,

570
00:34:24,610 --> 00:34:27,280
when we- so we have a range of words,

571
00:34:27,280 --> 00:34:30,190
it's a subpart of that sentence vector.

572
00:34:30,190 --> 00:34:36,310
And so, the convolutional filter is just being represented as a vector because

573
00:34:36,310 --> 00:34:42,100
here he's flattened everything out into one long vector for the entire sentence,

574
00:34:42,100 --> 00:34:44,515
whereas I'd sort of stepped into a matrix.

575
00:34:44,515 --> 00:34:51,070
Um, so a size three convolution is just a real vector of length hk,

576
00:34:51,070 --> 00:34:56,350
the size of the convolutional filter times the dimensionality of the words.

577
00:34:56,350 --> 00:35:01,210
Um, and so, what he's gonna do to build

578
00:35:01,210 --> 00:35:07,450
his text classifier is use convolutions made out of different sizes.

579
00:35:07,450 --> 00:35:10,765
So you can have size two convolutions,

580
00:35:10,765 --> 00:35:16,000
size three convolutions as shown here, and bigger convolutions.

581
00:35:16,000 --> 00:35:23,140
And so, um, so to compute a feature one channel for our CNN, we're

582
00:35:23,140 --> 00:35:26,620
then doing a dot product between the weight vector of

583
00:35:26,620 --> 00:35:30,415
the feature times this sub-sequence of the same terms,

584
00:35:30,415 --> 00:35:35,035
and he sort of also put in a bias which I sort of omitted.

585
00:35:35,035 --> 00:35:41,110
Um, and then putting it through a non-linearity,

586
00:35:41,110 --> 00:35:43,390
um, which I wasn't doing either.

587
00:35:43,390 --> 00:35:46,045
Um, but as sort of we've seen a ton of.

588
00:35:46,045 --> 00:35:49,810
Um, and so, what we're wanting to do is that's our,

589
00:35:49,810 --> 00:35:53,410
um, feature and we want to, um,

590
00:35:53,410 --> 00:35:58,150
do it through all this- for a feature of kernel size three,

591
00:35:58,150 --> 00:36:00,880
we're gonna go all the way through the sentence.

592
00:36:00,880 --> 00:36:04,735
The other thing he did though was slightly funnel funny is,

593
00:36:04,735 --> 00:36:08,920
his windows were sort of lopsided in the notation, right.

594
00:36:08,920 --> 00:36:11,695
There's a word and th- the,

595
00:36:11,695 --> 00:36:15,355
um, h minus 1 words to the right of it.

596
00:36:15,355 --> 00:36:20,095
So he has padding here just on the right end whereas

597
00:36:20,095 --> 00:36:25,810
most people do their convolutions symmetrically in both directions around things.

598
00:36:25,810 --> 00:36:31,630
Okay. And so, we're going to do that for a bunch of features or

599
00:36:31,630 --> 00:36:34,480
channels Ci and therefore compute

600
00:36:34,480 --> 00:36:38,680
our convolved representations just as we've talked about.

601
00:36:38,680 --> 00:36:43,435
Okay. Um, then he does just what we talked about.

602
00:36:43,435 --> 00:36:48,370
Um, there's max over time pooling in the pooling layer to capture

603
00:36:48,370 --> 00:36:53,650
the most relevant things and is giving us a single number for each channel.

604
00:36:53,650 --> 00:37:01,465
Um, and we have features that look at different that have different kernel sizes.

605
00:37:01,465 --> 00:37:08,230
Um, here's one other idea he used which is possibly a neat idea.

606
00:37:08,230 --> 00:37:13,659
Um, he knows one of the things that you could even think about in various ways,

607
00:37:13,659 --> 00:37:17,350
um, for say a question answering system among other things.

608
00:37:17,350 --> 00:37:21,605
Um, and so he used pre-trained word vectors.

609
00:37:21,605 --> 00:37:28,980
Um, but what he did was he actually kind of doubled the word vectors.

610
00:37:28,980 --> 00:37:32,475
So, for each word he had two copies of the word vector,

611
00:37:32,475 --> 00:37:37,290
and so you have sort of two channel sets and one set he

612
00:37:37,290 --> 00:37:42,375
froze and the other one he fine tuned as he trained.

613
00:37:42,375 --> 00:37:46,590
So it's sort of he tried to get the best of both worlds of sort of fine tuning

614
00:37:46,590 --> 00:37:51,770
and not fine tuning and all that went into the max pooling operation.

615
00:37:51,770 --> 00:38:01,614
Okay. Um, so, after the max pooling we get out one number for each channel and so,

616
00:38:01,614 --> 00:38:06,760
um, he has something of three size convolutions, three,

617
00:38:06,760 --> 00:38:10,390
four, five, 100 features for each size.

618
00:38:10,390 --> 00:38:13,435
So we're getting out a vector of size,

619
00:38:13,435 --> 00:38:15,670
um, 300 at that point,

620
00:38:15,670 --> 00:38:19,810
and at that point you're taking that final vector and just sticking it

621
00:38:19,810 --> 00:38:24,595
through a softmax and that's then giving your classification of the classes.

622
00:38:24,595 --> 00:38:31,495
Um, so all of that can be summarized in this picture if it's big enough to sort of read.

623
00:38:31,495 --> 00:38:32,800
So, here's our sentence.

624
00:38:32,800 --> 00:38:34,855
I like this movie very much,

625
00:38:34,855 --> 00:38:39,310
which has you know, our word embedding dimension is five,

626
00:38:39,310 --> 00:38:42,265
and so then doing it in this example,

627
00:38:42,265 --> 00:38:46,930
we are having two channels for each kernel size and

628
00:38:46,930 --> 00:38:52,030
we consider kernels of size two, three, and four.

629
00:38:52,030 --> 00:38:57,205
Um, and so and then we are getting two different ones.

630
00:38:57,205 --> 00:39:01,615
Um, so we're getting, um, six.

631
00:39:01,615 --> 00:39:04,405
This is showing six of our filters.

632
00:39:04,405 --> 00:39:07,180
Um, so we apply those.

633
00:39:07,180 --> 00:39:10,975
When we- when we apply those filters without any padding,

634
00:39:10,975 --> 00:39:15,880
we are then getting out these outputs of the filters which are of sizes four,

635
00:39:15,880 --> 00:39:18,985
five, and six respectively.

636
00:39:18,985 --> 00:39:23,065
Um, and so then once we've got these

637
00:39:23,065 --> 00:39:27,265
for each of these sets of numbers we're doing one max pooling.

638
00:39:27,265 --> 00:39:30,880
So, we're just taking the max of each of these,

639
00:39:30,880 --> 00:39:36,715
um, output features which gives us these six numbers.

640
00:39:36,715 --> 00:39:43,060
Um, we can concatenate them all together into one vector which we feed into,

641
00:39:43,060 --> 00:39:50,120
um, a softmax over two classes as to whether sentiment is positive or negative.

642
00:39:52,580 --> 00:39:55,590
Um, so that's basically the model.

643
00:39:55,590 --> 00:40:01,200
So something- so this is sort of really actually a very simple,

644
00:40:01,200 --> 00:40:03,630
very computationally efficient, uh,

645
00:40:03,630 --> 00:40:06,780
model as to how to build a text classifier.

646
00:40:06,780 --> 00:40:13,155
[NOISE] Um, yeah, just a couple more things to get through,

647
00:40:13,155 --> 00:40:15,210
um, so in one of the assignments,

648
00:40:15,210 --> 00:40:17,700
we talked about Dropout [NOISE] and you used it.

649
00:40:17,700 --> 00:40:19,065
So, um, you know,

650
00:40:19,065 --> 00:40:21,705
hopefully you're all masters of Dropout at this point.

651
00:40:21,705 --> 00:40:24,720
Um, so he was using Dropout, um,

652
00:40:24,720 --> 00:40:28,185
and this being 2014 and the,

653
00:40:28,185 --> 00:40:31,820
um, Dropout paper only coming out in 2014.

654
00:40:31,820 --> 00:40:34,895
I guess, there'd been an earlier version that came out a couple of years earlier.

655
00:40:34,895 --> 00:40:37,160
This was sort of still fairly early,

656
00:40:37,160 --> 00:40:39,425
um, to be taking advantage of Dropout.

657
00:40:39,425 --> 00:40:41,135
So that while training,

658
00:40:41,135 --> 00:40:44,105
you've got this sort of Dropout vector, um,

659
00:40:44,105 --> 00:40:49,010
where you sample your Bernoulli random variables and you're, sort of,

660
00:40:49,010 --> 00:40:54,825
um, sort of, designed to drop out some of the features each time you are doing things.

661
00:40:54,825 --> 00:40:58,200
At testing time, you don't do the dropout,

662
00:40:58,200 --> 00:41:02,130
but because before you were sort of dropping out a lot of stuff,

663
00:41:02,130 --> 00:41:07,455
you're scaling your weight matrix by the same probability that you use for dropping out,

664
00:41:07,455 --> 00:41:09,000
so that you get, sort of,

665
00:41:09,000 --> 00:41:12,000
vectors of the same scale as before.

666
00:41:12,000 --> 00:41:15,075
Um, so as we sort of discussed in the assignment,

667
00:41:15,075 --> 00:41:18,420
Dropout is a really effective form of regularization,

668
00:41:18,420 --> 00:41:20,580
widely used in neural networks.

669
00:41:20,580 --> 00:41:23,700
Um, he didn't only do that, he actually did,

670
00:41:23,700 --> 00:41:27,600
a kind of another sort of funky form of regularization.

671
00:41:27,600 --> 00:41:31,425
So that's for the softmax weight vector,

672
00:41:31,425 --> 00:41:35,280
he constrained the L2 norms,

673
00:41:35,280 --> 00:41:41,100
so the squared norms of the weight vectors and the softmax, [NOISE] um,

674
00:41:41,100 --> 00:41:45,405
matrix, um, to a fixed number S,

675
00:41:45,405 --> 00:41:47,460
which was sort of set of the hyper-parameters,

676
00:41:47,460 --> 00:41:49,515
actually set to the value three.

677
00:41:49,515 --> 00:41:53,055
Um, and if your weights were getting too large,

678
00:41:53,055 --> 00:41:55,590
they were being rescaled,

679
00:41:55,590 --> 00:41:57,345
um, so they didn't blow up.

680
00:41:57,345 --> 00:42:00,210
Um, this isn't a very common thing to do.

681
00:42:00,210 --> 00:42:03,690
I'm not sure it's very necessary, um, but, um,

682
00:42:03,690 --> 00:42:05,850
I guess it gives you some- I mean,

683
00:42:05,850 --> 00:42:09,045
I guess by showing you a few of the details of this one,

684
00:42:09,045 --> 00:42:10,590
my hope is, sort of,

685
00:42:10,590 --> 00:42:13,680
gives you some ideas about how there are lots of things you can play

686
00:42:13,680 --> 00:42:17,025
around with and muck with if you wanna try different things,

687
00:42:17,025 --> 00:42:19,020
um, for your final projects.

688
00:42:19,020 --> 00:42:21,000
Um, okay.

689
00:42:21,000 --> 00:42:24,120
So here are some of his final hyperparameters.

690
00:42:24,120 --> 00:42:27,360
So he's using ReLU nonlinearities,

691
00:42:27,360 --> 00:42:30,765
um, window sizes of three, four, and five,

692
00:42:30,765 --> 00:42:35,790
the convolutions, hundred features or channels for each size,

693
00:42:35,790 --> 00:42:38,535
um, Dropout of a half as usual.

694
00:42:38,535 --> 00:42:41,865
Um, you get several percentage improvements from dropout,

695
00:42:41,865 --> 00:42:43,860
which is quite common actually.

696
00:42:43,860 --> 00:42:47,835
Um, the sort of L2 constraint, s equals three,

697
00:42:47,835 --> 00:42:50,175
mini batch of 50,

698
00:42:50,175 --> 00:42:52,635
300 dimensional word vectors,

699
00:42:52,635 --> 00:42:55,755
train to maximize dev set performance.

700
00:42:55,755 --> 00:42:58,830
Okay. And here is the big table,

701
00:42:58,830 --> 00:43:00,690
you know, I was too lazy, um,

702
00:43:00,690 --> 00:43:06,570
to redo of performance on these different text classification data sets.

703
00:43:06,570 --> 00:43:08,460
Um, there are lots of different ones.

704
00:43:08,460 --> 00:43:11,820
So these two are both Stanford Sentiment Treebank.

705
00:43:11,820 --> 00:43:14,565
This is the Subjective Objective Language.

706
00:43:14,565 --> 00:43:19,650
This is the Question Classification, of is it asking for a person name and location,

707
00:43:19,650 --> 00:43:20,790
a company or whatever.

708
00:43:20,790 --> 00:43:24,150
Um, this is, um,

709
00:43:24,150 --> 00:43:26,280
talking about, sort of, a perspective,

710
00:43:26,280 --> 00:43:28,335
which is another classification thing.

711
00:43:28,335 --> 00:43:30,885
Consumer Reports is another sentiment one.

712
00:43:30,885 --> 00:43:36,210
Um, so lots of data sets and then here are lots of models.

713
00:43:36,210 --> 00:43:41,580
So the model- some of the models down here or here,

714
00:43:41,580 --> 00:43:46,020
are traditional feature-based, um, classifiers.

715
00:43:46,020 --> 00:43:48,000
Um, so in particular,

716
00:43:48,000 --> 00:43:52,230
um, sort of Wang and me back in 2012,

717
00:43:52,230 --> 00:43:56,025
had sort of pointed out that by taking certain steps

718
00:43:56,025 --> 00:44:00,720
with n-gram features and other forms of normalization,

719
00:44:00,720 --> 00:44:03,420
that you could actually get quite good results with

720
00:44:03,420 --> 00:44:06,960
just the traditional feature, um, based classifiers.

721
00:44:06,960 --> 00:44:12,045
So many people use that as a baseline for showing that you can do better things.

722
00:44:12,045 --> 00:44:14,360
Um, the ones up here,

723
00:44:14,360 --> 00:44:18,200
were tree structured neural networks that my group was very fond

724
00:44:18,200 --> 00:44:22,805
of in the early 2010s and then up at the very top,

725
00:44:22,805 --> 00:44:24,695
uh, his CNN models.

726
00:44:24,695 --> 00:44:26,510
And as you can see,

727
00:44:26,510 --> 00:44:27,875
it's sort of a mix.

728
00:44:27,875 --> 00:44:30,870
Sometimes the CNN model wins,

729
00:44:30,870 --> 00:44:33,015
like in this column and this column,

730
00:44:33,015 --> 00:44:36,015
sometimes it doesn't win like in these columns.

731
00:44:36,015 --> 00:44:38,010
Um, but in general, um,

732
00:44:38,010 --> 00:44:40,260
what you didn't see from this is that, you know,

733
00:44:40,260 --> 00:44:43,140
this is an extremely simple, um,

734
00:44:43,140 --> 00:44:46,335
convolutional neural network model and it actually does,

735
00:44:46,335 --> 00:44:48,720
um, kind of well on this system.

736
00:44:48,720 --> 00:44:54,720
Um, you can quibble with this results table,

737
00:44:54,720 --> 00:45:01,285
and again in terms of like writing your propos- project proposal, um,

738
00:45:01,285 --> 00:45:07,250
one thing that you should do is kind of think about what you're reading, um,

739
00:45:07,250 --> 00:45:10,100
because, you know, a lot of papers aren't perfect

740
00:45:10,100 --> 00:45:13,130
and there are reasons to quibble with what they claim.

741
00:45:13,130 --> 00:45:17,785
And sometimes if you think about what they're claiming and whether it's reasonable, um,

742
00:45:17,785 --> 00:45:20,895
there are reasons why it's not or there are ideas

743
00:45:20,895 --> 00:45:24,405
of how you could do things differently or show something different.

744
00:45:24,405 --> 00:45:27,315
I mean, the main reason why you could quibble with,

745
00:45:27,315 --> 00:45:31,365
um, Yoon Kim's results table is, well,

746
00:45:31,365 --> 00:45:35,385
he already said, as I had a couple of slides back, um,

747
00:45:35,385 --> 00:45:37,980
that the statement that Dropout gives you

748
00:45:37,980 --> 00:45:41,220
two to four percent accuracy improvement in this neural nets.

749
00:45:41,220 --> 00:45:45,210
[NOISE] Um, but most of these systems because they

750
00:45:45,210 --> 00:45:49,365
are older and were done before Dropout was invented,

751
00:45:49,365 --> 00:45:51,390
um, didn't make use of Dropout.

752
00:45:51,390 --> 00:45:55,170
But, you know, any of these sort of neural net systems up here

753
00:45:55,170 --> 00:45:59,445
could have used Dropout and presumably it would have given them a couple of,

754
00:45:59,445 --> 00:46:01,140
um, percent gain as well.

755
00:46:01,140 --> 00:46:05,385
So arguably, this is sort of a biased, unfair comparison.

756
00:46:05,385 --> 00:46:10,635
And the right thing would have been to be comparing all the systems, um, using Dropout.

757
00:46:10,635 --> 00:46:12,120
Um, but, you know,

758
00:46:12,120 --> 00:46:13,890
despite that, you know,

759
00:46:13,890 --> 00:46:16,980
this was still a prett- a lot of people noticed

760
00:46:16,980 --> 00:46:20,820
this paper because it showed that using this sort of very simple,

761
00:46:20,820 --> 00:46:23,190
very fast convolutional architecture,

762
00:46:23,190 --> 00:46:27,070
could give you strong results for text classification.

763
00:46:28,250 --> 00:46:31,005
Um, that's that.

764
00:46:31,005 --> 00:46:33,765
Yes. So in summary,

765
00:46:33,765 --> 00:46:38,475
you know, something that you should be thinking about for projects and otherwise,

766
00:46:38,475 --> 00:46:44,370
we're effectively building up a bigger toolkit of different tools you could be using,

767
00:46:44,370 --> 00:46:48,135
um, for projects or future work or whatever it is.

768
00:46:48,135 --> 00:46:49,635
So starting off with,

769
00:46:49,635 --> 00:46:53,250
we had word vectors and then we could build bag of

770
00:46:53,250 --> 00:46:57,105
vector models by just taking the word vectors and averaging them.

771
00:46:57,105 --> 00:47:01,080
And, you know, that's actually a surprisingly good baseline to start with.

772
00:47:01,080 --> 00:47:03,960
We suggest to you in many cases for things like projects,

773
00:47:03,960 --> 00:47:05,085
you should use that.

774
00:47:05,085 --> 00:47:06,270
See how well it does,

775
00:47:06,270 --> 00:47:07,965
make sure you're working better.

776
00:47:07,965 --> 00:47:10,605
I mean particularly, you can do even better with that,

777
00:47:10,605 --> 00:47:14,490
if you sort of add some extra ReLU layers on top,

778
00:47:14,490 --> 00:47:18,015
which is an idea that's been explored in deep averaging networks.

779
00:47:18,015 --> 00:47:22,290
Um, then we looked at window models which were very simple.

780
00:47:22,290 --> 00:47:23,850
You're just taking these sort of

781
00:47:23,850 --> 00:47:27,585
five word windows and computing a feed-forward network on them,

782
00:47:27,585 --> 00:47:32,835
and they work very well for word classification problems that only need local context.

783
00:47:32,835 --> 00:47:36,045
Things like, part of speech tagging or NER.

784
00:47:36,045 --> 00:47:39,390
But then we've gone ahead and looked at some other models.

785
00:47:39,390 --> 00:47:45,405
And so, um, CNN's are very good for text classification, um,

786
00:47:45,405 --> 00:47:49,590
and they're very good because they parallelize really well on GPUs,

787
00:47:49,590 --> 00:47:51,840
which is something I'll come back to again later.

788
00:47:51,840 --> 00:47:57,510
So they, they just sort- the general sort of representing sentence meaning.

789
00:47:57,510 --> 00:47:59,099
They're actually a efficient,

790
00:47:59,099 --> 00:48:02,295
versatile, good method, which has been used quite a bit.

791
00:48:02,295 --> 00:48:05,460
And then they sort of contrast with recurrent neural networks.

792
00:48:05,460 --> 00:48:07,800
Recurrent neural networks have some advantages.

793
00:48:07,800 --> 00:48:10,080
They're sort of more cognitively plausible,

794
00:48:10,080 --> 00:48:12,120
because you're sort of reading through the text and,

795
00:48:12,120 --> 00:48:14,145
um, getting its meaning.

796
00:48:14,145 --> 00:48:16,830
Um, recurrent neural networks are good for

797
00:48:16,830 --> 00:48:19,800
things like sequence tagging and classification,

798
00:48:19,800 --> 00:48:23,385
building language models to predict what's coming next.

799
00:48:23,385 --> 00:48:26,910
Um, they can do really well when combined with attention.

800
00:48:26,910 --> 00:48:29,565
Um, but they also have some disadvantages.

801
00:48:29,565 --> 00:48:33,870
They're way slower than convolutional neural networks and if what you wanna

802
00:48:33,870 --> 00:48:38,300
do is get out some kind of overall meaning representation of a sentence,

803
00:48:38,300 --> 00:48:39,845
you know, "What does this mean?

804
00:48:39,845 --> 00:48:41,375
Are these two, um,

805
00:48:41,375 --> 00:48:43,850
phrases paraphrases with each other?"

806
00:48:43,850 --> 00:48:46,730
There are now many results that show that people

807
00:48:46,730 --> 00:48:49,805
don't get better results with recurrent neural networks.

808
00:48:49,805 --> 00:48:55,440
They can get better results using techniques like convolutional neural networks.

809
00:48:55,550 --> 00:49:05,010
Okay. [NOISE] So in the next step then [NOISE] is to,

810
00:49:05,010 --> 00:49:09,675
sort of, head towards our com- our complex,

811
00:49:09,675 --> 00:49:12,375
um, convolutional architecture example.

812
00:49:12,375 --> 00:49:14,010
So before getting to that,

813
00:49:14,010 --> 00:49:18,525
I just wanna sort of introduce a few concepts that we haven't seen,

814
00:49:18,525 --> 00:49:22,625
all of which, um, start to turn up when we do this.

815
00:49:22,625 --> 00:49:26,360
So we spent a lot of time in the sequence models part,

816
00:49:26,360 --> 00:49:32,345
talking about gated models or the gated recurrent units and the LSTM units.

817
00:49:32,345 --> 00:49:36,080
But the idea of a gate is general that we can

818
00:49:36,080 --> 00:49:40,130
sort of have this idea that we can calculate something,

819
00:49:40,130 --> 00:49:42,175
put it through, um,

820
00:49:42,175 --> 00:49:47,370
a sigmoid nonlinearity and gets a value between zero and one,

821
00:49:47,370 --> 00:49:50,385
um, or a vector of values between zero and one.

822
00:49:50,385 --> 00:49:52,980
And then do a Hadamard product with a vector

823
00:49:52,980 --> 00:49:55,860
and sort of gate it between its value and zero.

824
00:49:55,860 --> 00:49:59,490
So that suggests the idea that you could also apply

825
00:49:59,490 --> 00:50:04,110
gates vertically when you're building multilayer networks.

826
00:50:04,110 --> 00:50:07,845
And after the successive LSTMs had been proven,

827
00:50:07,845 --> 00:50:11,780
that was, um, an idea that really took off,

828
00:50:11,780 --> 00:50:13,730
was people start exploring,

829
00:50:13,730 --> 00:50:19,445
how can we have, use these ideas of skip connections and gating in a,

830
00:50:19,445 --> 00:50:21,425
in a vertical direction?

831
00:50:21,425 --> 00:50:23,480
And here are two versions of it.

832
00:50:23,480 --> 00:50:26,450
This one is a very simple one,

833
00:50:26,450 --> 00:50:30,965
but a very successful one that's basically just about a skip connection.

834
00:50:30,965 --> 00:50:36,890
So and this is referred to as a residual block and- which is used in residual networks,

835
00:50:36,890 --> 00:50:38,690
otherwise known as ResNets.

836
00:50:38,690 --> 00:50:42,470
Um, so in a residual block, for each block,

837
00:50:42,470 --> 00:50:48,440
you allow a value just to skip ahead to the next, um, layer.

838
00:50:48,440 --> 00:50:52,535
Or you can stick it through a conv block,

839
00:50:52,535 --> 00:50:56,825
and the typical conv block is you go through a convolutional layer,

840
00:50:56,825 --> 00:50:59,600
you then go through a ReLU nonlinearity,

841
00:50:59,600 --> 00:51:03,250
another convolutional layer, and then when you come out,

842
00:51:03,250 --> 00:51:05,430
you just sum these two values.

843
00:51:05,430 --> 00:51:07,710
So this is the same idea that sort of

844
00:51:07,710 --> 00:51:11,820
summing values is magical in the same way as an LSTM.

845
00:51:11,820 --> 00:51:15,165
And then you put the output of that through another ReLU,

846
00:51:15,165 --> 00:51:18,705
and this thing here is called a residual block

847
00:51:18,705 --> 00:51:22,950
and then commonly you'll stack residual blocks on top of each other.

848
00:51:22,950 --> 00:51:25,230
Um, there's one little trick here,

849
00:51:25,230 --> 00:51:28,320
um, which is you need to use padding, right?

850
00:51:28,320 --> 00:51:33,000
Um, because at the end of the day since you want to sum these two pathways,

851
00:51:33,000 --> 00:51:35,355
you want them to be the same size.

852
00:51:35,355 --> 00:51:36,585
And if you, sort of,

853
00:51:36,585 --> 00:51:40,200
have them shrinking in the conv blocks you wouldn't be able to sum them.

854
00:51:40,200 --> 00:51:45,120
So you want to, sort of, have a padding at each stage so they stay the same size here,

855
00:51:45,120 --> 00:51:47,437
and so that you can add them together.

856
00:51:47,437 --> 00:51:54,500
Um, here's, um, a different version of a block which is

857
00:51:54,500 --> 00:51:57,470
sort of more LSTM-ish and indeed

858
00:51:57,470 --> 00:52:01,710
this block was developed by Jrgen Schmidhuber and students,

859
00:52:01,710 --> 00:52:06,000
who's the same guy who's behind LSTMs and you can see the same thinking.

860
00:52:06,000 --> 00:52:08,150
It's called a highway block.

861
00:52:08,150 --> 00:52:10,800
So in a way it's sort of similar.

862
00:52:10,800 --> 00:52:16,080
You've got, you know, kind of thinking of moving an identity x that skips

863
00:52:16,080 --> 00:52:23,085
a nonlinear block or you can have it go through exactly the same stuff conv, relu, conv.

864
00:52:23,085 --> 00:52:26,480
The difference is that unlike this one,

865
00:52:26,480 --> 00:52:29,165
this time there's explicit gates so there's,

866
00:52:29,165 --> 00:52:33,290
um, and this T-gate and the C-gate.

867
00:52:33,290 --> 00:52:39,230
And so you're multiplying both of the path through here and the path through here

868
00:52:39,230 --> 00:52:42,280
by a gate just kinda like the sort of

869
00:52:42,280 --> 00:52:47,130
the get input gates that we saw before and then summing them together.

870
00:52:47,130 --> 00:52:50,670
So that sort of feels more

871
00:52:50,670 --> 00:52:56,285
powerful but it's not actually clear that it is more powerful.

872
00:52:56,285 --> 00:52:59,460
I mean, this one actually has a very simple

873
00:52:59,460 --> 00:53:03,070
semantic because if you think of the semantics of this one

874
00:53:03,070 --> 00:53:05,930
is the default is just you walk

875
00:53:05,930 --> 00:53:11,015
this way and you just sort of carry forward your value and do nothing.

876
00:53:11,015 --> 00:53:14,900
Um, so, what this block's job to- is to do,

877
00:53:14,900 --> 00:53:18,155
is to learn a delta that is meant to learn

878
00:53:18,155 --> 00:53:21,750
what kind of deviation you have from doing nothing.

879
00:53:21,750 --> 00:53:25,210
Um, so that's a nice simple semantic which, um,

880
00:53:25,210 --> 00:53:28,680
seems to work well in neural networks to learn things.

881
00:53:28,680 --> 00:53:31,390
Um, this sort of has

882
00:53:31,390 --> 00:53:36,500
more complicated apparent semantics because you're taking, you know,

883
00:53:36,500 --> 00:53:43,005
some parts of the identity multiplying by this sort of gate in a Hadamard product

884
00:53:43,005 --> 00:53:49,880
and some parts of this conv block multiplied by this other gate T in a Hadamard product.

885
00:53:49,880 --> 00:53:53,980
So that sort of feels more powerful as that

886
00:53:53,980 --> 00:53:58,325
gives me a lot more control because I can take pieces of the different ones and so on.

887
00:53:58,325 --> 00:54:01,620
If you think about it for a bit longer, I mean,

888
00:54:01,620 --> 00:54:05,380
mathematically it's actually not any more powerful that you

889
00:54:05,380 --> 00:54:09,500
can represent anything you can do with this one with that one.

890
00:54:09,500 --> 00:54:13,530
And the way to think about that is well, um,

891
00:54:13,530 --> 00:54:19,410
you know, here you're kind of keeping only part of the identity,

892
00:54:19,410 --> 00:54:26,840
um, but what you could do is keep the whole of the identity and see it as your job

893
00:54:26,840 --> 00:54:30,095
to subtract off the bits that this one isn't keeping

894
00:54:30,095 --> 00:54:34,440
over here in the conv block which you can do theoretically.

895
00:54:34,440 --> 00:54:39,480
Um, and so, you can sort of anything you can compute with this as a function,

896
00:54:39,480 --> 00:54:42,830
you can actually compute with a, um, ResNet block.

897
00:54:42,830 --> 00:54:47,185
Um, and so then as quite often in neural network land,

898
00:54:47,185 --> 00:54:49,330
the question isn't sort of, um,

899
00:54:49,330 --> 00:54:53,190
some kind of proof of compute- can be computed or not.

900
00:54:53,190 --> 00:54:58,455
It sort of comes down to learning and regularization questions as to

901
00:54:58,455 --> 00:55:01,345
whether one or the other of these actually proves

902
00:55:01,345 --> 00:55:05,270
better as something to use in a learning architecture.

903
00:55:06,430 --> 00:55:09,680
Okay. Second concept.

904
00:55:09,680 --> 00:55:11,860
Um, batch normalization.

905
00:55:11,860 --> 00:55:17,405
So when people are building deep convolutional neural networks,

906
00:55:17,405 --> 00:55:21,680
um, in the 2015 pluses,

907
00:55:21,680 --> 00:55:27,065
um, they almost always use batch normalization layers because

908
00:55:27,065 --> 00:55:32,685
this makes your life a lot better and if they're not using batch normalization layers,

909
00:55:32,685 --> 00:55:37,070
they're normally using one of the other variant ideas that people have suggested

910
00:55:37,070 --> 00:55:42,165
such as layer normalization which is sort of meant to do about the same thing.

911
00:55:42,165 --> 00:55:46,090
Um, so what batch normalization does?

912
00:55:46,090 --> 00:55:50,650
I mean, I think many of you will have seen somewhere in steps or

913
00:55:50,650 --> 00:55:56,305
otherwise the idea of doing a Z-transform which means you take your data,

914
00:55:56,305 --> 00:55:59,100
you work out its mean and you work out its

915
00:55:59,100 --> 00:56:03,970
standard deviation and then you rescale by subtraction and

916
00:56:03,970 --> 00:56:07,710
multiplication so that you have a set of data which

917
00:56:07,710 --> 00:56:12,360
has a mean of zero and a standard deviation of one.

918
00:56:12,360 --> 00:56:14,680
Most people see that, right?

919
00:56:14,680 --> 00:56:23,500
Yeah? Um, so batch normalization is effectively doing exactly that but in a weird way.

920
00:56:23,500 --> 00:56:27,770
So what you're doing is that you're taking each mini batch.

921
00:56:27,770 --> 00:56:31,875
So whatever just random 32 examples you've stuck in a mini batch,

922
00:56:31,875 --> 00:56:34,040
you're running them through a layer of

923
00:56:34,040 --> 00:56:37,355
your neural network like a ConvBlock that we saw before

924
00:56:37,355 --> 00:56:43,190
and you take the output of that mini batch and then you do a Z-transform on it.

925
00:56:43,190 --> 00:56:47,295
Um, and then it goes forward into the next ConvBlock or whatever,

926
00:56:47,295 --> 00:56:49,410
and the next time you have a different mini batch,

927
00:56:49,410 --> 00:56:50,990
you just Z-transform it.

928
00:56:50,990 --> 00:56:52,290
So it seems a little bit weird.

929
00:56:52,290 --> 00:56:56,600
You're just doing it on the output of these mini batches.

930
00:56:56,600 --> 00:57:01,680
Um, but that's proven to be a very effective thing to do.

931
00:57:01,680 --> 00:57:05,980
So that it sort of means that what comes out of

932
00:57:05,980 --> 00:57:09,890
a ConvBlock sort of always has the same kind of scale.

933
00:57:09,890 --> 00:57:13,720
So it doesn't sort of fluctuate a lot and mess things up and it tends to

934
00:57:13,720 --> 00:57:18,225
make the models just much more reliably trainable because,

935
00:57:18,225 --> 00:57:22,860
you know, you just have to be much less fussy about a lot of things.

936
00:57:22,860 --> 00:57:25,505
Because, you know, a lot of the things we've talked about,

937
00:57:25,505 --> 00:57:28,180
about initializing your parameters and

938
00:57:28,180 --> 00:57:31,130
setting your learning rates is sort of about, well,

939
00:57:31,130 --> 00:57:34,310
you have to keep the scale of things about right so they don't get

940
00:57:34,310 --> 00:57:37,810
too big or too small and things like that.

941
00:57:37,810 --> 00:57:40,280
Whereas, if you're doing this batch normalization,

942
00:57:40,280 --> 00:57:42,490
you're sort of forcing scale,

943
00:57:42,490 --> 00:57:45,705
um, to being the same size each time.

944
00:57:45,705 --> 00:57:48,370
And s o therefore, you kind of don't have to do

945
00:57:48,370 --> 00:57:51,200
the other stuff as well and it still tends to,

946
00:57:51,200 --> 00:57:52,710
um, work pretty well.

947
00:57:52,710 --> 00:57:55,650
So that's a good technique to know about.

948
00:57:55,690 --> 00:57:59,800
Okay. Um, one last thing to learn about.

949
00:57:59,800 --> 00:58:02,070
Um, there's a concept of,

950
00:58:02,070 --> 00:58:07,015
um, size one convolutions.

951
00:58:07,015 --> 00:58:11,240
Um, and actually, I guess I really sort of, um,

952
00:58:11,240 --> 00:58:14,680
renamed it- I named this wrong because I wrote down

953
00:58:14,680 --> 00:58:18,240
one by one convolutions because that's the term you normally see.

954
00:58:18,240 --> 00:58:22,530
But that's, um, the vision world where you have 2D convolutions.

955
00:58:22,530 --> 00:58:26,135
So I guess I should have just called this one convolutions.

956
00:58:26,135 --> 00:58:28,890
So you can have convolutions, um,

957
00:58:28,890 --> 00:58:33,070
with a kernel size of one and when you first see that,

958
00:58:33,070 --> 00:58:37,840
it seems like that makes no sense whatsoever because the whole idea

959
00:58:37,840 --> 00:58:43,305
of a convolution was I was taking this patch and calculating something from it.

960
00:58:43,305 --> 00:58:48,330
If I'm not looking at any other words,

961
00:58:48,330 --> 00:58:50,510
surely I'm calculating nothing.

962
00:58:50,510 --> 00:58:54,975
But what actually happens in the size one convolution,

963
00:58:54,975 --> 00:58:59,160
is if you have a number of channels that

964
00:58:59,160 --> 00:59:03,850
sort of in a previous layer if you'd calculated whatever it was,

965
00:59:03,850 --> 00:59:06,605
32 channels or something like that.

966
00:59:06,605 --> 00:59:11,070
What the one by one convolution is doing is acting as

967
00:59:11,070 --> 00:59:16,625
a tiny little embedded fully-connected network over those channels.

968
00:59:16,625 --> 00:59:18,910
And so you're sort of doing a

969
00:59:18,910 --> 00:59:22,280
position specific fully-connected network,

970
00:59:22,280 --> 00:59:26,385
um, in- for each row of your data.

971
00:59:26,385 --> 00:59:28,050
And so you can do that,

972
00:59:28,050 --> 00:59:29,590
um, for various reasons.

973
00:59:29,590 --> 00:59:31,920
You can do it because you want to map down from having

974
00:59:31,920 --> 00:59:34,870
a lot of channels to having fewer channels or

975
00:59:34,870 --> 00:59:37,460
you can do it just because you think another non-linearity

976
00:59:37,460 --> 00:59:40,345
will help and this is a really cheap way to do it.

977
00:59:40,345 --> 00:59:44,150
Because the crucial thing to notice is that if you sort

978
00:59:44,150 --> 00:59:47,990
of put fully-connected layers over everything,

979
00:59:47,990 --> 00:59:52,930
they involve a lot of parameters whereas putting in these size

980
00:59:52,930 --> 00:59:56,650
one convolutions involve very few parameters

981
00:59:56,650 --> 01:00:00,310
because you're just doing it at the level of a single word.

982
01:00:00,670 --> 01:00:03,765
Um, okay.

983
01:00:03,765 --> 01:00:08,585
Um, two random things and then I'll go onto my complex model.

984
01:00:08,585 --> 01:00:10,540
Um, this is just a sort of

985
01:00:10,540 --> 01:00:13,660
almost a bias- aside but it just shows

986
01:00:13,660 --> 01:00:17,110
something different that you could do and it's something that you could play with.

987
01:00:17,110 --> 01:00:20,065
I mean, when we talked about machine translation,

988
01:00:20,065 --> 01:00:24,500
we talk about the SIC to SIC architecture that was introduced in

989
01:00:24,500 --> 01:00:29,930
2014 and has been very successful for machine translation.

990
01:00:29,930 --> 01:00:32,680
But actually, the year before that came out,

991
01:00:32,680 --> 01:00:34,855
um, there was a paper, um,

992
01:00:34,855 --> 01:00:41,255
doing neural machine translation by Nal Kalchbrenner and Phil Blunsom in the UK.

993
01:00:41,255 --> 01:00:44,015
And this sort of was actually essentially

994
01:00:44,015 --> 01:00:48,840
the first neural machine translation paper of the modern era.

995
01:00:48,840 --> 01:00:50,400
If you dig back far enough,

996
01:00:50,400 --> 01:00:52,130
there are actually a couple of people that tried to use

997
01:00:52,130 --> 01:00:54,135
neural networks for machine translation

998
01:00:54,135 --> 01:00:58,445
in the '80s and '90s but this was sort of the first one that restarted it,

999
01:00:58,445 --> 01:01:02,205
and they didn't actually use a SIC to SIC architecture.

1000
01:01:02,205 --> 01:01:05,690
So what they used was for the encoder,

1001
01:01:05,690 --> 01:01:08,490
they used the convolutional neural networks.

1002
01:01:08,490 --> 01:01:13,430
And so that they had a stack of convolutional neural networks that progressively shrunk

1003
01:01:13,430 --> 01:01:18,760
down the input and then finally pulled it to get a sentence representation,

1004
01:01:18,760 --> 01:01:22,960
and then they used a sequence model as the decoder.

1005
01:01:22,960 --> 01:01:26,880
Um, so, um, that's sort of something that you could

1006
01:01:26,880 --> 01:01:30,520
try in some other applications that for encoders,

1007
01:01:30,520 --> 01:01:33,800
it's really easy to use convolutional neural networks.

1008
01:01:33,800 --> 01:01:39,175
There has been work on using convolutional neural networks as decoders as well,

1009
01:01:39,175 --> 01:01:44,415
though that's a little bit harder to get your brain around and isn't used nearly as much.

1010
01:01:44,415 --> 01:01:50,960
Then the second thing I want to mention because we'll turn to it in just a minute is so,

1011
01:01:50,960 --> 01:01:57,305
so far we've done Convolutional models over words so that

1012
01:01:57,305 --> 01:02:00,890
our kernels are effectively picking up

1013
01:02:00,890 --> 01:02:06,050
these word n-gram units of two-word or three word sub-sequences.

1014
01:02:06,050 --> 01:02:10,190
And the idea that then developed fairly soon was well maybe

1015
01:02:10,190 --> 01:02:14,705
it would also be useful to use convolutions over characters.

1016
01:02:14,705 --> 01:02:17,105
So, you could run a convolutional neural network

1017
01:02:17,105 --> 01:02:19,970
over the characters of the word to try and,

1018
01:02:19,970 --> 01:02:22,640
um, generate a word embedding, um,

1019
01:02:22,640 --> 01:02:25,760
and this idea has been explored quite a lot, um,

1020
01:02:25,760 --> 01:02:28,505
it's part of what you guys are gonna do for assignment

1021
01:02:28,505 --> 01:02:31,715
five is build a character level ConvNet,

1022
01:02:31,715 --> 01:02:35,180
um, for your improved machine translation system.

1023
01:02:35,180 --> 01:02:40,250
I'm not going to say sort of a huge amount about the foundations of this today, um,

1024
01:02:40,250 --> 01:02:44,270
because Thursday's lecture is then talking about subword models

1025
01:02:44,270 --> 01:02:49,055
and we'll go through all the details of different subword models.

1026
01:02:49,055 --> 01:02:53,300
But, I wanted to show you a con- a complex

1027
01:02:53,300 --> 01:02:58,010
convolutional neural network which is also used for text classification.

1028
01:02:58,010 --> 01:03:01,685
So, essentially, the same task as Yoon Kim's model

1029
01:03:01,685 --> 01:03:06,230
and this model actually is built on characters,

1030
01:03:06,230 --> 01:03:07,700
it's not built on words.

1031
01:03:07,700 --> 01:03:10,640
So, we are at the foundation of it,

1032
01:03:10,640 --> 01:03:13,145
um, having a word-like model.

1033
01:03:13,145 --> 01:03:16,775
Um, so, this is a paper from 2017,

1034
01:03:16,775 --> 01:03:21,350
um, by, um, the four authors shown here, um,

1035
01:03:21,350 --> 01:03:24,170
people working at Facebook AI Research,

1036
01:03:24,170 --> 01:03:27,635
um, in France, um, and so,

1037
01:03:27,635 --> 01:03:30,320
they kind of had an interesting hypothesis for

1038
01:03:30,320 --> 01:03:34,205
this paper which was essentially to say, that, you know,

1039
01:03:34,205 --> 01:03:42,530
by 2017 people who are using deep learning for vision were building really,

1040
01:03:42,530 --> 01:03:47,600
really deep networks and fi- finding that they work much,

1041
01:03:47,600 --> 01:03:49,790
much better for vision tasks.

1042
01:03:49,790 --> 01:03:52,205
So, essentially to some extend,

1043
01:03:52,205 --> 01:03:58,490
the breakthrough was these guys that once these ideas that emerged,

1044
01:03:58,490 --> 01:04:04,445
it then proved that it wasn't just that you could build a six layer or an eight layer,

1045
01:04:04,445 --> 01:04:07,580
um, Convolutional Neural Network for vision tasks.

1046
01:04:07,580 --> 01:04:09,200
You could start building really,

1047
01:04:09,200 --> 01:04:14,270
really deep networks for vision tasks which had tens or even hundreds of

1048
01:04:14,270 --> 01:04:21,090
layers and that those models when trained on a lot of data proved to work even better.

1049
01:04:21,210 --> 01:04:27,115
So, um, if that's what's in your head and you then looked,

1050
01:04:27,115 --> 01:04:33,970
look at what was and indeed is happening in natural language processing,

1051
01:04:33,970 --> 01:04:36,410
the observation is, you know,

1052
01:04:36,410 --> 01:04:38,390
these NLP people are kind of pathetic,

1053
01:04:38,390 --> 01:04:43,550
they claim they're doing deep learning but they're still working with three layer LSTMs.

1054
01:04:43,550 --> 01:04:46,475
Surely, we can make some progress, um,

1055
01:04:46,475 --> 01:04:53,735
by building really deep networks that kinda look like vision networks and using them,

1056
01:04:53,735 --> 01:04:57,035
um, for natural language processing goals.

1057
01:04:57,035 --> 01:05:01,415
And so, that is precisely what they said about doing.

1058
01:05:01,415 --> 01:05:08,930
So, that they designed and built really deep network which sort of looks like a vision stack,

1059
01:05:08,930 --> 01:05:14,900
um, as a convolutional neural network that is built over characters.

1060
01:05:14,900 --> 01:05:20,660
Um, so, I've got the picture of it here but sufficiently deep that it's fitting it on

1061
01:05:20,660 --> 01:05:23,390
the slide and making it readable [LAUGHTER] is a little bit

1062
01:05:23,390 --> 01:05:26,150
of a challenge but we can try and look at this.

1063
01:05:26,150 --> 01:05:27,260
So, at the bottom,

1064
01:05:27,260 --> 01:05:29,240
we have the text, um,

1065
01:05:29,240 --> 01:05:33,965
which is a sequence of characters and so, um,

1066
01:05:33,965 --> 01:05:36,980
for the text, um, so,

1067
01:05:36,980 --> 01:05:40,640
when people do vision object recognition on

1068
01:05:40,640 --> 01:05:44,930
pictures normally all the pictures are made the same size.

1069
01:05:44,930 --> 01:05:50,225
Right. You make every picture 300 pixels by 300 pixels or something like that.

1070
01:05:50,225 --> 01:05:53,375
So, they do exactly the same for NLP, um,

1071
01:05:53,375 --> 01:05:55,490
they have a size, um,

1072
01:05:55,490 --> 01:05:59,690
for their document which is 1024 characters.

1073
01:05:59,690 --> 01:06:03,710
If it's longer than that they truncate it and keep the first part.

1074
01:06:03,710 --> 01:06:06,470
If it's shorter than that they pad it until it's of

1075
01:06:06,470 --> 01:06:11,315
size 1024 and then they're gonna stick it into their stack.

1076
01:06:11,315 --> 01:06:15,440
So, the first part is that for each character,

1077
01:06:15,440 --> 01:06:18,200
they're going to learn a character embedding now and

1078
01:06:18,200 --> 01:06:22,145
their character embeddings are of dimensionality 16.

1079
01:06:22,145 --> 01:06:29,540
So, that the piece of text is now 16 by 1024, um, so,

1080
01:06:29,540 --> 01:06:33,770
they're going to stick that through a convolutional layer where

1081
01:06:33,770 --> 01:06:38,210
you've got kernel size of three and 64 output channels.

1082
01:06:38,210 --> 01:06:44,150
So you now have something that's 64 times of 1024 in size.

1083
01:06:44,150 --> 01:06:47,900
You now stick this through a convolutional block.

1084
01:06:47,900 --> 01:06:52,085
I'll explain the details of that convolutional block on the next slide but,

1085
01:06:52,085 --> 01:06:56,360
you should be thinking of that ResNet picture I showed earlier where you

1086
01:06:56,360 --> 01:07:01,310
can either be going through some convolutions or taking this optional shortcut.

1087
01:07:01,310 --> 01:07:05,180
Another ResNet, another residual block

1088
01:07:05,180 --> 01:07:08,765
where you can be going through convolutions are an optional shortcut,

1089
01:07:08,765 --> 01:07:15,020
um, they're then doing local pooling in the same way people typically do envision.

1090
01:07:15,020 --> 01:07:17,990
So, commonly what people do in vision systems

1091
01:07:17,990 --> 01:07:21,530
is you are sort of shrinking the size of the images, um,

1092
01:07:21,530 --> 01:07:25,820
by doing pooling that halves the dimensions in each direction.

1093
01:07:25,820 --> 01:07:27,020
But, at the same time,

1094
01:07:27,020 --> 01:07:29,014
you do that in your neural network,

1095
01:07:29,014 --> 01:07:31,715
you expand the number of channels,

1096
01:07:31,715 --> 01:07:34,130
and so you make it deeper in terms of the number of

1097
01:07:34,130 --> 01:07:38,105
channels at the same time as you make it smaller in the x,

1098
01:07:38,105 --> 01:07:39,710
y size of the image.

1099
01:07:39,710 --> 01:07:44,120
So, they do exactly the same apart from these one-dimensional convolutions.

1100
01:07:44,120 --> 01:07:49,760
So, before we had 64 channels in our 1024 character,

1101
01:07:49,760 --> 01:07:54,425
um, embedding, um, document.

1102
01:07:54,425 --> 01:07:57,110
So, now we pool it, um, so,

1103
01:07:57,110 --> 01:08:03,605
we're going to have 512 positions which are sort of like pairs of characters,

1104
01:08:03,605 --> 01:08:06,440
um, but we now have 128 channels

1105
01:08:06,440 --> 01:08:09,380
and then they kind of repeat that over and over again, right?

1106
01:08:09,380 --> 01:08:11,690
So, there are two more convolutional blocks which I'll

1107
01:08:11,690 --> 01:08:14,285
explain more but they're sort of residual blocks.

1108
01:08:14,285 --> 01:08:17,960
They pool it again and they do exactly the same thing.

1109
01:08:17,960 --> 01:08:21,305
So, now there are 256, um,

1110
01:08:21,305 --> 01:08:26,900
positions which are like four character blocks and they have 256 channels,

1111
01:08:26,900 --> 01:08:31,460
um, I can't point high enough but they repeat that again and they pool again.

1112
01:08:31,460 --> 01:08:33,590
So, now they've got, um,

1113
01:08:33,590 --> 01:08:36,710
128 positions which are about eight characters

1114
01:08:36,710 --> 01:08:40,775
each and they have 512 channels representing that.

1115
01:08:40,775 --> 01:08:45,080
They pool again, they have convolutional blocks again, um,

1116
01:08:45,080 --> 01:08:47,570
then lo and behold because I said that even the

1117
01:08:47,570 --> 01:08:50,060
weird ideas are going to turn up, right up there,

1118
01:08:50,060 --> 01:08:55,325
they're doing k max pooling and they're keeping the eight strongest values,

1119
01:08:55,325 --> 01:08:57,290
um, in each channel.

1120
01:08:57,290 --> 01:08:59,300
Um, and so at that point,

1121
01:08:59,300 --> 01:09:05,195
they've got something of size 512 by eight, um, so,

1122
01:09:05,195 --> 01:09:08,510
sort of like eight of the eight character sequences

1123
01:09:08,510 --> 01:09:11,705
have been deemed important to the classification and they're

1124
01:09:11,705 --> 01:09:15,455
kept but they sort per channel and there are 512 of them

1125
01:09:15,455 --> 01:09:19,475
you're then putting that through three fully connected layers.

1126
01:09:19,475 --> 01:09:22,190
So, typically vision systems at the top

1127
01:09:22,190 --> 01:09:25,355
have a couple of fully connected layers at the end,

1128
01:09:25,355 --> 01:09:28,055
um, and the very last one of those,

1129
01:09:28,055 --> 01:09:31,835
is effectively sort of feeding into your Softmax.

1130
01:09:31,835 --> 01:09:36,080
So, it's size 2,048 times the number of

1131
01:09:36,080 --> 01:09:41,330
classes which might just be positive negative two class unlike the topical classes.

1132
01:09:41,330 --> 01:09:44,000
Um, so, yeah, so it's essentially like

1133
01:09:44,000 --> 01:09:47,180
a vision stack but they're going to use it for language.

1134
01:09:47,180 --> 01:09:48,890
Um, okay.

1135
01:09:48,890 --> 01:09:52,340
So, the bit that I hand quite explained was

1136
01:09:52,340 --> 01:09:57,515
these convolutional blocks but it sort of looks like the picture that we had before or,

1137
01:09:57,515 --> 01:09:59,975
um, departments slightly more complicated.

1138
01:09:59,975 --> 01:10:02,420
So you're doing, um,

1139
01:10:02,420 --> 01:10:05,840
a convolutional block of size three

1140
01:10:05,840 --> 01:10:10,430
convolutions some number of channels depending on where you are in the sequence.

1141
01:10:10,430 --> 01:10:13,490
You're then putting it through a batch norm as we just

1142
01:10:13,490 --> 01:10:17,075
talked about putting it through a ReLu non-linearity,

1143
01:10:17,075 --> 01:10:21,320
repeating all those three things again or remember there

1144
01:10:21,320 --> 01:10:25,550
was this sort of skipped connection that went right around the outside of this block.

1145
01:10:25,550 --> 01:10:31,190
And so this is sort of a residual style block, um, so,

1146
01:10:31,190 --> 01:10:34,550
that's the kind of complex architecture you can put together and

1147
01:10:34,550 --> 01:10:38,675
try in your final projects if you dare in PyTorch.

1148
01:10:38,675 --> 01:10:42,775
Um, yeah, um, so,

1149
01:10:42,775 --> 01:10:46,090
for experiments so- so one of

1150
01:10:46,090 --> 01:10:52,570
the things that they were interested in and wanted to make a point of is well some

1151
01:10:52,570 --> 01:10:55,670
of these traditional sentence and

1152
01:10:55,670 --> 01:10:58,970
text classification datasets have been used in other papers

1153
01:10:58,970 --> 01:11:02,465
like Yoon Kim's paper are effectively quite small.

1154
01:11:02,465 --> 01:11:10,550
So, something like that Rotten Tomatoes dataset is actually only 10,000 examples, 5,000,

1155
01:11:10,550 --> 01:11:13,550
positive 5,000 negative and they sort of have

1156
01:11:13,550 --> 01:11:17,180
the idea that just like ImageNet was needed for

1157
01:11:17,180 --> 01:11:20,435
deep learning models to really show their worth and vision

1158
01:11:20,435 --> 01:11:24,155
that probably does show the value of a huge model like that.

1159
01:11:24,155 --> 01:11:28,070
Um, you need to have really big datasets.

1160
01:11:28,070 --> 01:11:29,855
So, they get some much bigger,

1161
01:11:29,855 --> 01:11:32,000
um, text classification datasets.

1162
01:11:32,000 --> 01:11:36,065
So, here's an Amazon review positive-negative dataset, um,

1163
01:11:36,065 --> 01:11:39,500
with which they have sort of 3,6 million documents,

1164
01:11:39,500 --> 01:11:43,030
um, Yelp reviews 650,000 documents.

1165
01:11:43,030 --> 01:11:45,100
So much bigger datasets,

1166
01:11:45,100 --> 01:11:48,230
um, and here are their experiments.

1167
01:11:48,230 --> 01:11:50,930
Okay. So, the numbers at the top, uh,

1168
01:11:50,930 --> 01:11:55,940
for the different datasets of the best previous result printed in the literature,

1169
01:11:55,940 --> 01:11:58,640
and then if you read the, um,

1170
01:11:58,640 --> 01:12:03,200
footnotes, um, there are a few things that they want to sort of star.

1171
01:12:03,200 --> 01:12:07,040
So, the ones that have a star next to them use

1172
01:12:07,040 --> 01:12:13,225
an external thesaurus which they don't use. [NOISE]

1173
01:12:13,225 --> 01:12:15,640
And the Yang method, um,

1174
01:12:15,640 --> 01:12:18,610
use some special techniques as well that I cut off.

1175
01:12:18,610 --> 01:12:21,580
Um, and the other thing to mention is these numbers,

1176
01:12:21,580 --> 01:12:24,175
they're error rates, so low is good.

1177
01:12:24,175 --> 01:12:26,410
Um, so the lower you get them, the better.

1178
01:12:26,410 --> 01:12:30,940
And so then these are all of their results.

1179
01:12:30,940 --> 01:12:34,770
Um, and so what can you get out of these results?

1180
01:12:34,770 --> 01:12:39,545
Um, well, the first thing that you can notice is basically with these results,

1181
01:12:39,545 --> 01:12:42,100
the deeper networks are working better, right?

1182
01:12:42,100 --> 01:12:44,845
So, the one I showed you,

1183
01:12:44,845 --> 01:12:48,490
uh, well, no, I think the one that I have the picture of this isn't the full thing.

1184
01:12:48,490 --> 01:12:52,720
Um, but they have ones with depth 9, 17,

1185
01:12:52,720 --> 01:12:56,680
and 29 in terms of the number of convolutional layers,

1186
01:12:56,680 --> 01:13:01,150
and the deepest one is always the one that's working best.

1187
01:13:01,150 --> 01:13:04,255
So, that's a proof of deep networks.

1188
01:13:04,255 --> 01:13:07,570
Um, that didn't keep on working, um,

1189
01:13:07,570 --> 01:13:10,690
so an interesting footnote here is,

1190
01:13:10,690 --> 01:13:11,935
um, I guess they thought,

1191
01:13:11,935 --> 01:13:13,225
oh, this is cool.

1192
01:13:13,225 --> 01:13:19,315
Why don't we try an even deeper one that has 47 layers and see how well that works?

1193
01:13:19,315 --> 01:13:23,635
And, I mean, the results were sort of interesting for that.

1194
01:13:23,635 --> 01:13:26,125
So, for the 47 layer one,

1195
01:13:26,125 --> 01:13:28,855
it worked a fraction worse than this one.

1196
01:13:28,855 --> 01:13:32,050
Um, so in one sense you,

1197
01:13:32,050 --> 01:13:37,900
they showed the result of sort of residual layers work really well.

1198
01:13:37,900 --> 01:13:40,705
So, they did an experiment of let's try to train

1199
01:13:40,705 --> 01:13:45,325
a 47-layer network without using residual connections.

1200
01:13:45,325 --> 01:13:47,455
And, well, it was a lot worse.

1201
01:13:47,455 --> 01:13:49,870
The numbers went down about two percent.

1202
01:13:49,870 --> 01:13:52,825
And they trained one with residual connections,

1203
01:13:52,825 --> 01:13:58,870
and the fact of the matter is the numbers were just a teeny weeny bit worse.

1204
01:13:58,870 --> 01:14:02,485
They were sort of 0,1 of a percent worse.

1205
01:14:02,485 --> 01:14:05,515
So, you know, they sort of work just about as well.

1206
01:14:05,515 --> 01:14:10,300
But, nevertheless, that's kind of different to the situation in vision,

1207
01:14:10,300 --> 01:14:15,145
because for the sort of residual networks that people are using in vision,

1208
01:14:15,145 --> 01:14:19,990
this is sort of like the very minimum depth that people use.

1209
01:14:19,990 --> 01:14:23,485
So, if you're using residual networks in vision typically,

1210
01:14:23,485 --> 01:14:25,915
you might use ResNet-34.

1211
01:14:25,915 --> 01:14:29,215
If you're really short on memory and want to have a small model,

1212
01:14:29,215 --> 01:14:32,980
but you just know you'd get better results if you used ResNet-50,

1213
01:14:32,980 --> 01:14:36,730
and in fact, if you used ResNet-101 it'd work even better again.

1214
01:14:36,730 --> 01:14:39,625
Um, and so that somehow, you know,

1215
01:14:39,625 --> 01:14:41,410
whether it's got to do with the different nature of

1216
01:14:41,410 --> 01:14:44,350
language or the amounts of data or something,

1217
01:14:44,350 --> 01:14:47,915
you haven't yet gone to the same depth that you can in vision.

1218
01:14:47,915 --> 01:14:50,620
Um, but other results, um,

1219
01:14:50,620 --> 01:14:54,190
so the other thing they're comparing here is that they're comparing

1220
01:14:54,190 --> 01:14:59,245
three different ways of sort of stringing things down.

1221
01:14:59,245 --> 01:15:02,950
So, you could be using, um,

1222
01:15:02,950 --> 01:15:06,715
the stride in the Convolution,

1223
01:15:06,715 --> 01:15:09,745
you can be using local MaxPooling,

1224
01:15:09,745 --> 01:15:12,805
and you could be using KMaxPooling.

1225
01:15:12,805 --> 01:15:14,350
Um, and they're general,

1226
01:15:14,350 --> 01:15:16,945
they're slightly different numbers as you can see.

1227
01:15:16,945 --> 01:15:19,990
Each one, um, wins and one, uh,

1228
01:15:19,990 --> 01:15:23,890
at least one of these datasets or actually at least two of these datasets.

1229
01:15:23,890 --> 01:15:27,430
But not only does MaxPooling win for four of the datasets,

1230
01:15:27,430 --> 01:15:29,890
if you sort of look at the numbers,

1231
01:15:29,890 --> 01:15:32,245
MaxPooling always does pretty well.

1232
01:15:32,245 --> 01:15:34,495
Because MaxPooling does pretty well here,

1233
01:15:34,495 --> 01:15:38,140
whereas the convolutional stride works badly,

1234
01:15:38,140 --> 01:15:41,605
and over here MaxPooling works pretty well,

1235
01:15:41,605 --> 01:15:45,685
and the, um, KMaxPooling works kind of badly.

1236
01:15:45,685 --> 01:15:50,890
So, their recommendation at the end of the day is you should always use, um,

1237
01:15:50,890 --> 01:15:53,680
just MaxPooling of a simple kind,

1238
01:15:53,680 --> 01:15:55,389
that that seems to be fine,

1239
01:15:55,389 --> 01:15:57,340
um, and nothing else.

1240
01:15:57,340 --> 01:16:01,315
Um, it's actually worth the trouble of thinking about doing.

1241
01:16:01,315 --> 01:16:10,540
Okay. Um, was there any other conclusions I wanted to say?

1242
01:16:10,540 --> 01:16:13,285
Okay. Um, I think that was most of that.

1243
01:16:13,285 --> 01:16:17,440
I guess their overall message is you can build super good, um,

1244
01:16:17,440 --> 01:16:20,470
text classification systems using ConvNets,

1245
01:16:20,470 --> 01:16:22,630
and you should take away that message.

1246
01:16:22,630 --> 01:16:26,170
Okay. So, there are just a couple of minutes left.

1247
01:16:26,170 --> 01:16:30,145
There was sort of one other thing that I wanted to mention,

1248
01:16:30,145 --> 01:16:33,505
but I think I'll just sort of mention it very quickly,

1249
01:16:33,505 --> 01:16:36,505
and you can look in more detail if you want to.

1250
01:16:36,505 --> 01:16:38,785
So, we sort of have this situation

1251
01:16:38,785 --> 01:16:44,065
that re- recurrent neural networks are a very standard building block for NLP,

1252
01:16:44,065 --> 01:16:49,405
but they have this big problem that they just don't parallelize well.

1253
01:16:49,405 --> 01:16:53,800
And the way we get fast computation deep learning is we find

1254
01:16:53,800 --> 01:16:58,180
things that parallelize well so that we can stick them on GPUs.

1255
01:16:58,180 --> 01:17:05,425
GPUs only are fast if they can be simultaneously doing the same computation many times,

1256
01:17:05,425 --> 01:17:08,440
which is sort of trivial for a convolutional neural network,

1257
01:17:08,440 --> 01:17:13,225
because precisely, you're doing the same comput- computation every position.

1258
01:17:13,225 --> 01:17:17,740
But that's not what's happening in the recurrent neural network because you have to

1259
01:17:17,740 --> 01:17:19,990
work out the value of position one

1260
01:17:19,990 --> 01:17:22,930
before you can start to calculate the value of position two,

1261
01:17:22,930 --> 01:17:26,320
which is used for the value of position three.

1262
01:17:26,320 --> 01:17:28,975
Um, so this was a piece of work, um,

1263
01:17:28,975 --> 01:17:33,030
done by sometimes CS224N co-instructor

1264
01:17:33,030 --> 01:17:37,620
Richard Socher and some of his people at Salesforce Research

1265
01:17:37,620 --> 01:17:40,110
on saying, how can we get the best of both worlds?

1266
01:17:40,110 --> 01:17:43,485
How can we get something that's kind of like a

1267
01:17:43,485 --> 01:17:49,650
recurrent neural network, but doesn't have the bad computational properties?

1268
01:17:49,650 --> 01:17:53,160
And so the idea that they had was, well,

1269
01:17:53,160 --> 01:18:00,550
rather than doing the standard LSTM style thing where you're calculating, you know,

1270
01:18:00,550 --> 01:18:07,090
an updated candidate value and your gates in terms of the preceding time slice,

1271
01:18:07,090 --> 01:18:13,510
maybe what instead we could do is we could stick a relation between time

1272
01:18:13,510 --> 01:18:20,155
minus 1 and time into the MaxPooling layer of a convolutional neural network.

1273
01:18:20,155 --> 01:18:26,260
So, we're sort of calculating a candidate and a forget gate and an output gate.

1274
01:18:26,260 --> 01:18:30,700
But these, these candidate and the, um,

1275
01:18:30,700 --> 01:18:38,500
gated values are done inside the pooling layer via compute,

1276
01:18:38,500 --> 01:18:44,350
um, via, um, uh, uh, convolutional operation.

1277
01:18:44,350 --> 01:18:46,150
So, it sort of get,

1278
01:18:46,150 --> 01:18:47,650
it doesn't, it, you know,

1279
01:18:47,650 --> 01:18:53,065
if there's no free lunch you can't get true recurrence and not pay the penalty.

1280
01:18:53,065 --> 01:18:56,764
This is giving you sort of a pseudo-recurrence because you are

1281
01:18:56,764 --> 01:19:02,244
modeling an association between adjacent elements at each time slice,

1282
01:19:02,244 --> 01:19:06,310
but it's sort of just worked out locally rather than being carried forward,

1283
01:19:06,310 --> 01:19:08,200
um, in one layer.

1284
01:19:08,200 --> 01:19:10,240
But sort of what they found is,

1285
01:19:10,240 --> 01:19:14,320
if you made your networks deeper using this idea,

1286
01:19:14,320 --> 01:19:15,970
well then, you sort of start to, again,

1287
01:19:15,970 --> 01:19:18,010
expand your window of influence.

1288
01:19:18,010 --> 01:19:22,090
So, you got a certain amount of information being carried forward.

1289
01:19:22,090 --> 01:19:25,330
Um, so, their conclusions was that you could sort of

1290
01:19:25,330 --> 01:19:28,870
build these kind of models and get them to work,

1291
01:19:28,870 --> 01:19:32,050
you know, not necessarily better actually on this slide,

1292
01:19:32,050 --> 01:19:33,625
um, it says often better.

1293
01:19:33,625 --> 01:19:37,540
Um, you can get them to work kind of as well as an LSTM does,

1294
01:19:37,540 --> 01:19:41,650
but you could get them to work much faster because you're avoiding

1295
01:19:41,650 --> 01:19:46,555
the standard recurrent operation and keeping it as something that you can parallelize,

1296
01:19:46,555 --> 01:19:49,945
um, in the MaxPooling operations.

1297
01:19:49,945 --> 01:19:53,035
Um, yes, so that was a kind of

1298
01:19:53,035 --> 01:19:57,250
an interesting alternative way of sort of trying to get some of the benefits.

1299
01:19:57,250 --> 01:20:01,825
I think long-term this isn't the idea that's going to end up winning out.

1300
01:20:01,825 --> 01:20:05,740
And so next week we're going to talk about transformer networks,

1301
01:20:05,740 --> 01:20:09,655
which actually seems to be the idea that's gained the most steam at the moment.

1302
01:20:09,655 --> 01:20:12,830
Okay. I'll stop there for today. Thanks a lot.

