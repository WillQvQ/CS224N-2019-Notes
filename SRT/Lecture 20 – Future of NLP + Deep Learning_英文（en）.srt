1
00:00:04,760 --> 00:00:09,570
Let's get started. So welcome to the very final lecture of the class.

2
00:00:09,570 --> 00:00:11,475
I hope you're all surviving the last week and,

3
00:00:11,475 --> 00:00:13,830
uh, wrapping up your projects.

4
00:00:13,830 --> 00:00:18,540
So today we're going to be hearing about the future of NLP and deep learning.

5
00:00:18,540 --> 00:00:22,440
Uh, so Chris is still traveling and today we're going to be having Kevin Clark,

6
00:00:22,440 --> 00:00:24,825
who's one of the PhD students in the lab, uh,

7
00:00:24,825 --> 00:00:26,580
in the NLP lab,

8
00:00:26,580 --> 00:00:29,610
and he was also one of the head TAs for the class last year.

9
00:00:29,610 --> 00:00:31,785
So he's very familiar with the class as a whole.

10
00:00:31,785 --> 00:00:33,765
Um, so, take it away Kevin.

11
00:00:33,765 --> 00:00:37,830
Okay. Thanks, Abby. Um, yeah,

12
00:00:37,830 --> 00:00:40,440
it's great to be back after being a TA last year.

13
00:00:40,440 --> 00:00:45,350
Um, I'm really excited today to be talking about the future of deep learning and NLP.

14
00:00:45,350 --> 00:00:49,090
Um, obviously, trying to forecast the future, um,

15
00:00:49,090 --> 00:00:51,800
for deep learning or anything in that space is really

16
00:00:51,800 --> 00:00:54,800
difficult because the field is changing super quickly.

17
00:00:54,800 --> 00:00:57,080
Um, so as one reference point, um,

18
00:00:57,080 --> 00:01:00,050
let's look at what did deep learning for NLP,

19
00:01:00,050 --> 00:01:02,290
um, look like about five years ago.

20
00:01:02,290 --> 00:01:08,300
And really, a lot of ideas that are now considered to be pretty core techniques,

21
00:01:08,300 --> 00:01:10,445
um, when we think of deep learning and NLP,

22
00:01:10,445 --> 00:01:12,170
um, didn't even exist back then.

23
00:01:12,170 --> 00:01:14,870
Um, so things you learned in this class like Seq2Seq,

24
00:01:14,870 --> 00:01:17,180
attention mechanism, um, large-scale,

25
00:01:17,180 --> 00:01:20,105
reading comprehension, uh, even frameworks

26
00:01:20,105 --> 00:01:23,300
such as TensorFlow or Pytorch, um, didn't exist.

27
00:01:23,300 --> 00:01:27,145
And, uh, the point I want to make with this is that, um,

28
00:01:27,145 --> 00:01:31,205
because of this it's really difficult to, to look into the future and say,

29
00:01:31,205 --> 00:01:33,665
okay, what are things going to be like?

30
00:01:33,665 --> 00:01:38,065
Um, what I think we can do though is look at, um,

31
00:01:38,065 --> 00:01:41,870
areas that right now are really sort of taking off, um,

32
00:01:41,870 --> 00:01:43,640
so areas in which, um,

33
00:01:43,640 --> 00:01:46,370
there's a lot, been a lot of recent success and kind of, uh,

34
00:01:46,370 --> 00:01:48,100
project from that, that,

35
00:01:48,100 --> 00:01:50,900
those same areas will likely be important in the future.

36
00:01:50,900 --> 00:01:55,820
Um, and in this talk I'm going to be mostly focusing on one key idea of

37
00:01:55,820 --> 00:01:58,970
wh- key idea which is the idea of leveraging

38
00:01:58,970 --> 00:02:02,915
unlabeled examples when training our NLP systems.

39
00:02:02,915 --> 00:02:07,490
So I'll be talking a bit about doing that for machine translation, um,

40
00:02:07,490 --> 00:02:10,820
both in improving the quality of translation and even

41
00:02:10,820 --> 00:02:14,315
in doing a translation in an unsupervised way.

42
00:02:14,315 --> 00:02:16,170
So that means you don't have, um,

43
00:02:16,170 --> 00:02:19,230
paired sentences, uh, with, with their translations.

44
00:02:19,230 --> 00:02:23,365
Um, you try to learn a translation model only from a monolingual corpus.

45
00:02:23,365 --> 00:02:27,115
Um, the second thing I'll be talking a little bit about is, uh,

46
00:02:27,115 --> 00:02:29,330
OpenAI's GPT-2, um,

47
00:02:29,330 --> 00:02:32,435
and in general this phenomenon of really scaling up,

48
00:02:32,435 --> 00:02:34,045
um, deep learning models.

49
00:02:34,045 --> 00:02:38,330
Um, I know you saw a little bit of this in the lecture on contextual representations,

50
00:02:38,330 --> 00:02:40,340
but this, but this will be a little bit more in depth.

51
00:02:40,340 --> 00:02:42,335
Um, and I think, um,

52
00:02:42,335 --> 00:02:46,655
these new developments in NLP have had some,

53
00:02:46,655 --> 00:02:48,600
um, pretty big, uh,

54
00:02:48,600 --> 00:02:50,595
impacts in terms of,

55
00:02:50,595 --> 00:02:53,755
uh, more broadly kind of beyond even the technology we're using,

56
00:02:53,755 --> 00:02:55,070
and in particular, I mean,

57
00:02:55,070 --> 00:03:00,555
starting to raise more and more concerns about the social impact of NLP, um,

58
00:03:00,555 --> 00:03:03,520
both, um, in what our models can do and also in kind

59
00:03:03,520 --> 00:03:06,590
of plans of what, where people are looking to apply these models, um,

60
00:03:06,590 --> 00:03:09,755
and I think that really has some risks associated with it, um,

61
00:03:09,755 --> 00:03:13,160
in terms of security also in terms of areas like bias.

62
00:03:13,160 --> 00:03:16,465
Um, I'm also gonna talk a bit about future areas of research,

63
00:03:16,465 --> 00:03:19,140
um, these are mostly research areas now that are, um,

64
00:03:19,140 --> 00:03:22,060
over the past year have really kind of developed into

65
00:03:22,060 --> 00:03:27,180
promising areas and I expect they will continue to be important in the future.

66
00:03:27,190 --> 00:03:29,700
Okay, um, to start with,

67
00:03:29,700 --> 00:03:33,305
I wanna ask this question, why has deep learning been so successful recently?

68
00:03:33,305 --> 00:03:35,510
Um, I like this comic, um,

69
00:03:35,510 --> 00:03:38,045
here there's a statistical learning person,

70
00:03:38,045 --> 00:03:41,030
um, and they've got some really complicated,

71
00:03:41,030 --> 00:03:44,015
um, well-motivated, uh, method for doing, um,

72
00:03:44,015 --> 00:03:45,510
the task they care about,

73
00:03:45,510 --> 00:03:47,460
and then the neural net person just says,

74
00:03:47,460 --> 00:03:49,110
er, stack more layers.

75
00:03:49,110 --> 00:03:52,020
Um, so, so the point I want to make here is that, um,

76
00:03:52,020 --> 00:03:56,075
deep learning has not been successful recently because it's more

77
00:03:56,075 --> 00:04:01,530
theoretically motivated or it's more sophisticated than previous techniques, um.

78
00:04:01,530 --> 00:04:04,245
In fact I would say that actually a lot of, um,

79
00:04:04,245 --> 00:04:06,635
older statistical methods have more of

80
00:04:06,635 --> 00:04:10,175
a theoretical underpinning than some of the tricks we do in deep learning.

81
00:04:10,175 --> 00:04:14,050
Um, really the thing that makes deep learning so

82
00:04:14,050 --> 00:04:17,660
successful in recent years has been its ability to scale, right.

83
00:04:17,660 --> 00:04:22,310
So neural nets, as we increase the size of the data,

84
00:04:22,310 --> 00:04:24,265
as we increase the size of the models, um,

85
00:04:24,265 --> 00:04:26,170
they get a really big boost in accuracy,

86
00:04:26,170 --> 00:04:28,565
in ways other approaches do not.

87
00:04:28,565 --> 00:04:32,210
And, um, if you look to the '80s and '90s, um,

88
00:04:32,210 --> 00:04:36,190
there was actually plenty of research in neural nets going on, um.

89
00:04:36,190 --> 00:04:39,130
But it hadn't, doesn't have a hype around it that it does

90
00:04:39,130 --> 00:04:42,205
now and that seems likely to be because,

91
00:04:42,205 --> 00:04:45,020
um, in the past there wasn't, um,

92
00:04:45,020 --> 00:04:47,180
the same resources in terms of computers,

93
00:04:47,180 --> 00:04:49,235
in terms of data and, um,

94
00:04:49,235 --> 00:04:53,120
only now after we've reached sort of an inflection point where we can

95
00:04:53,120 --> 00:04:55,220
really take advantage of scale in

96
00:04:55,220 --> 00:04:57,965
our deep learning models and we started to see it become,

97
00:04:57,965 --> 00:05:01,520
um, a really successful paradigm for machine learning.

98
00:05:01,520 --> 00:05:04,080
Um, if we look at big, uh,

99
00:05:04,080 --> 00:05:06,075
deep learning success stories, um,

100
00:05:06,075 --> 00:05:10,200
I think, uh, you can see kind of this idea play out, right?

101
00:05:10,200 --> 00:05:16,490
So here are three of what are arguably the most famous successes of deep learning, right.

102
00:05:16,490 --> 00:05:18,620
So there's image recognition, where before,

103
00:05:18,620 --> 00:05:20,870
people used very highly engineered, um,

104
00:05:20,870 --> 00:05:25,870
features to classify images and now neural nets are much superior, um, to those methods.

105
00:05:25,870 --> 00:05:29,785
Um, machine translation has really closed the gap between, um,

106
00:05:29,785 --> 00:05:33,020
phrase-based systems and human quality translation,

107
00:05:33,020 --> 00:05:35,730
so this is widely used in things like Google Translate

108
00:05:35,730 --> 00:05:39,115
and the quality has actually gotten a lot better over the past five years.

109
00:05:39,115 --> 00:05:43,550
Um, another example that had a lot of hype around it is game-playing, so, um,

110
00:05:43,550 --> 00:05:46,460
there's been work on Atari games, there's been AlphaGo,

111
00:05:46,460 --> 00:05:50,395
uh, more recently there's been AlphaStar and OpenAI Five.

112
00:05:50,395 --> 00:05:53,599
Um, if you look at all three of these cases underlying

113
00:05:53,599 --> 00:05:57,200
these successes is really large amounts of data, right.

114
00:05:57,200 --> 00:05:58,550
So for ImageNet, um,

115
00:05:58,550 --> 00:06:00,020
for image recognition, um,

116
00:06:00,020 --> 00:06:03,035
there is the ImageNet dataset which has 14 million images,

117
00:06:03,035 --> 00:06:06,320
uh, machine translation datasets often have millions of examples.

118
00:06:06,320 --> 00:06:09,275
Um, for game playing you can actually

119
00:06:09,275 --> 00:06:12,470
generate as much training data as you want essentially,

120
00:06:12,470 --> 00:06:14,690
um, just by running your agent,

121
00:06:14,690 --> 00:06:16,040
um, within the game,

122
00:06:16,040 --> 00:06:18,660
um, over and over again.

123
00:06:19,120 --> 00:06:21,360
Um, so if we,

124
00:06:21,360 --> 00:06:23,590
if we look to NLP, um,

125
00:06:23,590 --> 00:06:27,740
the story is quite a bit different for a lot of tasks, um, right.

126
00:06:27,740 --> 00:06:32,030
So if you look at even pretty core kind of popular tasks,

127
00:06:32,030 --> 00:06:35,060
to say, reading comprehension in English, um,

128
00:06:35,060 --> 00:06:39,710
datasets like SQuAD are in the order of like 100,000 examples

129
00:06:39,710 --> 00:06:44,810
which is considerably less than the millions or tens of millions of examples,

130
00:06:44,810 --> 00:06:47,105
um, that these previous,

131
00:06:47,105 --> 00:06:50,285
um, successes have, have benefited from.

132
00:06:50,285 --> 00:06:54,210
Um, and that's of course only for English, right.

133
00:06:54,210 --> 00:06:55,770
Um, there are, um,

134
00:06:55,770 --> 00:06:59,570
thousands of other languages and this is I think

135
00:06:59,570 --> 00:07:03,770
a problem with NLP data as it exists today.

136
00:07:03,770 --> 00:07:06,455
Um, the vast majority of data is in English, um,

137
00:07:06,455 --> 00:07:10,070
when in reality fewer than 10% of the world's population,

138
00:07:10,070 --> 00:07:12,190
um, speak English as their first language.

139
00:07:12,190 --> 00:07:17,565
Um, so these problems with small datasets are only compounded if you look at,

140
00:07:17,565 --> 00:07:21,465
um, the full spectrum of languages, um, that exist.

141
00:07:21,465 --> 00:07:23,955
Um, so, as what do we do,

142
00:07:23,955 --> 00:07:25,800
uh, when we're limited by this data,

143
00:07:25,800 --> 00:07:30,560
but we want to take advantage of deep learning scale and train the biggest models we can.

144
00:07:30,560 --> 00:07:32,510
Um, the popular solution, um,

145
00:07:32,510 --> 00:07:36,230
that's especially had recent success is using unlabeled data, um,

146
00:07:36,230 --> 00:07:37,805
because unlike labeled data,

147
00:07:37,805 --> 00:07:40,840
unlabeled data is very easy to acquire for language.

148
00:07:40,840 --> 00:07:42,120
Um, you can just go to the Internet,

149
00:07:42,120 --> 00:07:44,685
you can go to books, you can get lots of text, um,

150
00:07:44,685 --> 00:07:49,370
whereas labeled data usually requires at the least crowdsourcing examples.

151
00:07:49,370 --> 00:07:54,730
Um, in some cases you even require someone who's an expert in something like linguistics,

152
00:07:54,730 --> 00:07:57,820
um, to, to annotate that data.

153
00:07:59,510 --> 00:08:03,890
Okay, so, um, this first part of the talk is going to be applying

154
00:08:03,890 --> 00:08:08,195
this idea of leveraging unlabeled data to improve our NLP models,

155
00:08:08,195 --> 00:08:11,070
um, to the task of machine translation.

156
00:08:11,990 --> 00:08:15,170
Um, so let's talk about machine translation data.

157
00:08:15,170 --> 00:08:20,520
Um, it is true that there do exist quite large datasets for machine translation.

158
00:08:20,520 --> 00:08:23,165
Um, those datasets don't exist because

159
00:08:23,165 --> 00:08:26,870
NLP researchers have annotated texts for the purpose of training their models, right.

160
00:08:26,870 --> 00:08:29,750
They exist because, er, in various settings,

161
00:08:29,750 --> 00:08:33,195
translation is done just because it's useful, so for example,

162
00:08:33,195 --> 00:08:35,070
proceedings of the European Parliament,

163
00:08:35,070 --> 00:08:37,019
um, proceedings of the United Nations,

164
00:08:37,019 --> 00:08:41,320
um, some, uh, news sites, they translate their articles into many languages.

165
00:08:41,320 --> 00:08:46,610
Um, so really, the machine translation data we use to train our models are often

166
00:08:46,610 --> 00:08:52,745
more of byproducts of existing cases where translation is wanted rather than,

167
00:08:52,745 --> 00:08:57,500
um, kind of a full sampling of the sort of text we see in the world.

168
00:08:57,500 --> 00:08:58,910
Um, so that means number one,

169
00:08:58,910 --> 00:09:00,680
it's quite limited in domain, right.

170
00:09:00,680 --> 00:09:03,580
So it's not easy to find translated tweets,

171
00:09:03,580 --> 00:09:05,410
um, unless you happen to work for Twitter.

172
00:09:05,410 --> 00:09:08,145
Um, in addition to that, um,

173
00:09:08,145 --> 00:09:12,230
there's limitations in terms of the languages that are covered, right.

174
00:09:12,230 --> 00:09:14,750
So some languages, say European languages,

175
00:09:14,750 --> 00:09:16,500
there's a lot of translation data, um,

176
00:09:16,500 --> 00:09:19,180
for other languages there's much less.

177
00:09:19,180 --> 00:09:22,040
Um, so in these settings where we want to work on

178
00:09:22,040 --> 00:09:25,220
a different domain or where we want to work with a low resource language,

179
00:09:25,220 --> 00:09:27,995
um, we're limited by labeled data, um,

180
00:09:27,995 --> 00:09:30,985
but what we can do is pretty easily find unlabeled data.

181
00:09:30,985 --> 00:09:33,620
Um, so it's actually a pretty solved problem, um,

182
00:09:33,620 --> 00:09:37,010
maybe not 100%, but we can with good accuracy look at

183
00:09:37,010 --> 00:09:41,195
some text and decide what language it's in and train a classifier to do that.

184
00:09:41,195 --> 00:09:43,610
Um, so this means it's really easy to find

185
00:09:43,610 --> 00:09:46,100
data in any language you care about because you can just go on

186
00:09:46,100 --> 00:09:48,440
the web and essentially search for data in

187
00:09:48,440 --> 00:09:52,650
that language and acquire a large corpus of monolingual data.

188
00:09:55,240 --> 00:10:00,775
Okay, um, I'm now going into the first approach,

189
00:10:00,775 --> 00:10:03,095
um, I'm going to talk about on using

190
00:10:03,095 --> 00:10:06,365
unlabeled data to improve machine translation models.

191
00:10:06,365 --> 00:10:09,410
Um, this technique is called pre-training and it's

192
00:10:09,410 --> 00:10:12,790
really reminiscent of ideas like, um, ELMo.

193
00:10:12,790 --> 00:10:16,580
Um, the idea is to pre-train by doing language modeling.

194
00:10:16,580 --> 00:10:18,350
So if we have, um,

195
00:10:18,350 --> 00:10:21,350
two languages we're interested in translating,

196
00:10:21,350 --> 00:10:22,535
um, from one end to the other,

197
00:10:22,535 --> 00:10:27,480
we'll collect large datasets for both of those languages and then we can train,

198
00:10:27,480 --> 00:10:29,040
uh, two language models,

199
00:10:29,040 --> 00:10:33,365
one each on that data and then, um,

200
00:10:33,365 --> 00:10:34,490
we can use those, uh,

201
00:10:34,490 --> 00:10:38,450
pre-trained language models as initialization for a machine translation system.

202
00:10:38,450 --> 00:10:41,720
Um, so the encoder will get initialized with

203
00:10:41,720 --> 00:10:45,485
the weights of the language model trained on the source side language, um,

204
00:10:45,485 --> 00:10:49,830
the decoder will get initialized with weights trained on the target size language, uh,

205
00:10:49,830 --> 00:10:51,225
and this will, um,

206
00:10:51,225 --> 00:10:55,490
improve the performance of your model because during this pre-training, um,

207
00:10:55,490 --> 00:10:59,750
we hope that our language models will be learning useful information such as, you know,

208
00:10:59,750 --> 00:11:02,460
the meaning of words or, um, uh,

209
00:11:02,460 --> 00:11:05,250
the kind of structure of the language, um,

210
00:11:05,250 --> 00:11:09,020
they are processing, um, and this can, uh,

211
00:11:09,020 --> 00:11:12,410
down the line help the machine translation model,

212
00:11:12,410 --> 00:11:15,020
um, when we fine tune it.

213
00:11:15,020 --> 00:11:17,464
Um, let me pause here and ask if there are any questions,

214
00:11:17,464 --> 00:11:18,620
and just in general, feel,

215
00:11:18,620 --> 00:11:24,870
feel free to ask questions throughout this talk. Okay.

216
00:11:25,920 --> 00:11:33,385
So, so here is a plot showing some results of this pre-training technique.

217
00:11:33,385 --> 00:11:36,040
Um, so this is English to German translation.

218
00:11:36,040 --> 00:11:39,805
Uh, the x-axis is how much training data,

219
00:11:39,805 --> 00:11:41,920
as in unsupervised training data, um,

220
00:11:41,920 --> 00:11:43,075
you provide these models,

221
00:11:43,075 --> 00:11:45,355
but of course they also have large amounts

222
00:11:45,355 --> 00:11:48,940
of monolingual data for this pre-training step.

223
00:11:48,940 --> 00:11:51,970
And you can see that this works pretty well, right?

224
00:11:51,970 --> 00:11:54,445
So you've got about two blue points, um,

225
00:11:54,445 --> 00:11:57,670
increase in performance, so that's this red line above the blue line,

226
00:11:57,670 --> 00:12:00,175
um, when doing this pre-training technique.

227
00:12:00,175 --> 00:12:01,689
And not too surprisingly,

228
00:12:01,689 --> 00:12:06,740
this gain is especially large when the amount of labeled data is small.

229
00:12:10,350 --> 00:12:14,080
Um, there is a problem with,

230
00:12:14,080 --> 00:12:17,260
uh, pre-training which I want to address, which is that, uh,

231
00:12:17,260 --> 00:12:18,850
in pre-training, you have

232
00:12:18,850 --> 00:12:20,890
these two separate language models and there's never

233
00:12:20,890 --> 00:12:23,035
really any interaction between the two,

234
00:12:23,035 --> 00:12:25,780
um, when you're running them on the unlabeled corpus.

235
00:12:25,780 --> 00:12:28,435
Um, so here's a simple technique, um,

236
00:12:28,435 --> 00:12:32,485
that tries to solve this problem and it's called self-training.

237
00:12:32,485 --> 00:12:37,090
Um, the idea is given a sentence from our monolingual corpus,

238
00:12:37,090 --> 00:12:40,210
so in this case, "I traveled to Belgium," that's an English sentence.

239
00:12:40,210 --> 00:12:45,400
Um, we won't have a human provided translation for this sentence, uh,

240
00:12:45,400 --> 00:12:48,925
but what we can do is we can run our machine translation model,

241
00:12:48,925 --> 00:12:52,750
and we'll get a translation in the target language.

242
00:12:52,750 --> 00:12:56,320
Um, since this is from a machine learning model it won't be perfect, uh,

243
00:12:56,320 --> 00:13:00,160
but we can hope that maybe our model can still learn from this kind

244
00:13:00,160 --> 00:13:03,580
of noisy labeled example, right?

245
00:13:03,580 --> 00:13:05,275
So we, we treat, um,

246
00:13:05,275 --> 00:13:08,230
our original monolingual sentence and it's machine-provided

247
00:13:08,230 --> 00:13:12,490
translation as though it were a human-provided translation and,

248
00:13:12,490 --> 00:13:17,030
uh, train our machine learning model as normal on this example.

249
00:13:19,800 --> 00:13:24,190
Um, I think this seems pretty strange actually as- as

250
00:13:24,190 --> 00:13:27,970
a method when you first see it because it seems really circular, right?

251
00:13:27,970 --> 00:13:31,315
So if you look at this, um, the, uh,

252
00:13:31,315 --> 00:13:33,850
translation that the model is being trained to

253
00:13:33,850 --> 00:13:38,095
produce is actually exactly what it already produces to begin with,

254
00:13:38,095 --> 00:13:43,420
right, because, um, this translation came from our model in the first place.

255
00:13:43,420 --> 00:13:45,700
Um, so actually in practice,

256
00:13:45,700 --> 00:13:49,480
this is not a technique that's very widely used due to this problem,

257
00:13:49,480 --> 00:13:53,365
um, but it motivates another technique called back-translation.

258
00:13:53,365 --> 00:13:56,740
And this technique is really a very popular, um,

259
00:13:56,740 --> 00:13:59,950
solution to that problem, and it's the method, um,

260
00:13:59,950 --> 00:14:04,240
that has had a lot of success in using unlabeled data for translation.

261
00:14:04,240 --> 00:14:06,940
So here's the approach rather than only

262
00:14:06,940 --> 00:14:10,855
having our translation system that goes from source language to target language,

263
00:14:10,855 --> 00:14:13,210
um, we're also going to train a model that

264
00:14:13,210 --> 00:14:16,375
goes from our target language to our source language.

265
00:14:16,375 --> 00:14:18,670
And so in this case, if,

266
00:14:18,670 --> 00:14:21,340
if at the end of the day we want a French to English model, um,

267
00:14:21,340 --> 00:14:24,910
we're gonna start by actually training an English to French model.

268
00:14:24,910 --> 00:14:27,880
And then we can do something that's a lot like self-labeling.

269
00:14:27,880 --> 00:14:30,205
So we take a English sentence.

270
00:14:30,205 --> 00:14:33,370
We run our English to French model and translate it.

271
00:14:33,370 --> 00:14:35,950
The difference to what we did before is that

272
00:14:35,950 --> 00:14:38,500
we're actually going to switch the source and target side.

273
00:14:38,500 --> 00:14:42,640
So now in this case the French sentence is the source sequence.

274
00:14:42,640 --> 00:14:45,985
Uh, the target sequence is, um,

275
00:14:45,985 --> 00:14:50,740
our original English sentence that came from  monolingual corpora.

276
00:14:50,740 --> 00:14:52,165
And now we're training the language, uh,

277
00:14:52,165 --> 00:14:54,040
the machine translation system that goes

278
00:14:54,040 --> 00:14:57,265
the other direction so that goes French to English.

279
00:14:57,265 --> 00:15:00,445
Um, so, so why do we think this will work better?

280
00:15:00,445 --> 00:15:02,320
Um, number one, um,

281
00:15:02,320 --> 00:15:05,230
there's no longer this kind of circularity to the training

282
00:15:05,230 --> 00:15:10,210
because what the model is being trained on is the output of a completely different model.

283
00:15:10,210 --> 00:15:14,845
Um, another thing that I think is pretty crucial here is that,

284
00:15:14,845 --> 00:15:18,970
um, the translations, the model is trained to produce.

285
00:15:18,970 --> 00:15:21,520
So the things that the decoder is actually learning to

286
00:15:21,520 --> 00:15:24,430
generate are never bad translations, right?

287
00:15:24,430 --> 00:15:26,574
So if you look at this example,

288
00:15:26,574 --> 00:15:29,545
the target sequence for our French to English model,

289
00:15:29,545 --> 00:15:31,165
I traveled to Belgium, um,

290
00:15:31,165 --> 00:15:34,645
that originally came from a monolingual corpus.

291
00:15:34,645 --> 00:15:37,420
Um, so I think intuitively this makes sense is

292
00:15:37,420 --> 00:15:40,435
that if we want to train a good translation model,

293
00:15:40,435 --> 00:15:44,620
um, it's probably okay to expose it to noisy inputs.

294
00:15:44,620 --> 00:15:47,515
So we expose it to the output of a system that's English to French,

295
00:15:47,515 --> 00:15:48,730
it might not be perfect.

296
00:15:48,730 --> 00:15:52,330
Um, but what we don't want to do is um, expose it to

297
00:15:52,330 --> 00:15:54,850
poor target sequences because then it

298
00:15:54,850 --> 00:15:58,370
won't learn how to generate in that language effectively.

299
00:15:58,560 --> 00:16:04,300
Any questions on back-translation before I get to results? Um, sure.

300
00:16:04,300 --> 00:16:08,980
[BACKGROUND]

301
00:16:08,980 --> 00:16:11,500
So this is assuming we have a large corpus of

302
00:16:11,500 --> 00:16:16,700
unlabeled data and we want to be using it to help our translation model.

303
00:16:17,330 --> 00:16:19,875
Does that, does that make sense?

304
00:16:19,875 --> 00:16:23,340
Um, maybe you could clarify the question.

305
00:16:23,340 --> 00:16:29,160
[BACKGROUND]

306
00:16:29,160 --> 00:16:32,830
Yeah, that's right. So we have a big corpus of English which includes the sentence,

307
00:16:32,830 --> 00:16:36,190
"I traveled to Belgium," and we don't know the translations but we'd still like to

308
00:16:36,190 --> 00:16:39,630
use this data. Yeah, another question.

309
00:16:39,630 --> 00:16:45,280
[BACKGROUND]

310
00:16:45,280 --> 00:16:47,110
Yeah, so that's a good question is how do you

311
00:16:47,110 --> 00:16:52,300
avoid both the models let's say sort of blowing up and producing garbage?

312
00:16:52,300 --> 00:16:54,400
And then they're just feeding garbage to each other.

313
00:16:54,400 --> 00:16:57,820
The answer is that there is some amount of labeled data here as well.

314
00:16:57,820 --> 00:17:00,820
So on unlabeled data you do this, but on labeled data,

315
00:17:00,820 --> 00:17:02,110
you do standard training,

316
00:17:02,110 --> 00:17:04,795
and that way you avoid, you,

317
00:17:04,795 --> 00:17:07,900
you make sure you kind of keep the models on track because they still have to fit to

318
00:17:07,900 --> 00:17:12,175
the labeled data. Yeah, another question.

319
00:17:12,175 --> 00:17:15,475
How do you schedule the training of the two models?

320
00:17:15,475 --> 00:17:17,500
Yeah, that is a good question.

321
00:17:17,500 --> 00:17:21,580
And I think that's basically almost like a hyper-parameter you can tweak.

322
00:17:21,580 --> 00:17:25,720
So I think a pretty common thing to do is first,

323
00:17:25,720 --> 00:17:28,270
train two models only on labeled data.

324
00:17:28,270 --> 00:17:32,965
Then label, um, so then do back-translation

325
00:17:32,965 --> 00:17:37,480
over a large corpus and kind of repeat that process over and over again.

326
00:17:37,480 --> 00:17:40,165
So each iteration, you train on the label data,

327
00:17:40,165 --> 00:17:43,510
label some unlabeled data and now you have more data to work with.

328
00:17:43,510 --> 00:17:46,270
But I think there'd be many kinds of scheduling that would be effective

329
00:17:46,270 --> 00:17:50,380
here. Okay. Another question.

330
00:17:50,380 --> 00:18:06,100
I'm curious as to the evaluation, considering if you have a very good French to English model, you could try to look up, or contest if you have a good French to English model, you could try to look up the original source and see if it matches.

331
00:18:06,100 --> 00:18:07,435
Yeah, I'm not, I'm not quite sure.

332
00:18:07,435 --> 00:18:10,135
Are you suggesting going like English to French to English and seeing if?

333
00:18:10,135 --> 00:18:11,635
I see, yeah, yeah,

334
00:18:11,635 --> 00:18:12,775
that's a really interesting idea.

335
00:18:12,775 --> 00:18:15,775
And we're actually going to talk a little bit about this sort of,

336
00:18:15,775 --> 00:18:17,290
it's called cycle consistency,

337
00:18:17,290 --> 00:18:20,000
this idea later in this talk.

338
00:18:20,970 --> 00:18:23,770
Okay, I'm going to move on to the results.

339
00:18:23,770 --> 00:18:28,120
So, so here's the method for using unlabeled data to improve translation.

340
00:18:28,120 --> 00:18:29,890
How well does it do?

341
00:18:29,890 --> 00:18:33,220
Um, the answer is that the improvements are at least to me, they

342
00:18:33,220 --> 00:18:36,490
were surprisingly extremely good, right?

343
00:18:36,490 --> 00:18:39,445
So, um, this is for English to German translation.

344
00:18:39,445 --> 00:18:44,515
This is from some work by Facebook, so they  used 5 million labeled sentence pairs.

345
00:18:44,515 --> 00:18:52,040
But they also used 230 monolingual sentences, so sentences without translations.

346
00:18:52,290 --> 00:18:56,425
And you can see that compared to previous state of the art,

347
00:18:56,425 --> 00:18:59,755
they get six BLEU points improvement which, um,

348
00:18:59,755 --> 00:19:03,010
if you compare it to most previous research and machine tran- machine translation

349
00:19:03,010 --> 00:19:04,180
is a really big gain, right?

350
00:19:04,180 --> 00:19:08,020
So even something like the invention of the transformer which most people would

351
00:19:08,020 --> 00:19:13,165
consider to be a really significant research development in NLP,

352
00:19:13,165 --> 00:19:16,825
that improved over prior work by about 2,5 BLEU points.

353
00:19:16,825 --> 00:19:22,330
And here without doing any sort of fancy model design just by using way more data,

354
00:19:22,330 --> 00:19:25,400
um, we get actually much larger improvements.

355
00:19:29,130 --> 00:19:34,390
Okay. So an interesting question to think about,

356
00:19:34,390 --> 00:19:38,125
um, is suppose we only have our monolingual corpora.

357
00:19:38,125 --> 00:19:41,155
So we don't have any sentences that had been human translated.

358
00:19:41,155 --> 00:19:43,390
We just have sentences in two languages.

359
00:19:43,390 --> 00:19:47,080
Um, so the scenario you can sort of imagine is suppose,

360
00:19:47,080 --> 00:19:48,985
um, an alien comes down and,

361
00:19:48,985 --> 00:19:50,740
um, starts talking to you and it's a

362
00:19:50,740 --> 00:19:53,965
weird alien language, um, and it talks a lot,

363
00:19:53,965 --> 00:19:58,120
would you eventually be able to translate what it's saying to English,

364
00:19:58,120 --> 00:20:01,670
um, just by having a really large amount of data?

365
00:20:03,300 --> 00:20:06,205
Um, so I'm going to start with, um,

366
00:20:06,205 --> 00:20:11,935
a simpler task than full-on translating when you only have unlabeled sentences.

367
00:20:11,935 --> 00:20:15,220
Um, instead of doing sentence to sentence translation,

368
00:20:15,220 --> 00:20:18,640
let's start by only worrying about word to word translation.

369
00:20:18,640 --> 00:20:21,490
So the goal here is given a word in one language,

370
00:20:21,490 --> 00:20:25,330
find its translation but without using any labeled data.

371
00:20:25,330 --> 00:20:27,100
Um, and the method,

372
00:20:27,100 --> 00:20:29,440
the method we're going to use to try to solve

373
00:20:29,440 --> 00:20:33,460
this task is called, uh, cross-lingual embeddings.

374
00:20:33,460 --> 00:20:35,830
Um, so the goal is to learn, uh,

375
00:20:35,830 --> 00:20:39,265
word vectors for words in both languages,

376
00:20:39,265 --> 00:20:41,935
and we'd like those word vectors to have

377
00:20:41,935 --> 00:20:45,550
all the nice properties you've already learned about word vectors having, um,

378
00:20:45,550 --> 00:20:49,149
but we also want word vectors for a particular language,

379
00:20:49,149 --> 00:20:52,860
um, to be close to the word vector of its translation.

380
00:20:52,860 --> 00:20:57,090
Um, so I'm not sure if it's visible in this figure but this fis- figure shows

381
00:20:57,090 --> 00:21:02,475
a large number of English and I think German words and you can see that,

382
00:21:02,475 --> 00:21:07,795
um, uh, the each English word has its corresponding German word,

383
00:21:07,795 --> 00:21:10,330
um, nearby to it in its embedding space.

384
00:21:10,330 --> 00:21:15,010
So if we learn embeddings like this then it's pretty easy to do word to word translation.

385
00:21:15,010 --> 00:21:16,705
Um, we just pick an English word,

386
00:21:16,705 --> 00:21:18,550
we find the nearest, uh,

387
00:21:18,550 --> 00:21:22,075
German word in this joint embedding space

388
00:21:22,075 --> 00:21:26,030
and that will give us a translation for the English word.

389
00:21:28,470 --> 00:21:32,185
Um, our key method for or the key

390
00:21:32,185 --> 00:21:35,500
assumption that we're going to be using to solve this is that,

391
00:21:35,500 --> 00:21:40,870
um, th- even though if you run word2vec twice you'll get really different embeddings.

392
00:21:40,870 --> 00:21:46,930
Um, the structure of that embedding space has a lot of regularity to it,

393
00:21:46,930 --> 00:21:49,675
and we can take advantage of that regularity, um,

394
00:21:49,675 --> 00:21:51,700
to help find when,

395
00:21:51,700 --> 00:21:54,370
um, an alignment between those embedding spaces.

396
00:21:54,370 --> 00:21:56,830
So to be kind of more concrete here.

397
00:21:56,830 --> 00:21:59,560
Here is a picture of two sets of word embeddings.

398
00:21:59,560 --> 00:22:00,820
So in red, we have, um,

399
00:22:00,820 --> 00:22:02,655
English words, in, uh,

400
00:22:02,655 --> 00:22:04,565
blue we have Italian words,

401
00:22:04,565 --> 00:22:09,280
and although, um, the vector spaces right now look very different to each other,

402
00:22:09,280 --> 00:22:12,400
um, you can see that they have a really similar structure, right?

403
00:22:12,400 --> 00:22:16,735
So you'd imagine distances are kind of similar that the distance from,

404
00:22:16,735 --> 00:22:19,345
uh, cat and feline in the, um,

405
00:22:19,345 --> 00:22:22,570
English embedding space should be pretty similar to the distance

406
00:22:22,570 --> 00:22:27,880
between gatto and felino in the, um, Italian space.

407
00:22:27,880 --> 00:22:34,310
Um, this kind of motivates an algorithm for learning these cross-lingual embeddings.

408
00:22:35,400 --> 00:22:38,440
Um, so here's the idea.

409
00:22:38,440 --> 00:22:40,960
What we're going to try to do is learn what's essentially

410
00:22:40,960 --> 00:22:44,080
a rotation such that we can transform,

411
00:22:44,080 --> 00:22:46,660
um, our set of English embeddings so

412
00:22:46,660 --> 00:22:50,515
that they match up with our Italian embe- embeddings.

413
00:22:50,515 --> 00:22:52,780
So mathematically, what this means is we're gonna learn

414
00:22:52,780 --> 00:22:55,660
a matrix W such that if we take let's say,

415
00:22:55,660 --> 00:23:00,355
uh, the word vector for cat in English and we multiply it by W, um,

416
00:23:00,355 --> 00:23:06,205
we end up with the vector for gatto in Spanish or Italian,

417
00:23:06,205 --> 00:23:09,550
um, and a detail here is that, um,

418
00:23:09,550 --> 00:23:12,580
we're going to constrain W to be orthogonal, um,

419
00:23:12,580 --> 00:23:15,070
and what that means geometrically is just that W is

420
00:23:15,070 --> 00:23:17,980
only going to be doing a rotation to the,

421
00:23:17,980 --> 00:23:19,945
uh, vectors, um, in X.

422
00:23:19,945 --> 00:23:24,320
It's not going to be doing some other weirder transformation.

423
00:23:24,870 --> 00:23:29,305
So this is our goal is to learn this W. Um,

424
00:23:29,305 --> 00:23:31,000
next I'm gonna talk about,

425
00:23:31,000 --> 00:23:36,985
talking about how actually do we learn this W. Um,

426
00:23:36,985 --> 00:23:41,665
and there's actually a bunch of techniques for learning this W matrix,

427
00:23:41,665 --> 00:23:44,740
um, but, um, here is one of

428
00:23:44,740 --> 00:23:48,310
them that I think is quite clever is called adversarial training.

429
00:23:48,310 --> 00:23:50,635
Um, so it works as follows,

430
00:23:50,635 --> 00:23:53,770
is in addition to trying to learn this W matrix,

431
00:23:53,770 --> 00:23:57,670
we're also going to be trying to learn a model that, uh,

432
00:23:57,670 --> 00:23:58,915
is called a discriminator,

433
00:23:58,915 --> 00:24:02,800
and what it'll do is take a vector and it will try to predict,

434
00:24:02,800 --> 00:24:05,080
is that vector originally, um,

435
00:24:05,080 --> 00:24:08,830
an English word embedding or is it originally an Italian word embedding?

436
00:24:08,830 --> 00:24:11,425
Um, in other words, if you think about, um,

437
00:24:11,425 --> 00:24:14,920
the diagram, what we're asking our discriminator to do is, uh,

438
00:24:14,920 --> 00:24:17,680
it's given one of these points and it's trying to predict is it

439
00:24:17,680 --> 00:24:21,055
basically a red point so an English word originally, or is it a blue point?

440
00:24:21,055 --> 00:24:24,010
Um, so if we have no W matrix and this is

441
00:24:24,010 --> 00:24:27,190
a really easy task for the discriminator because,

442
00:24:27,190 --> 00:24:32,425
um, the, uh, word embeddings for English and Italian are clearly separated.

443
00:24:32,425 --> 00:24:36,130
Um, however, if we learn a W matrix

444
00:24:36,130 --> 00:24:39,955
that succeeds in aligning all these embeddings on top of each other,

445
00:24:39,955 --> 00:24:43,270
then our discriminator will never do a good job, right.

446
00:24:43,270 --> 00:24:46,210
We can imagine it'll never really do better than 50%,

447
00:24:46,210 --> 00:24:48,835
um, because given a vector for say cat,

448
00:24:48,835 --> 00:24:51,190
it won't know is that the vector for cat that's been

449
00:24:51,190 --> 00:24:54,130
transformed by W or is it actually the vector for gatto?

450
00:24:54,130 --> 00:24:58,885
Um, because in this case those two vectors are aligned so they are on top of each other.

451
00:24:58,885 --> 00:25:03,715
Um, so, um, during training, you first, um,

452
00:25:03,715 --> 00:25:06,790
you alternate between training the discriminator a little bit which

453
00:25:06,790 --> 00:25:09,640
means making sure it's as good as possible at

454
00:25:09,640 --> 00:25:13,120
distinguishing the English from Italian words and then you

455
00:25:13,120 --> 00:25:16,930
train the W and the goal for training W is to,

456
00:25:16,930 --> 00:25:20,050
uh, essentially confuse the discriminator as much as possible.

457
00:25:20,050 --> 00:25:23,215
Um, so you want to have a situation where,

458
00:25:23,215 --> 00:25:26,170
um, you can't, um, with this machine learning model,

459
00:25:26,170 --> 00:25:29,290
figure out if a word embedding actually, um,

460
00:25:29,290 --> 00:25:33,625
was, um, originally from English or if it's an Italian word vector.

461
00:25:33,625 --> 00:25:36,085
Um, and so at the end of the day you have,

462
00:25:36,085 --> 00:25:39,410
you have vectors that are kind of aligned with each other.

463
00:25:39,420 --> 00:25:43,460
Um, any questions about this approach?

464
00:25:47,220 --> 00:25:50,650
Okay. Um, he- there's a link to a paper with more details.

465
00:25:50,650 --> 00:25:53,275
There's actually kind of a range of other tricks you can do,

466
00:25:53,275 --> 00:25:55,850
um, but this is kind of a key idea.

467
00:25:58,440 --> 00:26:04,810
Um, okay. So that was doing word to word unsupervised translation.

468
00:26:04,810 --> 00:26:08,930
Um, how do we do full sentence to sentence translation?

469
00:26:09,150 --> 00:26:11,725
Um, so we're going to use, um,

470
00:26:11,725 --> 00:26:13,750
a standard sort of seq2seq model,

471
00:26:13,750 --> 00:26:16,660
um, without even an attention mechanism.

472
00:26:16,660 --> 00:26:19,900
Um, there's one change to the standard seq2seq

473
00:26:19,900 --> 00:26:23,050
model going on here which is that, um,

474
00:26:23,050 --> 00:26:25,780
we're going to use the same encoder and decoder,

475
00:26:25,780 --> 00:26:30,160
uh, regardless of the input and output languages.

476
00:26:30,160 --> 00:26:31,930
So you can see, um,

477
00:26:31,930 --> 00:26:33,340
in this example, um,

478
00:26:33,340 --> 00:26:35,815
we could give the encoder an English sentence,

479
00:26:35,815 --> 00:26:40,360
we could also give it a French sentence and it'll have these cross-lingual embeddings.

480
00:26:40,360 --> 00:26:43,255
So it'll have vector representations for English words

481
00:26:43,255 --> 00:26:47,140
and French words which means it can handle sort of any input.

482
00:26:47,140 --> 00:26:49,375
Um, for the decoder,

483
00:26:49,375 --> 00:26:52,930
we need to give it some information about what language is it supposed to generate in.

484
00:26:52,930 --> 00:26:54,955
Is it going to generate in French or English?

485
00:26:54,955 --> 00:26:58,660
Um, so the way that is done is by, uh,

486
00:26:58,660 --> 00:27:01,915
feeding in a special token which here is Fr

487
00:27:01,915 --> 00:27:05,590
in brack- brackets to represent French that tells the model,

488
00:27:05,590 --> 00:27:07,975
okay, you should generate in French now.

489
00:27:07,975 --> 00:27:11,380
Um, here in this figure it's only French,

490
00:27:11,380 --> 00:27:13,975
but you could imagine also feeding this model, uh,

491
00:27:13,975 --> 00:27:17,635
English in brackets, and then that'll tell it to, uh, generate English.

492
00:27:17,635 --> 00:27:21,670
And one thing that you can see is that you could use this sort of model to g enerate,

493
00:27:21,670 --> 00:27:23,155
do go from English to French.

494
00:27:23,155 --> 00:27:25,450
You could also use this model as an auto-encoder, right.

495
00:27:25,450 --> 00:27:27,295
So, uh, at the bottom, um,

496
00:27:27,295 --> 00:27:31,510
it's taking in a French sentence as input and it's just generating French as

497
00:27:31,510 --> 00:27:37,580
output which here means just reproducing the original input sequence.

498
00:27:38,850 --> 00:27:43,104
Um, so just a small change to standard seq2seq.

499
00:27:43,104 --> 00:27:46,765
Here's how we're going to train the seq2seq model.

500
00:27:46,765 --> 00:27:50,170
Um, there's going to be two training objectives, um,

501
00:27:50,170 --> 00:27:51,940
and I'll explain sort of why they're, uh,

502
00:27:51,940 --> 00:27:55,060
present in this model in just a few slides.

503
00:27:55,060 --> 00:27:57,025
For now let's just say what they are.

504
00:27:57,025 --> 00:27:59,110
So the first one is, um,

505
00:27:59,110 --> 00:28:01,165
called a de-noising autoencoder.

506
00:28:01,165 --> 00:28:06,430
Um, what we're going to train our model to do in this case is take a, uh, sentence.

507
00:28:06,430 --> 00:28:08,140
So, um, and here it's going to be

508
00:28:08,140 --> 00:28:10,795
an English sentence but it could also be a French sentence.

509
00:28:10,795 --> 00:28:14,170
Um, we're going to scramble up the words a little bit,

510
00:28:14,170 --> 00:28:16,885
and then we're going to ask the model to, uh,

511
00:28:16,885 --> 00:28:20,560
de-noise that sentence which in other words means

512
00:28:20,560 --> 00:28:25,345
regenerating what the sentence actually was before it was scrambled.

513
00:28:25,345 --> 00:28:31,740
And, uh, maybe one idea of why this would be a useful training objective is that,

514
00:28:31,740 --> 00:28:35,505
uh, since we have an encoder-decoder without atten- attention,

515
00:28:35,505 --> 00:28:41,780
the encoder is converting the entirety of the source sentence into a single vector,

516
00:28:41,780 --> 00:28:47,110
what an auto-encoder does is ensure that that vector contains all the information about

517
00:28:47,110 --> 00:28:52,390
the sentence such that we are able to recover what the original sentence was,

518
00:28:52,390 --> 00:28:56,180
um, from the vector produced by the encoder.

519
00:28:57,960 --> 00:29:00,805
Um, so that was objective 1.

520
00:29:00,805 --> 00:29:05,005
Training objective 2 is now we're actually going to be trying to do a translation,

521
00:29:05,005 --> 00:29:07,479
um, but, um, as before,

522
00:29:07,479 --> 00:29:09,775
we're going to be using this back-translation idea.

523
00:29:09,775 --> 00:29:12,970
So remember, we only have unlabeled sentences,

524
00:29:12,970 --> 00:29:16,015
we don't have any human-provided translations,

525
00:29:16,015 --> 00:29:19,750
um, but what we can still do is, given, a,

526
00:29:19,750 --> 00:29:22,000
um, let's say an English sentence or let's say a French sentence,

527
00:29:22,000 --> 00:29:24,505
given a French sentence, we can translate it to English, um,

528
00:29:24,505 --> 00:29:28,120
using our model in its current state, uh,

529
00:29:28,120 --> 00:29:32,605
and then we can ask that model to translate from English or translate that- yeah,

530
00:29:32,605 --> 00:29:34,690
translate that English back into French.

531
00:29:34,690 --> 00:29:37,105
Um, so what you can imagine is in this setting, um,

532
00:29:37,105 --> 00:29:39,640
the input sequence is going to be somewhat messed

533
00:29:39,640 --> 00:29:42,820
up because it's the output of our imperfect machine learning model.

534
00:29:42,820 --> 00:29:47,050
So here the input sequence is just "I am student," um, a word has been dropped,

535
00:29:47,050 --> 00:29:51,820
but, um, we're now gonna train it to, even with this kind of bad input,

536
00:29:51,820 --> 00:29:55,330
to reproduce the original, um,

537
00:29:55,330 --> 00:29:58,270
French sentence, um, from our,

538
00:29:58,270 --> 00:30:01,270
uh, corpus of- of monolingual, um, French text.

539
00:30:01,270 --> 00:30:07,400
[NOISE] Um, let me- let me pause here actually and ask for questions.

540
00:30:08,910 --> 00:30:13,840
Sure.

541
00:30:13,840 --> 00:30:16,000
[NOISE] [inaudible] What if, um, the reason you have

542
00:30:16,000 --> 00:30:20,305
this orthogonality constraint for your words to be word embedding,

543
00:30:20,305 --> 00:30:22,900
is it to avoid overfitting?

544
00:30:22,900 --> 00:30:29,800
Have you tried to take that off, and you know, see what [inaudible]

545
00:30:29,800 --> 00:30:31,060
Yeah. That's a good question.

546
00:30:31,060 --> 00:30:35,305
Um, so this is going back to earlier when there was a word-word translation.

547
00:30:35,305 --> 00:30:39,325
Why would we constrain that W matrix to be orthogonal?

548
00:30:39,325 --> 00:30:43,030
Um, essentially, that's right. It's to avoid overfitting and in particular,

549
00:30:43,030 --> 00:30:46,060
it's making this assumption that our embedding spaces are so

550
00:30:46,060 --> 00:30:50,005
similar that there's actually just a rotation that distinguishes,

551
00:30:50,005 --> 00:30:53,500
um, our word vectors in English versus our word vectors in Italian.

552
00:30:53,500 --> 00:30:57,460
Um, I think there has been, um,

553
00:30:57,460 --> 00:31:01,360
there have been results that don't include that orthogonality constraint,

554
00:31:01,360 --> 00:31:04,480
and I think it slightly hurts performance to not have that in there.

555
00:31:04,480 --> 00:31:09,130
[NOISE] Okay.

556
00:31:09,130 --> 00:31:11,155
Um, so- so continuing with,

557
00:31:11,155 --> 00:31:13,765
um, unsupervised machine translation,

558
00:31:13,765 --> 00:31:17,290
um, I- I gave a training method.

559
00:31:17,290 --> 00:31:20,395
I didn't quite explain why it would work, so- so,

560
00:31:20,395 --> 00:31:24,790
um, here is some more intuition for- for this idea.

561
00:31:24,790 --> 00:31:27,730
Um, so remember, um,

562
00:31:27,730 --> 00:31:29,665
we're going to initialize

563
00:31:29,665 --> 00:31:33,264
our machine translation model with these cross-lingual embeddings,

564
00:31:33,264 --> 00:31:37,015
which mean the English and French word should look close to identically.

565
00:31:37,015 --> 00:31:42,565
Um, we're also using the shared, um, encoder.

566
00:31:42,565 --> 00:31:44,860
Um, so that means if you think about it,

567
00:31:44,860 --> 00:31:46,645
um, at the top, we have just,

568
00:31:46,645 --> 00:31:51,760
a auto-encoding objective and we can certainly believe that our model can learn this.

569
00:31:51,760 --> 00:31:54,250
Um, it's a pretty simple task.

570
00:31:54,250 --> 00:31:59,395
Um, now imagine we're giving our model a French sentence as input instead.

571
00:31:59,395 --> 00:32:01,555
Um, since the, uh,

572
00:32:01,555 --> 00:32:03,850
embeddings are going to look pretty similar,

573
00:32:03,850 --> 00:32:06,190
and since the encoder is the same, um,

574
00:32:06,190 --> 00:32:09,760
it's pretty likely that the model's representation of

575
00:32:09,760 --> 00:32:11,950
this French sentence should actually be very

576
00:32:11,950 --> 00:32:15,520
similar to the representation of the English sentence.

577
00:32:15,520 --> 00:32:19,870
Um, so when this representation is passed into the decoder, um,

578
00:32:19,870 --> 00:32:24,980
we can hope that we'll get the same output as before.

579
00:32:25,200 --> 00:32:28,495
Um, um, so here's like sort of as a starting point.

580
00:32:28,495 --> 00:32:30,865
We- we can hope that our model, um,

581
00:32:30,865 --> 00:32:33,430
already is able to have some translation capability.

582
00:32:33,430 --> 00:32:37,840
[NOISE] Um, another way of thinking about this is

583
00:32:37,840 --> 00:32:42,355
that what we really want our model to do is to be able to encode a sentence,

584
00:32:42,355 --> 00:32:44,335
such that the representation,

585
00:32:44,335 --> 00:32:47,410
um, is sort of a universal kind of Interlingua.

586
00:32:47,410 --> 00:32:49,885
So a universal, um, uh,

587
00:32:49,885 --> 00:32:53,680
universal representation of that sentence that doesn't,

588
00:32:53,680 --> 00:32:56,245
uh, that's not specific to the language.

589
00:32:56,245 --> 00:32:59,785
And so- so here's kind of a picture that's trying to get at this.

590
00:32:59,785 --> 00:33:03,160
So our autoencoder, um, and our, um,

591
00:33:03,160 --> 00:33:05,290
here in our back-translation example,

592
00:33:05,290 --> 00:33:07,210
um, here, the target sequence is the same.

593
00:33:07,210 --> 00:33:10,090
[NOISE] Um, so what that essentially means is

594
00:33:10,090 --> 00:33:14,200
that the vectors for the English sentence and the French sentence,

595
00:33:14,200 --> 00:33:17,410
um, are going to be trained to be the same, um, right?

596
00:33:17,410 --> 00:33:19,645
Because if they are different, our, uh,

597
00:33:19,645 --> 00:33:21,520
decoder would be generating different,

598
00:33:21,520 --> 00:33:25,040
uh, outputs on these two examples.

599
00:33:25,040 --> 00:33:29,640
Um, so here- this is just another sort of intuition is that what our model is

600
00:33:29,640 --> 00:33:31,290
trying to learn here is kind of a way of

601
00:33:31,290 --> 00:33:33,870
encoding the information of a sentence in a vector,

602
00:33:33,870 --> 00:33:37,100
um, but in a way that is language-agnostic.

603
00:33:37,100 --> 00:33:39,460
Um, any more questions about,

604
00:33:39,460 --> 00:33:42,740
uh, unsupervised machine translation?

605
00:33:44,220 --> 00:33:50,350
Okay. Um, so going on to results of this approach, um,

606
00:33:50,350 --> 00:33:53,020
here, the horizontal lines are,

607
00:33:53,020 --> 00:33:56,860
um, the results of an unsupervised machine translation model.

608
00:33:56,860 --> 00:34:00,775
Um, the lines that go up are for a supervised machine translation model,

609
00:34:00,775 --> 00:34:03,900
um, as we give it more and more data.

610
00:34:03,900 --> 00:34:06,300
Right? So unsurprisingly, um,

611
00:34:06,300 --> 00:34:09,405
given a large amount of supervised data, um,

612
00:34:09,405 --> 00:34:11,790
the supervised machine translation models

613
00:34:11,790 --> 00:34:15,725
work much better than the unsupervised machine translation model.

614
00:34:15,725 --> 00:34:19,285
Um, but, um, the unsupervised machine translation model,

615
00:34:19,285 --> 00:34:21,310
actually still does quite well.

616
00:34:21,310 --> 00:34:26,995
Um, so if you see it around 10,000 to 100,000 training examples,

617
00:34:26,995 --> 00:34:30,565
um, it actually does just as well or better than supervised translation,

618
00:34:30,565 --> 00:34:33,580
and I think that's a really promising result,

619
00:34:33,580 --> 00:34:36,640
uh, because if you think of, um,

620
00:34:36,640 --> 00:34:39,550
low-resource settings where there isn't much labeled examples, um,

621
00:34:39,550 --> 00:34:42,280
it suddenly becomes really nice that you can perform this well,

622
00:34:42,280 --> 00:34:46,550
um, without even needing to use a training set.

623
00:34:48,690 --> 00:34:51,745
Um, another thing kind of fun you can do with,

624
00:34:51,745 --> 00:34:55,195
an unsupervised machine translation model is attribute transfer.

625
00:34:55,195 --> 00:34:58,855
Um, so basically, you can, um, take, uh,

626
00:34:58,855 --> 00:35:00,520
collections of texts that,

627
00:35:00,520 --> 00:35:03,190
uh, split by any attribute you want.

628
00:35:03,190 --> 00:35:04,900
So for example, you could go on Twitter,

629
00:35:04,900 --> 00:35:08,650
look at hashtags to decide which tweets are annoyed and which tweets are relaxed,

630
00:35:08,650 --> 00:35:11,080
and then you can treat those two corpora as

631
00:35:11,080 --> 00:35:13,614
text as though they were two different languages,

632
00:35:13,614 --> 00:35:16,510
and you can train an unsupervised machine translation model,

633
00:35:16,510 --> 00:35:19,165
uh, to convert from one to the other.

634
00:35:19,165 --> 00:35:22,495
Uh, and you can see these examples, um,

635
00:35:22,495 --> 00:35:26,650
the model actually does a pretty good job of sort of minimally changing the sentence,

636
00:35:26,650 --> 00:35:29,680
kind of preserving a lot of that sentence's original semantics,

637
00:35:29,680 --> 00:35:33,620
um, such that the target attribute is changed.

638
00:35:36,540 --> 00:35:41,290
Um, I also wanna throw a little bit of cold water on this idea.

639
00:35:41,290 --> 00:35:44,410
So I do think it's really exciting and- and almost kind of

640
00:35:44,410 --> 00:35:47,650
mind-blowing that you can do this translation without labeled data.

641
00:35:47,650 --> 00:35:49,600
Um, certainly, right.

642
00:35:49,600 --> 00:35:54,520
It's really hard to imagine someone giving me a bunch of books in Italian and say, "Okay.

643
00:35:54,520 --> 00:35:56,410
We're in Italian," um, without, you know,

644
00:35:56,410 --> 00:35:59,755
teaching you how to specifically do the translation.

645
00:35:59,755 --> 00:36:03,955
Um, but, um, even though these methods show promise,

646
00:36:03,955 --> 00:36:08,095
um, mostly they have shown promise on languages that are quite closely related.

647
00:36:08,095 --> 00:36:09,775
So those previous results,

648
00:36:09,775 --> 00:36:11,065
those were all, um,

649
00:36:11,065 --> 00:36:13,750
some combination of English to French or English to German,

650
00:36:13,750 --> 00:36:16,270
um, or so on, and those languages are quite similar.

651
00:36:16,270 --> 00:36:18,280
[NOISE] Um, so if you look at, uh,

652
00:36:18,280 --> 00:36:20,320
a different language pair, let's say English to Turkish,

653
00:36:20,320 --> 00:36:24,685
where, um, the linguistics in those two languages are quite different, uh,

654
00:36:24,685 --> 00:36:27,610
these methods do still work to some extent, um,

655
00:36:27,610 --> 00:36:30,910
so they get around five BLEU points let's say, uh,

656
00:36:30,910 --> 00:36:33,190
but they don't work nearly as well,

657
00:36:33,190 --> 00:36:35,890
um, as they do in the f- uh, i- in the other settings, right?

658
00:36:35,890 --> 00:36:40,240
So there's still a huge gap to purely supervised learning. Um, right?

659
00:36:40,240 --> 00:36:41,350
So we're probably not, you know,

660
00:36:41,350 --> 00:36:45,040
quite at this stage where an alien could come down and it's sort of, no problem,

661
00:36:45,040 --> 00:36:48,220
let's use our unsupervised machine translation system, um,

662
00:36:48,220 --> 00:36:52,399
but I still think that's pretty exciting progress. Um, yeah. Question?

663
00:36:52,399 --> 00:36:55,270
Um, so what you're saying is that the genealogy of

664
00:36:55,270 --> 00:36:58,630
a language might need it to superimpose worse, right?

665
00:36:58,630 --> 00:37:01,510
Because my original thought was that if you took, for example,

666
00:37:01,510 --> 00:37:04,810
like Latin, which doesn't have a word for, you know,

667
00:37:04,810 --> 00:37:11,440
the modern classification of car, I thought that would do more poorly. But if- but, uh, basically,

668
00:37:11,440 --> 00:37:15,280
what I'm asking is, do you think the English maps better to Latin

669
00:37:15,280 --> 00:37:20,380
because they're both related, and worse to Turkish or is it the other way around?

670
00:37:20,380 --> 00:37:25,645
Um, I would expect English to map quite a lot better to Latin.

671
00:37:25,645 --> 00:37:28,930
And I think part of the issue here is that, um,

672
00:37:28,930 --> 00:37:33,460
the difficulty in translation I think is not really at the word level.

673
00:37:33,460 --> 00:37:35,410
So I mean that certainly is an issue that words exist

674
00:37:35,410 --> 00:37:37,495
in one language that don't exist in another,

675
00:37:37,495 --> 00:37:38,740
um, but I think actually,

676
00:37:38,740 --> 00:37:43,195
more substantial differences between language is at the level of like syntax,

677
00:37:43,195 --> 00:37:45,820
um, um, or you know, semantics, right?

678
00:37:45,820 --> 00:37:47,410
How ideas are expressed.

679
00:37:47,410 --> 00:37:53,515
Um, so- so I think I- I would expect Ital- Latin to have, you know,

680
00:37:53,515 --> 00:37:56,020
relatively similar syntax to English,

681
00:37:56,020 --> 00:37:57,580
um, compared to say Turkish,

682
00:37:57,580 --> 00:37:59,860
I imagine that is probably the bigger obstacle

683
00:37:59,860 --> 00:38:03,260
for unsupervised machine translation models.

684
00:38:07,190 --> 00:38:10,260
Um, I'm going to really quickly go into

685
00:38:10,260 --> 00:38:14,910
this last recent research paper which is basically taking BERT  which,

686
00:38:14,910 --> 00:38:17,265
which you've learned about, um, correct?

687
00:38:17,265 --> 00:38:20,190
Yes. Okay. And making it cross-lingual.

688
00:38:20,190 --> 00:38:23,730
Um, so, um, here's what regular BERT is, right?

689
00:38:23,730 --> 00:38:26,295
We have a sequence of sentences in English.

690
00:38:26,295 --> 00:38:28,215
We're going to mask out some of the words.

691
00:38:28,215 --> 00:38:31,500
And we're going to ask BERT which is our transformer model, um,

692
00:38:31,500 --> 00:38:36,675
to essentially fill in the blanks and predict what were the words that were dropped out.

693
00:38:36,675 --> 00:38:42,990
Um, what actually has already been done by Google is training a multilingual BERT .

694
00:38:42,990 --> 00:38:46,835
So what they did essentially is concatenate, um,

695
00:38:46,835 --> 00:38:51,560
a whole bunch of corpora in different languages and then train one model um,

696
00:38:51,560 --> 00:38:54,785
doing using this masked LM objective um,

697
00:38:54,785 --> 00:38:56,315
on all of that text at once.

698
00:38:56,315 --> 00:38:58,300
And that's a publicly released model.

699
00:38:58,300 --> 00:39:03,495
Um, the, the new kind of extension to this that has recently been uh,

700
00:39:03,495 --> 00:39:06,300
proposed by Facebook is to actually combine

701
00:39:06,300 --> 00:39:10,965
this masked LM training objective um, with uh, translation.

702
00:39:10,965 --> 00:39:17,130
So what they do is sometimes give this model a in this case,

703
00:39:17,130 --> 00:39:21,060
a sequence in English and a sequence in uh, French.

704
00:39:21,060 --> 00:39:24,300
Um, drop out some of the words and just as before,

705
00:39:24,300 --> 00:39:26,130
ask the model to fill it in.

706
00:39:26,130 --> 00:39:28,635
And the motivation here is that, um,

707
00:39:28,635 --> 00:39:31,080
this will much better cause the model

708
00:39:31,080 --> 00:39:33,525
to understand the relation between these two languages.

709
00:39:33,525 --> 00:39:37,950
Because if you're trying to find a fill in a English word that's been dropped,

710
00:39:37,950 --> 00:39:40,500
uh, the best way to do it if you have a translation is look

711
00:39:40,500 --> 00:39:43,005
at the French side and try to find that word.

712
00:39:43,005 --> 00:39:45,075
Hopefully, that one hasn't been dropped as well.

713
00:39:45,075 --> 00:39:48,525
And then you can um, much more easily fill in the blank.

714
00:39:48,525 --> 00:39:52,575
And uh, this actually leads to very uh,

715
00:39:52,575 --> 00:39:55,860
substantial improvements in unsupervised machine translation.

716
00:39:55,860 --> 00:39:59,670
So just like BERT is used for other tasks in NLP,

717
00:39:59,670 --> 00:40:02,010
they basically take this cross-lingual BERT.

718
00:40:02,010 --> 00:40:03,930
They use it as initialization for

719
00:40:03,930 --> 00:40:07,410
a unsupervised machine translation system and they get, you know,

720
00:40:07,410 --> 00:40:10,425
really large gains on the order of 10 BLEU points um,

721
00:40:10,425 --> 00:40:12,690
such that the gap between

722
00:40:12,690 --> 00:40:16,485
unsupervised machine translation and the current supervised state of the art,

723
00:40:16,485 --> 00:40:18,420
um, is much smaller.

724
00:40:18,420 --> 00:40:23,190
Uh, so this is a pretty recent idea but I think it also shows promise

725
00:40:23,190 --> 00:40:28,320
in really improving the quality of translation through using unlabeled data.

726
00:40:28,320 --> 00:40:30,945
Um, although I guess yeah, I guess in this case with BERT

727
00:40:30,945 --> 00:40:33,930
they are using labeled translation data as well.

728
00:40:33,930 --> 00:40:37,270
Any, any questions about this?

729
00:40:37,820 --> 00:40:46,350
Okay. Um, so that is all I'm going to say about using unlabeled data for translation.

730
00:40:46,350 --> 00:40:48,750
The next part of this talk is about um,

731
00:40:48,750 --> 00:40:54,720
what happens if we really scale up these unsupervised language models.

732
00:40:54,720 --> 00:40:59,865
Um, so in particular I'm gonna talk about GPT-2 which is a new model by OpenAI.

733
00:40:59,865 --> 00:41:02,265
That's essentially a really giant language model

734
00:41:02,265 --> 00:41:05,685
and I think it has some interesting implications.

735
00:41:05,685 --> 00:41:15,060
So first of all, here's just the sizes of a bunch of different NLP models and,

736
00:41:15,060 --> 00:41:18,165
um, you know, maybe a couple years ago the,

737
00:41:18,165 --> 00:41:19,230
the standard sort of

738
00:41:19,230 --> 00:41:24,135
LSTM medium-size model was on the order of about 10 million parameters.

739
00:41:24,135 --> 00:41:30,660
Where 10- where a parameter is just a single weight let's say in the neural net um,

740
00:41:30,660 --> 00:41:33,090
ELMo and uh, GPT.

741
00:41:33,090 --> 00:41:35,520
So the original OpenAI paper before they did

742
00:41:35,520 --> 00:41:38,820
this GPT-2 and we're about 10 times bigger than that.

743
00:41:38,820 --> 00:41:43,120
Um, GPT-2 is about another order of magnitude bigger.

744
00:41:44,120 --> 00:41:48,825
Um, one kind of interesting comparison point here is that uh,

745
00:41:48,825 --> 00:41:51,735
GPT-2 which is 1,5 billion parameters,

746
00:41:51,735 --> 00:41:55,635
actually has more parameters than a honey bee brain has synapses.

747
00:41:55,635 --> 00:41:58,440
Um, so that sounds kind of impressive, right?

748
00:41:58,440 --> 00:42:01,350
You know honeybees are not the smartest of

749
00:42:01,350 --> 00:42:05,325
animals but they can still fly around and find nectar or whatever.

750
00:42:05,325 --> 00:42:08,760
Um, but yeah. Of course, this isn't really an apples to apples comparison, right?

751
00:42:08,760 --> 00:42:11,970
So a synapse and a weight in a neural net are really quite different.

752
00:42:11,970 --> 00:42:14,490
But I just think it's one kind of interesting milestone

753
00:42:14,490 --> 00:42:16,815
let's say in terms of model size um,

754
00:42:16,815 --> 00:42:18,150
that has been surpassed.

755
00:42:18,150 --> 00:42:26,835
[NOISE] Um, one thing to point out here is that um,

756
00:42:26,835 --> 00:42:32,130
this increasing scaling of deep learning is really a general trend uh,

757
00:42:32,130 --> 00:42:34,845
in all of machine learning so beyond NLP.

758
00:42:34,845 --> 00:42:41,760
So this plot is showing time on the x-axis and the y-axis is log scaled um,

759
00:42:41,760 --> 00:42:45,255
the amount of petaFLOPS used to train this model.

760
00:42:45,255 --> 00:42:50,010
Um, so what this means is that the trend at least currently is that there is

761
00:42:50,010 --> 00:42:53,130
exponential growth in how much compute power

762
00:42:53,130 --> 00:42:55,735
we're throwing at our machine learning models.

763
00:42:55,735 --> 00:42:57,920
I guess it is kind of unclear, you know,

764
00:42:57,920 --> 00:43:00,695
will exponential growth continue but certainly um,

765
00:43:00,695 --> 00:43:03,560
there's rapid growth in the size of our models.

766
00:43:03,560 --> 00:43:06,200
And it's leading to some really amazing results, right?

767
00:43:06,200 --> 00:43:09,445
So here are results not from language but for vision.

768
00:43:09,445 --> 00:43:13,155
Um, this is a generative adversarial network

769
00:43:13,155 --> 00:43:16,920
that's been trained on a lot of data and it's been trained on really large scales.

770
00:43:16,920 --> 00:43:22,710
So it's a big model kind of in-between the size of ELMo and BERT let's say.

771
00:43:22,710 --> 00:43:27,510
And uh, these photos here are actually productions of the model.

772
00:43:27,510 --> 00:43:28,740
So those aren't real photos.

773
00:43:28,740 --> 00:43:31,515
Those are things the model has just kind of hallucinated out of thin air.

774
00:43:31,515 --> 00:43:34,770
And at least to me they look essentially photo-realistic.

775
00:43:34,770 --> 00:43:38,010
There's also a website that um, is fun to look at it.

776
00:43:38,010 --> 00:43:39,915
If you're not- if you're interested which is,

777
00:43:39,915 --> 00:43:42,195
thispersondoesnotexist.com.

778
00:43:42,195 --> 00:43:43,950
So if you go there, you'll see

779
00:43:43,950 --> 00:43:47,430
a very convincing photo of a person but it's not a real photo.

780
00:43:47,430 --> 00:43:51,400
It's again like a hallucinated image produced by a GAN.

781
00:43:51,440 --> 00:43:55,725
We're also seeing really huge models being used for image recognition.

782
00:43:55,725 --> 00:43:58,110
So this is recent work by Google where they trained

783
00:43:58,110 --> 00:44:02,010
an image net model with half a billion parameters.

784
00:44:02,010 --> 00:44:06,450
So that's bigger than BERT but not as big as GPT-2.

785
00:44:06,450 --> 00:44:09,420
Um, this plot here is showing a

786
00:44:09,420 --> 00:44:14,760
log scaled number of parameters on the x-axis and then accuracy at ImageNet

787
00:44:14,760 --> 00:44:20,520
on the y-axis- axis and sort of unsurprisingly bigger models perform better.

788
00:44:20,520 --> 00:44:24,000
And there seems to actually be a pretty consistent trend here which is uh,

789
00:44:24,000 --> 00:44:28,240
accuracy is increasing with the log of the, the model size.

790
00:44:31,010 --> 00:44:35,100
Um, I wanna go into a little bit more detail, how is it

791
00:44:35,100 --> 00:44:39,060
possible that we can scale up models and train models at such a large extent.

792
00:44:39,060 --> 00:44:41,190
One answer is just better hardware.

793
00:44:41,190 --> 00:44:42,675
And in particular, um,

794
00:44:42,675 --> 00:44:44,160
there's a growing uh,

795
00:44:44,160 --> 00:44:48,165
number of companies that are developing hardware specifically for deep learning.

796
00:44:48,165 --> 00:44:50,520
So these are even more kind of constrained and the

797
00:44:50,520 --> 00:44:53,190
kind of operations they can do than a GPU,

798
00:44:53,190 --> 00:44:55,950
um but they do those operations even faster.

799
00:44:55,950 --> 00:44:59,610
So Google's Tensor Processing Units is one example.

800
00:44:59,610 --> 00:45:03,180
There are actually a bunch of other companies working on this idea.

801
00:45:03,180 --> 00:45:06,930
Um, the other way to scale up models is by taking advantage of

802
00:45:06,930 --> 00:45:11,835
parallelism and there's two kinds of parallelism that I want to talk about very briefly.

803
00:45:11,835 --> 00:45:13,980
So one is data parallelism.

804
00:45:13,980 --> 00:45:16,785
In this case, each of your,

805
00:45:16,785 --> 00:45:19,380
let's say GPUs, will have a copy of the model.

806
00:45:19,380 --> 00:45:21,480
And what you essentially do is split

807
00:45:21,480 --> 00:45:25,350
the mini-batch that you're training on across these different models.

808
00:45:25,350 --> 00:45:27,165
So if you have, let's say,

809
00:45:27,165 --> 00:45:30,945
16 GPUs and each of them see a batch size of 32.

810
00:45:30,945 --> 00:45:35,670
You can aggregate the gradients of these 16 uh, uh,

811
00:45:35,670 --> 00:45:42,540
if you do a back-prop on these 16 GPUs and you end up with effectively a batch size of 512.

812
00:45:42,540 --> 00:45:44,700
So this allows you to train models much faster.

813
00:45:44,700 --> 00:45:50,340
Um, the other kind of parallelism that's growing in importance is model par- parallelism.

814
00:45:50,340 --> 00:45:54,510
Um, so eventually models get so big that they

815
00:45:54,510 --> 00:45:59,070
can't even fit on a single GPU and they can't even do a batch size of one.

816
00:45:59,070 --> 00:46:00,660
Um, in this case,

817
00:46:00,660 --> 00:46:02,985
you actually need to split up the model across

818
00:46:02,985 --> 00:46:06,075
multiple computers- multiple compute units.

819
00:46:06,075 --> 00:46:10,485
Um, and that's what's done for models kind of the size of,

820
00:46:10,485 --> 00:46:12,720
of let's say GPT-2.

821
00:46:12,720 --> 00:46:15,540
There are new frameworks such as Mesh-TensorFlow, um,

822
00:46:15,540 --> 00:46:21,430
which are basically designed to make this sort of model parallelism easier.

823
00:46:23,990 --> 00:46:27,390
Um, okay. So onto GPT-2, um,

824
00:46:27,390 --> 00:46:31,560
I know you already saw this a little bit in the contextualized uh,

825
00:46:31,560 --> 00:46:36,540
um, embeddings um, lecture but I'm going to go into some more depth here.

826
00:46:36,540 --> 00:46:41,265
[NOISE] So so essentially it's a really large transformer language model.

827
00:46:41,265 --> 00:46:45,165
Um, so there's nothing really kind of novel here in terms

828
00:46:45,165 --> 00:46:49,305
of new training algorithms or in terms of um,

829
00:46:49,305 --> 00:46:51,645
the loss function or anything like that.

830
00:46:51,645 --> 00:46:53,340
Um, the thing that makes it different from

831
00:46:53,340 --> 00:46:56,070
prior work is that it's just really really big.

832
00:46:56,070 --> 00:46:59,970
Uh, it's trained on a correspondingly huge amount of text.

833
00:46:59,970 --> 00:47:04,800
So it's trained on 40 gigabytes and that's roughly 10 times larger than previous uh,

834
00:47:04,800 --> 00:47:07,215
language models have been trained on.

835
00:47:07,215 --> 00:47:11,070
Um, when you have that size of dataset,

836
00:47:11,070 --> 00:47:14,325
um, the only way to get that much text is essentially to go to the web.

837
00:47:14,325 --> 00:47:18,840
Um, so one thing OpenAI put a quite a bit of effort into when they're developing

838
00:47:18,840 --> 00:47:23,570
this network was to ensure that that text was pretty high-quality.

839
00:47:23,570 --> 00:47:26,180
Um, and they did that in a kind of interesting way.

840
00:47:26,180 --> 00:47:28,970
They, they looked at Reddit which is this website where people uh,

841
00:47:28,970 --> 00:47:30,140
can vote on links.

842
00:47:30,140 --> 00:47:31,640
And then they said uh, if

843
00:47:31,640 --> 00:47:35,090
a link has a lot of votes then it's probably sort of a decent link.

844
00:47:35,090 --> 00:47:36,830
There's probably um, you know,

845
00:47:36,830 --> 00:47:39,750
reasonable text there for a model to learn.

846
00:47:40,610 --> 00:47:43,080
Um, okay, so if we have

847
00:47:43,080 --> 00:47:45,600
this super huge language model like

848
00:47:45,600 --> 00:47:49,515
GPT-2 on this question of what can you actually do with it,

849
00:47:49,515 --> 00:47:53,415
um, well obviously if you have a language model you can do language modelling with it.

850
00:47:53,415 --> 00:47:56,790
Uh, but one thing kind of interestingly interesting is that you

851
00:47:56,790 --> 00:48:00,525
can run this language model on er,

852
00:48:00,525 --> 00:48:03,435
existing benchmarks, um, for,

853
00:48:03,435 --> 00:48:05,250
for language modelling, um,

854
00:48:05,250 --> 00:48:08,520
and it gets state of the art perplexity on these benchmarks even

855
00:48:08,520 --> 00:48:11,700
though it never sees the training data for these benchmarks, right?

856
00:48:11,700 --> 00:48:16,770
So normally, if you want to say evaluate your language model on the Penn Treebank.

857
00:48:16,770 --> 00:48:21,510
You first train on the Penn Treebank and then you evaluate on this held-out set.

858
00:48:21,510 --> 00:48:23,790
Uh, in this case, uh,

859
00:48:23,790 --> 00:48:28,515
a GPT-2 just by virtue of having seen so much text and being such a large model,

860
00:48:28,515 --> 00:48:31,095
outperforms all these other uh,

861
00:48:31,095 --> 00:48:34,540
prior works even though it's not seeing that data.

862
00:48:34,580 --> 00:48:39,430
Um, on a bunch of different uh, language modelling benchmarks.

863
00:48:40,800 --> 00:48:46,315
Um, but there's a bunch of other interesting experiments that OpenAI

864
00:48:46,315 --> 00:48:51,700
ran with this language modeling and these were based on zero-shot learning.

865
00:48:51,700 --> 00:48:57,250
So zero-shot learning just means trying to do a task without ever training on it.

866
00:48:57,250 --> 00:49:00,445
And, uh, the way you can do this with a language model

867
00:49:00,445 --> 00:49:03,460
is by designing a prompt you feed into

868
00:49:03,460 --> 00:49:06,880
the language model and then have it just generate from there and

869
00:49:06,880 --> 00:49:11,065
hopefully it generates something relevant to the task you're trying to solve.

870
00:49:11,065 --> 00:49:13,225
So for example, for reading comprehension,

871
00:49:13,225 --> 00:49:16,090
what you can do is take the context paragraph,

872
00:49:16,090 --> 00:49:20,080
uh, concatenate the question to it and then add uh,

873
00:49:20,080 --> 00:49:21,430
a colon which is a way,

874
00:49:21,430 --> 00:49:22,705
I guess, of telling the model,

875
00:49:22,705 --> 00:49:25,210
''Okay you should be producing an answer to this question,''

876
00:49:25,210 --> 00:49:27,790
and then just have it generate text, um,

877
00:49:27,790 --> 00:49:30,940
and perhaps it'll generate something that is actually answering,

878
00:49:30,940 --> 00:49:32,365
um, the question and is,

879
00:49:32,365 --> 00:49:34,060
is paying attention to the context.

880
00:49:34,060 --> 00:49:37,390
[NOISE] Um, and similarly, for summarization,

881
00:49:37,390 --> 00:49:41,740
you can get the article then TL;DR and perhaps the model will produce the summary.

882
00:49:41,740 --> 00:49:43,795
Um, you can even do translation,

883
00:49:43,795 --> 00:49:45,655
where you give the model,

884
00:49:45,655 --> 00:49:49,720
um, some ex- a list of known English to French translations so you, sort of,

885
00:49:49,720 --> 00:49:53,770
prime it to tell it that it should be doing translation and then you give

886
00:49:53,770 --> 00:49:58,120
it the source sequence equals blank and have it just run and,

887
00:49:58,120 --> 00:49:59,920
um, perhaps it'll generate,

888
00:49:59,920 --> 00:50:02,690
um, the sequence in the target language.

889
00:50:03,300 --> 00:50:06,895
Um, okay. So so here's what the results look like.

890
00:50:06,895 --> 00:50:09,100
Um, for all of these,

891
00:50:09,100 --> 00:50:11,545
uh, the X-axis is,

892
00:50:11,545 --> 00:50:16,210
is log scaled model size and the Y-axis is accuracy, um,

893
00:50:16,210 --> 00:50:18,715
and the dotted lines basically correspond to,

894
00:50:18,715 --> 00:50:22,090
um, existing works on these tasks.

895
00:50:22,090 --> 00:50:26,290
Um, so for most of these tasks, um,

896
00:50:26,290 --> 00:50:31,765
GPT-2 is quite a bit below existing systems,

897
00:50:31,765 --> 00:50:33,625
um, but there's of course this big difference, right?

898
00:50:33,625 --> 00:50:37,195
Existing systems are trained specifically to do,

899
00:50:37,195 --> 00:50:39,775
um, whatever task they're being evaluated on,

900
00:50:39,775 --> 00:50:42,520
where GPT-2 is um,

901
00:50:42,520 --> 00:50:46,540
only trained to do language modeling and as it learns language modeling,

902
00:50:46,540 --> 00:50:48,865
it's sort of picking up on these other tasks.

903
00:50:48,865 --> 00:50:50,785
Um, so right. So for example, um,

904
00:50:50,785 --> 00:50:54,385
it does, uh, English to French machine translation, um,

905
00:50:54,385 --> 00:50:56,875
not as well as, uh,

906
00:50:56,875 --> 00:51:00,400
standard unsupervised machine translation which is those, uh,

907
00:51:00,400 --> 00:51:02,920
dotted lines, um, but it still,

908
00:51:02,920 --> 00:51:04,300
it still does quite well.

909
00:51:04,300 --> 00:51:06,370
And, um, one thing, kind of,

910
00:51:06,370 --> 00:51:07,810
interesting is the trend line, right,

911
00:51:07,810 --> 00:51:09,520
for almost all of these tasks.

912
00:51:09,520 --> 00:51:11,530
Um, performance is getting uh,

913
00:51:11,530 --> 00:51:13,600
much better as the model increases in size.

914
00:51:13,600 --> 00:51:18,535
[NOISE] Um, I think a particularly interesting,

915
00:51:18,535 --> 00:51:21,580
uh, one of these tasks is machine translation, right?

916
00:51:21,580 --> 00:51:23,290
So the question is, how can it be doing

917
00:51:23,290 --> 00:51:26,440
machine translation when all we're giving it as a bunch of

918
00:51:26,440 --> 00:51:28,540
web pages and those web pages are almost all in

919
00:51:28,540 --> 00:51:31,810
English and yet somehow it sort of magically picks up uh,

920
00:51:31,810 --> 00:51:33,340
a little bit of machine translation, right.

921
00:51:33,340 --> 00:51:35,395
So it's not a great model but it can still,

922
00:51:35,395 --> 00:51:38,260
um, you know, do a decent job in some cases.

923
00:51:38,260 --> 00:51:40,510
Um, and the answer is that,

924
00:51:40,510 --> 00:51:43,810
if you look at this giant corpus of English,

925
00:51:43,810 --> 00:51:47,050
occasionally, uh, within, within that corpus,

926
00:51:47,050 --> 00:51:48,880
you see examples of translations, right?

927
00:51:48,880 --> 00:51:50,290
So you see, um,

928
00:51:50,290 --> 00:51:52,810
a French idiom and its translation or

929
00:51:52,810 --> 00:51:56,035
a quote from someone who's French and then the translation in English.

930
00:51:56,035 --> 00:51:57,400
And, um, kind of,

931
00:51:57,400 --> 00:52:00,700
amazingly I think this big model, um,

932
00:52:00,700 --> 00:52:05,380
sees enough of these examples that it actually starts to learn how to generate French,

933
00:52:05,380 --> 00:52:07,030
um, even though that wasn't really,

934
00:52:07,030 --> 00:52:09,980
sort of, an intended part of its training.

935
00:52:11,970 --> 00:52:14,560
Um, another interesting, um,

936
00:52:14,560 --> 00:52:18,700
thing to dig a bit more into is its ability to do question answering.

937
00:52:18,700 --> 00:52:24,040
So uh, a simple baseline for question answering gets about 1% accuracy,

938
00:52:24,040 --> 00:52:27,295
GPT-2 barely does better at 4% accuracy.

939
00:52:27,295 --> 00:52:28,840
So this isn't, like, you know,

940
00:52:28,840 --> 00:52:32,440
super amazingly solved question answering, um, but, um,

941
00:52:32,440 --> 00:52:34,420
it's still pretty interesting in that,

942
00:52:34,420 --> 00:52:37,435
if you look at answers the model's most confident about,

943
00:52:37,435 --> 00:52:39,010
you can see that it sort of

944
00:52:39,010 --> 00:52:41,320
has learned some facts about the world, right.

945
00:52:41,320 --> 00:52:45,550
So it's learned that Charles Darwin wrote Origin of Species.

946
00:52:45,550 --> 00:52:50,740
Um, normally in the history of NLP, if you want to get, kind of,

947
00:52:50,740 --> 00:52:52,765
world knowledge into an NLP system,

948
00:52:52,765 --> 00:52:55,435
you'd need something like a big database of facts.

949
00:52:55,435 --> 00:52:57,340
And even though this is still,

950
00:52:57,340 --> 00:52:59,500
kind of, very early stages and that, um,

951
00:52:59,500 --> 00:53:04,000
there's still a huge gap between 4% accuracy and the, uh, you know,

952
00:53:04,000 --> 00:53:05,875
70% or so that, uh,

953
00:53:05,875 --> 00:53:09,550
state of the art open domain question answering systems can do,

954
00:53:09,550 --> 00:53:12,010
um, it, it, um,

955
00:53:12,010 --> 00:53:14,200
it still can, uh,

956
00:53:14,200 --> 00:53:17,740
pick up some world knowledge just by reading a lot of text, um, without,

957
00:53:17,740 --> 00:53:21,895
kind of, explicitly having that knowledge put into the model.

958
00:53:21,895 --> 00:53:26,810
Um, any questions by the way on GPT-2 so far?

959
00:53:28,050 --> 00:53:33,865
Okay. So one question that's interesting to think about is,

960
00:53:33,865 --> 00:53:36,505
what happens if our models get even bigger?

961
00:53:36,505 --> 00:53:38,305
Um, so here I've done the, um,

962
00:53:38,305 --> 00:53:42,565
very scientific thing of drawing some lines in PowerPoint and seeing where they meet up.

963
00:53:42,565 --> 00:53:44,655
Um, and you can see that, um,

964
00:53:44,655 --> 00:53:48,429
if the trend holds at about 1 trillion parameters,

965
00:53:48,429 --> 00:53:52,390
um, we get to human level reading comprehension performance.

966
00:53:52,390 --> 00:53:55,480
Um, so if that's true it would be really astonishing.

967
00:53:55,480 --> 00:54:00,505
I actually do expect that a 1 trillion parameter model would be attainable in,

968
00:54:00,505 --> 00:54:02,155
I don't know, ten years or so,

969
00:54:02,155 --> 00:54:04,240
um, but of course,

970
00:54:04,240 --> 00:54:05,665
right, the trend isn't clear.

971
00:54:05,665 --> 00:54:07,630
So if you look at summarization for example,

972
00:54:07,630 --> 00:54:09,040
it seems like performance is already,

973
00:54:09,040 --> 00:54:11,005
uh, uh, topped out.

974
00:54:11,005 --> 00:54:15,760
Um, so I think this will be a really interesting thing kinda going forward,

975
00:54:15,760 --> 00:54:17,980
looking at the future of NLP, um,

976
00:54:17,980 --> 00:54:20,710
is how the scaling will change,

977
00:54:20,710 --> 00:54:23,570
um, the way NLP is approached.

978
00:54:24,120 --> 00:54:29,755
Um, the other interesting thing about GPT-2 was its reaction from uh,

979
00:54:29,755 --> 00:54:32,125
the media and also from other researchers.

980
00:54:32,125 --> 00:54:35,455
Um, and the real cause of

981
00:54:35,455 --> 00:54:39,295
a lot of the controversy about it was this statement from OpenAI.

982
00:54:39,295 --> 00:54:43,000
They said that, ''We're not going to release our full language model,

983
00:54:43,000 --> 00:54:44,590
um, because it's too dangerous,

984
00:54:44,590 --> 00:54:46,015
you know, our language model is too good.''

985
00:54:46,015 --> 00:54:51,115
Um, so the media really enjoyed this and,

986
00:54:51,115 --> 00:54:52,330
you know, said that,

987
00:54:52,330 --> 00:54:55,135
uh, machine learning is going to break the Internet.

988
00:54:55,135 --> 00:55:00,580
Um, there's also some pretty interesting reactions from our researchers, right.

989
00:55:00,580 --> 00:55:02,020
So um, there's some,

990
00:55:02,020 --> 00:55:04,195
kind of, tongue-in-cheek responses here, right.

991
00:55:04,195 --> 00:55:05,755
You know, I trained the model on MNIST.

992
00:55:05,755 --> 00:55:07,915
Is it too dangerous for me to release it?

993
00:55:07,915 --> 00:55:11,530
Um, and similarly, we've done really great work

994
00:55:11,530 --> 00:55:15,715
but we can't release it it's too dangerous so you're just gonna have to trust us on this.

995
00:55:15,715 --> 00:55:18,970
Looking at more, kind of, reasoned, um,

996
00:55:18,970 --> 00:55:20,665
debate about this issue,

997
00:55:20,665 --> 00:55:22,885
you still see articles,

998
00:55:22,885 --> 00:55:24,610
um, arguing both sides.

999
00:55:24,610 --> 00:55:26,469
So these are two ar- articles,

1000
00:55:26,469 --> 00:55:29,545
um, from The Gradient which is a, sort of,

1001
00:55:29,545 --> 00:55:31,690
machine learning newsletter, um,

1002
00:55:31,690 --> 00:55:35,875
and they're arguing precisely opposite sides of this issue,

1003
00:55:35,875 --> 00:55:38,510
um, should it be released or not.

1004
00:55:40,770 --> 00:55:47,120
So I guess I can briefly go over a few arguments for or against.

1005
00:55:47,130 --> 00:55:50,185
There is, kind of, a lot of debate about this and I don't want to

1006
00:55:50,185 --> 00:55:53,480
go too deep into a controversial issue,

1007
00:55:54,150 --> 00:55:56,710
um, but here's a long list of,

1008
00:55:56,710 --> 00:55:58,570
kind of, things people have said about this, right.

1009
00:55:58,570 --> 00:56:01,450
So um, here's why you should release.

1010
00:56:01,450 --> 00:56:03,280
One complaint is that,

1011
00:56:03,280 --> 00:56:05,065
is this model really that special?

1012
00:56:05,065 --> 00:56:06,595
There's nothing new going on here.

1013
00:56:06,595 --> 00:56:09,640
It's just 10 times bigger than previous models, um,

1014
00:56:09,640 --> 00:56:11,860
and there's also some arguments that,

1015
00:56:11,860 --> 00:56:14,500
um, even if this one isn't released, you know,

1016
00:56:14,500 --> 00:56:17,185
in five years everybody can train a model this good, um,

1017
00:56:17,185 --> 00:56:22,270
and actually if you look at image recognition or look at images and speech data, um,

1018
00:56:22,270 --> 00:56:25,780
it already is possible to synthesize highly convincing,

1019
00:56:25,780 --> 00:56:28,405
um, fake images and fake speech.

1020
00:56:28,405 --> 00:56:34,750
So kinda, what makes this thing different from those other, um, systems.

1021
00:56:34,750 --> 00:56:36,310
And speaking of other systems, right,

1022
00:56:36,310 --> 00:56:38,335
Photoshop has existed for a long time,

1023
00:56:38,335 --> 00:56:41,950
so we can already convincingly fake images, um,

1024
00:56:41,950 --> 00:56:44,140
people have just learned to adjust and learned

1025
00:56:44,140 --> 00:56:46,645
that you shouldn't always trust what's in an image,

1026
00:56:46,645 --> 00:56:47,995
um, because it may have been,

1027
00:56:47,995 --> 00:56:50,065
um, altered in some way.

1028
00:56:50,065 --> 00:56:52,450
Um, on the other hand, you could say,

1029
00:56:52,450 --> 00:56:55,780
''Okay, uh, Photoshop exists but, um, you can't, sort of,

1030
00:56:55,780 --> 00:57:00,130
scale up Photoshop and start mass producing fake content the way you can with this sort

1031
00:57:00,130 --> 00:57:04,660
of model,'' and they pointed at the danger of uh, fake news, um,

1032
00:57:04,660 --> 00:57:08,950
fake reviews, um, in general just astroturfing, which means basically,

1033
00:57:08,950 --> 00:57:15,370
uh, creating fake user content that's supporting a view you want other people to hold.

1034
00:57:15,370 --> 00:57:18,870
Um, this is actually something that's already done,

1035
00:57:18,870 --> 00:57:21,660
um, pretty widely by country- companies and governments.

1036
00:57:21,660 --> 00:57:23,475
There's a lot of evidence for this, um,

1037
00:57:23,475 --> 00:57:25,500
but they are of course hiring people to

1038
00:57:25,500 --> 00:57:27,795
write all these comments on news articles let's say

1039
00:57:27,795 --> 00:57:30,390
and we don't want to make their job any easier

1040
00:57:30,390 --> 00:57:33,620
by producing a machine that could potentially do this.

1041
00:57:33,620 --> 00:57:37,330
So um, I'm not really gonna take a side here,

1042
00:57:37,330 --> 00:57:39,565
um, there's still a lot of debate about this.

1043
00:57:39,565 --> 00:57:41,110
I think, you know,

1044
00:57:41,110 --> 00:57:43,300
the main, the main takeaway here is that,

1045
00:57:43,300 --> 00:57:46,959
as a community on people in machine learning and NLP,

1046
00:57:46,959 --> 00:57:48,910
don't really have a handle on this, right?

1047
00:57:48,910 --> 00:57:51,355
We are sort of caught by surprise by, um,

1048
00:57:51,355 --> 00:57:56,095
OpenAI's, um, decision here and, um, uh,

1049
00:57:56,095 --> 00:57:57,760
that means that, you know,

1050
00:57:57,760 --> 00:58:01,120
there really is some figuring out that needs to be done on what

1051
00:58:01,120 --> 00:58:05,515
exactly is responsible to release publicly.

1052
00:58:05,515 --> 00:58:09,430
What kind of research problems should we be working on and so on.

1053
00:58:09,430 --> 00:58:11,530
[NOISE] So yeah.

1054
00:58:11,530 --> 00:58:13,795
Any questions about uh, this,

1055
00:58:13,795 --> 00:58:16,450
this reaction or this debate in general?

1056
00:58:16,450 --> 00:58:20,930
[NOISE] Okay.

1057
00:58:22,140 --> 00:58:27,610
Um, I think something arising from this debate is, um,

1058
00:58:27,610 --> 00:58:29,310
the question of, um,

1059
00:58:29,310 --> 00:58:32,580
should really the ML people be the people making these, sort of,

1060
00:58:32,580 --> 00:58:38,085
decisions or is there a need for more interdisciplinary science where we look at, um,

1061
00:58:38,085 --> 00:58:40,425
experts in say, computer security,

1062
00:58:40,425 --> 00:58:42,705
um, people from social sciences,

1063
00:58:42,705 --> 00:58:46,185
um, you know, people who are experts in ethics,

1064
00:58:46,185 --> 00:58:48,365
um, to look at these decisions.

1065
00:58:48,365 --> 00:58:54,595
Um, right. So GPT-2 was definitely one example of where suddenly it seems like,

1066
00:58:54,595 --> 00:58:58,420
um, our NLP technology has a lot of pitfalls, right.

1067
00:58:58,420 --> 00:59:02,005
Where they could be used in a malicious way or they could cause damage.

1068
00:59:02,005 --> 00:59:05,725
And I think this trend is only going to increase, um,

1069
00:59:05,725 --> 00:59:07,165
if you look at, kind of,

1070
00:59:07,165 --> 00:59:10,540
areas of NLP that people are working on, uh,

1071
00:59:10,540 --> 00:59:16,510
increasingly people are working on really high stakes applications of NLP,

1072
00:59:16,510 --> 00:59:19,570
um, and those often have really big, um,

1073
00:59:19,570 --> 00:59:25,280
ramifications, especially if you think from the angle of bias and fairness.

1074
00:59:25,980 --> 00:59:32,420
Um, so, so let's go over a couple examples of this, um-

1075
00:59:32,690 --> 00:59:35,955
Um, one- so some, some areas where,

1076
00:59:35,955 --> 00:59:37,875
where this is happening is people are looking at,

1077
00:59:37,875 --> 00:59:40,050
uh, NLP to look at judicial decisions.

1078
00:59:40,050 --> 00:59:41,895
So for example, should this person,

1079
00:59:41,895 --> 00:59:43,305
uh, get bail or not?

1080
00:59:43,305 --> 00:59:45,210
Um, for hiring decisions, right?

1081
00:59:45,210 --> 00:59:46,680
So you look at someone's resume,

1082
00:59:46,680 --> 00:59:48,000
you run NLP on it,

1083
00:59:48,000 --> 00:59:50,775
and then you'd make a decision automatically,

1084
00:59:50,775 --> 00:59:53,130
um, sh- should we throw out this resume or not?

1085
00:59:53,130 --> 00:59:56,850
So do some, sort of, screening, um, grading tests.

1086
00:59:56,850 --> 00:59:58,650
Um, if you take the GRE, um,

1087
00:59:58,650 --> 01:00:00,825
your, your tests will be graded by a machine.

1088
01:00:00,825 --> 01:00:03,090
Um, a person will also look at it, um,

1089
01:00:03,090 --> 01:00:05,295
but nevertheless, um, that's, you know,

1090
01:00:05,295 --> 01:00:09,090
a sometimes very impactful part of your life, um, when it's,

1091
01:00:09,090 --> 01:00:11,085
when it's the tests that, um, inf- you know,

1092
01:00:11,085 --> 01:00:14,490
affects your, um, acceptance into a school, let's say.

1093
01:00:14,490 --> 01:00:17,265
Um, so I think there is- are some,

1094
01:00:17,265 --> 01:00:20,790
some good sides of using Machine Learning in these kinds of contexts.

1095
01:00:20,790 --> 01:00:24,120
So one is that we can pretty quickly evaluate,

1096
01:00:24,120 --> 01:00:26,985
a machine learning system and search out.

1097
01:00:26,985 --> 01:00:28,680
Does it have some, kind of, bias,

1098
01:00:28,680 --> 01:00:31,350
just by running it on a bunch of data and seeing what it does,

1099
01:00:31,350 --> 01:00:34,350
and also perhaps even more importantly,

1100
01:00:34,350 --> 01:00:35,640
um, we can fix this, kind of,

1101
01:00:35,640 --> 01:00:37,080
problem if it arises, right?

1102
01:00:37,080 --> 01:00:42,240
So, um, it's probably easier to fix a machine learning system that screens resumes,

1103
01:00:42,240 --> 01:00:44,730
than it is to s- to fix having, you know,

1104
01:00:44,730 --> 01:00:48,300
5,000 executives that are slightly sexist or something, right?

1105
01:00:48,300 --> 01:00:49,725
So, so in this way,

1106
01:00:49,725 --> 01:00:51,180
um, there is a, sort of,

1107
01:00:51,180 --> 01:00:57,840
positive angle on using machine learning in these high-stakes, um, uh, decisions.

1108
01:00:57,840 --> 01:01:00,015
Um, on the other hand, um,

1109
01:01:00,015 --> 01:01:02,220
it's been pretty well, uh, s- known,

1110
01:01:02,220 --> 01:01:04,770
and I know you had a lecture on bias and fairness,

1111
01:01:04,770 --> 01:01:07,770
that machine learning often reflects bias in a data-set,

1112
01:01:07,770 --> 01:01:11,025
um, it can even amplify bias in the data-set.

1113
01:01:11,025 --> 01:01:12,660
Um, and there's concern of, kind of,

1114
01:01:12,660 --> 01:01:15,315
a feedback loop where a biased algorithm

1115
01:01:15,315 --> 01:01:18,360
actually will lead to the creation of more biased data,

1116
01:01:18,360 --> 01:01:23,050
um, in which case these problems will only compound and get worse.

1117
01:01:23,150 --> 01:01:28,950
Um, so for all of the, uh, high-impact decisions,

1118
01:01:28,950 --> 01:01:30,990
um, I, I had listed on that slide,

1119
01:01:30,990 --> 01:01:34,320
there are examples where things have gone awry, right?

1120
01:01:34,320 --> 01:01:36,690
So Amazon had some AI that was,

1121
01:01:36,690 --> 01:01:39,975
um, working as a recruiting tool and it turned out to be sexist.

1122
01:01:39,975 --> 01:01:42,255
Um, um, there have been some, kind of,

1123
01:01:42,255 --> 01:01:44,550
early pilots of using AI, um,

1124
01:01:44,550 --> 01:01:46,680
in the justice system and those also have had,

1125
01:01:46,680 --> 01:01:49,710
um, in some cases, really bad results.

1126
01:01:49,710 --> 01:01:52,920
Um, if you look at automatic,

1127
01:01:52,920 --> 01:01:54,855
automatic essay grading, um,

1128
01:01:54,855 --> 01:01:56,430
it's not really a great,

1129
01:01:56,430 --> 01:01:57,720
you know, NLP system, right?

1130
01:01:57,720 --> 01:01:59,730
So here's an example, um,

1131
01:01:59,730 --> 01:02:02,355
excerpt of an essay that, um,

1132
01:02:02,355 --> 01:02:06,240
a automatic grading system used by the GRE test gives, uh,

1133
01:02:06,240 --> 01:02:08,040
a very high score, um,

1134
01:02:08,040 --> 01:02:10,230
but really it's just, kind of, a solid of,

1135
01:02:10,230 --> 01:02:12,420
uh, big fancy words and that's

1136
01:02:12,420 --> 01:02:16,060
enough to convince the model that this is a, a great essay.

1137
01:02:17,240 --> 01:02:19,410
Um, the last, um,

1138
01:02:19,410 --> 01:02:21,555
area I wanna talk about where, where, um,

1139
01:02:21,555 --> 01:02:23,550
you can see there's really some risks and

1140
01:02:23,550 --> 01:02:26,655
some pitfalls with using NLP technology, is chatbots.

1141
01:02:26,655 --> 01:02:31,560
Um, so I think chatbots do have a side where they can be very beneficial.

1142
01:02:31,560 --> 01:02:33,930
Um, Woebot is one example,

1143
01:02:33,930 --> 01:02:37,545
is this company that has this chatbot you can talk to if you're not,

1144
01:02:37,545 --> 01:02:39,480
um, feeling too great and it'll try to,

1145
01:02:39,480 --> 01:02:41,565
um, I don't know, cheer you up.

1146
01:02:41,565 --> 01:02:43,830
Um, so, so that, you know,

1147
01:02:43,830 --> 01:02:46,770
could be a- a really nice piece of technology that helps people,

1148
01:02:46,770 --> 01:02:49,380
um, but on the other hand, there's some big risks.

1149
01:02:49,380 --> 01:02:53,520
So, so one example is Microsoft research had a chatbot trained on tweets,

1150
01:02:53,520 --> 01:02:56,850
and it started quickly saying racist things and had to be pulled.

1151
01:02:56,850 --> 01:02:59,625
Um, so I think all of this highlights that, um,

1152
01:02:59,625 --> 01:03:02,505
as NLP is becoming more effective,

1153
01:03:02,505 --> 01:03:05,835
people are seeing opportunities to use it in, um,

1154
01:03:05,835 --> 01:03:09,300
increasingly high-stakes decisions and although,

1155
01:03:09,300 --> 01:03:11,775
you know, there are some nice- there's some appeal to that,

1156
01:03:11,775 --> 01:03:14,310
um, there's also a lot of risk.

1157
01:03:14,310 --> 01:03:17,310
Um, any more questions on, uh,

1158
01:03:17,310 --> 01:03:20,620
this sort of social impact of NLP?

1159
01:03:21,650 --> 01:03:29,250
Okay. Um, last part of this lecture is looking more at future research, right?

1160
01:03:29,250 --> 01:03:30,465
And in particular, um,

1161
01:03:30,465 --> 01:03:33,510
I think a lot of the current research trends are,

1162
01:03:33,510 --> 01:03:35,760
kind of reactions to BERT, um, right?

1163
01:03:35,760 --> 01:03:40,080
So, so the question is what did BERT solve and- and what do we work on next?

1164
01:03:40,080 --> 01:03:44,295
Um, so here are results on the GLUE benchmark.

1165
01:03:44,295 --> 01:03:47,070
Um, that is, uh, a compendium of,

1166
01:03:47,070 --> 01:03:50,280
uh, 10 natural language understanding tasks.

1167
01:03:50,280 --> 01:03:54,420
Um, and you get an average score across those 10 tasks.

1168
01:03:54,420 --> 01:03:57,810
Um, the left, uh, two- the two are,

1169
01:03:57,810 --> 01:04:00,720
sorry the right- two right most models are,

1170
01:04:00,720 --> 01:04:03,330
um, uh, s- non, uh,

1171
01:04:03,330 --> 01:04:06,480
are just supervised trained machine learning systems, right?

1172
01:04:06,480 --> 01:04:08,355
So we have Bag-of-Vectors, um,

1173
01:04:08,355 --> 01:04:10,920
we instead use our fancy neural net architecture

1174
01:04:10,920 --> 01:04:13,650
of BiLSTM + Attention and we get about five points.

1175
01:04:13,650 --> 01:04:15,600
Um, but the gains from BERT,

1176
01:04:15,600 --> 01:04:17,520
uh, really dwarf that difference, right?

1177
01:04:17,520 --> 01:04:19,890
So, so BERT improves results by about, uh,

1178
01:04:19,890 --> 01:04:24,120
17 points and we end up being actually quite close,

1179
01:04:24,120 --> 01:04:26,925
um, to human performance on these tasks.

1180
01:04:26,925 --> 01:04:29,820
Um, so one, sort of,

1181
01:04:29,820 --> 01:04:32,220
implication of this that people are wondering about is,

1182
01:04:32,220 --> 01:04:35,115
is this, kind of, the death of architecture engineering?

1183
01:04:35,115 --> 01:04:39,225
Um, so I'm sure all of you who have worked on the default final project, um,

1184
01:04:39,225 --> 01:04:42,570
have seen a whole bunch of fancy pictures showing different,

1185
01:04:42,570 --> 01:04:44,490
uh, architectures for solving SQuAD.

1186
01:04:44,490 --> 01:04:46,710
Um, there are a lot of papers.

1187
01:04:46,710 --> 01:04:48,390
They all propose some, kind of,

1188
01:04:48,390 --> 01:04:50,895
uh, attention mechanism or something like that.

1189
01:04:50,895 --> 01:04:53,880
Um, and, um, right.

1190
01:04:53,880 --> 01:04:55,170
With BERT, it's, sort of,

1191
01:04:55,170 --> 01:04:56,970
um, you don't need to do any of that, right?

1192
01:04:56,970 --> 01:04:59,190
You just train a transformer and you give it enough data,

1193
01:04:59,190 --> 01:05:01,020
and actually you're doing great on SQuAD,

1194
01:05:01,020 --> 01:05:03,885
you know, maybe, um, these, uh,

1195
01:05:03,885 --> 01:05:07,800
architectural enhancements are not necessarily, um,

1196
01:05:07,800 --> 01:05:10,590
the key thing that'll drive progress in,

1197
01:05:10,590 --> 01:05:13,720
uh, improving results on these tasks.

1198
01:05:14,150 --> 01:05:16,740
Um, right. So, uh,

1199
01:05:16,740 --> 01:05:18,630
if you look at this with the perspective of a researcher,

1200
01:05:18,630 --> 01:05:20,610
you can think a researcher will say, "Okay,

1201
01:05:20,610 --> 01:05:23,520
I can spend six months designing a fancy new architecture for

1202
01:05:23,520 --> 01:05:27,930
SQuAD and if I do a good job maybe I'll improve results by 1, uh, F1 point."

1203
01:05:27,930 --> 01:05:30,030
Um, but in the case of BERT, um,

1204
01:05:30,030 --> 01:05:32,160
increasing the size of their model of 3x,

1205
01:05:32,160 --> 01:05:33,240
which is the difference between,

1206
01:05:33,240 --> 01:05:36,090
they've like a base size model and a large model,

1207
01:05:36,090 --> 01:05:39,585
um, that improve results by 5 F1 points.

1208
01:05:39,585 --> 01:05:42,150
Um, so it does seem to suggest we need to, sort of,

1209
01:05:42,150 --> 01:05:46,635
re-prioritize, um, which avenues of research we'd pursue,

1210
01:05:46,635 --> 01:05:49,500
because this architecture engineering isn't providing, kind of,

1211
01:05:49,500 --> 01:05:52,605
gains for its time investment the way,

1212
01:05:52,605 --> 01:05:54,765
uh, leveraging unlabeled data is.

1213
01:05:54,765 --> 01:05:57,735
Um, so now, if you look at the SQuAD leaderboard, um,

1214
01:05:57,735 --> 01:06:02,350
I think at least the top 20 entrants are all BERT plus something.

1215
01:06:04,190 --> 01:06:07,725
Um, one other issue, uh,

1216
01:06:07,725 --> 01:06:09,540
I think BERT has raised is that,

1217
01:06:09,540 --> 01:06:11,400
um, we need harder tasks, right?

1218
01:06:11,400 --> 01:06:13,560
BERT has almost solved SQuAD,

1219
01:06:13,560 --> 01:06:15,060
if you define it by, uh,

1220
01:06:15,060 --> 01:06:16,860
getting close to human performance.

1221
01:06:16,860 --> 01:06:19,230
Um, so there's been, um,

1222
01:06:19,230 --> 01:06:22,635
a growth in new datasets that are, uh,

1223
01:06:22,635 --> 01:06:25,020
more challenging and there are a couple of ways in which,

1224
01:06:25,020 --> 01:06:26,370
um, they can be more challenging.

1225
01:06:26,370 --> 01:06:28,140
So one is, um,

1226
01:06:28,140 --> 01:06:30,240
doing reading comprehension on longer documents,

1227
01:06:30,240 --> 01:06:32,625
or doing it across more than one document.

1228
01:06:32,625 --> 01:06:35,280
Um, one area is looking at c- uh,

1229
01:06:35,280 --> 01:06:38,850
coming up with harder questions that require a multi-hop reasoning.

1230
01:06:38,850 --> 01:06:41,550
Um, so that essentially meas- means you have to string

1231
01:06:41,550 --> 01:06:45,180
together multiple supporting facts from different places,

1232
01:06:45,180 --> 01:06:47,670
um, to produce the correct answer.

1233
01:06:47,670 --> 01:06:49,350
Um, and another area,

1234
01:06:49,350 --> 01:06:51,870
situating question-answering within a dialogue.

1235
01:06:51,870 --> 01:06:54,330
Um, there's also been a, kind of,

1236
01:06:54,330 --> 01:06:58,260
small detail with the construction of reading comprehension datasets,

1237
01:06:58,260 --> 01:07:00,600
that has actually really affected,

1238
01:07:00,600 --> 01:07:02,835
um, the, the difficulty of the task.

1239
01:07:02,835 --> 01:07:04,110
And that is whether, um,

1240
01:07:04,110 --> 01:07:06,495
when you create these datasets, um,

1241
01:07:06,495 --> 01:07:09,420
is the person who writes questions about a passage,

1242
01:07:09,420 --> 01:07:11,535
can they see that passage or not?

1243
01:07:11,535 --> 01:07:14,070
Um, so of course, it's much easier to come up

1244
01:07:14,070 --> 01:07:16,110
with a question that when you see the passage,

1245
01:07:16,110 --> 01:07:18,870
and if you come up with a question without seeing the passage,

1246
01:07:18,870 --> 01:07:21,810
you may not even have a answerable question.

1247
01:07:21,810 --> 01:07:23,730
Um, but the problem with looking at

1248
01:07:23,730 --> 01:07:26,460
the passage is that first of all it's not realistic, right?

1249
01:07:26,460 --> 01:07:28,845
So, uh, if I'm asking a question, you know,

1250
01:07:28,845 --> 01:07:30,585
I'm not going to have usually

1251
01:07:30,585 --> 01:07:33,870
the paragraph that answers that question sitting in front of me.

1252
01:07:33,870 --> 01:07:35,670
Um, on top of that,

1253
01:07:35,670 --> 01:07:37,560
it really encourages easy questions, right?

1254
01:07:37,560 --> 01:07:39,840
So, um, if you're a Mechanical Turker,

1255
01:07:39,840 --> 01:07:42,869
and you're paid to write as many questions as possible,

1256
01:07:42,869 --> 01:07:44,790
and then you see an article that says,

1257
01:07:44,790 --> 01:07:46,350
um, I don't know, you know,

1258
01:07:46,350 --> 01:07:50,040
uh, Abraham Lincoln was the 16th president of the United States,

1259
01:07:50,040 --> 01:07:51,600
um, what are you gonna write?

1260
01:07:51,600 --> 01:07:53,100
As your question, you're gonna write,

1261
01:07:53,100 --> 01:07:55,365
who was the 16th president of the United States.

1262
01:07:55,365 --> 01:07:58,035
You're not gonna write something more interesting that's harder to answer.

1263
01:07:58,035 --> 01:08:01,890
Um, so- so this is one way in which crowdsourced datasets have changed, um,

1264
01:08:01,890 --> 01:08:04,170
people are now making sure questions are,

1265
01:08:04,170 --> 01:08:07,410
sort of, independent of, of the contexts.

1266
01:08:07,410 --> 01:08:09,375
Um, so I'm gonna briefly, uh,

1267
01:08:09,375 --> 01:08:11,610
go over a couple of new datasets in this line.

1268
01:08:11,610 --> 01:08:15,150
So one is called QuAC, which stands for Question Answering in Context.

1269
01:08:15,150 --> 01:08:16,815
Um, in this dataset,

1270
01:08:16,815 --> 01:08:18,690
there is a teacher and a student,

1271
01:08:18,690 --> 01:08:21,390
um, the teacher sees a Wikipedia article.

1272
01:08:21,390 --> 01:08:24,195
The student wants to learn about this Wikipedia article,

1273
01:08:24,195 --> 01:08:28,005
and the goal is to train a machine learning model that acts as the teacher.

1274
01:08:28,005 --> 01:08:30,000
Um, so you can imagine maybe in the future, this,

1275
01:08:30,000 --> 01:08:32,190
sort of, technology would be useful for,

1276
01:08:32,190 --> 01:08:34,320
uh, um, education for, kind of,

1277
01:08:34,320 --> 01:08:37,035
having, uh, adding some automation.

1278
01:08:37,035 --> 01:08:42,495
Um, uh, one thing that makes this task difficult is that,

1279
01:08:42,495 --> 01:08:46,545
uh, questions depend on the entire history of the conversation.

1280
01:08:46,545 --> 01:08:48,225
Um, so for example, uh,

1281
01:08:48,225 --> 01:08:50,790
if you look, um, on the left here, uh,

1282
01:08:50,790 --> 01:08:54,810
the example, um, dialogue,

1283
01:08:54,810 --> 01:08:57,315
um, the third question is was he the star?

1284
01:08:57,315 --> 01:09:02,070
Um, clearly you can't answer that question unless you look back earlier in the dialogue,

1285
01:09:02,070 --> 01:09:04,095
and realize that the subject of this,

1286
01:09:04,095 --> 01:09:06,180
uh, conversation is Daffy Duck.

1287
01:09:06,180 --> 01:09:09,060
Um, a- and, sort of,

1288
01:09:09,060 --> 01:09:11,040
because this dataset is more challenging,

1289
01:09:11,040 --> 01:09:14,340
and you can see there's a, there's a much bigger gap to human performance, right?

1290
01:09:14,340 --> 01:09:17,610
So if you train some BERT with some extensions, you'll st- uh,

1291
01:09:17,610 --> 01:09:22,185
the results are still like 15 F1 points worse than human performance.

1292
01:09:22,185 --> 01:09:28,935
Um, um, here's one other dataset, um, called HotPotQA.

1293
01:09:28,935 --> 01:09:30,510
Um, it is, uh,

1294
01:09:30,510 --> 01:09:32,760
designed instead for multi-hop reasoning.

1295
01:09:32,760 --> 01:09:35,610
Um, so essentially, in order to answer a question,

1296
01:09:35,610 --> 01:09:37,875
you have to look at multiple documents,

1297
01:09:37,875 --> 01:09:40,350
you have to look at different facts from those documents,

1298
01:09:40,350 --> 01:09:41,925
and perform some inference,

1299
01:09:41,925 --> 01:09:44,655
um, to get what the correct answer is.

1300
01:09:44,655 --> 01:09:48,645
Um, so I think, you know, this is a- a much harder task.

1301
01:09:48,645 --> 01:09:54,040
And again, um, there's a much bigger gap between human performance.

1302
01:09:54,590 --> 01:09:57,390
Um, any questions on, uh,

1303
01:09:57,390 --> 01:10:01,720
new datasets, um, harder chi- tasks for NLP?

1304
01:10:01,900 --> 01:10:07,035
Okay. Um, I'm gonna,

1305
01:10:07,035 --> 01:10:09,360
kind of, rapid fire and go through, um,

1306
01:10:09,360 --> 01:10:12,210
a couple of more areas in the last minutes of this talk.

1307
01:10:12,210 --> 01:10:16,335
Um, so multitask learning I think is really growing in importance.

1308
01:10:16,335 --> 01:10:18,390
Um, of course, um,

1309
01:10:18,390 --> 01:10:20,190
you've had a whole lecture on this, right?

1310
01:10:20,190 --> 01:10:21,750
So I'm not gonna spend too much time on it.

1311
01:10:21,750 --> 01:10:24,330
Um, but maybe one, uh,

1312
01:10:24,330 --> 01:10:28,920
point of interest is that if you look at performance on this GLUE benchmark,

1313
01:10:28,920 --> 01:10:31,320
so this benchmark for natural language understanding,

1314
01:10:31,320 --> 01:10:34,920
um, all the top couple results, um,

1315
01:10:34,920 --> 01:10:37,980
are- that are now actually surpassing BERT in

1316
01:10:37,980 --> 01:10:42,390
performance are- is taking BERT and training it in a multi-task way.

1317
01:10:42,390 --> 01:10:47,370
Um, I think another interesting, uh,

1318
01:10:47,370 --> 01:10:52,020
motivation for multi-task learning is that if you are training BERT, you have a really,

1319
01:10:52,020 --> 01:10:54,480
really large model and one way to make

1320
01:10:54,480 --> 01:10:58,690
more efficient use of that model is training it to do many things at once.

1321
01:11:00,950 --> 01:11:04,920
Another area that's definitely important, um,

1322
01:11:04,920 --> 01:11:09,090
and I think will be important going in the future is dealing with low-resource settings.

1323
01:11:09,090 --> 01:11:10,890
Um, and here I'm using a really broad,

1324
01:11:10,890 --> 01:11:13,020
uh, definition of resources, right.

1325
01:11:13,020 --> 01:11:15,435
So that could mean compute power, um, you know,

1326
01:11:15,435 --> 01:11:18,990
BERT is great but it also takes huge amounts of compute to run it.

1327
01:11:18,990 --> 01:11:20,310
So it's not realistic to say,

1328
01:11:20,310 --> 01:11:22,545
um, if you're building, let's say a mobile, uh,

1329
01:11:22,545 --> 01:11:27,510
an app for a mobile device that you could run a model the size of BERT.

1330
01:11:27,510 --> 01:11:31,845
Um, as I already ga- went into earlier in this talk, um, you know,

1331
01:11:31,845 --> 01:11:36,225
low-resource languages is an area that I think is pretty, um,

1332
01:11:36,225 --> 01:11:39,120
under-represented in NLP research right now,

1333
01:11:39,120 --> 01:11:41,460
because most datasets are in English, um,

1334
01:11:41,460 --> 01:11:42,570
but I do think, right,

1335
01:11:42,570 --> 01:11:44,130
there's a really, you know,

1336
01:11:44,130 --> 01:11:49,245
large number of people that in order to benefit from NLP technology, um,

1337
01:11:49,245 --> 01:11:52,200
we'll need to have technologies that work well in a lot of

1338
01:11:52,200 --> 01:11:56,055
different languages especially those without much training data.

1339
01:11:56,055 --> 01:12:00,870
And, um, speaking of low- low amounts of training data, I think in general this is,

1340
01:12:00,870 --> 01:12:04,065
uh, a- an interesting area of research,

1341
01:12:04,065 --> 01:12:05,550
um, within machine learning.

1342
01:12:05,550 --> 01:12:07,305
Actually, people are, um,

1343
01:12:07,305 --> 01:12:09,315
working a lot on this as well.

1344
01:12:09,315 --> 01:12:11,460
Um, so a term is often, uh,

1345
01:12:11,460 --> 01:12:14,025
a term often used is few shot learning.

1346
01:12:14,025 --> 01:12:16,410
Um, and that essentially means being able to

1347
01:12:16,410 --> 01:12:18,720
train a machine learning model that only sees,

1348
01:12:18,720 --> 01:12:20,730
let's say five or ten examples.

1349
01:12:20,730 --> 01:12:23,370
Um, one motivation there is, um,

1350
01:12:23,370 --> 01:12:29,445
I think a clear distinction between how our existing machine learning systems learn,

1351
01:12:29,445 --> 01:12:31,875
and how humans learn is that, um,

1352
01:12:31,875 --> 01:12:35,550
humans can generalize very quickly from five or so examples.

1353
01:12:35,550 --> 01:12:37,185
Um, if you're training a neural net,

1354
01:12:37,185 --> 01:12:38,580
you normally need, you know,

1355
01:12:38,580 --> 01:12:41,610
thousands of examples or perhaps even tens of thousands,

1356
01:12:41,610 --> 01:12:45,060
hundreds of thousands of examples to get something that works.

1357
01:12:45,060 --> 01:12:49,650
Um, so I also see this being a pretty important area in the future.

1358
01:12:49,650 --> 01:12:53,730
Um, the last area where I want to go in, um,

1359
01:12:53,730 --> 01:12:57,600
a little bit more depth is interpreting and understanding models.

1360
01:12:57,600 --> 01:13:00,570
Um, so, so really there's two aspects of this.

1361
01:13:00,570 --> 01:13:04,095
One is if I have a machine learning model and it makes a prediction,

1362
01:13:04,095 --> 01:13:06,450
I would like to be able to, uh,

1363
01:13:06,450 --> 01:13:08,790
know why did it make that prediction?

1364
01:13:08,790 --> 01:13:11,385
So gets some rationale, get some explanation,

1365
01:13:11,385 --> 01:13:15,180
um, that would especially be important in an area like health care, right?

1366
01:13:15,180 --> 01:13:17,910
So if you're a doctor and you're making a decision, um,

1367
01:13:17,910 --> 01:13:21,090
it's probably not good enough for your machine learning model to say,

1368
01:13:21,090 --> 01:13:22,470
"Patient has disease X."

1369
01:13:22,470 --> 01:13:23,805
You really want it to say,

1370
01:13:23,805 --> 01:13:26,070
"Patient has disease X for these reasons."

1371
01:13:26,070 --> 01:13:28,589
Um, because then you as a doctor can double-check,

1372
01:13:28,589 --> 01:13:30,540
and, and try to validate the, the,

1373
01:13:30,540 --> 01:13:33,165
uh, machine's, um, thinking I guess,

1374
01:13:33,165 --> 01:13:35,610
um, to come up with that diagnosis.

1375
01:13:35,610 --> 01:13:38,640
Um, the other area of interpreting

1376
01:13:38,640 --> 01:13:41,370
understanding models is more of a scientific question, right?

1377
01:13:41,370 --> 01:13:43,860
Is we know things like BERT work really well,

1378
01:13:43,860 --> 01:13:45,960
um, we want to know why do they work well?

1379
01:13:45,960 --> 01:13:48,195
What -what what aspects of language do they model?

1380
01:13:48,195 --> 01:13:49,995
Um, what things don't they model?

1381
01:13:49,995 --> 01:13:52,020
Um, and that might lead to, um,

1382
01:13:52,020 --> 01:13:55,695
ideas of improving, um, those- those models.

1383
01:13:55,695 --> 01:13:59,580
Um, so, um, here is a, uh,

1384
01:13:59,580 --> 01:14:04,935
couple slides on the main approach for evalu- answering the sort of scientific questions.

1385
01:14:04,935 --> 01:14:06,975
What does a machine-learning model learn?

1386
01:14:06,975 --> 01:14:10,530
Um, what you do is you have a model so let's say it's BERT.

1387
01:14:10,530 --> 01:14:13,440
It takes as input a sequence of words, um,

1388
01:14:13,440 --> 01:14:16,470
it produces as output a sequence of vectors, um,

1389
01:14:16,470 --> 01:14:18,570
we want to ask does it know for example,

1390
01:14:18,570 --> 01:14:19,680
the part of speech of words?

1391
01:14:19,680 --> 01:14:22,455
So, so it does in its vector representations,

1392
01:14:22,455 --> 01:14:24,630
does that capture something about syntax?

1393
01:14:24,630 --> 01:14:29,850
Um, and a simple way of asking this question is train another classifier on top of BERT,

1394
01:14:29,850 --> 01:14:31,965
uh, that's trained to do,

1395
01:14:31,965 --> 01:14:34,395
um, let's say part-of-speech tagging.

1396
01:14:34,395 --> 01:14:36,825
Um, but we only, um,

1397
01:14:36,825 --> 01:14:39,945
backprop into that diagnostic classifier itself.

1398
01:14:39,945 --> 01:14:43,680
So in other words we're treating the output of BERT, um,

1399
01:14:43,680 --> 01:14:46,185
that sequence of vectors as a fixed input,

1400
01:14:46,185 --> 01:14:48,600
and we're sort of probing those vectors to see,

1401
01:14:48,600 --> 01:14:50,505
um, do they contain, um,

1402
01:14:50,505 --> 01:14:52,440
information about a part of speech that

1403
01:14:52,440 --> 01:14:56,445
this second diagnostic classifier on top can decode,

1404
01:14:56,445 --> 01:14:59,050
um, to get the correct labels?

1405
01:14:59,120 --> 01:15:03,690
Um, so, um, it was kind of quite a few concerns here.

1406
01:15:03,690 --> 01:15:06,540
Um, one concern is, uh,

1407
01:15:06,540 --> 01:15:09,915
if you make your diagnostic classifier too complicated,

1408
01:15:09,915 --> 01:15:13,200
it can just solve the classif- the task all on itself,

1409
01:15:13,200 --> 01:15:15,210
and it can basically ignore, uh,

1410
01:15:15,210 --> 01:15:17,565
whatever representations were produced by BERT.

1411
01:15:17,565 --> 01:15:20,040
Um, so- so the kind of standard thing right now is to use

1412
01:15:20,040 --> 01:15:23,205
a single softmax layer on top of BERT,

1413
01:15:23,205 --> 01:15:25,185
um, to do these decisions.

1414
01:15:25,185 --> 01:15:29,100
Um, and there's been a whole bunch of tasks proposed for

1415
01:15:29,100 --> 01:15:32,895
evaluating essentially the linguistic knowledge of these models.

1416
01:15:32,895 --> 01:15:34,785
Um, so you could do part-of-speech tagging,

1417
01:15:34,785 --> 01:15:37,080
you could do more semantic tasks like,

1418
01:15:37,080 --> 01:15:39,285
uh, relation extraction, um,

1419
01:15:39,285 --> 01:15:41,265
or- or something like co-reference.

1420
01:15:41,265 --> 01:15:44,280
Um, and this is a pretty active area of work.

1421
01:15:44,280 --> 01:15:47,055
Um, here is, uh, just one, uh,

1422
01:15:47,055 --> 01:15:51,195
plot showing some of the results, um, of this approach.

1423
01:15:51,195 --> 01:15:53,865
So here what we're doing is we're adding

1424
01:15:53,865 --> 01:15:56,955
diagnostic classifiers to different layers of BERT,

1425
01:15:56,955 --> 01:16:02,620
and we are seeing which layers of BERT are more useful for particular tasks.

1426
01:16:02,620 --> 01:16:07,025
Um, and, um, something kind of interesting comes out of this which is that, um,

1427
01:16:07,025 --> 01:16:10,310
the different layers of BERT seem to be corresponding, um,

1428
01:16:10,310 --> 01:16:12,890
fairly well with notions of,

1429
01:16:12,890 --> 01:16:15,395
uh, different layers of li- of linguistics.

1430
01:16:15,395 --> 01:16:19,110
Um, so, uh, dependency parsing which is a syntactic task,

1431
01:16:19,110 --> 01:16:20,940
um, it's, uh, considered sort of a, you know,

1432
01:16:20,940 --> 01:16:23,430
medium level task in understanding a sentence.

1433
01:16:23,430 --> 01:16:28,125
Um, the medium layers of BERT, so layers kind of 6 through 8 or something,

1434
01:16:28,125 --> 01:16:30,480
are the ones best at dependency parsing.

1435
01:16:30,480 --> 01:16:34,095
Um, if you have a se- very semantic task like sentiment analysis,

1436
01:16:34,095 --> 01:16:35,880
um, where you're trying to learn some kind of, uh,

1437
01:16:35,880 --> 01:16:38,325
semantic property of the whole sentence, um,

1438
01:16:38,325 --> 01:16:41,490
then the very last layers of BERT are the ones that seem

1439
01:16:41,490 --> 01:16:45,700
to encode the most information about- about this, uh, phenomenon.

1440
01:16:46,310 --> 01:16:48,690
Um, okay.

1441
01:16:48,690 --> 01:16:50,835
So this is almost it for the talk, um,

1442
01:16:50,835 --> 01:16:54,600
I just have one slide here of, uh, um,

1443
01:16:54,600 --> 01:16:57,870
NLP not in kind of the academic researching context,

1444
01:16:57,870 --> 01:17:00,735
which I have already been talking a lot about but NLP in industry,

1445
01:17:00,735 --> 01:17:03,075
and really there's rapid progress there.

1446
01:17:03,075 --> 01:17:06,315
And I wanted to point to you two areas where I think there's

1447
01:17:06,315 --> 01:17:10,650
especially a large interest in using NLP technology.

1448
01:17:10,650 --> 01:17:12,240
Um, one is dialogue,

1449
01:17:12,240 --> 01:17:14,010
um, so for things like chatbots, right?

1450
01:17:14,010 --> 01:17:17,580
There's the Alexa Prize where they're actually investing a lot of money in,

1451
01:17:17,580 --> 01:17:21,105
um, having groups figure out how to improve chitchat dialogue.

1452
01:17:21,105 --> 01:17:25,230
Um, there's also I think a lot of potential for customer service, right?

1453
01:17:25,230 --> 01:17:28,170
So improving basically automated systems that'll, um,

1454
01:17:28,170 --> 01:17:29,580
you know, book you a flight,

1455
01:17:29,580 --> 01:17:32,385
or help you cancel a subscription, or anything like that.

1456
01:17:32,385 --> 01:17:35,460
Um, and similarly, there's a lot of potential in health care.

1457
01:17:35,460 --> 01:17:39,180
Um, one is understanding the records of someone who,

1458
01:17:39,180 --> 01:17:42,060
um, is sick and to help them- to help with diagnoses.

1459
01:17:42,060 --> 01:17:43,935
Um, I think another, um,

1460
01:17:43,935 --> 01:17:46,215
equally important area is actually, uh,

1461
01:17:46,215 --> 01:17:49,020
parsing, uh, biomedical papers.

1462
01:17:49,020 --> 01:17:54,285
Um, so, um, the number of biomedical papers that are being written is really insane,

1463
01:17:54,285 --> 01:17:56,100
um, it's, it's way larger than the number

1464
01:17:56,100 --> 01:17:57,960
of computer science papers that are being written.

1465
01:17:57,960 --> 01:18:01,530
[NOISE] Um, often if you're a doctor,

1466
01:18:01,530 --> 01:18:03,150
or if you're a researcher, um,

1467
01:18:03,150 --> 01:18:06,360
in medicine, you might want to look up something very specific, right?

1468
01:18:06,360 --> 01:18:07,620
You might want to know what is

1469
01:18:07,620 --> 01:18:11,370
the effect of this particular drug on this particular gene,

1470
01:18:11,370 --> 01:18:13,140
or a cell with this particular gene.

1471
01:18:13,140 --> 01:18:16,710
Um, there's no good way right now of searching through, um,

1472
01:18:16,710 --> 01:18:20,175
hundreds of thousands of papers to find if someone has a- has, uh,

1473
01:18:20,175 --> 01:18:23,085
done this experiment and have results for this,

1474
01:18:23,085 --> 01:18:25,095
um, particular combination of things.

1475
01:18:25,095 --> 01:18:28,590
Um, so automated reading of all this biomedical literature,

1476
01:18:28,590 --> 01:18:30,850
um, could have a lot of value.

1477
01:18:31,100 --> 01:18:33,960
Okay, um, to conclude, um,

1478
01:18:33,960 --> 01:18:38,280
there's been rapid progress in the last five years due to deep learning, um, in NLP.

1479
01:18:38,280 --> 01:18:42,780
Um, in the last year, we've seen another really kind of, uh,

1480
01:18:42,780 --> 01:18:45,300
a dramatic increase in the capability of our systems,

1481
01:18:45,300 --> 01:18:47,610
thanks to, uh, using unlabeled data.

1482
01:18:47,610 --> 01:18:49,095
So that's methods like BERT.

1483
01:18:49,095 --> 01:18:54,210
Um, and, um, the other kind of thing that's I think important to think about is that,

1484
01:18:54,210 --> 01:18:58,170
NLP systems are starting to be at a place where they can have big social impact.

1485
01:18:58,170 --> 01:19:04,845
Um, so that makes some issues like bias and security very important. Um, thank you.

1486
01:19:04,845 --> 01:19:06,690
Uh, good luck finishing all your projects.

1487
01:19:06,690 --> 01:19:14,800
[APPLAUSE].

