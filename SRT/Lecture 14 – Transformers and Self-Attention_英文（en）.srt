1
00:00:04,520 --> 00:00:08,670
Okay. So I'm delighted to introduce,

2
00:00:08,670 --> 00:00:11,355
um, our first lot of invited speakers.

3
00:00:11,355 --> 00:00:14,460
And so we're gonna have two invited speakers, um, today.

4
00:00:14,460 --> 00:00:16,260
So starting off, um,

5
00:00:16,260 --> 00:00:18,960
we go and have Ashish Vaswani who's gonna be

6
00:00:18,960 --> 00:00:23,115
talking about self attention for generative models and in particular,

7
00:00:23,115 --> 00:00:25,530
um, we'll introduce some of the work on

8
00:00:25,530 --> 00:00:29,190
transformers that he is well-known for along with his colleagues.

9
00:00:29,190 --> 00:00:32,220
Um and then as a sort of, um,

10
00:00:32,220 --> 00:00:35,235
a special edition then we're also going to have

11
00:00:35,235 --> 00:00:39,440
Anna Huang talking about some applications of this work.

12
00:00:39,440 --> 00:00:41,540
There are actually at least a couple of people in the class who are

13
00:00:41,540 --> 00:00:43,790
actually interested in music applications.

14
00:00:43,790 --> 00:00:48,740
So this will be your one chance in the course to see music applications of deep learning.

15
00:00:48,740 --> 00:00:51,890
Okay, um, so I'll hand it over to Ashish.

16
00:00:51,890 --> 00:00:53,360
Thanks, Chris and, uh, thanks, Evie.

17
00:00:53,360 --> 00:00:55,940
Uh, Anna is actually here to make the class less dull.

18
00:00:55,940 --> 00:00:57,830
So [LAUGHTER] she's the highlight on this one.

19
00:00:57,830 --> 00:01:00,845
So uh, so, uh, hi everyone.

20
00:01:00,845 --> 00:01:03,395
Um, um, uh excited to be here.

21
00:01:03,395 --> 00:01:06,205
This is a very large class.

22
00:01:06,205 --> 00:01:07,920
Uh, first invited speaker,

23
00:01:07,920 --> 00:01:09,920
no pressure, so hopefully this will all go well.

24
00:01:09,920 --> 00:01:14,975
Uh, so yes, so the talk is going to be about, uh, self attention.

25
00:01:14,975 --> 00:01:18,345
Um, and so the purpose is,

26
00:01:18,345 --> 00:01:22,685
is not going to be just to talk about a particular model, but, as,

27
00:01:22,685 --> 00:01:25,850
as, as, as empiricists and, and,

28
00:01:25,850 --> 00:01:27,830
like, well, I'm an empiricist and I

29
00:01:27,830 --> 00:01:30,380
consume machine learning to apply it to various tasks.

30
00:01:30,380 --> 00:01:35,210
And, and, and, well, starting point always is to ask this question, you know,

31
00:01:35,210 --> 00:01:36,800
what are the- what's the structure in

32
00:01:36,800 --> 00:01:38,720
my dataset or what are the symmetries in my dataset,

33
00:01:38,720 --> 00:01:41,880
and is there a model that exists that that's a very good- that,

34
00:01:41,880 --> 00:01:46,010
that has the inductive biases to model these properties that exist in my dataset.

35
00:01:46,010 --> 00:01:48,455
So hopefully, over the course of this, uh,

36
00:01:48,455 --> 00:01:51,680
this, this lecture Anna and I will convince you that, uh,

37
00:01:51,680 --> 00:01:54,410
self attention indeed does have some- has

38
00:01:54,410 --> 00:01:56,165
the ability models and inductive biases that

39
00:01:56,165 --> 00:01:58,685
potentially could be useful for the problems that you care about.

40
00:01:58,685 --> 00:02:04,790
Um, so, um, this talk is going to be our learning representations primarily of,

41
00:02:04,790 --> 00:02:07,445
uh, variable length data where we have images but,

42
00:02:07,445 --> 00:02:10,190
uh, most of it is going to be variable length data.

43
00:02:10,190 --> 00:02:11,960
And, uh, and, and,

44
00:02:11,960 --> 00:02:14,975
and all of us care about this problem because we- in

45
00:02:14,975 --> 00:02:17,990
deep learning, and deep learning is all about representation learning.

46
00:02:17,990 --> 00:02:22,655
And if- and building the right tools for learning representations as,

47
00:02:22,655 --> 00:02:24,830
as, as, as sort of- is an important factor in,

48
00:02:24,830 --> 00:02:26,600
in achieving empirical success.

49
00:02:26,600 --> 00:02:29,810
Um, now, uh, the models of choice,

50
00:02:29,810 --> 00:02:32,165
the primary workhorse for

51
00:02:32,165 --> 00:02:35,600
perhaps even now and or up to this point had been recurrent neural networks.

52
00:02:35,600 --> 00:02:40,730
Um, um, how, how many people here are familiar with RNNs?

53
00:02:40,730 --> 00:02:43,055
[LAUGHTER] Okay.

54
00:02:43,055 --> 00:02:45,260
So definitely up to this point,

55
00:02:45,260 --> 00:02:47,690
the primary workhorse have been recurrent neural networks,

56
00:02:47,690 --> 00:02:50,465
and some of the more, uh, some, uh,

57
00:02:50,465 --> 00:02:54,540
some gated variants that explicitly add multiplicative interactions like LSTMs,

58
00:02:54,540 --> 00:02:58,175
they also, they also have mechanisms that allow for better gradient transfer.

59
00:02:58,175 --> 00:03:00,620
And some recent variants like gated, uh,

60
00:03:00,620 --> 00:03:02,300
recurrent units that are simplification,

61
00:03:02,300 --> 00:03:06,860
they're kind of the- they're- they dominate this, this recurrent landscape.

62
00:03:06,860 --> 00:03:09,605
Um, and typically how did recurrent neural networks, uh,

63
00:03:09,605 --> 00:03:12,675
learn or, um, produce representations?

64
00:03:12,675 --> 00:03:15,765
They consume a string or a sentence, um,

65
00:03:15,765 --> 00:03:17,625
even an image, imagine, you know,

66
00:03:17,625 --> 00:03:21,135
in a particular- in sequentially and, uh, at each,

67
00:03:21,135 --> 00:03:22,530
at each, uh, position,

68
00:03:22,530 --> 00:03:24,900
at each timestep they produce, they produce a,

69
00:03:24,900 --> 00:03:26,899
a continuous representation that's

70
00:03:26,899 --> 00:03:30,845
summarization of, of everything that they've actually crunched through.

71
00:03:30,845 --> 00:03:36,960
Um, now, so in, in, in the,

72
00:03:36,960 --> 00:03:39,480
in the realm of large data, uh,

73
00:03:39,480 --> 00:03:41,310
par- having parallel models is,

74
00:03:41,310 --> 00:03:42,890
is quite, is quite beneficial.

75
00:03:42,890 --> 00:03:45,140
In fact, I was actually reading Oliver Selfridge.

76
00:03:45,140 --> 00:03:46,370
Uh, he was a,

77
00:03:46,370 --> 00:03:48,770
he was a professor at MIT and, uh, he had this,

78
00:03:48,770 --> 00:03:53,360
uh, sorry, he wrote the precursor to deep nets its it's called Pandemoniums.

79
00:03:53,360 --> 00:03:54,590
I would recommend everybody to read it.

80
00:03:54,590 --> 00:03:56,450
And he has this fascinating note that, you know,

81
00:03:56,450 --> 00:03:57,960
if you give me more parallel computation,

82
00:03:57,960 --> 00:03:59,900
I'll just add more data and make it slower.

83
00:03:59,900 --> 00:04:02,180
So you can consume more data.

84
00:04:02,180 --> 00:04:07,260
Um, and, and recurrence, uh, recurrence sort of just by construction, um,

85
00:04:07,260 --> 00:04:08,915
limits parallelization because you have to,

86
00:04:08,915 --> 00:04:11,105
you have to wait until- your wait un-

87
00:04:11,105 --> 00:04:14,030
for a particular time point to produce a representation.

88
00:04:14,030 --> 00:04:16,275
Um, but if there's any questions,

89
00:04:16,275 --> 00:04:17,325
please raise your hands, I'll

90
00:04:17,325 --> 00:04:18,900
hopefully look around and, and,

91
00:04:18,900 --> 00:04:21,230
uh, be able to attend to your question.

92
00:04:21,230 --> 00:04:24,920
Um, and again, and, and now because we're actually producing these representations,

93
00:04:24,920 --> 00:04:26,150
we're sort of summarizing,

94
00:04:26,150 --> 00:04:27,740
you know, if you want to pass information,

95
00:04:27,740 --> 00:04:29,765
if you want to pass co-reference information,

96
00:04:29,765 --> 00:04:32,350
then we kind of have to shove all of this inside

97
00:04:32,350 --> 00:04:36,110
this fixed size vector, so it could potentially be difficult to model.

98
00:04:36,110 --> 00:04:39,635
And, uh, while they have been successful in language, uh,

99
00:04:39,635 --> 00:04:42,530
explicit they don't have- the architecture

100
00:04:42,530 --> 00:04:45,890
doesn't have a very clear explicit way to model hierarchy which is,

101
00:04:45,890 --> 00:04:48,155
which is something that's very important in language.

102
00:04:48,155 --> 00:04:54,390
Um, now, um, so they have been devin- it has been excellent work of,

103
00:04:54,390 --> 00:04:58,640
a precursor to self attention that actually surmounted some of these difficulties.

104
00:04:58,640 --> 00:05:01,550
And what were these difficulties basically is a convolutional sequence models

105
00:05:01,550 --> 00:05:05,180
where you have these limited receptive field convolutions that,

106
00:05:05,180 --> 00:05:07,220
again, consumed the sentence now not,

107
00:05:07,220 --> 00:05:09,590
not sequentially but in depth.

108
00:05:09,590 --> 00:05:12,130
And they produce representations for every-

109
00:05:12,130 --> 00:05:14,720
they produce representations of your variable length sequences.

110
00:05:14,720 --> 00:05:17,720
Um, and, uh, they're trivial to

111
00:05:17,720 --> 00:05:21,110
parallelize because you can apply these convolutions simultaneously at every position.

112
00:05:21,110 --> 00:05:23,030
Each layer is trivial to parallelize.

113
00:05:23,030 --> 00:05:26,165
Uh, the, the, the serial dependencies are only in the number of layers.

114
00:05:26,165 --> 00:05:28,235
Um, you can get, uh,

115
00:05:28,235 --> 00:05:29,960
you can- you can get

116
00:05:29,960 --> 00:05:32,750
these local dependencies efficiently because that a single application of

117
00:05:32,750 --> 00:05:37,475
a convolution can consume all the information inside its local receptive field.

118
00:05:37,475 --> 00:05:39,320
Um, now if you want to have

119
00:05:39,320 --> 00:05:42,170
these really long distance interactions while you

120
00:05:42,170 --> 00:05:45,020
don't have to pass through a linear number of steps,

121
00:05:45,020 --> 00:05:46,060
you still because these,

122
00:05:46,060 --> 00:05:49,670
because these receptive fields are local you might need something like linear

123
00:05:49,670 --> 00:05:53,525
and depth or logarithmic if you're doing something like dilated convolutions.

124
00:05:53,525 --> 00:05:56,030
So there's still need- the number of layers that are needed are

125
00:05:56,030 --> 00:05:59,215
still a function of the length of the of, of your string.

126
00:05:59,215 --> 00:06:01,070
Uh, but they're a great development and they

127
00:06:01,070 --> 00:06:03,320
actually pushed a lot of research like WaveRNN, for example,

128
00:06:03,320 --> 00:06:05,225
is a classic sort of success story of

129
00:06:05,225 --> 00:06:08,825
convolutio- convolutional sequence models even by net.

130
00:06:08,825 --> 00:06:15,085
Um, now, so far attention has been like one of the most important components,

131
00:06:15,085 --> 00:06:16,810
the sort of content-based,

132
00:06:16,810 --> 00:06:19,060
you know, memory retrieval mechanism.

133
00:06:19,060 --> 00:06:23,560
And it's content-based because you have your decoder that attends to all this content,

134
00:06:23,560 --> 00:06:26,625
that's your encoder and then just sort of decides what to wha- what,

135
00:06:26,625 --> 00:06:28,580
what information to absorb based on how similar

136
00:06:28,580 --> 00:06:30,980
this content is to every position in the memory.

137
00:06:30,980 --> 00:06:33,440
So this has been a very critical mechanism in,

138
00:06:33,440 --> 00:06:34,955
uh, in neural machine translation.

139
00:06:34,955 --> 00:06:36,950
So now the question that we asked was, like, why,

140
00:06:36,950 --> 00:06:40,460
why not just use attention for representations and, uh,

141
00:06:40,460 --> 00:06:43,790
now here's what sort of a rough framework of this,

142
00:06:43,790 --> 00:06:46,445
this representation mechanism would look like, uh,

143
00:06:46,445 --> 00:06:49,635
the way- just sort of repeating what attention is essentially.

144
00:06:49,635 --> 00:06:52,360
Now imagine you have- you want to represent the word,

145
00:06:52,360 --> 00:06:55,730
re-represent the word representing, you want to construct its new representation.

146
00:06:55,730 --> 00:06:58,610
And then first, uh, you, you attend or you,

147
00:06:58,610 --> 00:07:00,710
you compare yourself, you compare your content,

148
00:07:00,710 --> 00:07:02,765
and in the beginning it could just be a word embedding.

149
00:07:02,765 --> 00:07:05,540
Your compare content with all your words, and with all,

150
00:07:05,540 --> 00:07:07,340
with all the embeddings and based on these,

151
00:07:07,340 --> 00:07:09,905
based on these compatibilities or these comparisons,

152
00:07:09,905 --> 00:07:14,180
you produce, uh, you produce a weighted combination of your entire neighborhood,

153
00:07:14,180 --> 00:07:16,180
and based on that weighted combination you,

154
00:07:16,180 --> 00:07:17,870
you summarize all that information.

155
00:07:17,870 --> 00:07:20,150
So it's, like, you're re-expressing yourself in certain terms

156
00:07:20,150 --> 00:07:22,730
of a weighted combination of your entire neighborhood.

157
00:07:22,730 --> 00:07:23,930
That's what attention does,

158
00:07:23,930 --> 00:07:28,955
and you can add feed-forward layers to basically sort of compute new features for you.

159
00:07:28,955 --> 00:07:34,700
Um, now, um so the first part is going to be about how, like,

160
00:07:34,700 --> 00:07:37,760
some of the properties of self attention actually help us in text generation, like,

161
00:07:37,760 --> 00:07:39,320
what inductive biases are actually useful,

162
00:07:39,320 --> 00:07:40,950
and we empirically showed that indeed they,

163
00:07:40,950 --> 00:07:43,055
they move the needle in text generation.

164
00:07:43,055 --> 00:07:44,990
And this is going to be about machine translation,

165
00:07:44,990 --> 00:07:47,420
but there were other work also that we'll talk about later.

166
00:07:47,420 --> 00:07:49,880
So [NOISE] now with this, uh,

167
00:07:49,880 --> 00:07:51,885
with this sort of, uh,

168
00:07:51,885 --> 00:07:55,470
with this attention mechanism you get this- we get a constant path length.

169
00:07:55,470 --> 00:07:58,000
So all pairs or a word can in-

170
00:07:58,000 --> 00:08:01,100
position can interact with any position, every position simultaneously.

171
00:08:01,100 --> 00:08:04,250
Um, hopefully if the number of positions is not too many.

172
00:08:04,250 --> 00:08:06,410
Uh, attention just by virtue of, like,

173
00:08:06,410 --> 00:08:08,060
it's a construction, you have a softmax,

174
00:08:08,060 --> 00:08:10,205
you have these gating and multiplicative interactions.

175
00:08:10,205 --> 00:08:12,680
And again, I'm not gonna be able to explain why,

176
00:08:12,680 --> 00:08:14,195
but it's, it's interesting, like,

177
00:08:14,195 --> 00:08:15,290
you've seen these models, like,

178
00:08:15,290 --> 00:08:16,400
even, even the, uh,

179
00:08:16,400 --> 00:08:19,980
even Pixel, PixelCNN, uh, or, um,

180
00:08:19,980 --> 00:08:21,660
when it was actually modeling images,

181
00:08:21,660 --> 00:08:24,965
they explicitly had to add these multiplicative interactions inside the model to,

182
00:08:24,965 --> 00:08:26,885
to basically beat RNNs,

183
00:08:26,885 --> 00:08:29,390
and attention just by construction gets this because you're,

184
00:08:29,390 --> 00:08:33,035
you're multiplying the attention probabilities with your, with your activations.

185
00:08:33,035 --> 00:08:34,580
It's trivial to parallelize, why?

186
00:08:34,580 --> 00:08:39,440
Because you can just do attention with matmuls, especially the variant that we use in our paper,

187
00:08:39,440 --> 00:08:40,865
uh, in our work.

188
00:08:40,865 --> 00:08:43,895
And, uh, so now the question is

189
00:08:43,895 --> 00:08:49,160
convolutional sequence to- convolutional sequence models have been very successful in,

190
00:08:49,160 --> 00:08:52,325
in, in, in ge- generative tasks for text.

191
00:08:52,325 --> 00:08:54,830
Can we actually do the same or achieved the same with, uh,

192
00:08:54,830 --> 00:08:58,580
with, uh, attention as our primary workhorse for representation learning.

193
00:08:58,580 --> 00:09:03,485
Um, so just to sort of add some context and there's been some,

194
00:09:03,485 --> 00:09:07,430
there's been some- up to- up to the transformer there have been a lot of

195
00:09:07,430 --> 00:09:12,025
great work on using self attention primarily for classification within.

196
00:09:12,025 --> 00:09:15,290
There was, there was work on self attention within the confines of,

197
00:09:15,290 --> 00:09:16,610
like, recurrent neural networks.

198
00:09:16,610 --> 00:09:19,370
Um, perhaps the closest to us is the,

199
00:09:19,370 --> 00:09:20,915
is the memory networks,

200
00:09:20,915 --> 00:09:22,820
uh, by Weston, Sukhbaatar,

201
00:09:22,820 --> 00:09:25,715
where they actually had a version of recurrent attention,

202
00:09:25,715 --> 00:09:27,290
but they didn't have, uh,

203
00:09:27,290 --> 00:09:30,705
but they didn't actually- empirically,

204
00:09:30,705 --> 00:09:33,500
they didn't show it to work on sort of conditional modeling, like,

205
00:09:33,500 --> 00:09:37,370
uh, translation and their mechanism was, uh, like,

206
00:09:37,370 --> 00:09:41,555
they were using sort of a fixed- they were using a fixed query at every step.

207
00:09:41,555 --> 00:09:43,495
So there's- it, it leaves something to be desired.

208
00:09:43,495 --> 00:09:47,060
They still had this question, is it actually going to work, um, on,

209
00:09:47,060 --> 00:09:50,870
on, on large scale machine translation systems or large-scale text generation systems.

210
00:09:50,870 --> 00:09:54,100
So this is sort of the, the culmination of, um,

211
00:09:54,100 --> 00:09:57,435
of the, the self attention, our self attention work.

212
00:09:57,435 --> 00:10:00,495
This is the tran- the- and we put it together in the transformer model.

213
00:10:00,495 --> 00:10:03,195
And, uh, so how does this look like?

214
00:10:03,195 --> 00:10:05,975
So we're going to use attention pri- we're going to use

215
00:10:05,975 --> 00:10:09,395
attention primarily for computing representations so- of your input.

216
00:10:09,395 --> 00:10:11,480
Imagine you're doing English to German translation.

217
00:10:11,480 --> 00:10:14,030
So you have your words, and notice that,

218
00:10:14,030 --> 00:10:16,610
uh, attention is, uh, permutation invariant.

219
00:10:16,610 --> 00:10:19,220
So you just change the order of your positions.

220
00:10:19,220 --> 00:10:20,910
You change the order of your words and, and,

221
00:10:20,910 --> 00:10:23,315
uh, it's not going to affect the actual output.

222
00:10:23,315 --> 00:10:25,340
So in ord- in order to maintain order we add,

223
00:10:25,340 --> 00:10:26,985
we add position representations.

224
00:10:26,985 --> 00:10:29,710
And, uh, there's two kinds that we tried in the paper,

225
00:10:29,710 --> 00:10:33,185
these, these fantastic sinusoids with no entropy invented.

226
00:10:33,185 --> 00:10:35,625
And we also use learned representations which are

227
00:10:35,625 --> 00:10:38,090
very plain vanilla both of them work equally well.

228
00:10:38,090 --> 00:10:40,420
Um, and, uh, so,

229
00:10:40,420 --> 00:10:42,895
so first we have- so the encoder looks as follows, right?

230
00:10:42,895 --> 00:10:46,970
So we have a self attention layer that just recomputes the representation, uh,

231
00:10:46,970 --> 00:10:50,090
for every position simultaneously using attention,

232
00:10:50,090 --> 00:10:51,545
then we have a feed-forward layer.

233
00:10:51,545 --> 00:10:52,820
And we also have residual,

234
00:10:52,820 --> 00:10:54,380
residual connections and I'll,

235
00:10:54,380 --> 00:10:56,600
I'll sort of give you a glimpse of what these residual connections

236
00:10:56,600 --> 00:10:59,090
might be bringing that is between every,

237
00:10:59,090 --> 00:11:02,990
every layer, and the input we have a skip connection that just adds the activations.

238
00:11:02,990 --> 00:11:05,330
Uh, and then this tuple of, uh,

239
00:11:05,330 --> 00:11:08,135
self attention and feed-forward layer just essentially repeats.

240
00:11:08,135 --> 00:11:10,215
Now, on the decoder side, uh,

241
00:11:10,215 --> 00:11:13,925
we've- we, we have a sort of standard encoder decoder architecture.

242
00:11:13,925 --> 00:11:17,500
On the decoder side, we mimic a language model using self attention,

243
00:11:17,500 --> 00:11:20,300
and the way to mimic a language model using self attention is to impose

244
00:11:20,300 --> 00:11:23,540
causality by just masking out the positions that you can look at.

245
00:11:23,540 --> 00:11:25,660
So basically, uh,

246
00:11:25,660 --> 00:11:29,285
the first position it's- it can't look forward, it's illegal to look forward.

247
00:11:29,285 --> 00:11:32,075
It can look at itself because we actually shift the input.

248
00:11:32,075 --> 00:11:34,750
Um, so it's not copying, uh.

249
00:11:34,750 --> 00:11:37,240
It's kind of surprising that parti- with these models,

250
00:11:37,240 --> 00:11:38,790
it's very easy to copy at one point,

251
00:11:38,790 --> 00:11:41,585
when early on it was even harder to ge- you know,

252
00:11:41,585 --> 00:11:43,355
do copying with recurrent models.

253
00:11:43,355 --> 00:11:44,860
But now, at least, you can copy really well,

254
00:11:44,860 --> 00:11:46,850
which is a positive sign, I think overall.

255
00:11:46,850 --> 00:11:49,830
Um, but, uh, so now on the decoder side, uh,

256
00:11:49,830 --> 00:11:51,150
we have, uh, we have

257
00:11:51,150 --> 00:11:54,380
this causal self attention layer followed by encoder-decoder attention,

258
00:11:54,380 --> 00:11:56,180
where we actually attend to the, uh,

259
00:11:56,180 --> 00:11:59,450
last layer of the encoder and a feed-forward layer, and this tripled,

260
00:11:59,450 --> 00:12:00,670
repeats a mul- a few times,

261
00:12:00,670 --> 00:12:02,945
and at the end we have the standard cross-entropy loss.

262
00:12:02,945 --> 00:12:08,465
Um, and, um, so, um, sort of,

263
00:12:08,465 --> 00:12:10,655
staring at the- at,

264
00:12:10,655 --> 00:12:12,740
at our parti- at the particular variant of the self-

265
00:12:12,740 --> 00:12:15,205
of the attention mechanis- mechanism that we use,

266
00:12:15,205 --> 00:12:17,965
we went for both- we went for simplicity and speed.

267
00:12:17,965 --> 00:12:21,635
So, um, so how do you actually compute attention?

268
00:12:21,635 --> 00:12:24,470
So imagine you want to re-represent the position e2.

269
00:12:24,470 --> 00:12:26,930
And, uh, we're going to first linearly,

270
00:12:26,930 --> 00:12:30,220
linearly transform it into, uh, a query,

271
00:12:30,220 --> 00:12:32,150
and then we're gonna linearly transform

272
00:12:32,150 --> 00:12:34,520
every position in your neighborhood

273
00:12:34,520 --> 00:12:36,510
or let's say every position at the input because this is the,

274
00:12:36,510 --> 00:12:37,805
uh, uh, the encoder side,

275
00:12:37,805 --> 00:12:39,175
to, uh, a key.

276
00:12:39,175 --> 00:12:41,680
And these linear transformations can actually be thought as features,

277
00:12:41,680 --> 00:12:43,100
and I'll talk more about it later on.

278
00:12:43,100 --> 00:12:45,500
So it's like- it's, it's basically a bilinear form.

279
00:12:45,500 --> 00:12:48,355
You're projecting these vectors into a space where dot product is

280
00:12:48,355 --> 00:12:51,655
a good- where just a dot product is a good proxy for similarity.

281
00:12:51,655 --> 00:12:53,200
Okay? So now, you have your logit,

282
00:12:53,200 --> 00:12:55,810
so you just do a so- softmax computer convex combination.

283
00:12:55,810 --> 00:12:57,930
And now based on this convex combination,

284
00:12:57,930 --> 00:13:01,475
you're going to then re-express e2 or in

285
00:13:01,475 --> 00:13:05,380
terms of this convex combination of all the vectors of all these positions.

286
00:13:05,380 --> 00:13:08,105
And before doing- before doing the convex combination,

287
00:13:08,105 --> 00:13:10,505
we again do a linear transformation to produce values.

288
00:13:10,505 --> 00:13:13,940
And then we do a second linear transformation just to

289
00:13:13,940 --> 00:13:17,620
mix this information and pass it through a- pass it through a feedforward layer.

290
00:13:17,620 --> 00:13:19,075
And this is- um,

291
00:13:19,075 --> 00:13:21,915
and all of this can be expressed basically

292
00:13:21,915 --> 00:13:24,900
in two- in two- in two-matrix multiplications,

293
00:13:24,900 --> 00:13:27,620
and the square root factor is just to make sure that these,

294
00:13:27,620 --> 00:13:29,080
these dot products don't blow up.

295
00:13:29,080 --> 00:13:30,425
It's just a scaling factor.

296
00:13:30,425 --> 00:13:32,145
And, uh, and, and,

297
00:13:32,145 --> 00:13:33,605
wha- why is this particular- why is

298
00:13:33,605 --> 00:13:35,735
this mechanism attractive? Well, it's just really fast.

299
00:13:35,735 --> 00:13:37,340
You can do this very quickly on a GPU,

300
00:13:37,340 --> 00:13:39,010
and simul- you can do it simultaneously for

301
00:13:39,010 --> 00:13:43,045
all positions with just two matmuls and a softmax.

302
00:13:43,045 --> 00:13:45,320
Um, on the decoder side it's,

303
00:13:45,320 --> 00:13:46,640
it's exactly the same,

304
00:13:46,640 --> 00:13:54,585
except we impose causality by just adding 10 e- minus 10 e9 to the logits.

305
00:13:54,585 --> 00:13:58,135
So it basi- it's just- you just get zero probabilities on those positions.

306
00:13:58,135 --> 00:14:00,820
So we just impose causality by, by adding these,

307
00:14:00,820 --> 00:14:04,410
uh, highly negative values on the attention- on the attention logits.

308
00:14:04,410 --> 00:14:06,745
Um, is, is everything-

309
00:14:06,745 --> 00:14:07,410
[LAUGHTER]

310
00:14:07,410 --> 00:14:13,600
I thought that was a question.

311
00:14:13,600 --> 00:14:18,455
So, um, [LAUGHTER] okay so attention is really, uh, attention is cheap.

312
00:14:18,455 --> 00:14:20,890
So because it's- because this variant of

313
00:14:20,890 --> 00:14:23,905
attention just involve two- involves two matrix multiplications,

314
00:14:23,905 --> 00:14:26,510
it's quadratic in the length of your sequence.

315
00:14:26,510 --> 00:14:30,815
And now what's the computational profile of RNNs or convolutions?

316
00:14:30,815 --> 00:14:32,370
They're quadratic in the dimension.

317
00:14:32,370 --> 00:14:35,045
Because, basically, you can just think of a convolution just flattening

318
00:14:35,045 --> 00:14:38,170
your input or just applying a linear transformation on top of it, right?

319
00:14:38,170 --> 00:14:40,835
So- and when does this actually become very attractive?

320
00:14:40,835 --> 00:14:44,140
This becomes very, very attractive when your dimension is,

321
00:14:44,140 --> 00:14:46,635
uh, much larger than your length.

322
00:14:46,635 --> 00:14:48,455
Which is the case for machine translation.

323
00:14:48,455 --> 00:14:51,335
Now, we will talk about cases when there's- when the- when this is not true,

324
00:14:51,335 --> 00:14:54,280
and we have to- we have to do a- we have to make other model developments.

325
00:14:54,280 --> 00:14:56,440
Um, but, uh, but for

326
00:14:56,440 --> 00:14:58,020
short sequences or sequences where

327
00:14:58,020 --> 00:15:00,020
your length does- where your dimension dominates length,

328
00:15:00,020 --> 00:15:02,890
attention is a very- has a very favorable computation profile.

329
00:15:02,890 --> 00:15:06,355
And as you can see, it's about four times faster than an RNN.

330
00:15:06,355 --> 00:15:08,920
Um, um, and, and faster than

331
00:15:08,920 --> 00:15:12,995
a convolutional model where the- you have a kernel of- like filter with, uh, three.

332
00:15:12,995 --> 00:15:19,320
So, so there's still one problem.

333
00:15:19,320 --> 00:15:21,455
Now, here's something- so in language,

334
00:15:21,455 --> 00:15:22,595
typically, we want to know, like,

335
00:15:22,595 --> 00:15:23,900
who did what to whom, right?

336
00:15:23,900 --> 00:15:25,770
So now, imagine you applied a convolutional filter.

337
00:15:25,770 --> 00:15:26,825
Because you actually have

338
00:15:26,825 --> 00:15:30,445
different linear transformations based on let- relative distances,

339
00:15:30,445 --> 00:15:31,770
like this, this, this, this,

340
00:15:31,770 --> 00:15:35,320
linear transformation on the word who, uh, o- o- on the concept,

341
00:15:35,320 --> 00:15:37,690
we can have- can learn this concept of who and, and, and,

342
00:15:37,690 --> 00:15:40,355
pick out different information from this embedding of the word I.

343
00:15:40,355 --> 00:15:41,930
And this linear transformation,

344
00:15:41,930 --> 00:15:44,530
the lre- the red linear transformation can pick out different information

345
00:15:44,530 --> 00:15:47,765
from kicked and the blue linear transformation can pick out different,

346
00:15:47,765 --> 00:15:49,445
different information from ball.

347
00:15:49,445 --> 00:15:53,230
Now, when you have a single attention layer, this is difficult.

348
00:15:53,230 --> 00:15:55,330
Because all- because they're just a convex combination

349
00:15:55,330 --> 00:15:57,320
where you have the same linear transformation everywhere.

350
00:15:57,320 --> 00:16:00,215
All that's available to you is just a- is just mixing proportions.

351
00:16:00,215 --> 00:16:03,670
So you can't pick out different pieces of information from different places.

352
00:16:03,670 --> 00:16:10,270
Well, what if we had one attention layer for who?

353
00:16:10,270 --> 00:16:13,595
So you can think of an attention layer as something like a feature detector almost,

354
00:16:13,595 --> 00:16:15,040
like, because a particular- it,

355
00:16:15,040 --> 00:16:18,340
it might try to- it might- because it carries with it a linear transformation,

356
00:16:18,340 --> 00:16:21,700
so it's projecting them in a space that- which starts caring maybe about syntax,

357
00:16:21,700 --> 00:16:24,485
or it's projecting in this space which starts caring about who or what.

358
00:16:24,485 --> 00:16:28,940
Uh, then we can have another attention layer for or attention head for what,

359
00:16:28,940 --> 00:16:31,490
did what, and other- another attention head for,

360
00:16:31,490 --> 00:16:34,115
for, for whom- to whom.

361
00:16:34,115 --> 00:16:37,300
And all of this can actually be done in parallel,

362
00:16:37,300 --> 00:16:39,210
and that's actually- and that's exactly what we do.

363
00:16:39,210 --> 00:16:41,255
And for efficiency, instead of actually

364
00:16:41,255 --> 00:16:44,230
having these dimensions operating in a large space,

365
00:16:44,230 --> 00:16:46,990
we just- we just reduce the dimensionality of all these heads

366
00:16:46,990 --> 00:16:50,225
and we operate these attention layers in parallel, sort of bridging the gap.

367
00:16:50,225 --> 00:16:51,670
Now, here's a, uh,

368
00:16:51,670 --> 00:16:53,660
perhaps, well, here's a little quiz.

369
00:16:53,660 --> 00:16:56,400
I mean, can you actually- is there

370
00:16:56,400 --> 00:17:01,105
a combination of heads or is there a configuration in which you can,

371
00:17:01,105 --> 00:17:04,260
actually, exactly simulate a convolution probably with more parameters?

372
00:17:04,260 --> 00:17:06,390
I think there should be a simple way to show that if you

373
00:17:06,390 --> 00:17:09,650
had mo- more heads or heads are a function of positions,

374
00:17:09,650 --> 00:17:11,875
you could probably just simulate a convolution,

375
00:17:11,875 --> 00:17:13,375
but- although with a lot of parameters.

376
00:17:13,375 --> 00:17:15,150
Uh, so it can- in, in,

377
00:17:15,150 --> 00:17:17,140
in the limit, it can actually simulate a convolution.

378
00:17:17,140 --> 00:17:21,280
Uh, and it also- we can al- we can continue to enjoy the benefits of parallelism,

379
00:17:21,280 --> 00:17:23,045
but we did increase the number of softmaxes

380
00:17:23,045 --> 00:17:24,820
because each head then carries with it a softmax.

381
00:17:24,820 --> 00:17:27,190
But the amount of FLOPS didn't change because we-

382
00:17:27,190 --> 00:17:30,010
instead of actually having these heads operating in very large dimensions,

383
00:17:30,010 --> 00:17:32,220
they're operating in very small dimensions.

384
00:17:32,220 --> 00:17:35,105
Um, so, uh, when we applied this on, on,

385
00:17:35,105 --> 00:17:37,535
on machine translation, um,

386
00:17:37,535 --> 00:17:40,295
we were able to drama- uh, dramatically outperform,

387
00:17:40,295 --> 00:17:43,640
uh, previous results on English-German and English-French translation.

388
00:17:43,640 --> 00:17:47,075
So we had a pretty standard setup: 32,000-word vocabularies,

389
00:17:47,075 --> 00:17:50,315
WordPiece encodings, WMT14-, uh,

390
00:17:50,315 --> 00:17:52,540
WMT 2014, uh, was our test set,

391
00:17:52,540 --> 00:17:53,970
2013 did the dev set.

392
00:17:53,970 --> 00:17:59,120
And, uh, and some of these results were much stronger than even our previous ensemble models.

393
00:17:59,120 --> 00:18:02,690
And, um, and on English-French also,

394
00:18:02,690 --> 00:18:05,400
we had some- we had some very favorabl- favorable results.

395
00:18:05,400 --> 00:18:06,630
Uh, and we- and we are,

396
00:18:06,630 --> 00:18:08,425
we, we, we achieved state of the art.

397
00:18:08,425 --> 00:18:11,260
Now, ste- stepping back a bit, uh,

398
00:18:11,260 --> 00:18:13,905
I- I'm not claiming that we,

399
00:18:13,905 --> 00:18:17,105
we arrived at an architecture that has better expressivity than an LSTM.

400
00:18:17,105 --> 00:18:18,600
I mean, there's, there's, there's,

401
00:18:18,600 --> 00:18:22,430
there's theorems that are- that say that LSTMs can model any function.

402
00:18:22,880 --> 00:18:27,870
Um, perhaps, all we did was just build an architecture that was good for SGD.

403
00:18:27,870 --> 00:18:30,895
Because stochastic gradient descent could just train this architecture really well,

404
00:18:30,895 --> 00:18:33,400
because the gradient dynamics and attention are very simple attentions,

405
00:18:33,400 --> 00:18:34,615
just a linear combination.

406
00:18:34,615 --> 00:18:37,820
And, uh, um, I think that's- I,

407
00:18:37,820 --> 00:18:39,560
I think that's actually favorable.

408
00:18:39,560 --> 00:18:42,235
But hopefully, uh, as we- as we go on,

409
00:18:42,235 --> 00:18:43,480
but the- well, I'd,

410
00:18:43,480 --> 00:18:44,860
I'd also like to point out that, you know,

411
00:18:44,860 --> 00:18:47,770
we do explicit mo- we do explicitly model all,

412
00:18:47,770 --> 00:18:49,440
all path connection, all, all,

413
00:18:49,440 --> 00:18:54,065
all pairwise connections and it has its adva- advantage of a very clear modeling,

414
00:18:54,065 --> 00:18:56,575
very clear relationships directly between, between any two words.

415
00:18:56,575 --> 00:19:00,640
Um, and, like, hopefully we'll be able to also

416
00:19:00,640 --> 00:19:02,260
show that there are other inductive biases.

417
00:19:02,260 --> 00:19:05,605
That it's not just like building more architectures that,

418
00:19:05,605 --> 00:19:08,720
that are good for- that are good inductive biases for SGD.

419
00:19:08,720 --> 00:19:13,000
So frameworks, a lot of our work was initially pushed out in tensor2tensor.

420
00:19:13,000 --> 00:19:15,985
Maybe that might change in the future with the arrival of JAX.

421
00:19:15,985 --> 00:19:18,790
There's ano- there's a framework also from Amazon called Sockeye.

422
00:19:18,790 --> 00:19:20,810
There's also Fairseq, uh, the se- the

423
00:19:20,810 --> 00:19:23,640
convolutional sequence-to-sequence toolkit from Facebook that the,

424
00:19:23,640 --> 00:19:26,290
they prob- I'm actually not sure if it has a transformer implementation,

425
00:19:26,290 --> 00:19:29,480
but they have some really good sequence-to-sequence models as well.

426
00:19:29,480 --> 00:19:31,700
Um, okay.

427
00:19:31,700 --> 00:19:32,845
So the importance of residuals.

428
00:19:32,845 --> 00:19:37,875
So, uh, we have these resil- residual connections, uh, between, um,

429
00:19:37,875 --> 00:19:41,795
so we have these residual connections that go from here to- here to here,

430
00:19:41,795 --> 00:19:45,240
here to here, like between every pair of layers, and it's interesting.

431
00:19:45,240 --> 00:19:47,895
So we, um, we- so what we do is we just

432
00:19:47,895 --> 00:19:51,025
add the position informations at the input to the model.

433
00:19:51,025 --> 00:19:53,525
And, uh, we don't infuse- we don't infuse

434
00:19:53,525 --> 00:19:56,150
or we don't inject position information at every layer.

435
00:19:56,150 --> 00:20:02,185
So when, uh, we severed these residual connections and we loo- stared at these,

436
00:20:02,185 --> 00:20:04,805
uh, stared at these attention distributions, this is the center or,

437
00:20:04,805 --> 00:20:07,545
sort of, the middle map is this attention distribution.

438
00:20:07,545 --> 00:20:10,750
You actually- basically, it- it's been unable to pick this diagonal.

439
00:20:10,750 --> 00:20:13,370
It should have a very strong diagonal focus.

440
00:20:13,370 --> 00:20:15,330
And so what has happened was these residuals

441
00:20:15,330 --> 00:20:18,155
were carrying this position information to every layer.

442
00:20:18,155 --> 00:20:20,820
And because these subsequent layers had no notion of position,

443
00:20:20,820 --> 00:20:22,900
they were fi- finding it hard to actually attend.

444
00:20:22,900 --> 00:20:25,875
This is the encoder-decoder attention which typically ends up being diagonal.

445
00:20:25,875 --> 00:20:27,380
Now, so then we, uh, we said okay.

446
00:20:27,380 --> 00:20:30,700
So then we actually continued with- continued to sever the residuals,

447
00:20:30,700 --> 00:20:32,950
but we added position information back in at every layer.

448
00:20:32,950 --> 00:20:34,835
We injected position information back in.

449
00:20:34,835 --> 00:20:36,390
And we didn't recover the accuracy,

450
00:20:36,390 --> 00:20:37,790
but we did get some of this,

451
00:20:37,790 --> 00:20:39,300
sort of, diagonal focus back in.

452
00:20:39,300 --> 00:20:41,400
So the residuals are doing more, but they're certainly,

453
00:20:41,400 --> 00:20:44,160
definitely moving this position information to the model there.

454
00:20:44,160 --> 00:20:46,805
They're pumping this position information through the model.

455
00:20:46,805 --> 00:20:49,070
Um, okay.

456
00:20:49,070 --> 00:20:51,365
So, so that was- that was- so, so now we saw that,

457
00:20:51,365 --> 00:20:52,445
you know, being able to, sort of,

458
00:20:52,445 --> 00:20:53,895
model both long- and short-,

459
00:20:53,895 --> 00:20:56,510
short-term relationships, uh, sh- uh, long and,

460
00:20:56,510 --> 00:20:58,435
long- and short-distance relationships with,

461
00:20:58,435 --> 00:21:01,745
with attention is beneficial for, for text generation.

462
00:21:01,745 --> 00:21:03,525
Um, what kind of inductive,

463
00:21:03,525 --> 00:21:06,780
inductive biases lay- actually, uh, appear, or what,

464
00:21:06,780 --> 00:21:10,860
what kind of phenomena appear in images and something that we constantly see- constantly

465
00:21:10,860 --> 00:21:12,745
see in images and music is this notion of

466
00:21:12,745 --> 00:21:15,180
repeating structure that's very similar to each other?

467
00:21:15,180 --> 00:21:18,055
You have these motifs that repeat in, in different scales.

468
00:21:18,055 --> 00:21:21,420
So, for example, there's a b- it's another artificial but beautiful example of

469
00:21:21,420 --> 00:21:24,900
self-similarity where you have this Van Gogh painting where this texture or these,

470
00:21:24,900 --> 00:21:26,410
these little objects just repeat.

471
00:21:26,410 --> 00:21:30,280
These images are- these different pieces of the image are very sa- similar to each other,

472
00:21:30,280 --> 00:21:31,595
but they might have different scales.

473
00:21:31,595 --> 00:21:32,950
Uh, again in music,

474
00:21:32,950 --> 00:21:34,615
here's a motif that repeats, uh,

475
00:21:34,615 --> 00:21:36,605
that could have- it could have, like,

476
00:21:36,605 --> 00:21:40,115
di- various, like, spans of time between in, in, between it.

477
00:21:40,115 --> 00:21:43,250
So, um, so, so this,

478
00:21:43,250 --> 00:21:44,440
so we, we, we,

479
00:21:44,440 --> 00:21:45,775
we attempted after this to see, well,

480
00:21:45,775 --> 00:21:49,720
to ask this question: can self-attention help us in modeling other objects like images?

481
00:21:49,720 --> 00:21:51,715
So the, the path we took was, sort of,

482
00:21:51,715 --> 00:21:57,450
standard auto-regressive image modeling the- or probabilistic image modeling, not GANs.

483
00:21:57,450 --> 00:21:58,910
Because it was- well, one, it was very easy.

484
00:21:58,910 --> 00:22:00,085
We had a language model almost.

485
00:22:00,085 --> 00:22:02,245
So this is just like language modeling on images.

486
00:22:02,245 --> 00:22:03,910
Uh, and also training at maximum,

487
00:22:03,910 --> 00:22:04,930
likely, it allows you to, sort of,

488
00:22:04,930 --> 00:22:06,720
measure, measure how well you're doing on,

489
00:22:06,720 --> 00:22:08,780
uh, on, on your held-out set.

490
00:22:08,780 --> 00:22:10,840
Uh, and it also gives you diversity,

491
00:22:10,840 --> 00:22:12,785
so you hopefully are covering all possible, uh,

492
00:22:12,785 --> 00:22:15,900
different kinds of images you- So, um,

493
00:22:15,900 --> 00:22:17,290
and to this point there's al- we had

494
00:22:17,290 --> 00:22:18,880
an advantage that's also been- there are- there've been

495
00:22:18,880 --> 00:22:22,425
good work on using recurrent models like PixelRNN and PixelCNN,

496
00:22:22,425 --> 00:22:26,330
that, that we're actually getting some very good compression rates. Um-

497
00:22:26,330 --> 00:22:31,805
And, um, again here,

498
00:22:31,805 --> 00:22:35,610
originally the argument was that, well, you know,

499
00:22:35,610 --> 00:22:37,920
in images because there- because you want symmetry,

500
00:22:37,920 --> 00:22:39,300
because you want like if you have a face,

501
00:22:39,300 --> 00:22:41,580
you want, you want one ear to sort of match with the other.

502
00:22:41,580 --> 00:22:43,575
If you had a large receptive field,

503
00:22:43,575 --> 00:22:47,130
which you could potentially get with attention at a lower computational cost,

504
00:22:47,130 --> 00:22:50,820
then it should benefit- then it should be quite beneficial for, for images,

505
00:22:50,820 --> 00:22:53,640
for images and you wouldn't need many layers like you do in

506
00:22:53,640 --> 00:22:57,955
convolutions to actually get dependencies between these far away pixels.

507
00:22:57,955 --> 00:23:00,670
So it seem like self-attention would have been a- what, what,

508
00:23:00,670 --> 00:23:03,745
what was already a good computational mechanism, right?

509
00:23:03,745 --> 00:23:06,580
But this sort of- but it was actually interesting to see

510
00:23:06,580 --> 00:23:09,700
how it even modeled- naturally modeled self-similarity,

511
00:23:09,700 --> 00:23:12,460
and people have used self-similarity in image generation like, you know, uh,

512
00:23:12,460 --> 00:23:15,760
there's this really cool work by Efros where they actually see, okay,

513
00:23:15,760 --> 00:23:18,670
in the training set, what are those patches that are really,

514
00:23:18,670 --> 00:23:19,810
that are really similar to me?

515
00:23:19,810 --> 00:23:21,640
And based on the patches that are really similar to me,

516
00:23:21,640 --> 00:23:23,035
I'm going to fill up the information.

517
00:23:23,035 --> 00:23:25,495
So it's like actually doing image generation.

518
00:23:25,495 --> 00:23:27,430
Uh, there is this really classic work called

519
00:23:27,430 --> 00:23:29,980
non-local means where they do image denoising,

520
00:23:29,980 --> 00:23:31,915
where they want to denoise this sort of,

521
00:23:31,915 --> 00:23:34,450
this patch P. And they say,

522
00:23:34,450 --> 00:23:38,290
I'm going to- based on my similarity between all other patches in my image,

523
00:23:38,290 --> 00:23:41,125
I'm going to compute some function of content-based similarity,

524
00:23:41,125 --> 00:23:43,525
and based on the similarity I'm going to pull information.

525
00:23:43,525 --> 00:23:46,645
So as- and exploiting this fact that images are very self-similar.

526
00:23:46,645 --> 00:23:50,440
And, uh, uh, this has also been sort of,

527
00:23:50,440 --> 00:23:52,390
uh, applied in some recent work.

528
00:23:52,390 --> 00:23:55,030
Now if you just took this encoder self-attention mechanism

529
00:23:55,030 --> 00:23:57,175
and just replace these word embeddings with patches,

530
00:23:57,175 --> 00:23:58,765
and that's kind of exactly what it's doing.

531
00:23:58,765 --> 00:24:01,330
It's, it's computing this notion of content-based similarity

532
00:24:01,330 --> 00:24:04,120
between these elements and then based on this content-based similarity,

533
00:24:04,120 --> 00:24:07,510
it constructs a convex combination that essentially brings these things together.

534
00:24:07,510 --> 00:24:09,385
So it's, it's a very ni- it was,

535
00:24:09,385 --> 00:24:11,545
it was quite- it was very pleasant to see that,

536
00:24:11,545 --> 00:24:13,975
oh, this is a differentiable way of doing non-local means.

537
00:24:13,975 --> 00:24:22,030
And, uh, and we took the transformer architecture and replaced words with pixels.

538
00:24:22,030 --> 00:24:26,005
Uh, there was some- there were some architecture adjustments to do.

539
00:24:26,005 --> 00:24:28,300
And, uh, so this was but- this was

540
00:24:28,300 --> 00:24:31,090
basically the kind of- it was very similar to the original work,

541
00:24:31,090 --> 00:24:34,360
and here the position representations instead of being, you know,

542
00:24:34,360 --> 00:24:36,760
one-dimensional, they were- because we are not dealing with sequences,

543
00:24:36,760 --> 00:24:38,350
we have two-dimensional position representations.

544
00:24:38,350 --> 00:24:40,365
Um, okay.

545
00:24:40,365 --> 00:24:42,070
So I pointed out before,

546
00:24:42,070 --> 00:24:45,775
attention is a very com- very favorable computational profile

547
00:24:45,775 --> 00:24:49,270
if your length- if your dimension dominates length,

548
00:24:49,270 --> 00:24:51,250
which if- which is absolutely untrue for,

549
00:24:51,250 --> 00:24:52,540
absolutely untrue for images.

550
00:24:52,540 --> 00:24:56,170
Uh, because even for like 32 by- even for 32 by 32 images,

551
00:24:56,170 --> 00:24:59,260
when you flatten them and you- and you flatten them, you have 30- you get 30,

552
00:24:59,260 --> 00:25:02,960
72 positions, uh, so it's your standard CFIR image.

553
00:25:02,960 --> 00:25:06,400
Um, so simple solution, uh,

554
00:25:06,400 --> 00:25:09,220
because like convolutions of- I mean,

555
00:25:09,220 --> 00:25:11,140
you get- convolutions are basically looked

556
00:25:11,140 --> 00:25:13,345
at local windows and you get translational equivariance.

557
00:25:13,345 --> 00:25:16,660
We said, "Okay. Let's adopt the same strategy."

558
00:25:16,660 --> 00:25:19,225
And also there's a lot of spatial locality and images.

559
00:25:19,225 --> 00:25:24,265
Uh, but now, we will still have a better computational profile.

560
00:25:24,265 --> 00:25:27,340
If your- if your receptive field is still smaller than your dimension,

561
00:25:27,340 --> 00:25:29,485
you can afford- you can actually still do

562
00:25:29,485 --> 00:25:34,600
much more long distance computation than a standard convolution because you're,

563
00:25:34,600 --> 00:25:37,615
uh, because you're quadratic in length.

564
00:25:37,615 --> 00:25:40,390
So as long as we didn't increase our length beyond the dimension,

565
00:25:40,390 --> 00:25:42,415
we still had a favorable computational profile.

566
00:25:42,415 --> 00:25:44,440
And so the way we did it was, uh,

567
00:25:44,440 --> 00:25:46,195
we essentially had, uh,

568
00:25:46,195 --> 00:25:47,950
two kinds of rasterizations.

569
00:25:47,950 --> 00:25:52,210
So we had a one-dimensional rasterization where you had a sort of single query block,

570
00:25:52,210 --> 00:25:54,790
uh, which was, uh,

571
00:25:54,790 --> 00:25:58,660
which was then attending or to the- into a larger memory block,

572
00:25:58,660 --> 00:26:02,680
uh, in this rasterized fashion along the- along, along the rows.

573
00:26:02,680 --> 00:26:05,320
Um, then we tried another form of rasterization,

574
00:26:05,320 --> 00:26:07,780
falling standard two-dimensional locality,

575
00:26:07,780 --> 00:26:10,360
where you had- where we actually produced the image in,

576
00:26:10,360 --> 00:26:13,300
uh, in blocks and within each block we had a rasterization scheme.

577
00:26:13,300 --> 00:26:18,640
Um, again, these- the image transformer layer was very similar.

578
00:26:18,640 --> 00:26:21,280
We had two-dimensional position representations along

579
00:26:21,280 --> 00:26:24,670
with query- with the same- with a very similar attention mechanism.

580
00:26:24,670 --> 00:26:27,220
Um, and we tried

581
00:26:27,220 --> 00:26:30,550
both super-resolution and unconditional and conditional image generation.

582
00:26:30,550 --> 00:26:34,074
Uh, this is- this is Ne- Niki Parmar,

583
00:26:34,074 --> 00:26:37,255
I and a co- and a few other authors from Brain,

584
00:26:37,255 --> 00:26:39,580
um, and we presented it at ICML.

585
00:26:39,580 --> 00:26:44,815
And, uh, we were able to achieve better perplexity than existing models.

586
00:26:44,815 --> 00:26:47,650
So PixelSNAIL is actually another model that used- mixed

587
00:26:47,650 --> 00:26:50,695
both convolutions and self-attention and they- they outperformed us on,

588
00:26:50,695 --> 00:26:52,675
on, on, on, on, bits per dimension.

589
00:26:52,675 --> 00:26:54,310
So we were measuring perplexity because these are

590
00:26:54,310 --> 00:26:56,560
probabilistic- these are probabilistic models.

591
00:26:56,560 --> 00:26:58,675
It's like basically a language model of images and,

592
00:26:58,675 --> 00:27:01,120
and it just- and your- and the factorization

593
00:27:01,120 --> 00:27:03,580
of your language model just depends on how you rasterize.

594
00:27:03,580 --> 00:27:05,620
In the- in this- in the one-D rasterization,

595
00:27:05,620 --> 00:27:07,000
we went first rows and then columns.

596
00:27:07,000 --> 00:27:08,185
In the two-D rasterization,

597
00:27:08,185 --> 00:27:11,155
we went blockwise and inside each block we rasterized.

598
00:27:11,155 --> 00:27:14,650
On ImageNet, we achieved better perplexities, and,

599
00:27:14,650 --> 00:27:18,775
uh, so yeah, I mean we're at a GAN level, right?

600
00:27:18,775 --> 00:27:23,694
I mean this weird- this is- I think probabilist auto-regressive Image generation,

601
00:27:23,694 --> 00:27:26,725
uh, by this point had not reached GANs.

602
00:27:26,725 --> 00:27:31,090
At ICLR 2019, there's a paper by Nal that actually uses self-attention and gets very,

603
00:27:31,090 --> 00:27:32,575
very good quality images.

604
00:27:32,575 --> 00:27:34,705
But what we, what we observed was,

605
00:27:34,705 --> 00:27:36,670
we were getting structured objects fairly well.

606
00:27:36,670 --> 00:27:39,930
Like can people recognize what the second row is?

607
00:27:39,930 --> 00:27:43,770
Cars. [OVERLAPPING]

608
00:27:43,770 --> 00:27:46,050
I heard- I said- most- almost everyone said cars.

609
00:27:46,050 --> 00:27:48,720
I'm not going to ask who said something else, but yes, they're cars.

610
00:27:48,720 --> 00:27:53,350
yeah. And, uh, so the- and the last row is another vehicles like,

611
00:27:53,350 --> 00:27:58,330
uh, so essentially when structured jo- structured objects were easy to capture.

612
00:27:58,330 --> 00:28:01,165
Um, like frogs and sort of,

613
00:28:01,165 --> 00:28:04,150
you know, objects that were camouflaged just turned into this mush.

614
00:28:04,150 --> 00:28:07,090
Um, and- but on super resolution,

615
00:28:07,090 --> 00:28:08,650
now super-resolution is interesting because

616
00:28:08,650 --> 00:28:10,375
there's a lot of conditioning information, right?

617
00:28:10,375 --> 00:28:13,525
And, uh, when you have a lot of conditioning information, the,

618
00:28:13,525 --> 00:28:15,520
the sort of possible- you break- you,

619
00:28:15,520 --> 00:28:17,905
you actually lock quite a few of the modes.

620
00:28:17,905 --> 00:28:20,020
So there's only a few options you can have at the output.

621
00:28:20,020 --> 00:28:22,390
And super- our super resolution results are much better.

622
00:28:22,390 --> 00:28:26,770
We were able to get better facial orientation and structure than previous work.

623
00:28:26,770 --> 00:28:31,390
And these are samples at different temperatures and, uh, and, uh,

624
00:28:31,390 --> 00:28:34,690
and we wou- when we quantify this with actual human evaluators,

625
00:28:34,690 --> 00:28:36,160
we- like we flash an image and said,

626
00:28:36,160 --> 00:28:37,345
is this real, is this false?

627
00:28:37,345 --> 00:28:38,630
And we were able to, uh,

628
00:28:38,630 --> 00:28:40,750
we were able to fool humans like four

629
00:28:40,750 --> 00:28:43,285
times better than previous results in super resolution.

630
00:28:43,285 --> 00:28:46,990
Again, these are not- these results like I, I guess the,

631
00:28:46,990 --> 00:28:50,470
the latest GAN result from Nvidia makes us look like a joke.

632
00:28:50,470 --> 00:28:51,715
But, I mean this is,

633
00:28:51,715 --> 00:28:53,050
I mean, we're starting later than GAN.

634
00:28:53,050 --> 00:28:54,130
So hopefully we'll catch up.

635
00:28:54,130 --> 00:28:57,250
But, but the point here is that this is an interesting inductive bias for images,

636
00:28:57,250 --> 00:28:59,500
so very natural inductive bias for images.

637
00:28:59,500 --> 00:29:01,375
Um, and, uh, and,

638
00:29:01,375 --> 00:29:05,650
and there is hope to apply it- for applying in classification and other such tasks also.

639
00:29:05,650 --> 00:29:07,450
Um, so one interesting thing,

640
00:29:07,450 --> 00:29:09,640
just to sort of both out of curiosity and

641
00:29:09,640 --> 00:29:12,745
asking how good is maximum or like does maximum likelihood.

642
00:29:12,745 --> 00:29:16,180
Well, one, does the model actually capture some interesting structure in the role?

643
00:29:16,180 --> 00:29:17,650
Second, do you get diversity?

644
00:29:17,650 --> 00:29:19,540
Well, maximum likelihood should get diversity,

645
00:29:19,540 --> 00:29:21,955
by, by virtue, by virtue of what it does.

646
00:29:21,955 --> 00:29:23,860
Uh, so then we just- we did image completion.

647
00:29:23,860 --> 00:29:25,870
And why is- why image completion because as soon as you

648
00:29:25,870 --> 00:29:28,000
lock down half the image to the goal truth,

649
00:29:28,000 --> 00:29:30,610
you're actually shaving off a lot of the possible modes.

650
00:29:30,610 --> 00:29:32,230
So you have a much easier time sampling.

651
00:29:32,230 --> 00:29:34,090
So, uh, so the first is,

652
00:29:34,090 --> 00:29:35,935
uh, first is what we supply to the model.

653
00:29:35,935 --> 00:29:38,785
The, the, the right row- the right most column is,

654
00:29:38,785 --> 00:29:41,155
is gold, and we were able to generate different samples.

655
00:29:41,155 --> 00:29:43,270
But what was really interesting is the third row.

656
00:29:43,270 --> 00:29:46,180
Uh, so the rightmost column is- the rightmost column is gold.

657
00:29:46,180 --> 00:29:48,655
Uh, now if you look at the third row, this horse.

658
00:29:48,655 --> 00:29:52,135
So actually there's this sort of glimpse or a suggestion of a pull,

659
00:29:52,135 --> 00:29:55,045
but the model hallucinated a human in some of these,

660
00:29:55,045 --> 00:29:56,170
in some of these images,

661
00:29:56,170 --> 00:29:58,450
which is interesting like in- it does capture at least

662
00:29:58,450 --> 00:30:02,230
the data teaches it to capture some structure about the world.

663
00:30:02,230 --> 00:30:06,190
Um, the dog is just cute and I guess it also shows that, you know,

664
00:30:06,190 --> 00:30:07,480
there was this entire object,

665
00:30:07,480 --> 00:30:10,660
this chair, that the model just completely refused to imagine.

666
00:30:10,660 --> 00:30:12,835
So there's a lot of difficulty.

667
00:30:12,835 --> 00:30:15,070
And I guess Anna is gonna talk about

668
00:30:15,070 --> 00:30:19,465
[NOISE] the another way to exploit self- self-similarity.

669
00:30:19,465 --> 00:30:20,080
Thank you.

670
00:30:20,080 --> 00:30:31,600
[APPLAUSE]

671
00:30:31,600 --> 00:30:34,060
So thank you Ashish for the introduction.

672
00:30:34,060 --> 00:30:37,105
Uh, so there's a lot of self-similarity in images.

673
00:30:37,105 --> 00:30:39,460
There's also a lot of self-similarity in, in music.

674
00:30:39,460 --> 00:30:43,180
So we can imagine, transformer being a, a good model for it.

675
00:30:43,180 --> 00:30:46,060
Uh, we- we're going to show how,

676
00:30:46,060 --> 00:30:48,100
uh, we can add more to,

677
00:30:48,100 --> 00:30:50,350
to the self attention, to think more about kind of

678
00:30:50,350 --> 00:30:54,220
relational information and how that could help, uh, music generation.

679
00:30:54,220 --> 00:30:57,160
[NOISE] So, uh, first I want to

680
00:30:57,160 --> 00:31:01,225
clarify what is the raw representation that we're working with right now.

681
00:31:01,225 --> 00:31:03,280
So analogous to language,

682
00:31:03,280 --> 00:31:07,060
you can think about there's text and somebody is reading out a text,

683
00:31:07,060 --> 00:31:09,430
so they add their kind of own intonations to it,

684
00:31:09,430 --> 00:31:12,385
and then you have sound waves coming out of that speech.

685
00:31:12,385 --> 00:31:16,100
So for music there's a va- very similar kind of, uh,

686
00:31:16,100 --> 00:31:21,270
line of a generation where you say the composer has an idea,

687
00:31:21,270 --> 00:31:23,355
uh, writes down the score and then,

688
00:31:23,355 --> 00:31:25,575
a performer performs it and then you get sound.

689
00:31:25,575 --> 00:31:29,535
So what we're going to focus on today is mostly, uh,

690
00:31:29,535 --> 00:31:31,410
you can think of the score but it's actually,

691
00:31:31,410 --> 00:31:34,075
er, a performance, um,

692
00:31:34,075 --> 00:31:41,545
in that it's a symbolic representation where MIDI pianos were used and,

693
00:31:41,545 --> 00:31:44,065
uh, um, professional amateur, uh,

694
00:31:44,065 --> 00:31:46,645
musicians were performing on the pianos.

695
00:31:46,645 --> 00:31:47,890
So we have the recorded,

696
00:31:47,890 --> 00:31:49,660
uh, information of their playing.

697
00:31:49,660 --> 00:31:51,295
So in particular, um,

698
00:31:51,295 --> 00:31:55,810
at each time se- step modeling music as this sequential, uh,

699
00:31:55,810 --> 00:31:58,720
process, what is being output are, okay,

700
00:31:58,720 --> 00:32:00,145
turn this note on, ah,

701
00:32:00,145 --> 00:32:01,960
advance the clock by this much,

702
00:32:01,960 --> 00:32:03,220
and then turn this note off.

703
00:32:03,220 --> 00:32:05,965
And also there is, uh, dynamics information,

704
00:32:05,965 --> 00:32:07,660
so when you turn the note on, you first say like,

705
00:32:07,660 --> 00:32:09,985
how loud it's going to be.

706
00:32:09,985 --> 00:32:13,090
Uh, so traditionally, uh, modeling, uh,

707
00:32:13,090 --> 00:32:15,085
music as kind of a language,

708
00:32:15,085 --> 00:32:18,130
we've been using, uh, recurrent neural networks.

709
00:32:18,130 --> 00:32:23,350
And, um, because as Ashish introduced and, and talked about,

710
00:32:23,350 --> 00:32:25,510
there is a lot of compression that needs to happen,

711
00:32:25,510 --> 00:32:29,830
like a long sequence has to be embedded into like a fixed length vector.

712
00:32:29,830 --> 00:32:32,200
And that becomes hard when, uh,

713
00:32:32,200 --> 00:32:35,200
in music you have- you have repetition coming,

714
00:32:35,200 --> 00:32:37,150
um, at a distance.

715
00:32:37,150 --> 00:32:39,490
So, uh, I'm first going to show you,

716
00:32:39,490 --> 00:32:43,270
um, samples from, from the RNNs,

717
00:32:43,270 --> 00:32:46,450
from a transformer and then from a music transformer that has

718
00:32:46,450 --> 00:32:48,580
the relative attention and kind of let you hear

719
00:32:48,580 --> 00:32:51,970
the differences and then I'll go into how we,

720
00:32:51,970 --> 00:32:54,580
uh, what are, what are the, uh,

721
00:32:54,580 --> 00:32:58,660
modifications we needed to do on top of the, uh, transformer model.

722
00:32:58,660 --> 00:33:00,745
Uh, so here, uh,

723
00:33:00,745 --> 00:33:03,295
this task is kind of the image completion task.

724
00:33:03,295 --> 00:33:08,335
So we give it an initial motif and then we ask the model to do continuations.

725
00:33:08,335 --> 00:33:10,660
So this is the motif that we fed.

726
00:33:10,660 --> 00:33:16,225
[MUSIC] How many people recognize that?

727
00:33:16,225 --> 00:33:19,090
Awesome. Okay. [LAUGHTER] Yeah,

728
00:33:19,090 --> 00:33:20,380
so this is a, uh,

729
00:33:20,380 --> 00:33:22,900
kind of a fragment from a Chopin Etude piece.

730
00:33:22,900 --> 00:33:24,910
And we're going to ask, uh,

731
00:33:24,910 --> 00:33:26,680
the RNN to do a continuation.

732
00:33:26,680 --> 00:33:34,990
[NOISE]

733
00:33:34,990 --> 00:33:48,325
[MUSIC]

734
00:33:48,325 --> 00:33:50,950
So in here, like in the beginning, it was trying to repeat it.

735
00:33:50,950 --> 00:33:52,330
But very fast, it, er,

736
00:33:52,330 --> 00:33:55,870
wandered off into, its other different ideas.

737
00:33:55,870 --> 00:33:58,120
So that's one challenge because it's, uh,

738
00:33:58,120 --> 00:34:01,705
not able to directly look back to what happened in the past, uh, and,

739
00:34:01,705 --> 00:34:04,060
and can just look at kind of a blu- blurry version,

740
00:34:04,060 --> 00:34:06,400
and that blurry version becomes more and more blurry.

741
00:34:06,400 --> 00:34:08,455
Uh, so this is what the transformer does.

742
00:34:08,455 --> 00:34:10,990
Uh, so so, uh, a detail is, uh,

743
00:34:10,990 --> 00:34:14,455
these models are trained on half the length that you're hearing.

744
00:34:14,455 --> 00:34:18,760
So we're kinda asking the model to generalize beyond the length that it's trained on.

745
00:34:18,760 --> 00:34:20,170
And you can see for this transformer,

746
00:34:20,170 --> 00:34:22,285
it, it deteriorates beyond that.

747
00:34:22,285 --> 00:34:25,150
But it can hold the motif pretty consistent.

748
00:34:25,150 --> 00:34:34,690
[MUSIC] Okay. You, you,

749
00:34:34,690 --> 00:34:35,770
you ge- you get the idea.

750
00:34:35,770 --> 00:34:40,690
[LAUGHTER] So initially, it was able to do this repetition really well.

751
00:34:40,690 --> 00:34:42,400
Uh, so it was able to copy it very well.

752
00:34:42,400 --> 00:34:44,170
But beyond the length that was trained on,

753
00:34:44,170 --> 00:34:47,440
it kinda didn't know how to cope with, like longer contexts.

754
00:34:47,440 --> 00:34:48,880
And, uh, what you see,

755
00:34:48,880 --> 00:34:51,325
uh, the, the last one is from the music transformer.

756
00:34:51,325 --> 00:34:53,350
I think so that kind of [NOISE] the relational information.

757
00:34:53,350 --> 00:34:56,470
And you can just see visually how it's very consistent and kinda

758
00:34:56,470 --> 00:34:59,940
repeating these [NOISE] these larger, uh, arcs.

759
00:34:59,940 --> 00:35:21,190
[MUSIC]

760
00:35:21,190 --> 00:35:23,815
Yeah. So that was, uh, music transformer.

761
00:35:23,815 --> 00:35:27,070
And so in music,

762
00:35:27,070 --> 00:35:30,415
the, the self similarity that we talked about, uh,

763
00:35:30,415 --> 00:35:31,765
so we see, uh,

764
00:35:31,765 --> 00:35:32,950
the motif here, and so,

765
00:35:32,950 --> 00:35:35,005
so there we primed the model with a motif,

766
00:35:35,005 --> 00:35:36,445
and this is actually a sample,

767
00:35:36,445 --> 00:35:37,870
unconditioned sample from the model.

768
00:35:37,870 --> 00:35:40,690
So nothing, er, there was no priming that the, uh,

769
00:35:40,690 --> 00:35:42,880
model kinda had to create its own motif and then,

770
00:35:42,880 --> 00:35:45,115
uh, do, uh, continuations from there.

771
00:35:45,115 --> 00:35:49,210
And here, uh, if we kinda look at it and analyze it a bit, you see,

772
00:35:49,210 --> 00:35:51,805
uh, a lot of repetition,

773
00:35:51,805 --> 00:35:54,040
uh, with gaps in between.

774
00:35:54,040 --> 00:35:56,635
And if you look at the self attention structure,

775
00:35:56,635 --> 00:35:58,870
we actually do see the model,

776
00:35:58,870 --> 00:36:00,625
uh, looking at the relevant parts.

777
00:36:00,625 --> 00:36:04,075
Even if, if it was not immediately, uh, preceding it.

778
00:36:04,075 --> 00:36:05,500
So, so here, uh,

779
00:36:05,500 --> 00:36:09,970
what I colored shaded out is where the motif, um, occurs.

780
00:36:09,970 --> 00:36:11,830
Uh, and you can, uh, see the different colors,

781
00:36:11,830 --> 00:36:14,710
there's a different attention heads and they're kinda focusing,

782
00:36:14,710 --> 00:36:16,810
uh, among those, uh, grayed out sections.

783
00:36:16,810 --> 00:36:19,750
[NOISE] So I'll play the sample and we also have

784
00:36:19,750 --> 00:36:23,695
a visualization that kind of shows you as the music is pa- uh,

785
00:36:23,695 --> 00:36:28,930
is being played or what notes it was attending to as it was predicting that note.

786
00:36:28,930 --> 00:36:31,150
And, uh, this was generated from scratch.

787
00:36:31,150 --> 00:36:33,880
And, uh, so the self attention is, um,

788
00:36:33,880 --> 00:36:37,270
from, from kind of note to note level or event to event level.

789
00:36:37,270 --> 00:36:39,325
So it's, it's quite low level.

790
00:36:39,325 --> 00:36:40,975
Uh, so when you look at it, it's,

791
00:36:40,975 --> 00:36:42,655
it's ki- a little bit overwhelming.

792
00:36:42,655 --> 00:36:44,350
It has like multiple heads and,

793
00:36:44,350 --> 00:36:45,925
er, a lot of things moving.

794
00:36:45,925 --> 00:36:47,950
Uh, but there's kind of these structural moments

795
00:36:47,950 --> 00:36:50,275
where you would kind of see more of this, uh,

796
00:36:50,275 --> 00:36:52,795
clean, uh, kind of,

797
00:36:52,795 --> 00:36:55,270
uh, sections where it's attending to.

798
00:36:55,270 --> 00:37:52,390
[MUSIC]

799
00:37:52,390 --> 00:37:53,710
VOkay. So, um,

800
00:37:53,710 --> 00:37:55,690
how, how did we do that?

801
00:37:55,690 --> 00:37:59,440
And so starting from kind of the the regular attention mechanism,

802
00:37:59,440 --> 00:38:02,695
we know it's, uh, a weighted average of the past history.

803
00:38:02,695 --> 00:38:04,690
Uh, and the nice thing is, uh,

804
00:38:04,690 --> 00:38:07,165
however far it is, we have direct access to it.

805
00:38:07,165 --> 00:38:08,845
So if we know, uh,

806
00:38:08,845 --> 00:38:10,870
there are kind of motifs that occurred,

807
00:38:10,870 --> 00:38:13,000
uh, in in early on in the piece,

808
00:38:13,000 --> 00:38:15,385
we're still able to based on, uh,

809
00:38:15,385 --> 00:38:17,080
the fact that things that are similar,

810
00:38:17,080 --> 00:38:19,240
uh, to be able to retrieve those.

811
00:38:19,240 --> 00:38:22,915
Um, but, uh, it also becomes,

812
00:38:22,915 --> 00:38:25,030
all the past becomes kind of a bag of words,

813
00:38:25,030 --> 00:38:27,310
like there is no structure of which came,

814
00:38:27,310 --> 00:38:28,570
uh, before or after.

815
00:38:28,570 --> 00:38:31,195
So there's the positional sinusoids that Ashish talked about.

816
00:38:31,195 --> 00:38:33,595
That, uh, basically in this, uh,

817
00:38:33,595 --> 00:38:38,395
indices indexes into a sinusoids that are moving at different speeds.

818
00:38:38,395 --> 00:38:40,645
And so close-by positions would have, uh,

819
00:38:40,645 --> 00:38:42,160
a very similar kind of, uh,

820
00:38:42,160 --> 00:38:45,680
cross section into those multiple sinusoids.

821
00:38:46,320 --> 00:38:48,805
Uh, in contrast for, er,

822
00:38:48,805 --> 00:38:50,920
for convolutions, you kinda have this, uh,

823
00:38:50,920 --> 00:38:54,940
fixed filter that's moving around that captures the relative distance.

824
00:38:54,940 --> 00:38:56,875
Like 1B4, 2B4.

825
00:38:56,875 --> 00:38:59,185
And these are kind of, uh,

826
00:38:59,185 --> 00:39:02,935
in some ways like a rigid structure that allows you to be, uh,

827
00:39:02,935 --> 00:39:04,925
a kind of, uh, bring in the,

828
00:39:04,925 --> 00:39:07,440
the distance information very explicitly.

829
00:39:07,440 --> 00:39:10,770
Um, you can imagine relative attention, um,

830
00:39:10,770 --> 00:39:13,080
with the multiple heads, uh, at play,

831
00:39:13,080 --> 00:39:15,395
uh, to be some combination of these.

832
00:39:15,395 --> 00:39:17,170
So, uh, on one hand,

833
00:39:17,170 --> 00:39:18,580
you can access, uh,

834
00:39:18,580 --> 00:39:20,485
the the history very directly.

835
00:39:20,485 --> 00:39:22,510
On the other hand, you also know, er,

836
00:39:22,510 --> 00:39:25,210
how you rel- relate to this history.

837
00:39:25,210 --> 00:39:26,860
Uh, capturing for example,

838
00:39:26,860 --> 00:39:29,575
like translational invariance and, er,

839
00:39:29,575 --> 00:39:32,440
and we, uh, and for example,

840
00:39:32,440 --> 00:39:35,455
we think one of the reasons why in the beginning, uh,

841
00:39:35,455 --> 00:39:38,830
priming samples that you heard that the, uh,

842
00:39:38,830 --> 00:39:40,945
music transformer was able to generate

843
00:39:40,945 --> 00:39:43,735
beyond the length that it was trained on at a very coherent way,

844
00:39:43,735 --> 00:39:47,830
is that it's able to kind of rely on this translational invariance to to carry,

845
00:39:47,830 --> 00:39:50,785
uh, the relational information forward.

846
00:39:50,785 --> 00:39:55,000
So, if we take a closer look at how how how the,

847
00:39:55,000 --> 00:39:56,545
how this works is, uh,

848
00:39:56,545 --> 00:39:58,540
the regular transformer you have,

849
00:39:58,540 --> 00:40:00,250
you compare all the queries and keys,

850
00:40:00,250 --> 00:40:02,260
so you get kind of this, uh, square matrix.

851
00:40:02,260 --> 00:40:04,390
You can think of it as like a self similarity,

852
00:40:04,390 --> 00:40:06,010
uh, matrix, so it's, uh, a square.

853
00:40:06,010 --> 00:40:08,890
Uh, what relative attention does is,

854
00:40:08,890 --> 00:40:12,355
to add an additional term that thinks, uh,

855
00:40:12,355 --> 00:40:14,530
that thinks about whenever you're comparing two things,

856
00:40:14,530 --> 00:40:16,210
how far are you apart?

857
00:40:16,210 --> 00:40:18,820
And also based on the content, do I,

858
00:40:18,820 --> 00:40:21,340
do I care about things that are two steps away or

859
00:40:21,340 --> 00:40:24,175
three steps away or I maybe care about things that are recurring,

860
00:40:24,175 --> 00:40:26,275
at kind of a periodical distance.

861
00:40:26,275 --> 00:40:29,305
And, uh, with that information gathered,

862
00:40:29,305 --> 00:40:33,835
that influences, uh, the the similarity between positions.

863
00:40:33,835 --> 00:40:35,815
And in particular, uh,

864
00:40:35,815 --> 00:40:39,460
this extra term is based on, um, the distance.

865
00:40:39,460 --> 00:40:40,510
So you wanna, uh,

866
00:40:40,510 --> 00:40:41,950
gather the embeddings, uh,

867
00:40:41,950 --> 00:40:44,500
that's irrelevant to the, uh,

868
00:40:44,500 --> 00:40:46,180
the query key distances,

869
00:40:46,180 --> 00:40:49,285
uh, on the [NOISE] on the logits.

870
00:40:49,285 --> 00:40:51,715
So, in translation, this,

871
00:40:51,715 --> 00:40:53,230
uh, has shown, uh,

872
00:40:53,230 --> 00:40:55,015
a lot of improvement in,

873
00:40:55,015 --> 00:40:57,730
um, for example English to to German translation.

874
00:40:57,730 --> 00:41:00,010
Uh, but in translation,

875
00:41:00,010 --> 00:41:01,765
the sequences are usually quite short.

876
00:41:01,765 --> 00:41:03,415
It's only a sentence to sentence.

877
00:41:03,415 --> 00:41:05,110
Uh, a translation for example,

878
00:41:05,110 --> 00:41:07,210
maybe 50 words or 100 words.

879
00:41:07,210 --> 00:41:12,010
But the music, er, samples that you've heard are in the range of 2,000 time-steps.

880
00:41:12,010 --> 00:41:16,015
So it's like 2,000 tokens need to be able to fit in memory.

881
00:41:16,015 --> 00:41:17,500
So this was a problem, uh,

882
00:41:17,500 --> 00:41:23,350
because the original formulation relied on building this 3D tensor that's,

883
00:41:23,350 --> 00:41:25,795
uh, that's very large in memory.

884
00:41:25,795 --> 00:41:27,715
Um, and and why this is the case?

885
00:41:27,715 --> 00:41:30,055
It's because for every pair,

886
00:41:30,055 --> 00:41:32,710
uh, you look up what the,

887
00:41:32,710 --> 00:41:35,200
what the re- so you can compute what the relative distance is,

888
00:41:35,200 --> 00:41:38,320
and then you look up an embedding that corresponds to that distance.

889
00:41:38,320 --> 00:41:43,540
So, um, for like this there's a length by length, like L by L, uh, matrix.

890
00:41:43,540 --> 00:41:44,815
You need like, uh,

891
00:41:44,815 --> 00:41:47,635
to collect embeddings for each of the positions and that's, uh,

892
00:41:47,635 --> 00:41:51,070
depth D. So that gives us the 3D.

893
00:41:51,070 --> 00:41:52,900
What we realized is,

894
00:41:52,900 --> 00:41:58,480
you can actually just directly multiply the queries and the embedding distances.

895
00:41:58,480 --> 00:42:00,670
[NOISE] And they, uh,

896
00:42:00,670 --> 00:42:02,080
come out kind of in a different order,

897
00:42:02,080 --> 00:42:04,630
because now you have the queries ordered by a relative distance,

898
00:42:04,630 --> 00:42:07,930
but you need the queries ordered by keys, uh,

899
00:42:07,930 --> 00:42:11,440
which is kind of a absolute by absolute, uh, configuration.

900
00:42:11,440 --> 00:42:13,360
So what we could do is just, uh,

901
00:42:13,360 --> 00:42:16,705
do a series of skewing, uh,

902
00:42:16,705 --> 00:42:20,515
to to put it into the right, uh, configuration.

903
00:42:20,515 --> 00:42:23,875
And this is, uh, yeah.

904
00:42:23,875 --> 00:42:25,570
Just a, just a quick contrast to,

905
00:42:25,570 --> 00:42:28,480
to show, um, the difference in memory requirements.

906
00:42:28,480 --> 00:42:31,705
So, er, a lot of the times the challenge is in, uh,

907
00:42:31,705 --> 00:42:33,820
being able to scale, uh, you know,

908
00:42:33,820 --> 00:42:37,660
being able to be more memory efficient so that [NOISE] you can model longer sequences.

909
00:42:37,660 --> 00:42:40,420
So with that, uh, this is,

910
00:42:40,420 --> 00:42:42,850
um, I can play you one more example if we have time.

911
00:42:42,850 --> 00:42:45,125
But if we don't have time, we can, go ahead.

912
00:42:45,125 --> 00:42:46,175
We'll see more of that.

913
00:42:46,175 --> 00:42:47,980
Okay. [LAUGHTER] So this is,

914
00:42:47,980 --> 00:42:49,930
this is, uh, maybe a one, uh,

915
00:42:49,930 --> 00:42:54,480
about a one-minute sample and I- I hope you like it.

916
00:42:54,480 --> 00:44:06,385
Thanks. [MUSIC]

917
00:44:06,385 --> 00:44:07,720
Thank you for listening.

918
00:44:07,720 --> 00:44:18,610
[APPLAUSE].

919
00:44:18,610 --> 00:44:23,830
[LAUGHTER] Thanks, Anna. Um, um, great.

920
00:44:23,830 --> 00:44:26,930
Um, so to sort to, um,

921
00:44:26,970 --> 00:44:31,615
so relative attention has been a powerful mechanism for,

922
00:44:31,615 --> 00:44:35,185
um, a very powerful mechanism for music.

923
00:44:35,185 --> 00:44:37,285
It's also helped in machine translation.

924
00:44:37,285 --> 00:44:39,355
Um, one really interesting, uh,

925
00:44:39,355 --> 00:44:41,545
consequences of, uh, of, um,

926
00:44:41,545 --> 00:44:44,395
one really interesting consequence of relative attention in,

927
00:44:44,395 --> 00:44:46,030
uh, images, is that,

928
00:44:46,030 --> 00:44:48,370
um, like convolutions achieve,

929
00:44:48,370 --> 00:44:50,740
uh, convolutions achieve translational equivariance.

930
00:44:50,740 --> 00:44:51,970
So if you have,

931
00:44:51,970 --> 00:44:54,640
let's say, you wa- uh, you have this,

932
00:44:54,640 --> 00:44:58,345
this red dot or this feature that you're computing at this red dot,

933
00:44:58,345 --> 00:45:01,465
it doesn't depend on where the image of the dog is in the image,

934
00:45:01,465 --> 00:45:04,720
is in the the larger image. It just doesn't depend on its absolute location.

935
00:45:04,720 --> 00:45:07,000
It's going to, it's going to produce the same activation.

936
00:45:07,000 --> 00:45:10,915
So you have- convolutions have this nice, uh, translation equivariance.

937
00:45:10,915 --> 00:45:13,135
Now, with, with relative,

938
00:45:13,135 --> 00:45:15,220
uh, positions or relative attention,

939
00:45:15,220 --> 00:45:18,550
you get exactly the same effect because you don't have any- once you just

940
00:45:18,550 --> 00:45:22,495
remove this notion of absolute position that you are injecting [NOISE] into the model,

941
00:45:22,495 --> 00:45:24,280
uh, once you've, once you've removed that,

942
00:45:24,280 --> 00:45:26,470
then your attention computation,

943
00:45:26,470 --> 00:45:28,840
because it actually includes I mean, we've,

944
00:45:28,840 --> 00:45:32,215
we've- Niki and I couple of others have actually,

945
00:45:32,215 --> 00:45:34,870
and Anna were actually working on images and seems-

946
00:45:34,870 --> 00:45:37,480
and it seems to actually show, uh, better results.

947
00:45:37,480 --> 00:45:42,040
Um, this actio- this now satisfies this,

948
00:45:42,040 --> 00:45:44,440
uh, uh, the- I mean, it,

949
00:45:44,440 --> 00:45:47,470
it can achieve translation equivariance which is a great property for images.

950
00:45:47,470 --> 00:45:49,300
So there's a lot of- it seems like this might be

951
00:45:49,300 --> 00:45:51,250
an interesting direction to pursue if you want to push,

952
00:45:51,250 --> 00:45:55,090
uh, Self-Attention in images for a self-supervised learning.

953
00:45:55,090 --> 00:45:59,785
Um, I guess on, on self-supervised learning so the geni- generative modeling work that,

954
00:45:59,785 --> 00:46:01,450
that I talked about before in,

955
00:46:01,450 --> 00:46:05,320
in itself just having probabilistic models of images is, I mean,

956
00:46:05,320 --> 00:46:06,895
I guess the best model of an image is I,

957
00:46:06,895 --> 00:46:09,580
I go to Google search and I pick up an image and I just give it to you,

958
00:46:09,580 --> 00:46:12,325
but I guess generative models of images are useful because,

959
00:46:12,325 --> 00:46:14,470
if you want to do something like semis-, uh, uh,

960
00:46:14,470 --> 00:46:16,810
self supervised learning where you just pre-train a model on

961
00:46:16,810 --> 00:46:19,375
a lot of- on a lot of unlabeled data then you transfer it.

962
00:46:19,375 --> 00:46:22,765
So hopefully, this is gonna help and this is gonna be a part of that machinery.

963
00:46:22,765 --> 00:46:26,890
Um, another interesting, uh,

964
00:46:26,890 --> 00:46:30,520
another indus-interesting structure that relative attention allows you to model,

965
00:46:30,520 --> 00:46:31,960
is, uh, is, is kind of a graph.

966
00:46:31,960 --> 00:46:33,520
So imagine you have this, uh,

967
00:46:33,520 --> 00:46:36,265
you have this similarity graph where these red edges are,

968
00:46:36,265 --> 00:46:37,600
are this notion of companies,

969
00:46:37,600 --> 00:46:40,180
and the blue edge is a notion of a fruit, uh,

970
00:46:40,180 --> 00:46:44,500
and um, an apple takes these two forms.

971
00:46:44,500 --> 00:46:47,140
And, uh, and you could just imagine

972
00:46:47,140 --> 00:46:50,650
relative attention just modeling this- just being able to model,

973
00:46:50,650 --> 00:46:52,285
or being able to- you, you,

974
00:46:52,285 --> 00:46:56,170
yourself being able to impose these different notions of similarity uh,

975
00:46:56,170 --> 00:46:58,375
between, uh, between, uh, different elements.

976
00:46:58,375 --> 00:47:00,715
Uh, so if you have like, if you have graph problems, um,

977
00:47:00,715 --> 00:47:03,925
then relative self-attention might be a good fit for you.

978
00:47:03,925 --> 00:47:08,530
Um, there's also, there's also a simi- quite a position paper by Battaglia et al from

979
00:47:08,530 --> 00:47:13,930
Deep Mind that talks about relative attention and how it can be used, um, within graphs.

980
00:47:13,930 --> 00:47:15,580
So while we're on graphs,

981
00:47:15,580 --> 00:47:18,685
I just wanted to- perhaps might be interesting to connect,

982
00:47:18,685 --> 00:47:21,490
um, uh, of- some, uh,

983
00:47:21,490 --> 00:47:22,810
excellent work that was done on, uh,

984
00:47:22,810 --> 00:47:25,030
on graphs called Message Passing Neural Networks.

985
00:47:25,030 --> 00:47:27,265
And it's quite funny, so if you look at,

986
00:47:27,265 --> 00:47:30,730
if you look at the message passing function, um,

987
00:47:30,730 --> 00:47:34,480
what it's saying is you're actually just passing messages between pairs of nodes.

988
00:47:34,480 --> 00:47:37,090
So you can just think of self attention as imposing a fully connect- it's

989
00:47:37,090 --> 00:47:39,970
like a bipe- a full, a complete bipartite graph,

990
00:47:39,970 --> 00:47:42,250
and, uh, you're, you're passing messages between,

991
00:47:42,250 --> 00:47:43,750
you're passing messages between nodes.

992
00:47:43,750 --> 00:47:46,540
Now message passing, message passing neural networks did exactly that.

993
00:47:46,540 --> 00:47:49,420
They were passing messages between nodes as well. And how are they different?

994
00:47:49,420 --> 00:47:51,580
Well, the only way that when- well, mathematically,

995
00:47:51,580 --> 00:47:53,965
they were only different in that message passing was,

996
00:47:53,965 --> 00:47:57,370
was, uh, forcing the messages to be between pairs of nodes,

997
00:47:57,370 --> 00:48:00,790
but just because of the Softmax function where you get interaction between all the nodes,

998
00:48:00,790 --> 00:48:03,175
self attention is like a message passing mechanism,

999
00:48:03,175 --> 00:48:05,470
where the interactions are between all, all nodes.

1000
00:48:05,470 --> 00:48:07,315
So, uh, they're, they're like,

1001
00:48:07,315 --> 00:48:08,800
they're not too far mathematically,

1002
00:48:08,800 --> 00:48:11,320
and also the me- the Message Passing Paper introduces

1003
00:48:11,320 --> 00:48:14,620
an interesting concept called Multiple Towers that are similar to multi-head attention,

1004
00:48:14,620 --> 00:48:16,585
uh, that, that Norman invented.

1005
00:48:16,585 --> 00:48:21,160
And, uh, it's like you run k copies of these message passing neural networks in parallel.

1006
00:48:21,160 --> 00:48:23,590
So there's a lot of similarity between existing, you know,

1007
00:48:23,590 --> 00:48:27,805
this connects to work that existed before but these connections sort of came in later.

1008
00:48:27,805 --> 00:48:31,930
Um, we have a graph library where we kind of connected these both,

1009
00:48:31,930 --> 00:48:34,150
both these strands message passing and, uh, we,

1010
00:48:34,150 --> 00:48:37,495
uh, we put it out in tensor2tensor.

1011
00:48:37,495 --> 00:48:40,705
Um, so to sort of summarize, um,

1012
00:48:40,705 --> 00:48:43,510
the properties that Self-Attention has been able to help

1013
00:48:43,510 --> 00:48:46,240
us model is this constant path length between any two,

1014
00:48:46,240 --> 00:48:47,870
any two positions, and it's been,

1015
00:48:47,870 --> 00:48:49,600
it's been shown to be quite useful in,

1016
00:48:49,600 --> 00:48:52,165
in, in, uh, in sequence modeling.

1017
00:48:52,165 --> 00:48:56,200
This advantage of having unbounded memory not having to pack information in finite,

1018
00:48:56,200 --> 00:48:58,360
in, in sort of a finite amount of- in a,

1019
00:48:58,360 --> 00:48:59,575
in a fixed amount of space,

1020
00:48:59,575 --> 00:49:03,625
uh, where in, in our case our memory essentially grows with the sequences is,

1021
00:49:03,625 --> 00:49:07,180
is helps you computationally, uh, it's trivial to parallelize.

1022
00:49:07,180 --> 00:49:09,110
You can, you can crunch a lot of data, it's uh,

1023
00:49:09,110 --> 00:49:12,040
which is useful if you wanna have your large data sets.

1024
00:49:12,040 --> 00:49:14,275
We found that it can model Self-Similarity.

1025
00:49:14,275 --> 00:49:16,330
Uh, It seems to be a very natural thing, uh,

1026
00:49:16,330 --> 00:49:20,350
a very, a very natural phenomenon if you're dealing with images or music.

1027
00:49:20,350 --> 00:49:23,200
Also, relative attention allows you to sort of, gives you this added dimension

1028
00:49:23,200 --> 00:49:26,080
of being able to model expressive timing and music,

1029
00:49:26,080 --> 00:49:27,925
well, this translational equivariance,

1030
00:49:27,925 --> 00:49:30,475
uh, it extends naturally to graphs.

1031
00:49:30,475 --> 00:49:37,030
Um, so this part or everything that I talked so far was about sort of parallel training.

1032
00:49:37,030 --> 00:49:41,905
Um, so there's a very active area of research now using the Self-Attention models for,

1033
00:49:41,905 --> 00:49:43,975
for, for less auto-regressive generation.

1034
00:49:43,975 --> 00:49:45,790
So notice a- at generation time,

1035
00:49:45,790 --> 00:49:47,575
notice that the decoder mask was causal,

1036
00:49:47,575 --> 00:49:48,670
we couldn't look into the future.

1037
00:49:48,670 --> 00:49:51,190
So when we're, when we're generating we're still

1038
00:49:51,190 --> 00:49:54,250
generating sequentially left to right on the target side.

1039
00:49:54,250 --> 00:49:56,845
Um, so, um, and, and,

1040
00:49:56,845 --> 00:49:59,170
and, and why, why is generation hard?

1041
00:49:59,170 --> 00:50:00,670
Well, because your outputs are multi-modal.

1042
00:50:00,670 --> 00:50:02,845
I f you had- if you want to translate English to German,

1043
00:50:02,845 --> 00:50:04,285
there's multiple ways and,

1044
00:50:04,285 --> 00:50:08,410
and, and your, your second word that you're translating will depend on the first word.

1045
00:50:08,410 --> 00:50:11,605
For example, if you, if you first- the first word that you predict was danke,

1046
00:50:11,605 --> 00:50:13,675
then that's going to change the second word that you predict.

1047
00:50:13,675 --> 00:50:15,670
And if you just predicted them independently,

1048
00:50:15,670 --> 00:50:17,620
then you can imagine you can just have all sorts of

1049
00:50:17,620 --> 00:50:20,185
permutations of these which will be incorrect.

1050
00:50:20,185 --> 00:50:22,690
Uh, and the way we actually break modes is

1051
00:50:22,690 --> 00:50:24,940
just- or we make decisions is just sequential generation.

1052
00:50:24,940 --> 00:50:27,700
Once we commit to a word that makes a decision,

1053
00:50:27,700 --> 00:50:30,490
and then that nails down what's the next word that you're going to predict.

1054
00:50:30,490 --> 00:50:34,210
So there's been some, there's been some work on, it's an active research area, uh,

1055
00:50:34,210 --> 00:50:36,700
and you can kind of categorize some of these papers like

1056
00:50:36,700 --> 00:50:41,740
the non-autogressive transformer of the fast- the third paper, fast decoding.

1057
00:50:41,740 --> 00:50:43,870
Um, the fourth paper towards a better understanding

1058
00:50:43,870 --> 00:50:46,000
of all Vector Quantized Auto-encoders into this group,

1059
00:50:46,000 --> 00:50:49,255
where they're actually make- doing the decision making in a latent space,

1060
00:50:49,255 --> 00:50:53,470
that's being, uh, it's e- either being learned using word alignments,

1061
00:50:53,470 --> 00:50:56,860
uh, fertilities, or that's being learned using Auto-encoders.

1062
00:50:56,860 --> 00:50:59,680
So you make- you do the decision making in latent space,

1063
00:50:59,680 --> 00:51:02,275
and then you- once you've made the decisions in latent space,

1064
00:51:02,275 --> 00:51:04,030
you assume that all your outputs,

1065
00:51:04,030 --> 00:51:05,725
are actually conditionally independent,

1066
00:51:05,725 --> 00:51:07,180
given that you've made these decisions.

1067
00:51:07,180 --> 00:51:08,485
So that's how they actually speed up.

1068
00:51:08,485 --> 00:51:10,600
There's also- there's ano- there's another paper.

1069
00:51:10,600 --> 00:51:11,860
The second one is a

1070
00:51:11,860 --> 00:51:14,020
paper that does Iterative Refinement.

1071
00:51:14,020 --> 00:51:17,665
There is also a Blockwise Parallel Decoding paper by Mitchell Stern,

1072
00:51:17,665 --> 00:51:20,215
uh, Noam Shazeer, and Jakob Uszkoreit, uh,

1073
00:51:20,215 --> 00:51:23,440
where they essentially just run multiple models like, uh,

1074
00:51:23,440 --> 00:51:29,440
and rescore using a more- a decode using a faster model and score,

1075
00:51:29,440 --> 00:51:31,405
using the more expensive model.

1076
00:51:31,405 --> 00:51:33,580
So that's how it sort of it speeds it up.

1077
00:51:33,580 --> 00:51:38,350
Um, [NOISE] transfer learning has had the- Self-Attention has been beneficial in transfer

1078
00:51:38,350 --> 00:51:42,820
learning, GPT from OpenAI and BERT are two classic examples.

1079
00:51:42,820 --> 00:51:44,995
There's been some work on actually, scaling this up,

1080
00:51:44,995 --> 00:51:48,085
like add a factor as, uh, efficient optimizer.

1081
00:51:48,085 --> 00:51:52,120
Um, there's a, there's a recent paper by Rohan Anil and Yoram Singer.

1082
00:51:52,120 --> 00:51:54,445
Um, there's also Mesh-Tensorflow,

1083
00:51:54,445 --> 00:51:57,850
which actually they've been able to train models

1084
00:51:57,850 --> 00:52:02,530
of just several orders of magnitude larger than the original models have been trained.

1085
00:52:02,530 --> 00:52:05,710
So there's, I mean, when you're working this large data regime you would probably want to

1086
00:52:05,710 --> 00:52:07,720
memorize a lot of- you want to memorize

1087
00:52:07,720 --> 00:52:10,270
a lot of things inside your parameters used to train a larger model.

1088
00:52:10,270 --> 00:52:12,415
Uh, Mesh-Tensorflow can uh, can let you do that.

1089
00:52:12,415 --> 00:52:16,300
Um, there has been a lot of interesting work, universal transformers,

1090
00:52:16,300 --> 00:52:19,240
sort of recurrent neural networks can actually count very nicely.

1091
00:52:19,240 --> 00:52:21,910
There's these cute papers by Schmidhuber where he actually shows

1092
00:52:21,910 --> 00:52:25,480
that recurring neural, the count- the cell mechanism just learns a nice counter,

1093
00:52:25,480 --> 00:52:27,340
like if you're- you can learn kind of a to the n,

1094
00:52:27,340 --> 00:52:29,230
b to the n, uh, with LSTM.

1095
00:52:29,230 --> 00:52:31,735
So then, uh, universals transformers

1096
00:52:31,735 --> 00:52:34,660
brings back recurrence in depth inside the transformer.

1097
00:52:34,660 --> 00:52:37,195
Uh, there is a really cool Wikipedia paper,

1098
00:52:37,195 --> 00:52:40,900
um, simultaneously with the image transformer paper that also uses local attention.

1099
00:52:40,900 --> 00:52:46,060
Transformer-XL paper that sort of combines recurrence with Self-Attention,

1100
00:52:46,060 --> 00:52:47,290
so they do Self-Attention in chunks,

1101
00:52:47,290 --> 00:52:50,275
but they sort of summarize history by using recurrence, it's kinda cute.

1102
00:52:50,275 --> 00:52:52,135
It's been used in speech but I don't know if there's been

1103
00:52:52,135 --> 00:52:55,315
some fairly big success stories of Self-Attention in speech.

1104
00:52:55,315 --> 00:52:58,350
Uh, again, similar issues where you have very large, uh,

1105
00:52:58,350 --> 00:53:00,900
um as positions to,

1106
00:53:00,900 --> 00:53:03,165
uh, to do Self-Attention over.

1107
00:53:03,165 --> 00:53:08,050
So yeah, um, self supervision is a- if it works it would be,

1108
00:53:08,050 --> 00:53:09,640
it would be, it would be very beneficial.

1109
00:53:09,640 --> 00:53:12,910
We wouldn't need large label datasets, understanding transfer,

1110
00:53:12,910 --> 00:53:15,490
transfers is becoming very succe- becoming- is becoming

1111
00:53:15,490 --> 00:53:18,960
a reality in NLP with BERT and some of these other models.

1112
00:53:18,960 --> 00:53:21,630
So understanding how these, what's actually happening is a-

1113
00:53:21,630 --> 00:53:24,555
is an interesting area of ongoing research for me and a couple.

1114
00:53:24,555 --> 00:53:29,505
And a few of my collaborators and uh, multitask learning and surmounting this,

1115
00:53:29,505 --> 00:53:32,860
this quadratic problem with Self-Attention is

1116
00:53:32,860 --> 00:53:38,150
an interesting area of research that I- that I'd like to pursue. Thank you.

