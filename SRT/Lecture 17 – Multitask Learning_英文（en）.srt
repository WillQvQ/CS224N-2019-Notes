1
00:00:04,190 --> 00:00:08,790
So today, we're very pleased to have as our second, um,

2
00:00:08,790 --> 00:00:10,980
invited speaker, Richard Socher,

3
00:00:10,980 --> 00:00:14,085
he is the chief scientist at Salesforce.

4
00:00:14,085 --> 00:00:18,040
Um, Richard actually also has a lot more connection to this class,

5
00:00:18,040 --> 00:00:21,840
um, because, um, for several years, um,

6
00:00:21,840 --> 00:00:24,970
Richard was involved either as instructor or, um,

7
00:00:24,970 --> 00:00:29,110
co-instructor in teaching this material at Stanford,

8
00:00:29,110 --> 00:00:32,605
um, so he sort of knows the course, um, pretty well.

9
00:00:32,605 --> 00:00:34,030
Um, and so today,

10
00:00:34,030 --> 00:00:38,830
he's going to be talking about some of the challenges and recent work

11
00:00:38,830 --> 00:00:43,690
in doing multitask learning in natural language processing. So welcome, Richard.

12
00:00:43,690 --> 00:00:46,595
Thank you. Hello, everybody. I'm excited to be here.

13
00:00:46,595 --> 00:00:49,380
Uh, yeah, I want to talk to you today about what we,

14
00:00:49,380 --> 00:00:51,280
in short, called decaNLP.

15
00:00:51,280 --> 00:00:54,635
I want to first give a big shout out to Bryan McCann.

16
00:00:54,635 --> 00:00:56,900
He's the first author of this, uh, paper,

17
00:00:56,900 --> 00:01:00,200
and I've pitched this idea to a lot of people in the last, like,

18
00:01:00,200 --> 00:01:01,280
three to four years,

19
00:01:01,280 --> 00:01:02,405
and most people were like,

20
00:01:02,405 --> 00:01:04,730
"This is too much pre-processing because you're trying to

21
00:01:04,730 --> 00:01:07,290
do 10 different tasks in one model."

22
00:01:07,290 --> 00:01:09,510
That's sort of where the decathlon, uh,

23
00:01:09,510 --> 00:01:11,805
wording comes in, uh, but he,

24
00:01:11,805 --> 00:01:13,305
he really stuck to it, uh,

25
00:01:13,305 --> 00:01:16,730
did all the pre-processing and all the things that you now know like tokenization,

26
00:01:16,730 --> 00:01:18,500
and it turns out a lot of different data sets,

27
00:01:18,500 --> 00:01:20,270
have a different conception of what a word is.

28
00:01:20,270 --> 00:01:21,710
This wasn't two words,

29
00:01:21,710 --> 00:01:23,480
uh, or one word,

30
00:01:23,480 --> 00:01:25,355
and things like that, and that changes how you

31
00:01:25,355 --> 00:01:27,470
write all your evaluation scripts and all of that.

32
00:01:27,470 --> 00:01:29,165
So Bryan, uh, is,

33
00:01:29,165 --> 00:01:30,770
is a really phenomenal researcher,

34
00:01:30,770 --> 00:01:31,985
uh, with us in the group,

35
00:01:31,985 --> 00:01:35,340
and Nitish has helped us a lot on the optimization side of this,

36
00:01:35,340 --> 00:01:36,480
uh, and then Caiming Xiong,

37
00:01:36,480 --> 00:01:38,415
the Director of Research, has done a lot of, uh,

38
00:01:38,415 --> 00:01:41,735
really phenomenal work that's kind of helpful in pretty much all our projects.

39
00:01:41,735 --> 00:01:44,830
So I'm going to tell you a couple of different, uh,

40
00:01:44,830 --> 00:01:48,560
lines of reasoning that led us to,

41
00:01:48,560 --> 00:01:50,525
uh, this idea of multitask learning.

42
00:01:50,525 --> 00:01:54,170
And the first one was sort of trying to take a step back and looking at the field,

43
00:01:54,170 --> 00:01:58,955
and I noticed not like that much of a historical class but basically pre-2010,

44
00:01:58,955 --> 00:02:04,340
most natural language processing had kind of these very hand-designed features,

45
00:02:04,340 --> 00:02:05,660
and we basically just had,

46
00:02:05,660 --> 00:02:08,710
uh, machine learning kind of learned weights,

47
00:02:08,710 --> 00:02:12,680
uh, in the optimization procedure for these human-designed features.

48
00:02:12,680 --> 00:02:20,780
And so in 2010, Chris and I and others sort of started to work in deep learning for feature learning.

49
00:02:20,780 --> 00:02:22,150
So everything was a word vector and now,

50
00:02:22,150 --> 00:02:25,910
we can back-propagate into them and actually learn those representations.

51
00:02:25,910 --> 00:02:27,410
And I think currently,

52
00:02:27,410 --> 00:02:28,880
we're kind of in a state where we do a lot of

53
00:02:28,880 --> 00:02:31,940
deep architecture engineering for specific tasks,

54
00:02:31,940 --> 00:02:33,110
and you've seen this already.

55
00:02:33,110 --> 00:02:34,700
You have like an NER model,

56
00:02:34,700 --> 00:02:36,350
you have a question and answering model,

57
00:02:36,350 --> 00:02:37,745
you have a translation model,

58
00:02:37,745 --> 00:02:39,110
and we basically now,

59
00:02:39,110 --> 00:02:41,990
each of these communities has at least, uh,

60
00:02:41,990 --> 00:02:44,660
converged on is probably some kind of neural network,

61
00:02:44,660 --> 00:02:47,570
but there's still a lot of different kinds of architectures of

62
00:02:47,570 --> 00:02:51,045
these neural networks that you're working on for each different task.

63
00:02:51,045 --> 00:02:52,590
And so the question is like, okay,

64
00:02:52,590 --> 00:02:54,125
we're gonna probably do that for

65
00:02:54,125 --> 00:02:57,170
another couple of years because we're making good progress,

66
00:02:57,170 --> 00:02:58,550
but what's sort of next,

67
00:02:58,550 --> 00:02:59,990
uh, on the research side?

68
00:02:59,990 --> 00:03:02,480
And what I actually love about this class so much is that

69
00:03:02,480 --> 00:03:05,000
you go from like maybe not knowing much about NLP at

70
00:03:05,000 --> 00:03:07,715
all to you can basically understand

71
00:03:07,715 --> 00:03:10,880
the state-of-the-art research papers as they come out now,

72
00:03:10,880 --> 00:03:12,950
uh, and this, this is one of those.

73
00:03:12,950 --> 00:03:15,480
Uh, so [NOISE] why,

74
00:03:15,480 --> 00:03:17,840
why not continue to work in this multitask regime?

75
00:03:17,840 --> 00:03:19,280
In some ways, I feel like, uh,

76
00:03:19,280 --> 00:03:20,960
the community is a little bit, uh,

77
00:03:20,960 --> 00:03:22,700
like this cute dog, where we, kind of,

78
00:03:22,700 --> 00:03:25,960
randomly restart, uh, after every project.

79
00:03:25,960 --> 00:03:29,840
And it's kind of clear to me that if you have a lot of training data, uh,

80
00:03:29,840 --> 00:03:34,920
and you define a specific data set and task on that data set,

81
00:03:34,920 --> 00:03:39,080
you start to architecture engineer in your model to hill-climb on a particular metric,

82
00:03:39,080 --> 00:03:41,420
or leaderboard, or publications,

83
00:03:41,420 --> 00:03:43,655
or products, or whatever it is, uh,

84
00:03:43,655 --> 00:03:45,710
then as long as your data set has

85
00:03:45,710 --> 00:03:48,090
roughly a good representative set of

86
00:03:48,090 --> 00:03:50,875
1,000 times the number of output classes that you have,

87
00:03:50,875 --> 00:03:56,210
you'll probably get it into a regi- regime where you're in the 80 to 90 percent accuracy,

88
00:03:56,210 --> 00:03:59,360
or if one, where you're basically doing pretty okay.

89
00:03:59,360 --> 00:04:02,300
And of course, now when you look at trends on ImageNet,

90
00:04:02,300 --> 00:04:05,000
you have 1,000 different classes in computer vision,

91
00:04:05,000 --> 00:04:08,640
1,000 different classes, each has 1,000 images.

92
00:04:08,640 --> 00:04:11,460
So if you have roughly a million images, you do pretty well.

93
00:04:11,460 --> 00:04:13,740
And in machine translation, ideally,

94
00:04:13,740 --> 00:04:16,250
you know, I have many more, I have like hundreds of thousands of words,

95
00:04:16,250 --> 00:04:21,725
so you want many millions of examples of each of the word in their,

96
00:04:21,725 --> 00:04:23,090
uh, words in their context.

97
00:04:23,090 --> 00:04:24,830
And of course, you know, that the caveat is

98
00:04:24,830 --> 00:04:27,620
machine translation doesn't work to the level of humans,

99
00:04:27,620 --> 00:04:30,110
but it works well enough to have it at least in products,

100
00:04:30,110 --> 00:04:34,750
and even the best human translators use it as sort of a pre-translation and then,

101
00:04:34,750 --> 00:04:37,030
uh, sort of, clean it up.

102
00:04:37,030 --> 00:04:39,985
And so it's also clear to me that in this regime,

103
00:04:39,985 --> 00:04:41,480
and if we want to get to, sort of,

104
00:04:41,480 --> 00:04:43,550
more general AI features, uh,

105
00:04:43,550 --> 00:04:47,360
we need to have some kind of more continuous learning of a single model.

106
00:04:47,360 --> 00:04:49,840
Because if we keep restarting at every project,

107
00:04:49,840 --> 00:04:51,830
we're never going to get to a single model that, kind of,

108
00:04:51,830 --> 00:04:55,715
encompasses more and more of the complexity of natural language.

109
00:04:55,715 --> 00:04:59,115
And, uh, when I say we start from random,

110
00:04:59,115 --> 00:05:01,295
you of course know that that's not quite true

111
00:05:01,295 --> 00:05:04,190
because we do have some things that we pre-train,

112
00:05:04,190 --> 00:05:06,290
namely word vectors, and in computer vision,

113
00:05:06,290 --> 00:05:07,520
we have even more things.

114
00:05:07,520 --> 00:05:09,020
And so in some ways that is, ah,

115
00:05:09,020 --> 00:05:11,745
an aspiring ideal for NLP,

116
00:05:11,745 --> 00:05:13,860
because in computer vision, you would be, kind of,

117
00:05:13,860 --> 00:05:15,590
crazy to not use some kind of

118
00:05:15,590 --> 00:05:19,610
convolution neural network that has pre-train- has been pre-trained on some kind of

119
00:05:19,610 --> 00:05:22,520
tasks like ImageNet when you start with your project and

120
00:05:22,520 --> 00:05:25,985
try to classify objects or do object detection and a lot of other things.

121
00:05:25,985 --> 00:05:29,750
And in some ways that the whole community could get behind it very quickly,

122
00:05:29,750 --> 00:05:32,460
because I mean, you know, once it worked, uh,

123
00:05:32,460 --> 00:05:34,125
reasonably well, because there was a, sort of,

124
00:05:34,125 --> 00:05:35,990
single blocking task in computer vision.

125
00:05:35,990 --> 00:05:38,610
If you can't even tell apart a dog from a cat from a house,

126
00:05:38,610 --> 00:05:42,420
it doesn't really make sense to think of even larger, uh, vision projects.

127
00:05:42,420 --> 00:05:45,210
And in NLP, we've had a lot of success with word vectors,

128
00:05:45,210 --> 00:05:46,650
you know a lot of those now,

129
00:05:46,650 --> 00:05:48,750
and it started for, sort of, just a small, uh,

130
00:05:48,750 --> 00:05:51,780
window-based approach or Word2Vec and GloVe, uh,

131
00:05:51,780 --> 00:05:55,020
then we had, uh, context vectors that were trained, uh,

132
00:05:55,020 --> 00:05:57,295
on machine translation, but basically,

133
00:05:57,295 --> 00:06:00,050
instead of just having a single set of words,

134
00:06:00,050 --> 00:06:04,455
we actually pre-trained some of the NLSTMs that came on top of those word vectors,

135
00:06:04,455 --> 00:06:06,925
and, uh, the way we train that, uh,

136
00:06:06,925 --> 00:06:09,050
was also actually Bryan McCann's paper on

137
00:06:09,050 --> 00:06:12,530
contextual vectors with machine translation and then ELMo,

138
00:06:12,530 --> 00:06:16,260
kind of, replaced machine translation with, uh, language modeling,

139
00:06:16,260 --> 00:06:18,570
which of course is even better because there's even more training data,

140
00:06:18,570 --> 00:06:20,340
and it still tells you a lot, uh,

141
00:06:20,340 --> 00:06:23,210
and kind of captures in some ways a more complex version of

142
00:06:23,210 --> 00:06:26,900
distributional sort of hypotheses that we had in simpler word vectors,

143
00:06:26,900 --> 00:06:29,640
and BERT, not quite a language model but also, kind of,

144
00:06:29,640 --> 00:06:31,610
trying to predict words in their context, uh,

145
00:06:31,610 --> 00:06:34,400
but pre-training a lot more layers and a lot deeper networks.

146
00:06:34,400 --> 00:06:39,700
And so we see the success of pre-training a certain set of weights.

147
00:06:39,700 --> 00:06:41,265
And so the question is,

148
00:06:41,265 --> 00:06:44,310
why not try to pre-train the entire model?

149
00:06:44,310 --> 00:06:46,650
As in including your output,

150
00:06:46,650 --> 00:06:50,145
your softmax, your pointer mechanisms and everything,

151
00:06:50,145 --> 00:06:54,240
and then just taking a completely pre-trained model and trying to do something,

152
00:06:54,240 --> 00:06:56,895
and that is, kind of, the goal that we have.

153
00:06:56,895 --> 00:06:58,890
And so, uh, we, sort of,

154
00:06:58,890 --> 00:07:00,525
ask ourselves why hasn't this happened?

155
00:07:00,525 --> 00:07:01,740
Why are we, you know,

156
00:07:01,740 --> 00:07:03,430
the first to think about, like,

157
00:07:03,430 --> 00:07:05,810
trying to pre-train the entirety of the model,

158
00:07:05,810 --> 00:07:07,370
the encoders, and decoders,

159
00:07:07,370 --> 00:07:08,420
and outputs, and everything.

160
00:07:08,420 --> 00:07:12,740
Uh, and I think part of it is that NLP requires a lot of different kinds of reasoning.

161
00:07:12,740 --> 00:07:14,420
You've seen many of them already.

162
00:07:14,420 --> 00:07:18,290
You have some logical reasoning like 550 people in this room,

163
00:07:18,290 --> 00:07:20,300
25 leave, are there still people in the room,

164
00:07:20,300 --> 00:07:22,790
and you logically can answer that question,

165
00:07:22,790 --> 00:07:25,930
and you have lots of different kinds of linguistic and emotional reasoning,

166
00:07:25,930 --> 00:07:27,470
sentiment analysis, you know,

167
00:07:27,470 --> 00:07:30,140
this is a typical Nicolas Cage movie and then you need to know that that's a

168
00:07:30,140 --> 00:07:33,590
probably negative review unless you like Nicolas Cage movies.

169
00:07:33,590 --> 00:07:36,470
Um, no judgment. And, uh,

170
00:07:36,470 --> 00:07:38,180
you know, visual types of reasoning and so on.

171
00:07:38,180 --> 00:07:41,450
And so I think partly because of that complexity in the beginning to feel,

172
00:07:41,450 --> 00:07:46,580
didn't really make much progress and now and then kind of separate it.

173
00:07:46,580 --> 00:07:50,675
And I think in some cases, kind of artificially separated into all these separate tasks,

174
00:07:50,675 --> 00:07:52,340
like you have named entity recognition,

175
00:07:52,340 --> 00:07:55,795
part of speech tagging, and semantic role labeling and, and so on.

176
00:07:55,795 --> 00:07:58,560
And, and in some ways- and it sounds kind of snarky but,

177
00:07:58,560 --> 00:07:59,990
you know, it made a lot of sense at the time,

178
00:07:59,990 --> 00:08:02,540
and it allowed us to make a lot of progress in the community,

179
00:08:02,540 --> 00:08:04,850
but basically we started chasing these benchmarks,

180
00:08:04,850 --> 00:08:06,290
and all these different communities, kind of,

181
00:08:06,290 --> 00:08:08,610
started going off in their own ways.

182
00:08:08,610 --> 00:08:10,320
And we even have some communities that say,

183
00:08:10,320 --> 00:08:11,950
"We do general question answering,

184
00:08:11,950 --> 00:08:14,990
and there's literally workshops on general question answering, and when I asked,

185
00:08:14,990 --> 00:08:18,350
uh, the organizers, "Can I ask your model what the sentiment is of this tweet?"

186
00:08:18,350 --> 00:08:21,240
They're like, "No, that's sentiment analysis. Go to that different workshop.

187
00:08:21,240 --> 00:08:22,515
It's down, down the hall."

188
00:08:22,515 --> 00:08:24,270
But I'm like, "That's a- that's a question.

189
00:08:24,270 --> 00:08:27,335
Why can't you answer it in the general question answering workshop?"

190
00:08:27,335 --> 00:08:29,940
Um, and so a lot of people then say,

191
00:08:29,940 --> 00:08:31,540
"Well, if you want to work on more general stuff,

192
00:08:31,540 --> 00:08:33,860
it has to be an unsupervised, kind of,

193
00:08:33,860 --> 00:08:36,695
task and the, the feature will not be supervised."

194
00:08:36,695 --> 00:08:40,490
I don't think NLP will be completely unsupervised,

195
00:08:40,490 --> 00:08:42,830
and we won't solve it, uh, completely unsupervised,

196
00:08:42,830 --> 00:08:45,410
because in the end, language has a lot of supervision for people,

197
00:08:45,410 --> 00:08:49,020
uh, and, uh, I think for, for systems also.

198
00:08:49,020 --> 00:08:52,620
Uh, and you won't, you know,

199
00:08:52,620 --> 00:08:54,600
if you have- there's a child and it's in a jungle,

200
00:08:54,600 --> 00:08:57,290
it will probably develop a pretty good visual cortex by itself,

201
00:08:57,290 --> 00:08:59,365
but it won't develop language by itself.

202
00:08:59,365 --> 00:09:01,230
And then- and then also, like,

203
00:09:01,230 --> 00:09:03,720
I think if you'll just allow AI's to talk to one another,

204
00:09:03,720 --> 00:09:06,200
it makes very little sense for them to try to come up with as

205
00:09:06,200 --> 00:09:09,140
inefficient of a communication protocol as humans have with, you know,

206
00:09:09,140 --> 00:09:13,970
sequential processing of language because algorithms and computers could,

207
00:09:13,970 --> 00:09:16,070
if there's no supervision of human language,

208
00:09:16,070 --> 00:09:19,455
they could just communicate in much more efficient ways with one another.

209
00:09:19,455 --> 00:09:21,055
So I think it's fairly clear,

210
00:09:21,055 --> 00:09:24,490
we need a lot of supervision, uh, in NLP.

211
00:09:24,490 --> 00:09:27,840
And so basically, all of this has led us, uh,

212
00:09:27,840 --> 00:09:34,340
to trying to think about a unified multitask model for a lot of different NLP tasks.

213
00:09:34,340 --> 00:09:36,515
By the way, if you have any questions, just raise your hand.

214
00:09:36,515 --> 00:09:39,110
Okay, let's make this very interactive.

215
00:09:39,110 --> 00:09:42,555
Um, basically, we want this unified model, uh,

216
00:09:42,555 --> 00:09:45,570
to decide how to transfer knowledge,

217
00:09:45,570 --> 00:09:47,885
uh, and not have it, sort of, be manually assigned.

218
00:09:47,885 --> 00:09:49,280
Like in most cases,

219
00:09:49,280 --> 00:09:50,870
when you assign your project you say, "Oh,

220
00:09:50,870 --> 00:09:55,030
well I know that named entity recognition part of speech tagging help each other.

221
00:09:55,030 --> 00:09:56,870
Because once you know something is a noun,

222
00:09:56,870 --> 00:10:00,730
then it's more likely that it's also a named entity."

223
00:10:00,730 --> 00:10:05,090
And in this case, we want to basically allow for the single unified model

224
00:10:05,090 --> 00:10:09,890
to know itself how to do domain adaptation and wha- how to share the weights,

225
00:10:09,890 --> 00:10:12,650
and that will hopefully then lead to a lot of,

226
00:10:12,650 --> 00:10:15,935
uh, transfer learning and zero shot learning capabilities.

227
00:10:15,935 --> 00:10:19,100
I also think that if we get to this, sort of,

228
00:10:19,100 --> 00:10:23,265
hard goal of having a single fa- single unified multitask model,

229
00:10:23,265 --> 00:10:27,140
then we'll easy-  be able to more easily adapt it to

230
00:10:27,140 --> 00:10:31,085
new tasks and we'll be also able to deploy it in production more quickly.

231
00:10:31,085 --> 00:10:32,405
If nowadays you want to build

232
00:10:32,405 --> 00:10:35,570
a little squirrel detector and connect it to your sprinkler system,

233
00:10:35,570 --> 00:10:37,895
you can just download some off-the-shelf software,

234
00:10:37,895 --> 00:10:40,200
and it will basically, kind of, work.

235
00:10:40,200 --> 00:10:42,170
That is not the case if you try to do

236
00:10:42,170 --> 00:10:44,390
a pretty complex language project where you

237
00:10:44,390 --> 00:10:46,955
want to translate into some completely new language or,

238
00:10:46,955 --> 00:10:50,240
you know, analyze some website and then do something else afterwards.

239
00:10:50,240 --> 00:10:51,890
So, uh, you also,

240
00:10:51,890 --> 00:10:56,370
when you actually try to deploy and use these kinds of tools and companies,

241
00:10:56,370 --> 00:10:59,075
you'll realize that there are a lot of different kinds of groups.

242
00:10:59,075 --> 00:11:00,200
There's the search group,

243
00:11:00,200 --> 00:11:01,310
and the chatbot team,

244
00:11:01,310 --> 00:11:02,540
and the translation team,

245
00:11:02,540 --> 00:11:05,930
and, uh, and the social sentiment analysis team,

246
00:11:05,930 --> 00:11:07,100
and they all use different models,

247
00:11:07,100 --> 00:11:08,390
and they all deploy different models,

248
00:11:08,390 --> 00:11:10,850
and they all have to build a lot of overhead into

249
00:11:10,850 --> 00:11:15,150
the core of the- or around that core of an AI model.

250
00:11:15,150 --> 00:11:18,240
So basically, um, lastly,

251
00:11:18,240 --> 00:11:20,435
it was, sort of, what we had with, with this dog.

252
00:11:20,435 --> 00:11:22,170
I think that once we have this unified model,

253
00:11:22,170 --> 00:11:24,380
it will also be a first step to being able to

254
00:11:24,380 --> 00:11:26,870
then continually learn this and just have a single model that just

255
00:11:26,870 --> 00:11:28,880
gets better and better over time and starts

256
00:11:28,880 --> 00:11:32,030
to capture more and more of the complexity of language.

257
00:11:32,030 --> 00:11:33,980
All right, any questions around, sort of,

258
00:11:33,980 --> 00:11:37,560
the motivation high level?

259
00:11:41,700 --> 00:11:44,860
All right. So then, uh,

260
00:11:44,860 --> 00:11:48,370
it's sort of the question, how do we actually make that happen?

261
00:11:48,370 --> 00:11:52,135
And then we -- I first sort of sat down and looked at, like,

262
00:11:52,135 --> 00:11:56,560
the general sort of formats of all the tasks that you may experience in

263
00:11:56,560 --> 00:11:58,510
this class and that NLP sort of has as a field in

264
00:11:58,510 --> 00:12:01,000
general and I think they can broadly classified,

265
00:12:01,000 --> 00:12:03,100
be classified into these three different categories.

266
00:12:03,100 --> 00:12:04,900
Sequence tagging, you already know.

267
00:12:04,900 --> 00:12:07,840
Things like NER or aspect-specific sentiment or in

268
00:12:07,840 --> 00:12:12,250
a specific context we want to classify if a word is positive or negative.

269
00:12:12,250 --> 00:12:14,380
Uh, and then text classification,

270
00:12:14,380 --> 00:12:17,290
just a single label for the entire piece of text

271
00:12:17,290 --> 00:12:20,335
and then sequence the sequence a lot of different, you know,

272
00:12:20,335 --> 00:12:23,575
problems fall into that and I actually personally love, uh,

273
00:12:23,575 --> 00:12:27,490
these three particular tasks: machine translation, summarization, question answering.

274
00:12:27,490 --> 00:12:31,450
Because they are immediately useful that you don't have to explain to somebody,

275
00:12:31,450 --> 00:12:34,195
"Oh, but why do you need the semantic role labeller or parser? "

276
00:12:34,195 --> 00:12:36,490
If you're a layman and you, you know,

277
00:12:36,490 --> 00:12:38,620
on the Internet you understand immediately why it's

278
00:12:38,620 --> 00:12:41,140
useful to do summarization, question answering,

279
00:12:41,140 --> 00:12:43,240
or translation and an improvement in

280
00:12:43,240 --> 00:12:46,840
those tasks kind of immediately translates in- into better products,

281
00:12:46,840 --> 00:12:51,430
uh, and people being able to communicate better and more efficiently with language.

282
00:12:51,430 --> 00:12:57,400
So, that, uh, kind of analysis led us to think,

283
00:12:57,400 --> 00:13:01,030
uh, about these what I call three equivalent supertasks of NLP.

284
00:13:01,030 --> 00:13:03,910
Uh, and basically they are

285
00:13:03,910 --> 00:13:07,780
language modeling, question answer now- question answering and dialogue systems.

286
00:13:07,780 --> 00:13:11,410
Uh, language modeling, basically trying to predin- predict the next word,

287
00:13:11,410 --> 00:13:12,430
you've already worked on that.

288
00:13:12,430 --> 00:13:18,775
Uh, and usually it's only used to rescore or basically to pre-train these days.

289
00:13:18,775 --> 00:13:22,645
But really if you ask me a question and then you try to predict the next couple of words,

290
00:13:22,645 --> 00:13:25,435
then that is also language modeling

291
00:13:25,435 --> 00:13:28,810
and if you're able to predict the next couple of words after a question, like,

292
00:13:28,810 --> 00:13:32,350
what were the named entities in the sentence and then you just generate, you know,

293
00:13:32,350 --> 00:13:34,120
Dresden was a location,

294
00:13:34,120 --> 00:13:36,430
Richard was a person and whatnot.

295
00:13:36,430 --> 00:13:41,140
Uh, then you can kind of cast almost all of these tasks into language modeling.

296
00:13:41,140 --> 00:13:42,580
Uh, similarly question answering,

297
00:13:42,580 --> 00:13:44,080
you can ask any kind of question,

298
00:13:44,080 --> 00:13:45,430
what is the translation,

299
00:13:45,430 --> 00:13:48,115
what's the summary, uh, and so on,

300
00:13:48,115 --> 00:13:50,770
and then with dialogue right now it's kind of tricky because there are

301
00:13:50,770 --> 00:13:55,930
no really good dialogue datasets out there and a lot of times you want some interaction,

302
00:13:55,930 --> 00:14:00,010
you have to run user studies and most of the existing NLP task would

303
00:14:00,010 --> 00:14:04,360
basically be pretty short one-step dialogues like what are the named entity tags,

304
00:14:04,360 --> 00:14:05,560
and you give them and that's it.

305
00:14:05,560 --> 00:14:09,850
So it's a little bit overkill and because of that we basically converged,

306
00:14:09,850 --> 00:14:13,525
uh, on question answering as our main formalism.

307
00:14:13,525 --> 00:14:18,355
And here is now an overview of the 10 different tasks that we have,

308
00:14:18,355 --> 00:14:21,610
uh, and we cast all of them as question answering.

309
00:14:21,610 --> 00:14:25,120
These are literally the tr- the training,

310
00:14:25,120 --> 00:14:27,700
uh, the format of the training dataset, uh,

311
00:14:27,700 --> 00:14:30,880
and eventually also the way we formulate

312
00:14:30,880 --> 00:14:35,530
the test set and you'll see basically for every single task,

313
00:14:35,530 --> 00:14:38,605
you have a context as some kind of document.

314
00:14:38,605 --> 00:14:39,700
It could be a Wikipedia article,

315
00:14:39,700 --> 00:14:41,500
it could be a tweet, it could be a longer document,

316
00:14:41,500 --> 00:14:45,550
whatever, and you ask a question about it and you want to generate an answer.

317
00:14:45,550 --> 00:14:49,090
And I'm actually -- I'm curious if you can think of any task in NLP

318
00:14:49,090 --> 00:14:52,795
that couldn't be formulated in this kind of structure.

319
00:14:52,795 --> 00:14:55,720
Uh, so, let's go over some of these.

320
00:14:55,720 --> 00:14:57,865
Uh, the first one is sort of the standard,

321
00:14:57,865 --> 00:15:00,145
uh, task that all- you're all familiar with now.

322
00:15:00,145 --> 00:15:02,440
The SQuAD, Stanford Question Answering Dataset.

323
00:15:02,440 --> 00:15:06,880
Uh, where the answer is essentially a phrase somewhere in the context.

324
00:15:06,880 --> 00:15:12,265
But then, uh, the second one is something that you would never see in most,

325
00:15:12,265 --> 00:15:16,900
uh, generalized, uh, question answering workshops and that is, uh,

326
00:15:16,900 --> 00:15:20,560
having a context of the single sentence asking what is the translation from

327
00:15:20,560 --> 00:15:25,090
English into German and the output is again a sequence of words but in this case,

328
00:15:25,090 --> 00:15:26,500
and we color them differently here.

329
00:15:26,500 --> 00:15:31,870
Uh, this is blue because all these words are basically not in the context and not in

330
00:15:31,870 --> 00:15:35,110
the question and we will just generate them

331
00:15:35,110 --> 00:15:39,280
with a standard softmax to basically answer this question.

332
00:15:39,280 --> 00:15:43,390
We can also ask what is the summary and you can see that those

333
00:15:43,390 --> 00:15:47,290
two in some ways is artificial to make them into a natural language question.

334
00:15:47,290 --> 00:15:51,250
You could just say translate or summarize and this is just like

335
00:15:51,250 --> 00:15:56,140
one kind of task token in your network but actually half of these tasks.

336
00:15:56,140 --> 00:16:02,305
It makes sense because the question also has ac- is different for every example.

337
00:16:02,305 --> 00:16:06,040
So this one here is natural language inference, NLI, uh,

338
00:16:06,040 --> 00:16:10,920
She covered also where we want to ask whether two sentences entail each other,

339
00:16:10,920 --> 00:16:14,810
contradict each other or there's some neutral relationship between them.

340
00:16:14,810 --> 00:16:16,900
You've seen a lot of sentiment.

341
00:16:16,900 --> 00:16:18,580
And this here is kind of important.

342
00:16:18,580 --> 00:16:22,600
We actually asked is this sentence positive or negative versus just what is the sentiment

343
00:16:22,600 --> 00:16:27,745
and what- why that is important is that you see here in green,

344
00:16:27,745 --> 00:16:30,760
this answer here actually comes from

345
00:16:30,760 --> 00:16:34,380
a word into question and if we formulate it that way,

346
00:16:34,380 --> 00:16:39,330
we can eventually do zero-shot learning where we ask a new question that was

347
00:16:39,330 --> 00:16:44,155
never asked before for a new set of labels and magically, in some cases,

348
00:16:44,155 --> 00:16:46,180
it still actually works and we'll, you know,

349
00:16:46,180 --> 00:16:50,500
ask que- we can ask questions like is this story happy or sad and it will still

350
00:16:50,500 --> 00:16:52,120
give us an answer even though we've never given

351
00:16:52,120 --> 00:16:55,195
it a trained dataset of a bunch of happy and sad stories.

352
00:16:55,195 --> 00:16:59,740
So, it's kind of zero-shot classification that you get to in

353
00:16:59,740 --> 00:17:02,230
some cases if you formulate your questions in a way

354
00:17:02,230 --> 00:17:05,270
that the answer is part as a word in the question.

355
00:17:05,270 --> 00:17:08,340
Then we have semantic role labeling here.

356
00:17:08,340 --> 00:17:15,540
So what has something experienced, kind of a random weird question.

357
00:17:15,540 --> 00:17:18,450
Then we have a zero-shot relation extraction who is

358
00:17:18,450 --> 00:17:22,255
the illustrator of Cycle of the Werewolf,

359
00:17:22,255 --> 00:17:24,580
we also have some dialogue state tracking.

360
00:17:24,580 --> 00:17:28,615
What is the current state in- in a dialogue and the context just keeps on

361
00:17:28,615 --> 00:17:33,985
growing with the dialogue and then we also have SQL,

362
00:17:33,985 --> 00:17:37,690
Wiki SQL translation tasks but not translating into

363
00:17:37,690 --> 00:17:42,025
another natural language translating into a SQL database query.

364
00:17:42,025 --> 00:17:43,720
It's actually a super-helpful task.

365
00:17:43,720 --> 00:17:47,830
There's a, you know, a lot of data out there that is stored in databases.

366
00:17:47,830 --> 00:17:50,440
If you can access it without having to ask

367
00:17:50,440 --> 00:17:53,380
somebody who knows how to program SQL it will make

368
00:17:53,380 --> 00:17:56,200
that data available to a lot more people so

369
00:17:56,200 --> 00:17:59,260
they can analyze it and like business analytics and so on.

370
00:17:59,260 --> 00:18:02,740
And then here, Winograd Schemas and anaphora resolution.

371
00:18:02,740 --> 00:18:06,100
Uh, some people call this kind of common sense reasoning but it's kind of,

372
00:18:06,100 --> 00:18:10,225
you know, mostly just anaphora resolution trying to understand in this context.

373
00:18:10,225 --> 00:18:12,385
Uh, what -- who's, you know,

374
00:18:12,385 --> 00:18:15,550
uh, the word like who had given help,

375
00:18:15,550 --> 00:18:19,030
was it Susan or Joanne, and then based on this context,

376
00:18:19,030 --> 00:18:22,900
you can kind of should be able to figure that out and again here,

377
00:18:22,900 --> 00:18:26,860
the question is different for every single example. All right, yeah?

378
00:18:26,860 --> 00:18:29,890
When you're testing it -- like when you ask,

379
00:18:29,890 --> 00:18:31,795
is this sentence positive or negative,

380
00:18:31,795 --> 00:18:35,290
does it sometimes, like, [inaudible]?

381
00:18:35,290 --> 00:18:37,765
Great question. So, the question is when I ask,

382
00:18:37,765 --> 00:18:40,510
is this sentence positive or negative will it sometimes eventually

383
00:18:40,510 --> 00:18:43,915
accidentally switch to a different one of the task and, uh,

384
00:18:43,915 --> 00:18:47,110
we actually have a slide on that and the answer is it's surprisingly good at

385
00:18:47,110 --> 00:18:52,780
knowing how to go about doing the task and where to get the answer where it's from.

386
00:18:52,780 --> 00:18:56,860
Um, and yeah, they'll make more sense in a couple of slides once we go over the model.

387
00:18:56,860 --> 00:18:58,555
Any other questions about,

388
00:18:58,555 --> 00:19:00,820
uh, the question answering formalism?

389
00:19:00,820 --> 00:19:04,930
Are you able to formulate text generation in the question answer format as well?

390
00:19:04,930 --> 00:19:06,685
Like, tell me a story.

391
00:19:06,685 --> 00:19:10,195
Good question. So can we do text generation, uh,

392
00:19:10,195 --> 00:19:11,800
like tell me a story, uh,

393
00:19:11,800 --> 00:19:14,590
from a random kind of -- or in this kind of formalism.

394
00:19:14,590 --> 00:19:19,450
Uh, we don't have that as a task because largely it's really hard to evaluate.

395
00:19:19,450 --> 00:19:22,120
It'll tell you some random stuff and then is that a good story or not,

396
00:19:22,120 --> 00:19:24,325
is it grammatical, you have to come up with a lot of,

397
00:19:24,325 --> 00:19:25,750
uh, sort of, uh,

398
00:19:25,750 --> 00:19:28,420
evaluation metrics which we actually are doing for

399
00:19:28,420 --> 00:19:31,330
some of the dialogue systems and in case of dialogue,

400
00:19:31,330 --> 00:19:33,280
why does -- why are they equivalent because

401
00:19:33,280 --> 00:19:36,160
the context can just keep on growing and every time, uh,

402
00:19:36,160 --> 00:19:38,395
the user said something, uh,

403
00:19:38,395 --> 00:19:43,525
you basically try to then predict the next answer in that dialogue.

404
00:19:43,525 --> 00:19:48,700
And so I think you could very easily [NOISE] use this to generate texts.

405
00:19:48,700 --> 00:19:51,220
Uh, you basically just ask -- tell it like what is, you know,

406
00:19:51,220 --> 00:19:54,490
what's a good ending of the story and you maybe start the context with like

407
00:19:54,490 --> 00:19:58,420
two or three words and then you ask the model to generate more and more words,

408
00:19:58,420 --> 00:20:01,975
uh, in the form of this network I'll describe in a second. Yeah?

409
00:20:01,975 --> 00:20:04,720
I was wondering like, uh, when you're training

410
00:20:04,720 --> 00:20:07,795
it and you're trying to research like a new task.

411
00:20:07,795 --> 00:20:11,470
Uh, does it like learn with less data?

412
00:20:11,470 --> 00:20:14,320
That is an amazingly thoughtful question

413
00:20:14,320 --> 00:20:16,930
and it's- it's so important we'll have a bunch of slides on it.

414
00:20:16,930 --> 00:20:20,980
So maybe we'll- we'll go -- we'll continue and we'll get to that question, uh,

415
00:20:20,980 --> 00:20:25,075
in a lot of detail because it's sort of why we're doing it and, the short answer is yes.

416
00:20:25,075 --> 00:20:27,855
But we'll get to more details. All right.

417
00:20:27,855 --> 00:20:30,210
So these are basically the 10 tasks.

418
00:20:30,210 --> 00:20:33,970
Uh, and again this is the actual format for it.

419
00:20:33,970 --> 00:20:35,890
So if you have a problem,

420
00:20:35,890 --> 00:20:37,810
and you can cast it in this format, uh,

421
00:20:37,810 --> 00:20:40,630
you can just take, uh, the open source code and run it and,

422
00:20:40,630 --> 00:20:42,025
uh, it'll- it'll work.

423
00:20:42,025 --> 00:20:45,010
And so when you kind of analyze and think about what we've done here.

424
00:20:45,010 --> 00:20:47,680
In some ways, we've taken the tasks that

425
00:20:47,680 --> 00:20:50,950
usually is kind of in your head but it's not given to the model.

426
00:20:50,950 --> 00:20:54,730
The model is just given an input x and an output y in almost all of

427
00:20:54,730 --> 00:21:00,760
the supervised systems and instead we're actually including the task in the inputs,

428
00:21:00,760 --> 00:21:05,950
uh, in the set of inputs to the model. So you can kind of call this meta-supervised learning.

429
00:21:05,950 --> 00:21:08,260
So again the question, uh,

430
00:21:08,260 --> 00:21:11,140
is kind of our task definition for each of these different tasks.

431
00:21:11,140 --> 00:21:13,570
The model has to figure out itself when to ask the question

432
00:21:13,570 --> 00:21:16,180
that way it can also figure out itself when to

433
00:21:16,180 --> 00:21:21,565
transfer knowledge from these other tasks and y is again just the answer.

434
00:21:21,565 --> 00:21:25,330
So, in some ways it's meta-supervised learning and I'm quite excited

435
00:21:25,330 --> 00:21:29,560
because once you allow the task to be given to the model as input,

436
00:21:29,560 --> 00:21:32,170
it can kind of decide itself how to go about

437
00:21:32,170 --> 00:21:35,020
solving that particular task and now you can learn,

438
00:21:35,020 --> 00:21:36,835
uh, a lot more powerful models.

439
00:21:36,835 --> 00:21:39,310
So once we had the dataset,

440
00:21:39,310 --> 00:21:42,265
we thought "Okay, how do we now solve this problem?"

441
00:21:42,265 --> 00:21:43,960
The simplest way is you could just say, "Well,

442
00:21:43,960 --> 00:21:45,010
I have a big if statement,

443
00:21:45,010 --> 00:21:47,260
I have a classifier in the beginning and then I classify.

444
00:21:47,260 --> 00:21:49,225
If this is a machine translation task,

445
00:21:49,225 --> 00:21:51,015
then run my machine translation model."

446
00:21:51,015 --> 00:21:54,300
And in general, in Python that would still be just like one big python,

447
00:21:54,300 --> 00:21:56,430
uh, model with a bunch of if statements, right?

448
00:21:56,430 --> 00:21:58,770
And that's not the goal because then we wouldn't get to any of

449
00:21:58,770 --> 00:22:02,195
the transfer learning and zero-shot capabilities that we're hoping for.

450
00:22:02,195 --> 00:22:07,630
So [NOISE] we want to have the model wanted

451
00:22:07,630 --> 00:22:10,105
to have the capability to internally adjust

452
00:22:10,105 --> 00:22:13,880
to these different tasks and make these decisions itself.

453
00:22:15,360 --> 00:22:18,490
And basically, all of those considerations and all

454
00:22:18,490 --> 00:22:20,620
of those thoughts led us, uh, to this model.

455
00:22:20,620 --> 00:22:22,120
So before I go, uh,

456
00:22:22,120 --> 00:22:23,455
into a little bit more detail.

457
00:22:23,455 --> 00:22:25,825
I'll just like sort of give you the high-level overview.

458
00:22:25,825 --> 00:22:27,925
Again, you start with the context.

459
00:22:27,925 --> 00:22:30,715
Um, you start- you ask a question about, uh,

460
00:22:30,715 --> 00:22:33,700
that context document, and then we're going to generate,

461
00:22:33,700 --> 00:22:38,560
uh, the answer one word at a time by either pointing to the context,

462
00:22:38,560 --> 00:22:40,045
and you've had pointers already, right?

463
00:22:40,045 --> 00:22:44,035
Pointer networks, all that? Great. Um, pointing to a question word,

464
00:22:44,035 --> 00:22:48,190
or choosing a word from an external vocabulary with your standard softmax classifier.

465
00:22:48,190 --> 00:22:52,630
Uh, and we'll have a pointer switch mechanism that will kind

466
00:22:52,630 --> 00:22:57,415
of choose how much to weight [NOISE] each of these three generation mechanisms.

467
00:22:57,415 --> 00:23:00,760
So, uh, let's dig into a little bit into this model.

468
00:23:00,760 --> 00:23:04,600
Fortunately, uh, in some ways it's kind of just taking the best, uh,

469
00:23:04,600 --> 00:23:09,160
of the current sort of the state of the art techniques and putting them together in a way,

470
00:23:09,160 --> 00:23:11,560
uh, that- that generalize well enough.

471
00:23:11,560 --> 00:23:14,140
Uh, you can look at all the code on decanlp.com,

472
00:23:14,140 --> 00:23:16,870
[NOISE] it has like thousands of, uh,

473
00:23:16,870 --> 00:23:20,400
stars and, uh, and forks and stuff combined, uh,

474
00:23:20,400 --> 00:23:21,795
and you can, you know,

475
00:23:21,795 --> 00:23:24,180
basically run everything, uh,

476
00:23:24,180 --> 00:23:29,755
in this, uh, on these experiments with just one command.

477
00:23:29,755 --> 00:23:33,610
It'll double, you get all the datasets and everything and- and run everything,

478
00:23:33,610 --> 00:23:36,340
you can really explore what it looks like but let's- let's

479
00:23:36,340 --> 00:23:39,370
dive a little bit into the details of what this model told us.

480
00:23:39,370 --> 00:23:41,065
In some ways again, it just kind of takes

481
00:23:41,065 --> 00:23:43,870
all the best ingredients from deep learning [NOISE] NLP,

482
00:23:43,870 --> 00:23:48,490
most of which you've already learned about and puts them together in a reasonable way.

483
00:23:48,490 --> 00:23:50,470
So we start with fixed GloVe embeddings.

484
00:23:50,470 --> 00:23:52,630
Eventually, we'll- we updated, uh,

485
00:23:52,630 --> 00:23:54,730
the embeddings to CoVe embeddings, uh,

486
00:23:54,730 --> 00:23:57,715
and probably it'll work even better if you update them to BERT embeddings.

487
00:23:57,715 --> 00:24:00,820
Uh, but at some point we kind of have to move on and do other things.

488
00:24:00,820 --> 00:24:03,460
Uh, but basically, you have a fixed set of word vectors,

489
00:24:03,460 --> 00:24:05,860
and that is kind of important because in some of these,

490
00:24:05,860 --> 00:24:08,545
uh, data sets, they're much smaller than others.

491
00:24:08,545 --> 00:24:10,360
Uh, and as you know from SQuAD,

492
00:24:10,360 --> 00:24:12,580
if you actually backpropagate into the word vectors,

493
00:24:12,580 --> 00:24:14,680
you just do really, really well on your trained dataset,

494
00:24:14,680 --> 00:24:18,310
but then you won't generalize because of most of the [NOISE] text,

495
00:24:18,310 --> 00:24:21,430
uh, test documents will include words you've never seen before.

496
00:24:21,430 --> 00:24:24,640
So if you change all the word vectors during training, uh,

497
00:24:24,640 --> 00:24:28,300
it won't- it won't work very well at test time and won't generalize the unseen words.

498
00:24:28,300 --> 00:24:30,355
So, uh, fixed GloVe embeddings,

499
00:24:30,355 --> 00:24:31,990
if you don't have word vectors, uh,

500
00:24:31,990 --> 00:24:35,140
for unseen words, we also have character n-gram embeddings.

501
00:24:35,140 --> 00:24:37,870
Then we pipe them through a simple linear layer,

502
00:24:37,870 --> 00:24:39,250
and then we have a shared, uh,

503
00:24:39,250 --> 00:24:42,535
bidirectional LSTM with skip connections.

504
00:24:42,535 --> 00:24:46,255
And so, uh, it's a deep- deep one so you skip to higher layers,

505
00:24:46,255 --> 00:24:49,090
and it's shared between the context and the questions.

506
00:24:49,090 --> 00:24:51,850
So they have basically the same [NOISE] set of weights.

507
00:24:51,850 --> 00:24:56,440
[NOISE] Then, uh, we have a co-attention layer.

508
00:24:56,440 --> 00:24:58,840
Uh, where we basically just have outer products, uh,

509
00:24:58,840 --> 00:25:03,400
between all the hidden states of those two sequences,

510
00:25:03,400 --> 00:25:06,070
and again, have skip connections, uh,

511
00:25:06,070 --> 00:25:08,050
to circumvent, uh, those as well.

512
00:25:08,050 --> 00:25:11,200
So now you have kind of context or question dependent, uh,

513
00:25:11,200 --> 00:25:15,460
contextual representations [NOISE] or- or representations of that context.

514
00:25:15,460 --> 00:25:18,970
[NOISE] Uh, then we feed those into our transformer layers,

515
00:25:18,970 --> 00:25:23,575
uh, and we actually tried to use transformers for all the things,

516
00:25:23,575 --> 00:25:25,765
with having no LSTMs or any of that.

517
00:25:25,765 --> 00:25:28,735
Uh, unfortunately, transformer layers were still, uh,

518
00:25:28,735 --> 00:25:32,590
very, uh, finicky and very hard to optimize,

519
00:25:32,590 --> 00:25:35,020
and there's a lot of trickery with- of the learning rates,

520
00:25:35,020 --> 00:25:38,515
and we could just not get them to perform really well,

521
00:25:38,515 --> 00:25:41,755
uh, on- on these 10 different tasks.

522
00:25:41,755 --> 00:25:45,760
Uh, [NOISE] sometimes you had one transformer layer, one transformer network,

523
00:25:45,760 --> 00:25:46,930
that worked really well in one task,

524
00:25:46,930 --> 00:25:49,330
but the only other transformer network that worked well

525
00:25:49,330 --> 00:25:51,895
on the second task had like half the layers.

526
00:25:51,895 --> 00:25:55,150
And once you tried to have one network with the same number of layers,

527
00:25:55,150 --> 00:25:57,715
it just wouldn't work on either of the two tasks anymore.

528
00:25:57,715 --> 00:26:00,640
Uh, and so- so yeah, unfortunately as nice as they

529
00:26:00,640 --> 00:26:03,579
are because they're nicely paralyzable in GPUs,

530
00:26:03,579 --> 00:26:05,110
uh, they weren't yet robust enough,

531
00:26:05,110 --> 00:26:06,820
uh, to- to be used for this.

532
00:26:06,820 --> 00:26:09,280
[NOISE] So we have to have these LSTMs,

533
00:26:09,280 --> 00:26:11,200
uh, before and after the transformer layers.

534
00:26:11,200 --> 00:26:15,295
[NOISE] And then we essentially just have a standard sort of autoregressive, uh,

535
00:26:15,295 --> 00:26:17,770
decoder where given the last state,

536
00:26:17,770 --> 00:26:19,720
uh, we generate the next word.

537
00:26:19,720 --> 00:26:22,090
And then we have these three pointer mechanisms.

538
00:26:22,090 --> 00:26:24,460
Uh, they're very similar to the pointer ne- mechanisms you already know.

539
00:26:24,460 --> 00:26:28,405
But now on top of these very contextualized representations, uh,

540
00:26:28,405 --> 00:26:30,580
at the end of this encoder, uh,

541
00:26:30,580 --> 00:26:33,640
and it basically learns to either point to question words,

542
00:26:33,640 --> 00:26:35,770
context words based on the hidden states,

543
00:26:35,770 --> 00:26:38,125
or have also a standard softmax,

544
00:26:38,125 --> 00:26:41,395
and then we just basically have a weighted sum,

545
00:26:41,395 --> 00:26:45,490
convex sum, of these three different distributions of output words.

546
00:26:45,490 --> 00:26:48,115
[NOISE] All right.

547
00:26:48,115 --> 00:26:52,690
So I think these are mostly standard components that you've already saw,

548
00:26:52,690 --> 00:26:54,610
uh, for you- already seen all their details.

549
00:26:54,610 --> 00:26:55,944
But if you have any questions,

550
00:26:55,944 --> 00:26:58,690
um, about how we put it together? Yeah?

551
00:26:58,690 --> 00:27:02,920
[NOISE] So the output- the output has to be a word.

552
00:27:02,920 --> 00:27:06,610
That's right. The output has to be a word and it's always either a word from the context,

553
00:27:06,610 --> 00:27:08,470
a word from the question or a word from the softmax.

554
00:27:08,470 --> 00:27:11,050
[NOISE]

555
00:27:11,050 --> 00:27:15,610
That's- the data preprocessing I guess it's different with each task.

556
00:27:15,610 --> 00:27:18,220
So the data preprocessing is different for each task,

557
00:27:18,220 --> 00:27:20,950
but we basically had to normalize everything to have

558
00:27:20,950 --> 00:27:23,710
the same tokenization and- and all of that. [NOISE]

559
00:27:23,710 --> 00:27:29,770
Uh, so do the double arrows in the encoding just represent there's a bidirectional?

560
00:27:29,770 --> 00:27:30,125
Yeah.

561
00:27:30,125 --> 00:27:30,775
Okay.

562
00:27:30,775 --> 00:27:32,395
Yeah. But the double arrows,

563
00:27:32,395 --> 00:27:34,000
uh, here are just bidirectional.

564
00:27:34,000 --> 00:27:38,080
So left to right and right to left for the LSTMs. All right.

565
00:27:38,080 --> 00:27:41,050
So what datasets, uh, are we using?

566
00:27:41,050 --> 00:27:44,125
Uh, I mentioned that that was a big headache in the beginning.

567
00:27:44,125 --> 00:27:46,540
Uh, we definitely wanted to include a lot of the sequence to

568
00:27:46,540 --> 00:27:49,720
sequence tasks that we felt like are very,

569
00:27:49,720 --> 00:27:54,055
um, sort of high level and I- immediately useful, uh,

570
00:27:54,055 --> 00:27:57,955
and in some ways what this also shows you is that

571
00:27:57,955 --> 00:28:03,310
nowadays you don't have to work as much on some of the intermediate representations,

572
00:28:03,310 --> 00:28:05,275
uh, in NLP anymore.

573
00:28:05,275 --> 00:28:09,490
Uh, you can just directly go for the end tasks that that real users might care about,

574
00:28:09,490 --> 00:28:12,340
and then have these end-to-end trainable systems,

575
00:28:12,340 --> 00:28:14,695
uh, that really do quite well.

576
00:28:14,695 --> 00:28:17,290
And, uh, I've myself worked a lot on parsing.

577
00:28:17,290 --> 00:28:18,415
And so I don't wanna, you know,

578
00:28:18,415 --> 00:28:19,540
say we- we don't need it.

579
00:28:19,540 --> 00:28:21,580
There's certainly still tasks that you do need it for,

580
00:28:21,580 --> 00:28:26,095
but it's kind of surprising that you can just go directly to translation or summarization

581
00:28:26,095 --> 00:28:28,870
without having intermediate representations that

582
00:28:28,870 --> 00:28:32,035
were sort of very specifically hand-designed.

583
00:28:32,035 --> 00:28:36,310
Um, so we had those three really interesting, uh, and hard tasks.

584
00:28:36,310 --> 00:28:38,380
Question answering, machine translation, summarization.

585
00:28:38,380 --> 00:28:41,260
They actually also have the three biggest datasets,

586
00:28:41,260 --> 00:28:42,820
uh, of all of these.

587
00:28:42,820 --> 00:28:46,960
Uh, then we had NLI, and basically, um,

588
00:28:46,960 --> 00:28:52,195
all of these, uh, 10 datasets [NOISE] were, uh,

589
00:28:52,195 --> 00:28:56,875
publicly available, uh, and in several cases especially for translation,

590
00:28:56,875 --> 00:29:01,030
you could actually find much larger, uh, translation datasets,

591
00:29:01,030 --> 00:29:03,790
but we also tried to keep it, uh,

592
00:29:03,790 --> 00:29:08,530
to a- to a size where normal people that don't work in gigantic companies with huge, uh,

593
00:29:08,530 --> 00:29:13,540
GPU infrastructures could still run experiments, [NOISE] uh, themselves.

594
00:29:13,540 --> 00:29:16,630
So universities and folks, uh, can still run it on.

595
00:29:16,630 --> 00:29:18,985
Basically if you have just a single GPU,

596
00:29:18,985 --> 00:29:21,385
it'll probably take about a week or so, uh,

597
00:29:21,385 --> 00:29:23,680
to run an experiment.

598
00:29:23,680 --> 00:29:26,635
If you have multiple GPUs on one large AWS machine,

599
00:29:26,635 --> 00:29:29,560
you can kind of run an experiment in a day or two.

600
00:29:29,560 --> 00:29:31,750
And so especially for translation, right,

601
00:29:31,750 --> 00:29:35,605
you could get a lot more data, uh, than IWSLT.

602
00:29:35,605 --> 00:29:38,470
And each of these, uh,

603
00:29:38,470 --> 00:29:42,100
communities and datasets and- and tasks has their own metric.

604
00:29:42,100 --> 00:29:44,050
We actually tried to, in the beginning,

605
00:29:44,050 --> 00:29:46,330
we had a lot of discussion about how we should

606
00:29:46,330 --> 00:29:49,870
define the measure of success for this project.

607
00:29:49,870 --> 00:29:51,565
Uh, it doesn't make sense, uh,

608
00:29:51,565 --> 00:29:55,300
to have a normalized F1 score for basically all the different tasks,

609
00:29:55,300 --> 00:29:57,310
but then we basically realized that

610
00:29:57,310 --> 00:30:00,250
these different communities have different metrics for a reason.

611
00:30:00,250 --> 00:30:05,005
Uh, unfortunately at least all of these metrics are from 0-100 in theory.

612
00:30:05,005 --> 00:30:07,405
Of course, in practice, you rarely ever see, uh,

613
00:30:07,405 --> 00:30:10,270
a translation system of a 100, uh,

614
00:30:10,270 --> 00:30:12,280
or even high 90s of a BLEU score,

615
00:30:12,280 --> 00:30:14,935
uh, or these really, really high ROUGE scores.

616
00:30:14,935 --> 00:30:18,550
But, you know, in theory they go from 0-100, and so, uh,

617
00:30:18,550 --> 00:30:24,039
we kept basically intact the different evaluation metrics for each of these communities,

618
00:30:24,039 --> 00:30:26,440
and we just said we're going to sum them up.

619
00:30:26,440 --> 00:30:29,380
And, uh, when we first talked about this,

620
00:30:29,380 --> 00:30:31,150
we have- had a lot of discussion,

621
00:30:31,150 --> 00:30:32,890
uh, with- with others also like, oh,

622
00:30:32,890 --> 00:30:35,530
but translation is so much more important because it's much

623
00:30:35,530 --> 00:30:38,245
bigger and it's a much more useful task than you still,

624
00:30:38,245 --> 00:30:40,630
you know, silly like pronoun resolution Winograd Schemas

625
00:30:40,630 --> 00:30:43,150
which only have a couple hundred training samples.

626
00:30:43,150 --> 00:30:45,730
And so you should have weighted translation more and

627
00:30:45,730 --> 00:30:48,310
then literally five questions later somebody's like,

628
00:30:48,310 --> 00:30:50,140
"Why didn't you weight pronoun resolution more?

629
00:30:50,140 --> 00:30:54,370
That is a really hard task that captures sort of common sense reasoning and, you know,

630
00:30:54,370 --> 00:30:56,590
the complexity of language and semantics,

631
00:30:56,590 --> 00:31:00,340
and unlike all this, like, statistical pattern matching [NOISE] that you do in translation."

632
00:31:00,340 --> 00:31:03,190
And I was like, I used to talk to that guy [LAUGHTER] and like,

633
00:31:03,190 --> 00:31:04,510
uh, hopefully in the end,

634
00:31:04,510 --> 00:31:08,050
we'll just all agree that like it's reasonable to sum them up, uh,

635
00:31:08,050 --> 00:31:13,645
and of course, you also have to tackle when you run experiments in this.

636
00:31:13,645 --> 00:31:17,845
Uh, a lot of the complexity that you have in machine learning and,

637
00:31:17,845 --> 00:31:21,625
you know, stuff that very few people talk about like having very skewed distributions.

638
00:31:21,625 --> 00:31:24,610
So you have translation which has, uh,

639
00:31:24,610 --> 00:31:26,620
millions or hundreds of thousands of examples,

640
00:31:26,620 --> 00:31:27,730
and you have Winograd Schemas,

641
00:31:27,730 --> 00:31:29,920
uh, that only have a couple hundred.

642
00:31:29,920 --> 00:31:34,750
How do you train that such that you don't just completely ignore the smaller dataset.

643
00:31:34,750 --> 00:31:38,350
Uh, so we'll get to some of the optimization trickery,

644
00:31:38,350 --> 00:31:42,010
uh, that Nitish spent several months on in a bit.

645
00:31:42,010 --> 00:31:45,310
But I first wanna sort of give you the first set of experiments.

646
00:31:45,310 --> 00:31:46,960
So as you can see from all the numbers,

647
00:31:46,960 --> 00:31:48,565
there's a lot of experiments, uh,

648
00:31:48,565 --> 00:31:50,695
that we ran to even get to this,

649
00:31:50,695 --> 00:31:52,960
and so we'll walk through this, uh, quite carefully.

650
00:31:52,960 --> 00:31:56,110
I think hopefully you'll get some ideas also for- for ablations,

651
00:31:56,110 --> 00:31:59,800
or experiments that you might wanna run in your, um,

652
00:31:59,800 --> 00:32:01,210
in your experiments and in your,

653
00:32:01,210 --> 00:32:03,670
uh, problem- final- final projects.

654
00:32:03,670 --> 00:32:05,290
So what are we looking at here?

655
00:32:05,290 --> 00:32:07,405
So basically, uh, on the left side,

656
00:32:07,405 --> 00:32:08,770
we have single task performance.

657
00:32:08,770 --> 00:32:13,475
So here, each number comes from its different model that was trained,

658
00:32:13,475 --> 00:32:16,330
um, separately on just one task.

659
00:32:16,330 --> 00:32:22,540
Uh, each row- each column here is the same architecture, uh,

660
00:32:22,540 --> 00:32:23,935
and [NOISE] on the right side here,

661
00:32:23,935 --> 00:32:25,435
we basically have, uh,

662
00:32:25,435 --> 00:32:31,165
for each column is basically the same architecture and the same exact model.

663
00:32:31,165 --> 00:32:34,675
So here, we have four different models and here, uh,

664
00:32:34,675 --> 00:32:37,165
we have 40 different models,

665
00:32:37,165 --> 00:32:40,105
and each column again is the same architecture.

666
00:32:40,105 --> 00:32:41,725
And so the simplest, uh,

667
00:32:41,725 --> 00:32:44,620
first column here is just a standard sequence to sequence

668
00:32:44,620 --> 00:32:48,280
model with very few bells and whistles and some pointers,

669
00:32:48,280 --> 00:32:49,960
but nothing sort of major.

670
00:32:49,960 --> 00:32:51,270
It's pretty deep, you know,

671
00:32:51,270 --> 00:32:53,550
stack bidirectional LSTM skip connections,

672
00:32:53,550 --> 00:32:57,780
all the standard good well-tuned stuff for sequence to sequence models.

673
00:32:57,780 --> 00:33:00,945
And, uh, then we added self-attention.

674
00:33:00,945 --> 00:33:03,405
Um, this- this sort of, uh,

675
00:33:03,405 --> 00:33:06,310
basically, uh, transformer layers.

676
00:33:06,310 --> 00:33:08,110
[NOISE] Then we have this co-attention layer of

677
00:33:08,110 --> 00:33:10,225
the outer products that we mentioned in the beginning,

678
00:33:10,225 --> 00:33:12,715
and then we also added the question pointer.

679
00:33:12,715 --> 00:33:17,060
So having the ability to point to a word in a question.

680
00:33:18,330 --> 00:33:21,670
All right. Any questions about this table?

681
00:33:21,670 --> 00:33:23,320
We'll dig into some of the details.

682
00:33:23,320 --> 00:33:25,090
Uh, okay. Well, we'll dig into

683
00:33:25,090 --> 00:33:27,760
the details first and then maybe you can think of some questions.

684
00:33:27,760 --> 00:33:29,830
So let's analyze, uh,

685
00:33:29,830 --> 00:33:32,740
what's going on in this table because there are a lot of numbers, uh,

686
00:33:32,740 --> 00:33:36,510
and you really want to carefully analyze and sort of distinguish.

687
00:33:36,510 --> 00:33:37,890
I think my first, uh,

688
00:33:37,890 --> 00:33:40,590
observation was, wow, we can have a single architecture.

689
00:33:40,590 --> 00:33:43,170
Like, even, even this is not quite what we want, right?

690
00:33:43,170 --> 00:33:44,535
We want a single model.

691
00:33:44,535 --> 00:33:46,140
But even this kind of showed us, wow,

692
00:33:46,140 --> 00:33:51,429
you can have a single architecture that actually does really well and somewhat randomly,

693
00:33:51,429 --> 00:33:53,920
in some cases, it actually had gotten state-of-the-art results.

694
00:33:53,920 --> 00:33:56,020
So Wiki SQL, for instance,

695
00:33:56,020 --> 00:33:59,200
this architecture had the best model

696
00:33:59,200 --> 00:34:02,245
to translate natural language English questions into SQL queries,

697
00:34:02,245 --> 00:34:05,530
which was a surprise to us because it is the ninth dataset.

698
00:34:05,530 --> 00:34:08,950
It was really not like a priority for us and when we designed

699
00:34:08,950 --> 00:34:12,970
the model and thought about how to generate words and pointer mechanisms and so on.

700
00:34:12,970 --> 00:34:16,390
We just kind of had the standard context of SQL words

701
00:34:16,390 --> 00:34:19,990
and we asked the question what's the translation to SQL, and then, uh,

702
00:34:19,990 --> 00:34:24,790
somewhat surprisingly to us this particular architecture had the state-of-the-art, uh,

703
00:34:24,790 --> 00:34:27,820
on SQL generation and bunch of folks in that community kind

704
00:34:27,820 --> 00:34:30,865
of picked it up more quickly because it had state-of-the-art.

705
00:34:30,865 --> 00:34:32,590
And that's- uh, unfortunately,

706
00:34:32,590 --> 00:34:34,915
it doesn't have that many other state-of-the-art numbers, uh,

707
00:34:34,915 --> 00:34:36,400
which is why it's harder, uh,

708
00:34:36,400 --> 00:34:37,750
it's actually a much harder task.

709
00:34:37,750 --> 00:34:40,195
And what you also observe is that,

710
00:34:40,195 --> 00:34:42,325
uh, in several of the cases, uh,

711
00:34:42,325 --> 00:34:44,080
using the multitask model,

712
00:34:44,080 --> 00:34:46,645
so having a single model for all the 10 tasks,

713
00:34:46,645 --> 00:34:48,880
uh, actually hurts performance at first.

714
00:34:48,880 --> 00:34:52,120
And this is also something you rarely read in papers because papers

715
00:34:52,120 --> 00:34:55,210
have a strong selection bias to only publish positive results.

716
00:34:55,210 --> 00:35:00,310
Uh, and when you look at most transfer learning and multitask learning papers,

717
00:35:00,310 --> 00:35:04,660
they're sort of an outside of the actual model consideration of like,

718
00:35:04,660 --> 00:35:09,100
well, let's only combine tasks that we know will work well with one another.

719
00:35:09,100 --> 00:35:11,050
And if they don't work and hurt performance,

720
00:35:11,050 --> 00:35:13,285
then we'd just exclude them from our experiments.

721
00:35:13,285 --> 00:35:16,615
And so you don't see many negative task results, uh,

722
00:35:16,615 --> 00:35:20,215
in the literature and there are a few papers here and there that, uh,

723
00:35:20,215 --> 00:35:24,910
study basically the opposite side of transfer learning and that is,

724
00:35:24,910 --> 00:35:28,315
uh, catastrophic interference and catastrophic forgetting.

725
00:35:28,315 --> 00:35:32,110
So interference is when you train two different tasks in the same model,

726
00:35:32,110 --> 00:35:35,155
and to interfere with one another next, you hurt each other's performance.

727
00:35:35,155 --> 00:35:37,960
And catastrophic forgetting is if you train continually

728
00:35:37,960 --> 00:35:41,305
your first train in one task then you train on a second task,

729
00:35:41,305 --> 00:35:42,895
people used to think,

730
00:35:42,895 --> 00:35:44,080
"Oh, well, you know,

731
00:35:44,080 --> 00:35:45,790
basically the first task will be completely

732
00:35:45,790 --> 00:35:48,970
forgotten," and you just work well on the second task.

733
00:35:48,970 --> 00:35:52,750
If you train neural networks sort of in a sequential way one task and then

734
00:35:52,750 --> 00:35:56,850
another and somewhat surprisingly, uh,

735
00:35:56,850 --> 00:35:59,160
we- we found that things aren't actually

736
00:35:59,160 --> 00:36:01,935
catastrophically being forgotten in these models,

737
00:36:01,935 --> 00:36:04,410
turns out that if you train them sequentially and

738
00:36:04,410 --> 00:36:07,065
you add a little bit of the original to the first task,

739
00:36:07,065 --> 00:36:08,760
it comes back very, very quickly.

740
00:36:08,760 --> 00:36:10,655
So while the performance is really bad,

741
00:36:10,655 --> 00:36:12,910
you can get to the really good performance very,

742
00:36:12,910 --> 00:36:14,470
very quickly in very few iterations.

743
00:36:14,470 --> 00:36:18,115
So but it's one of the many interesting sort of tidbits that we found,

744
00:36:18,115 --> 00:36:20,905
uh, in the course of this that we haven't even published yet. All right.

745
00:36:20,905 --> 00:36:24,055
So, uh, focusing on, uh,

746
00:36:24,055 --> 00:36:26,560
the transformer layers here we basically find transformers

747
00:36:26,560 --> 00:36:29,275
do help the original sequence to sequence model a lot.

748
00:36:29,275 --> 00:36:33,415
So if you tune them carefully and you combine them with, uh,

749
00:36:33,415 --> 00:36:36,235
some bidirectional LSTMs and so on, uh,

750
00:36:36,235 --> 00:36:38,410
they were very helpful and improved, uh,

751
00:36:38,410 --> 00:36:41,800
across a bunch of different datasets, in some cases quite significantly.

752
00:36:41,800 --> 00:36:46,390
Another observation is question-answering and semantic role labeling,

753
00:36:46,390 --> 00:36:49,660
uh, actually can predict each other's performance quite well.

754
00:36:49,660 --> 00:36:51,670
If one works well, the other works well,

755
00:36:51,670 --> 00:36:53,140
uh, and- and vice-versa.

756
00:36:53,140 --> 00:36:54,400
If they don't work well,

757
00:36:54,400 --> 00:36:56,590
uh, both of them don't work very well.

758
00:36:56,590 --> 00:37:00,849
Um, and it's also interesting because both of those tasks have different questions for,

759
00:37:00,849 --> 00:37:04,075
uh, every training example.

760
00:37:04,075 --> 00:37:07,780
Pointing. Uh, so the question pointing,

761
00:37:07,780 --> 00:37:09,520
uh, is super important.

762
00:37:09,520 --> 00:37:11,695
Uh, we actually have in some cases, uh,

763
00:37:11,695 --> 00:37:13,915
twice the performance even for,

764
00:37:13,915 --> 00:37:15,565
and this is kind of surprising to us,

765
00:37:15,565 --> 00:37:18,700
a simple classification task where you could just have a standard Softmax.

766
00:37:18,700 --> 00:37:22,645
But instead of saying you have a Softmax of entailment, contradiction, and so on,

767
00:37:22,645 --> 00:37:25,015
you just basically, uh,

768
00:37:25,015 --> 00:37:28,015
point to the word entailment in the question.

769
00:37:28,015 --> 00:37:32,050
And that was also the case for Winograd Schemas that also benefited a lot,

770
00:37:32,050 --> 00:37:34,000
uh, from this pointer mechanism.

771
00:37:34,000 --> 00:37:36,190
[NOISE]

772
00:37:36,190 --> 00:37:36,880
Can you explain that?

773
00:37:36,880 --> 00:37:39,490
Sure. Um, can we explain it? Why-

774
00:37:39,490 --> 00:37:41,470
[inaudible]

775
00:37:41,470 --> 00:37:42,760
Why does it help so much?

776
00:37:42,760 --> 00:37:44,980
Um, in some ways,

777
00:37:44,980 --> 00:37:47,860
I think partly is the whole architecture

778
00:37:47,860 --> 00:37:51,160
has been gotten- has gotten better and better at pointing.

779
00:37:51,160 --> 00:37:53,320
And part of the reason we actually do very,

780
00:37:53,320 --> 00:37:54,730
very poorly in translation,

781
00:37:54,730 --> 00:37:59,020
which is the only task that hurt in the- our first experiments a lot, uh,

782
00:37:59,020 --> 00:38:02,500
in the multitask setting is that that is the only task that now has to generate,

783
00:38:02,500 --> 00:38:05,440
uh, results from a completely separate Softmax,

784
00:38:05,440 --> 00:38:07,660
whereas the rest of the architecture got really,

785
00:38:07,660 --> 00:38:12,535
really good at pointing to things to answer questions, any kind of question.

786
00:38:12,535 --> 00:38:15,550
Uh, and so but in some ways,

787
00:38:15,550 --> 00:38:17,560
I think that is one explanation,

788
00:38:17,560 --> 00:38:19,720
but I- I don't think it's- it's all of it.

789
00:38:19,720 --> 00:38:29,005
I think we still need to figure out more why this happens. All right.

790
00:38:29,005 --> 00:38:32,200
Now, multitask learning is the most

791
00:38:32,200 --> 00:38:35,470
helpful when it comes to zero-shot and I'm actually very excited about that.

792
00:38:35,470 --> 00:38:39,835
So this is a zero-shot relation extraction where you have different kinds of, uh,

793
00:38:39,835 --> 00:38:42,430
relations that you might wanna extract and you might have never

794
00:38:42,430 --> 00:38:45,550
seen like the student-teacher relationship that you're trying

795
00:38:45,550 --> 00:38:47,860
to identify in a certain context or

796
00:38:47,860 --> 00:38:51,745
a product company relationship or something like that.

797
00:38:51,745 --> 00:38:55,480
And so, uh, that one actually, uh,

798
00:38:55,480 --> 00:38:58,180
benefited a lot and almost got twice, uh,

799
00:38:58,180 --> 00:39:00,280
as high in terms of the accuracy, uh,

800
00:39:00,280 --> 00:39:02,380
when you learned it with everything else.

801
00:39:02,380 --> 00:39:04,360
So these were questions, it's never seen before,

802
00:39:04,360 --> 00:39:06,265
relations that it's never seen before,

803
00:39:06,265 --> 00:39:08,725
and it got twice as good, uh,

804
00:39:08,725 --> 00:39:13,210
and benefited a lot especially from having seen other kinds of questions.

805
00:39:13,210 --> 00:39:16,870
And in some ways, we have to give a lot of credit to SQuAD too,

806
00:39:16,870 --> 00:39:18,895
uh, because SQuAD as a dataset,

807
00:39:18,895 --> 00:39:24,760
uh, kind of pushed people into thinking about pointers as a mechanism to generate answers.

808
00:39:24,760 --> 00:39:28,750
And pointers, we kind of see them like as a given and they don't get that much credit,

809
00:39:28,750 --> 00:39:33,535
but they allow you to predict answers that you've never seen before at training time.

810
00:39:33,535 --> 00:39:36,040
To generate words, you've never seen before at training time,

811
00:39:36,040 --> 00:39:39,850
which is actually quite- quite amazing. All right.

812
00:39:39,850 --> 00:39:43,090
Now, the main observation though

813
00:39:43,090 --> 00:39:46,810
here is that you still if you had an Oracle that would tell you

814
00:39:46,810 --> 00:39:50,275
exactly which task you're currently in

815
00:39:50,275 --> 00:39:54,685
and you would be perfectly kind of separating these into 10 different models,

816
00:39:54,685 --> 00:39:58,945
maybe they're all the same architecture but there's still 10 different models, then, uh,

817
00:39:58,945 --> 00:40:02,410
you would actually still do slightly better,

818
00:40:02,410 --> 00:40:06,535
uh, than the first version of this multitask learning model.

819
00:40:06,535 --> 00:40:09,070
And that is largely because we

820
00:40:09,070 --> 00:40:12,430
chose to include a bunch of different tasks that have nothing to do

821
00:40:12,430 --> 00:40:15,130
with one another and we wanted the community to start

822
00:40:15,130 --> 00:40:18,310
thinking about tackling catastrophic interference, right?

823
00:40:18,310 --> 00:40:21,685
If you learn like a new language or, you know,

824
00:40:21,685 --> 00:40:24,670
you learn how to understand social media on Twitter,

825
00:40:24,670 --> 00:40:26,860
you don't replace all your language,

826
00:40:26,860 --> 00:40:28,825
uh, you know, in- in your brain.

827
00:40:28,825 --> 00:40:30,820
You have one brain, it keeps getting smarter,

828
00:40:30,820 --> 00:40:32,065
you keep learning new skills,

829
00:40:32,065 --> 00:40:35,140
even when that skills that are new to you are very,

830
00:40:35,140 --> 00:40:36,520
very different from old skills.

831
00:40:36,520 --> 00:40:40,420
So in some ways we may have made our lives too hard,

832
00:40:40,420 --> 00:40:41,770
and now we're actually thinking, okay,

833
00:40:41,770 --> 00:40:44,620
maybe if you wanna publish a nicer paper on multitask learning,

834
00:40:44,620 --> 00:40:46,810
we'll just look at all the tasks that do help each other,

835
00:40:46,810 --> 00:40:48,880
and then we'll just, you know, have groups of tasks,

836
00:40:48,880 --> 00:40:51,445
and then I can very quickly publish,

837
00:40:51,445 --> 00:40:54,010
uh, some, some nice state-of-the-art papers.

838
00:40:54,010 --> 00:40:57,370
But basically here, uh, we're still, uh,

839
00:40:57,370 --> 00:41:03,910
quite significantly away in the decaScore between 10 different models and a single model.

840
00:41:03,910 --> 00:41:06,280
Now, this of course is kind of an oracle score,

841
00:41:06,280 --> 00:41:09,805
that's why we put it in parentheses because you don't actually have this oracle.

842
00:41:09,805 --> 00:41:11,260
And in some cases,

843
00:41:11,260 --> 00:41:13,780
it's quite easy to build an almost perfect classifier.

844
00:41:13,780 --> 00:41:16,615
So, you know, separating what is the summary

845
00:41:16,615 --> 00:41:19,810
based on that question and what is the translation from English to German,

846
00:41:19,810 --> 00:41:21,610
you can do with almost 100 percent accuracy.

847
00:41:21,610 --> 00:41:25,090
Uh, but, uh, SQuAD, question-answering,

848
00:41:25,090 --> 00:41:26,665
and zero-shot relation extraction,

849
00:41:26,665 --> 00:41:29,575
and question-answering as a semantic role labeling,

850
00:41:29,575 --> 00:41:33,220
those are actually easily confused in terms of how

851
00:41:33,220 --> 00:41:37,330
to generate the answers and you wouldn't quite know,

852
00:41:37,330 --> 00:41:40,870
uh, which into which model to route, uh, this.

853
00:41:40,870 --> 00:41:44,935
So in some sense, this is kind of theoretical. All right.

854
00:41:44,935 --> 00:41:47,710
Now, I mentioned that we have this prob- this

855
00:41:47,710 --> 00:41:51,730
complexity in the optimization strategy and this is one of the many,

856
00:41:51,730 --> 00:41:55,795
um, sort of problems that don't get that much, uh, coverage.

857
00:41:55,795 --> 00:41:57,535
But when you have a very,

858
00:41:57,535 --> 00:41:59,785
uh, imbalanced or skewed dataset,

859
00:41:59,785 --> 00:42:05,005
it's easy to lose track and basically overpower the smaller dataset tasks.

860
00:42:05,005 --> 00:42:07,510
And so, uh, the first, uh,

861
00:42:07,510 --> 00:42:10,780
simplest training- we actually tried a ton of different training strategies,

862
00:42:10,780 --> 00:42:13,600
but in the end, this fully joint one worked quite well.

863
00:42:13,600 --> 00:42:18,160
But actually promised to ask go wait for questions, uh, on this table.

864
00:42:18,160 --> 00:42:20,680
So any questions on all these results so far? Yeah?

865
00:42:20,680 --> 00:42:24,550
So, uh, [NOISE] since you mentioned that if you had

866
00:42:24,550 --> 00:42:26,740
an oracle that will tell you which task it is and

867
00:42:26,740 --> 00:42:29,215
you have two better ways having 10 different ones.

868
00:42:29,215 --> 00:42:32,440
So really try training a model on

869
00:42:32,440 --> 00:42:35,710
like data meaning what task is interested in this particular version?

870
00:42:35,710 --> 00:42:38,310
We did. And so it- it confused, you know,

871
00:42:38,310 --> 00:42:42,240
SQuAD and- and those too the quest- the other- basically the other,

872
00:42:42,240 --> 00:42:47,265
uh, two types of problems that were also cast, ask question answering.

873
00:42:47,265 --> 00:42:49,350
So it confused those.

874
00:42:49,350 --> 00:42:53,490
Um, but then a lot of the others, it was able to like, very perfectly do it.

875
00:42:53,490 --> 00:42:56,190
But then you basically, as soon as you,

876
00:42:56,190 --> 00:43:01,105
uh, were to try to then build a whole model and get a decaScore,

877
00:43:01,105 --> 00:43:05,395
if your- if your classifier is even like 90 percent accurate,

878
00:43:05,395 --> 00:43:08,530
you basically multiply this by 0,9 and

879
00:43:08,530 --> 00:43:11,680
you get dinged so hard that it- it's not competitive anymore.

880
00:43:11,680 --> 00:43:14,350
So it is actually hard if you try to just build

881
00:43:14,350 --> 00:43:17,080
that whole system and keep adding sort of if-then else statements,

882
00:43:17,080 --> 00:43:18,880
uh, to make that, uh,

883
00:43:18,880 --> 00:43:20,885
into sort of a single system. Yeah?

884
00:43:20,885 --> 00:43:24,090
Have you tried telling the model what kind of task this it's doing,

885
00:43:24,090 --> 00:43:27,330
just giving that indicator of the kind of task quickly?

886
00:43:27,330 --> 00:43:29,010
I mean, in some ways,

887
00:43:29,010 --> 00:43:30,120
we did in this case,

888
00:43:30,120 --> 00:43:33,360
because we only trained each model separately on it.

889
00:43:33,360 --> 00:43:34,280
[inaudible]

890
00:43:34,280 --> 00:43:36,905
Um, only through the question.

891
00:43:36,905 --> 00:43:39,185
Yeah. Because I was thinking the

892
00:43:39,185 --> 00:43:42,760
um, maybe it's not that important that the model figure out what we want it to

893
00:43:42,760 --> 00:43:44,965
do in- in a practical [NOISE] application

894
00:43:44,965 --> 00:43:47,560
if we could just tell it what we want it to do right now?

895
00:43:47,560 --> 00:43:49,420
In some cases, you could tell.

896
00:43:49,420 --> 00:43:51,430
Uh, so the question is sort of,

897
00:43:51,430 --> 00:43:53,260
uh, and even in the multitask setting,

898
00:43:53,260 --> 00:43:56,095
you could have like an extra kind of token to say,

899
00:43:56,095 --> 00:43:58,150
"Now, you're doing summarization.

900
00:43:58,150 --> 00:43:59,950
So, and that's another input."

901
00:43:59,950 --> 00:44:01,255
Uh, in some ways,

902
00:44:01,255 --> 00:44:03,610
whether you have a summarization token,

903
00:44:03,610 --> 00:44:05,650
uh, or you ask what is the summary?

904
00:44:05,650 --> 00:44:08,125
It actually I don't think makes that big of a difference.

905
00:44:08,125 --> 00:44:11,190
It's just now you can query this model in

906
00:44:11,190 --> 00:44:13,140
very natural language rather than having to know

907
00:44:13,140 --> 00:44:15,600
kind of a special token to, to query the model.

908
00:44:15,600 --> 00:44:19,710
Uh, and we'll see actually in a couple of slides that the model is not confused,

909
00:44:19,710 --> 00:44:22,860
uh, when it comes to how to generate the answers.

910
00:44:22,860 --> 00:44:24,710
So, for every of the task,

911
00:44:24,710 --> 00:44:28,660
it knows very clearly how to generate the words to get to the right,

912
00:44:28,660 --> 00:44:30,700
to get to, you know, a reasonably accurate answer.

913
00:44:30,700 --> 00:44:36,520
[NOISE] Um, in the- [inaudible] does the model

914
00:44:36,520 --> 00:44:42,580
see all of the data and then [inaudible] that class or does it only include a [inaudible]?

915
00:44:42,580 --> 00:44:45,400
Oh, great question. So, how do we train, uh, the single task models?

916
00:44:45,400 --> 00:44:47,980
They're only trained on that dataset.

917
00:44:47,980 --> 00:44:51,700
So, the SQuAD number here is just a single model that has only seen SQuAD training.

918
00:44:51,700 --> 00:44:57,250
[NOISE] So, your point about the,

919
00:44:57,250 --> 00:44:59,050
um, the pointer exception for the, uh,

920
00:44:59,050 --> 00:45:02,310
[inaudible] generally more helpful than [inaudible]?

921
00:45:02,310 --> 00:45:04,830
Somewhat surprisingly, even, ah,

922
00:45:04,830 --> 00:45:06,315
in the case here, uh,

923
00:45:06,315 --> 00:45:09,065
where we had, um, this is MultiNLI,

924
00:45:09,065 --> 00:45:10,690
this particular model, I mean,

925
00:45:10,690 --> 00:45:12,550
if you just have the standard sequence to sequence,

926
00:45:12,550 --> 00:45:14,035
it just generates, you know,

927
00:45:14,035 --> 00:45:16,660
also with a softmax, uh, that label.

928
00:45:16,660 --> 00:45:18,640
So in that sense, it's quite similar.

929
00:45:18,640 --> 00:45:23,650
Uh, but yeah, it was actually better able to just point, which actually led us, uh,

930
00:45:23,650 --> 00:45:27,730
for a while into thinking about maybe we should have a project where we just say point to

931
00:45:27,730 --> 00:45:32,125
all the things and just get rid of softmax classifiers forever.

932
00:45:32,125 --> 00:45:35,890
Um, the problem is when you then try to do translation also,

933
00:45:35,890 --> 00:45:37,210
it's like okay wow,

934
00:45:37,210 --> 00:45:38,395
what do you point to,

935
00:45:38,395 --> 00:45:40,420
and then you kind of pre-train it and do

936
00:45:40,420 --> 00:45:43,750
some alignment and it gets kinda very large and you point to a lot of different like,

937
00:45:43,750 --> 00:45:46,360
you may have like- like tens of thousands of potential candidates.

938
00:45:46,360 --> 00:45:49,540
So we kinda discarded it as like a single unifying model for all the things,

939
00:45:49,540 --> 00:45:51,895
but you could point to a lot of different,

940
00:45:51,895 --> 00:45:52,990
like a lot of these tasks,

941
00:45:52,990 --> 00:45:54,280
you could actually point to and

942
00:45:54,280 --> 00:45:59,030
I think it's another interesting side project that could spawn from this, yeah.

943
00:46:01,440 --> 00:46:03,745
Just a quick question to how,

944
00:46:03,745 --> 00:46:06,910
how sensitive [inaudible] how sensitive, uh,

945
00:46:06,910 --> 00:46:09,850
the individual components [inaudible] was when you

946
00:46:09,850 --> 00:46:13,240
slightly perturb the relative weights of them in the loss function?

947
00:46:13,240 --> 00:46:16,855
So, we -- the question is, uh, how, um,

948
00:46:16,855 --> 00:46:19,795
sensitive were the tasks if we were to,

949
00:46:19,795 --> 00:46:22,825
um, add weights to the different tasks?

950
00:46:22,825 --> 00:46:27,490
We [NOISE] did in the optimization kind of did a lot of trickery on

951
00:46:27,490 --> 00:46:32,080
how to train it but we never said this task only matters like 0,5 or something.

952
00:46:32,080 --> 00:46:34,930
So, we didn't do that analysis. Yeah?

953
00:46:34,930 --> 00:46:37,990
Co-attention seems to be a burden a little bit.

954
00:46:37,990 --> 00:46:39,070
In some cases, yeah.

955
00:46:39,070 --> 00:46:44,425
Is it the [inaudible] co-attention and order but no co-attention or is that kind of like,

956
00:46:44,425 --> 00:46:47,320
"Oh, you already saw the test data so, like, you can't use these."

957
00:46:47,320 --> 00:46:49,045
I mean, these are all dep sets.

958
00:46:49,045 --> 00:46:53,560
Um, but it's, you could definitely do even more architecture engineering.

959
00:46:53,560 --> 00:46:55,900
In fact, there's this whole field which I don't think

960
00:46:55,900 --> 00:46:58,690
you gotten to, right, neural architecture search?

961
00:46:58,690 --> 00:47:02,515
Yeah. So like you can actually combine your reinforcement learning, um,

962
00:47:02,515 --> 00:47:05,695
and you say the action space for the reinforcement learning agent

963
00:47:05,695 --> 00:47:07,360
are trying to have a couple of

964
00:47:07,360 --> 00:47:09,580
different modules of neural nets like maybe you want to have

965
00:47:09,580 --> 00:47:11,185
like a CNN layer and then like

966
00:47:11,185 --> 00:47:14,320
a memory layer and then an LSTM layer and maybe it's bidirectional and you

967
00:47:14,320 --> 00:47:19,465
basically let a reinforcement learning agent figure out all of these decisions.

968
00:47:19,465 --> 00:47:22,855
Uh, so I think it would be phenomenal to try to apply

969
00:47:22,855 --> 00:47:25,210
neural architecture search not to what's

970
00:47:25,210 --> 00:47:27,790
usually being done which is we already know how to do image classification,

971
00:47:27,790 --> 00:47:30,715
we'll just do it slightly better with NAS, neural architecture search.

972
00:47:30,715 --> 00:47:31,930
But we actually try to find

973
00:47:31,930 --> 00:47:34,810
a single architecture for multi-task learning which we don't know.

974
00:47:34,810 --> 00:47:38,620
The problem of course is that already getting to these.

975
00:47:38,620 --> 00:47:41,470
All these numbers took a lot of compute time and a lot of

976
00:47:41,470 --> 00:47:44,875
fiddling around with stuff and it is, I can,

977
00:47:44,875 --> 00:47:48,985
I can only give you sort of an idea of like how often we'd say,

978
00:47:48,985 --> 00:47:50,890
"Oh man, we got like this really amazing result

979
00:47:50,890 --> 00:47:53,110
in this task but it needed this learning rate."

980
00:47:53,110 --> 00:47:55,000
And it turns out the same model,

981
00:47:55,000 --> 00:47:57,100
same set of hyperparameters everything,

982
00:47:57,100 --> 00:48:01,555
but this other task to get to good performance needed a much higher learning rate.

983
00:48:01,555 --> 00:48:05,650
And now, you try to combine those two tasks only together and you're like,

984
00:48:05,650 --> 00:48:07,345
"Okay, how do you choose your learning rate now?"

985
00:48:07,345 --> 00:48:09,070
You choose the, you know,

986
00:48:09,070 --> 00:48:11,650
if you choose the task, the learning rate from the task that is, you know,

987
00:48:11,650 --> 00:48:13,780
bigger than the smaller tasks just doesn't work

988
00:48:13,780 --> 00:48:15,970
well at all because it needed this higher learning rate.

989
00:48:15,970 --> 00:48:19,405
If you'd use the higher learning rate that the smaller task and the smaller dataset,

990
00:48:19,405 --> 00:48:23,995
uh, did really well on then the large one just overfits and doesn't work well either.

991
00:48:23,995 --> 00:48:25,960
If you try to do the average, neither of the two work.

992
00:48:25,960 --> 00:48:29,560
Like there's a lot of complexity in trying to do multitask learning.

993
00:48:29,560 --> 00:48:33,860
That's why, that's why it's such an interesting I think, uh, research challenge.

994
00:48:35,100 --> 00:48:38,415
All right, any more questions about this first set of results?

995
00:48:38,415 --> 00:48:39,780
They get, they will get better.

996
00:48:39,780 --> 00:48:42,270
We, we have, we have had some ideas already,

997
00:48:42,270 --> 00:48:45,370
uh, on, on how to improve them.

998
00:48:47,250 --> 00:48:49,780
All right. So, uh,

999
00:48:49,780 --> 00:48:51,775
how did we actually train this whole thing?

1000
00:48:51,775 --> 00:48:54,895
Um, we had tried a lot of different things but in the end, uh,

1001
00:48:54,895 --> 00:48:58,990
this very simple fully joint training strategy actually worked the best.

1002
00:48:58,990 --> 00:49:02,800
Uh, and that is you basically take a mini batch from each of

1003
00:49:02,800 --> 00:49:07,540
the different tasks and you just train on that mini batch from that task.

1004
00:49:07,540 --> 00:49:11,470
So basically just going through all the 10 tasks and then round robin,

1005
00:49:11,470 --> 00:49:13,690
uh, go through them.

1006
00:49:13,690 --> 00:49:16,825
Um, now it turns out, ah,

1007
00:49:16,825 --> 00:49:19,090
that that does not work,

1008
00:49:19,090 --> 00:49:21,460
uh, quite as well, uh,

1009
00:49:21,460 --> 00:49:26,050
as another training strategy and if you look into optimization,

1010
00:49:26,050 --> 00:49:27,685
uh, strategies in neural nets, uh,

1011
00:49:27,685 --> 00:49:29,170
there are actually a couple of papers on

1012
00:49:29,170 --> 00:49:31,720
so-called curriculum learning, where the idea is,

1013
00:49:31,720 --> 00:49:36,430
you start with training your model with simple pro- simple instances of your problems.

1014
00:49:36,430 --> 00:49:38,830
So, in translation, for instance you start training with

1015
00:49:38,830 --> 00:49:41,995
very short sentences and then you go to larger and larger,

1016
00:49:41,995 --> 00:49:44,560
uh, sentences, uh, or longer and longer sentences.

1017
00:49:44,560 --> 00:49:47,545
Uh, now it turns out for multi-task learning,

1018
00:49:47,545 --> 00:49:49,285
you actually want to do the opposite.

1019
00:49:49,285 --> 00:49:52,045
You wanna do anti-curriculum learning.

1020
00:49:52,045 --> 00:49:55,330
Uh, and that is you start with the hardest tasks and you iterate on

1021
00:49:55,330 --> 00:49:58,930
those for a while and then you add the simple tasks later on.

1022
00:49:58,930 --> 00:50:02,050
And to some degree, I think this is intuitive because when

1023
00:50:02,050 --> 00:50:07,780
you train this very gigantic and powerful model,

1024
00:50:07,780 --> 00:50:11,020
uh, on a very simple task like

1025
00:50:11,020 --> 00:50:14,515
sentiment and you just need to classify everything to be positive or negative.

1026
00:50:14,515 --> 00:50:18,220
You train all of these weights and you arrive at sort of, uh,

1027
00:50:18,220 --> 00:50:20,710
local optima that are quite deep and very

1028
00:50:20,710 --> 00:50:24,370
specific to just generating these two words and if you then try to get out of that,

1029
00:50:24,370 --> 00:50:27,430
out of this local optimum for that very simple task

1030
00:50:27,430 --> 00:50:30,655
and then try to generate all these other kinds of words and point to different,

1031
00:50:30,655 --> 00:50:33,925
you know, words it's never seen before then SQuAD,

1032
00:50:33,925 --> 00:50:36,940
it's very very hard to come out of that local optimum.

1033
00:50:36,940 --> 00:50:40,975
And that is sort of my intuition of why it actually makes more sense to say,

1034
00:50:40,975 --> 00:50:44,935
"Let's start with SQuAD and machine translation and a couple of these harder tasks.

1035
00:50:44,935 --> 00:50:47,020
We'll make the model very general purpose.

1036
00:50:47,020 --> 00:50:48,910
It has to generate a lot of different things,

1037
00:50:48,910 --> 00:50:52,240
create a softmax, German words,

1038
00:50:52,240 --> 00:50:54,460
it has to point to all kinds of

1039
00:50:54,460 --> 00:50:57,895
different words and be able to parse all kinds of different Wikipedia paragraphs."

1040
00:50:57,895 --> 00:51:01,315
And you do that a couple of times and then once you've finished,

1041
00:51:01,315 --> 00:51:03,190
uh, this sort of pre-training, uh,

1042
00:51:03,190 --> 00:51:09,220
stage or anti-curriculum, then you move on and add sort of the simpler smaller tasks.

1043
00:51:09,220 --> 00:51:11,590
So [NOISE] with that, uh,

1044
00:51:11,590 --> 00:51:15,085
relatively simple change that did take us,

1045
00:51:15,085 --> 00:51:17,455
uh, a lot of different experiments to get to.

1046
00:51:17,455 --> 00:51:20,200
Um, we actually, uh,

1047
00:51:20,200 --> 00:51:22,045
closed or, uh, um,

1048
00:51:22,045 --> 00:51:25,570
went closer to closing that gap and now, um,

1049
00:51:25,570 --> 00:51:30,330
we're only sort of, um, 14, uh, away.

1050
00:51:30,330 --> 00:51:32,780
Right, yeah, uh, 14 or so.

1051
00:51:32,780 --> 00:51:35,180
Uh, but there's still, uh,

1052
00:51:35,180 --> 00:51:37,700
a big gap and the biggest, uh,

1053
00:51:37,700 --> 00:51:40,880
nuisance and issue that we had was with a translation.

1054
00:51:40,880 --> 00:51:42,845
Basically, if you look at all of these,

1055
00:51:42,845 --> 00:51:44,914
most things are kind of similar,

1056
00:51:44,914 --> 00:51:49,160
get slightly better, um and it's sort of a toss up but then and,

1057
00:51:49,160 --> 00:51:52,130
and roughly similar, but translation was really bad.

1058
00:51:52,130 --> 00:51:53,450
It's almost only half, uh,

1059
00:51:53,450 --> 00:51:56,420
the performance in the multitask learning setup,

1060
00:51:56,420 --> 00:52:00,110
and part of that is because translation was the only task that had

1061
00:52:00,110 --> 00:52:05,960
a very large Softmax vocabulary of words that were in no other task.

1062
00:52:05,960 --> 00:52:08,075
And most of the other tasks,

1063
00:52:08,075 --> 00:52:10,430
actually were doing really well with pointing.

1064
00:52:10,430 --> 00:52:14,570
And so, uh, my interpretation of this was that the intermediate layers,

1065
00:52:14,570 --> 00:52:16,550
all these representations that we learned with

1066
00:52:16,550 --> 00:52:19,520
bi-directional LSTMs and transformers, they got really,

1067
00:52:19,520 --> 00:52:21,875
really good at being pointed to,

1068
00:52:21,875 --> 00:52:27,560
like creating hidden representations that the answer module can point to very accurately.

1069
00:52:27,560 --> 00:52:29,465
And then you have this one task that is like,

1070
00:52:29,465 --> 00:52:31,085
I don't point to almost anything,

1071
00:52:31,085 --> 00:52:34,235
I basically just generate other words and then different vocabulary.

1072
00:52:34,235 --> 00:52:37,610
And so those hidden representations became less useful for that task.

1073
00:52:37,610 --> 00:52:41,360
And so, that was one of the insights and that led

1074
00:52:41,360 --> 00:52:45,020
to one of the ways of trying to improve this.

1075
00:52:45,020 --> 00:52:47,615
Now, one of the interesting issues that we had is,

1076
00:52:47,615 --> 00:52:49,040
when we improved the model,

1077
00:52:49,040 --> 00:52:51,500
the multi-single model for all 10 tasks,

1078
00:52:51,500 --> 00:52:53,090
a lot of times we said, well,

1079
00:52:53,090 --> 00:52:55,280
but now we also have to go back and run

1080
00:52:55,280 --> 00:52:59,060
10 more experiments on all the single tasks to have a proper comparison, right?

1081
00:52:59,060 --> 00:53:01,280
Because if you tune the thing you care about,

1082
00:53:01,280 --> 00:53:04,790
and you stop tuning the thing you wanna show you can do better than,

1083
00:53:04,790 --> 00:53:06,275
then that's not fair.

1084
00:53:06,275 --> 00:53:09,470
Uh, so you always wanna give as much, uh,

1085
00:53:09,470 --> 00:53:13,655
TLC and focus and experiment time to your baselines.

1086
00:53:13,655 --> 00:53:17,789
And so, uh, in some cases we actually,

1087
00:53:18,670 --> 00:53:22,415
uh, improved some- improved something.

1088
00:53:22,415 --> 00:53:26,495
But then, we improve both the 10 separate models and our model,

1089
00:53:26,495 --> 00:53:29,090
and some cases like the 10 separate models improved, even more.

1090
00:53:29,090 --> 00:53:30,485
So the gap got even larger.

1091
00:53:30,485 --> 00:53:32,720
It's kind of the opposite of what we wanted to show, but in general,

1092
00:53:32,720 --> 00:53:34,220
it's better for both tests,

1093
00:53:34,220 --> 00:53:36,530
uh, for the architecture overall.

1094
00:53:36,530 --> 00:53:37,970
So basically, we started, uh,

1095
00:53:37,970 --> 00:53:40,220
with this fully joint training and we have

1096
00:53:40,220 --> 00:53:42,515
this sort of set of single models that we could,

1097
00:53:42,515 --> 00:53:44,150
in theory with some oracle,

1098
00:53:44,150 --> 00:53:45,335
kind of just sum up, uh,

1099
00:53:45,335 --> 00:53:47,015
in their scores, to get a decaScore.

1100
00:53:47,015 --> 00:53:49,115
So the gap started at 23.

1101
00:53:49,115 --> 00:53:53,030
And then, uh, we basically did this anti-curriculum training,

1102
00:53:53,030 --> 00:53:55,790
uh, which, uh, lowered the gap to 15.

1103
00:53:55,790 --> 00:53:57,380
So we're kind of excited,

1104
00:53:57,380 --> 00:53:58,760
uh, making good progress.

1105
00:53:58,760 --> 00:53:59,930
Then we switched, uh,

1106
00:53:59,930 --> 00:54:01,880
from GloVe and use CoVe.

1107
00:54:01,880 --> 00:54:04,055
So contextual vectors, um,

1108
00:54:04,055 --> 00:54:06,320
which actually increased the gap a lot again.

1109
00:54:06,320 --> 00:54:09,325
So everything got better, but the 10 separate models got

1110
00:54:09,325 --> 00:54:13,000
even better than the one single model that does the 10 tasks.

1111
00:54:13,000 --> 00:54:14,650
Um, so the gap got bigger,

1112
00:54:14,650 --> 00:54:17,140
but everybody's performance increased.

1113
00:54:17,140 --> 00:54:19,510
So it was still overall a good thing.

1114
00:54:19,510 --> 00:54:22,780
Uh, and then, uh, we basically figured,

1115
00:54:22,780 --> 00:54:24,610
especially with this machine translation issue,

1116
00:54:24,610 --> 00:54:26,470
we shouldn't just pre-train on SQuAD,

1117
00:54:26,470 --> 00:54:30,100
but we also should include machine translation in

1118
00:54:30,100 --> 00:54:34,845
this pre-training in the beginning so the model doesn't just start learning to point.

1119
00:54:34,845 --> 00:54:37,625
Um, and that helped us, uh,

1120
00:54:37,625 --> 00:54:40,160
to reduce the gap between the 10 separate models,

1121
00:54:40,160 --> 00:54:43,085
Oracle, and the single model to about five points.

1122
00:54:43,085 --> 00:54:44,690
And then, uh, we basically said,

1123
00:54:44,690 --> 00:54:46,640
okay, translation is still not that good.

1124
00:54:46,640 --> 00:54:47,780
We just keep oversampling.

1125
00:54:47,780 --> 00:54:52,760
So, every time we go through one of these round robin mini-batch sets,

1126
00:54:52,760 --> 00:54:54,740
we just always include machine translation.

1127
00:54:54,740 --> 00:54:59,270
And that basically allowed us to then reduce the gap,

1128
00:54:59,270 --> 00:55:01,025
uh, to just a single point.

1129
00:55:01,025 --> 00:55:03,590
So now, uh, we started, uh,

1130
00:55:03,590 --> 00:55:06,650
couple of, several months ago, uh, at 586.

1131
00:55:06,650 --> 00:55:08,960
And now the single, uh,

1132
00:55:08,960 --> 00:55:11,330
oracle with 10 different models,

1133
00:55:11,330 --> 00:55:12,560
if you were to sum them up,

1134
00:55:12,560 --> 00:55:16,100
get 618, uh, and the, you know,

1135
00:55:16,100 --> 00:55:19,985
better contextual vectors and tuning and adding a lot more translation,

1136
00:55:19,985 --> 00:55:23,210
and translation is still not as good as we would like it to be, uh,

1137
00:55:23,210 --> 00:55:26,525
but now, several of the other tasks benefited a bunch.

1138
00:55:26,525 --> 00:55:30,140
And now we're basically one decaScore away from

1139
00:55:30,140 --> 00:55:33,740
having a single model that does as well as 10 different ones.

1140
00:55:33,740 --> 00:55:36,395
And you can basically,

1141
00:55:36,395 --> 00:55:38,525
you could run even more experiments,

1142
00:55:38,525 --> 00:55:41,930
in some ways you could burn millions of dollars on AWS cost here,

1143
00:55:41,930 --> 00:55:47,180
because most of the time we kept the hyperparameters of these different models the same.

1144
00:55:47,180 --> 00:55:49,385
Like each of these, you could also say, well,

1145
00:55:49,385 --> 00:55:52,010
maybe this multitask model needs to have 50 more layers,

1146
00:55:52,010 --> 00:55:53,720
or maybe 19 more layers,

1147
00:55:53,720 --> 00:55:56,225
or maybe five more layers and maybe they should be 1000,

1148
00:55:56,225 --> 00:55:57,860
you know, wider in their hidden dimensions.

1149
00:55:57,860 --> 00:56:01,310
And you could basically run a lot more experiments.

1150
00:56:01,310 --> 00:56:03,830
Maybe hopefully, eventually, the community jointly does that,

1151
00:56:03,830 --> 00:56:06,170
and then we can kind of move, move towards that.

1152
00:56:06,170 --> 00:56:08,480
But we figured, okay, we're pretty close,

1153
00:56:08,480 --> 00:56:13,850
so we moved on to some other things which maybe I'll tell you about next year.

1154
00:56:13,850 --> 00:56:16,715
[LAUGHTER] But basically, um,

1155
00:56:16,715 --> 00:56:18,980
let's do some analysis of what happened in this project.

1156
00:56:18,980 --> 00:56:22,235
And this is kind of, I think something that I would encourage you all to do as well.

1157
00:56:22,235 --> 00:56:25,460
Like you, you can chase the numbers for a while and in some ways,

1158
00:56:25,460 --> 00:56:28,385
you should always be skeptical about your evaluations.

1159
00:56:28,385 --> 00:56:29,780
And in some cases,

1160
00:56:29,780 --> 00:56:33,230
you've seen- we've seen in the NLP community people

1161
00:56:33,230 --> 00:56:36,935
like basically just optimize BLEU scores for translation for years.

1162
00:56:36,935 --> 00:56:38,690
And then somebody came out with a paper and said, well,

1163
00:56:38,690 --> 00:56:44,510
it turns out BLEU metrics and human evaluations on how good of a translation is this,

1164
00:56:44,510 --> 00:56:46,175
aren't actually that correlated.

1165
00:56:46,175 --> 00:56:48,320
And you're like, ah, that that sucks,

1166
00:56:48,320 --> 00:56:53,000
we just spent years of our lives tuning that metric and publishing a bunch of papers.

1167
00:56:53,000 --> 00:56:57,290
Um, and so in some ways all of these metrics have flaws, uh, you know,

1168
00:56:57,290 --> 00:57:00,140
root scores summarization is a super,

1169
00:57:00,140 --> 00:57:03,380
uh, subjective kind of a task.

1170
00:57:03,380 --> 00:57:05,465
And summarization, for instance,

1171
00:57:05,465 --> 00:57:07,730
when you analyze the errors, uh,

1172
00:57:07,730 --> 00:57:10,595
you often realize that word vectors have problems too.

1173
00:57:10,595 --> 00:57:12,920
So, for instance, the word vector for Jason, John,

1174
00:57:12,920 --> 00:57:15,290
and Jeremy are all kind of the same, right?

1175
00:57:15,290 --> 00:57:16,940
They all have similar, uh,

1176
00:57:16,940 --> 00:57:20,045
distributions, similar contexts, windows, and so on.

1177
00:57:20,045 --> 00:57:22,610
And so word vectors of names are very similar.

1178
00:57:22,610 --> 00:57:25,835
And so in summarization errors, you realize, oh,

1179
00:57:25,835 --> 00:57:29,300
well, you know, this article, news article talked about Jeremy being kidnapped.

1180
00:57:29,300 --> 00:57:31,160
But the summary said that Jason was kidnapped.

1181
00:57:31,160 --> 00:57:33,650
And you like, well, you know, in the evaluation metric

1182
00:57:33,650 --> 00:57:36,320
that's just one word is off and like, all the rest is correct,

1183
00:57:36,320 --> 00:57:38,000
but it's a pretty important word.

1184
00:57:38,000 --> 00:57:40,970
And so, word vectors have like issues

1185
00:57:40,970 --> 00:57:44,075
for summarization that are pretty fundamental and I don't think,

1186
00:57:44,075 --> 00:57:46,835
uh, anybody's tackling really well right now.

1187
00:57:46,835 --> 00:57:48,875
Uh, and so all of these metrics have issues.

1188
00:57:48,875 --> 00:57:51,620
I would argue though that combining the 10 actually

1189
00:57:51,620 --> 00:57:54,440
makes it less problematic and more meaningful,

1190
00:57:54,440 --> 00:57:56,630
than looking at each one separately.

1191
00:57:56,630 --> 00:58:00,725
Uh, because now you can't use the idiosyncrasies of

1192
00:58:00,725 --> 00:58:04,970
one particular evaluation metric to just get like your score a little bit higher.

1193
00:58:04,970 --> 00:58:09,740
Um, because then, if you just tune with that particular thing in mind,

1194
00:58:09,740 --> 00:58:13,370
it will hurt some of the other tasks and you won't get to the sort of general,

1195
00:58:13,370 --> 00:58:15,950
uh, NLP model that much more easily.

1196
00:58:15,950 --> 00:58:18,605
All right. So now, let's do some analysis uh,

1197
00:58:18,605 --> 00:58:20,645
of this model and, uh,

1198
00:58:20,645 --> 00:58:24,140
look at, and this is the kinda thing that comes to one of the questions that was asked.

1199
00:58:24,140 --> 00:58:28,295
Uh, is this model able to kind of generate the right words for the right tasks?

1200
00:58:28,295 --> 00:58:31,775
And here, we basically looked at the distributions of how often, uh,

1201
00:58:31,775 --> 00:58:37,100
the model generated words in these differen- with these three different mechanisms,

1202
00:58:37,100 --> 00:58:40,370
Softmax vocabulary, context pointers, or question pointers.

1203
00:58:40,370 --> 00:58:42,515
And, uh, as you can see,

1204
00:58:42,515 --> 00:58:45,500
in the majority of cases it knows exactly how to generate.

1205
00:58:45,500 --> 00:58:47,915
So, uh, for, uh,

1206
00:58:47,915 --> 00:58:51,110
question, answering, and semantic role labeling,

1207
00:58:51,110 --> 00:58:55,355
and SQuAD and Wiki SQL and,

1208
00:58:55,355 --> 00:58:59,150
um, summarization, it basically uses the context pointer.

1209
00:58:59,150 --> 00:59:01,565
So it just points into the context document.

1210
00:59:01,565 --> 00:59:02,795
And we know for SQuAD,

1211
00:59:02,795 --> 00:59:05,990
that is basically [NOISE] how the data set was generated.

1212
00:59:05,990 --> 00:59:08,600
So that's the only thing that that really makes a lot of sense.

1213
00:59:08,600 --> 00:59:11,930
Uh, what's kind of cool is that in some cases like summarization,

1214
00:59:11,930 --> 00:59:14,240
it sometimes creates new words or, you know,

1215
00:59:14,240 --> 00:59:17,330
that weren't in the context document wherein pointed to.

1216
00:59:17,330 --> 00:59:19,910
Uh, and for zero-shot relation extraction,

1217
00:59:19,910 --> 00:59:21,455
also sometimes uses, uh,

1218
00:59:21,455 --> 00:59:24,050
this external vocabulary and in some cases the context pointer.

1219
00:59:24,050 --> 00:59:26,210
So for the most part, uh,

1220
00:59:26,210 --> 00:59:31,970
this model doesn't- is not confused how to execute on a task given, uh,

1221
00:59:31,970 --> 00:59:35,180
this question formalism rather than, uh, the,

1222
00:59:35,180 --> 00:59:37,370
uh, format of sort of this is the task,

1223
00:59:37,370 --> 00:59:39,840
just do this particular test.

1224
00:59:41,200 --> 00:59:44,030
Now, um, you might argue,

1225
00:59:44,030 --> 00:59:45,830
okay, I'm not that impressed by, you know,

1226
00:59:45,830 --> 00:59:48,500
having the performance be slightly the same with one model versus

1227
00:59:48,500 --> 00:59:51,590
10 separate models even though it's nice if you wanna deploy it right,

1228
00:59:51,590 --> 00:59:53,255
like, uses less RAM and all of that,

1229
00:59:53,255 --> 00:59:54,965
assuming they're the same size,

1230
00:59:54,965 --> 00:59:57,080
uh, while, you know, one-tenth the size.

1231
00:59:57,080 --> 01:00:00,710
But what I'm excited about is more like the next couple of results.

1232
01:00:00,710 --> 01:00:02,750
And namely, sort of this transfer learning,

1233
01:00:02,750 --> 01:00:04,550
domain adaptation, and zero-shot,

1234
01:00:04,550 --> 01:00:06,020
uh, these kinds of capabilities.

1235
01:00:06,020 --> 01:00:11,630
So here, uh, we chose two data sets that weren't included in the original 10.

1236
01:00:11,630 --> 01:00:17,795
And we basically trained a pre-trained model on this versus a random model.

1237
01:00:17,795 --> 01:00:20,510
And, uh, randomly here again,

1238
01:00:20,510 --> 01:00:21,859
they're the same architecture,

1239
01:00:21,859 --> 01:00:25,295
and pre-trained means the entirety of the model was pre-trained.

1240
01:00:25,295 --> 01:00:26,945
All the, you know,

1241
01:00:26,945 --> 01:00:31,325
encoders including the decoder in the Softmax and everything, uh,

1242
01:00:31,325 --> 01:00:36,140
and to two other tasks where another IWSLT language pair namely,

1243
01:00:36,140 --> 01:00:37,685
translating from English to Czech, uh,

1244
01:00:37,685 --> 01:00:40,880
and named entity recognition tasks that you all know very well.

1245
01:00:40,880 --> 01:00:43,460
So basically what we found is that,

1246
01:00:43,460 --> 01:00:45,935
uh, it converges much more quickly,

1247
01:00:45,935 --> 01:00:47,810
uh, in the beginning, uh, and then,

1248
01:00:47,810 --> 01:00:51,200
there's still a significant but not gigantic gap.

1249
01:00:51,200 --> 01:00:55,595
So this pre-training on these completely separate kinds of task had helped.

1250
01:00:55,595 --> 01:00:58,745
And, uh, I think that's,

1251
01:00:58,745 --> 01:01:00,365
that's pretty exciting, um,

1252
01:01:00,365 --> 01:01:02,420
especially sort of the quicker convergence, like,

1253
01:01:02,420 --> 01:01:04,165
learning more quickly, uh,

1254
01:01:04,165 --> 01:01:06,310
whatever new task you, you come up with,

1255
01:01:06,310 --> 01:01:09,010
which also means in some cases you can get away with

1256
01:01:09,010 --> 01:01:11,950
less training data on these new- on these new tasks.

1257
01:01:11,950 --> 01:01:15,970
Uh, now domain adaptation is kind of the simpler form of transfer learning,

1258
01:01:15,970 --> 01:01:19,280
where you basically just have a different,

1259
01:01:19,280 --> 01:01:21,410
uh, type of, uh,

1260
01:01:21,410 --> 01:01:23,060
you know, distribution for your words.

1261
01:01:23,060 --> 01:01:26,750
Uh, we mentioned we have the Stanford Sentiment Treebank for sentiment analysis.

1262
01:01:26,750 --> 01:01:29,780
Uh, and then we analyze this on different,

1263
01:01:29,780 --> 01:01:31,610
uh, sentiment data sets,

1264
01:01:31,610 --> 01:01:34,505
namely Amazon product reviews and Yelp restaurant reviews,

1265
01:01:34,505 --> 01:01:36,605
and out of the box without any training,

1266
01:01:36,605 --> 01:01:39,965
the model just got 80% accuracy on both of those data sets.

1267
01:01:39,965 --> 01:01:42,319
Uh, and I think for practitioners,

1268
01:01:42,319 --> 01:01:45,140
that is pretty exciting because you basically didn't have to train anything,

1269
01:01:45,140 --> 01:01:46,610
it just kind of worked out of the box,

1270
01:01:46,610 --> 01:01:48,830
download it from GitHub, and run it.

1271
01:01:48,830 --> 01:01:51,620
Uh, SNLI, that was slightly different.

1272
01:01:51,620 --> 01:01:53,330
It didn't quite work as well.

1273
01:01:53,330 --> 01:01:55,280
It's another natural language inference data set,

1274
01:01:55,280 --> 01:01:59,135
but has very different- a very different distribution, different, uh,

1275
01:01:59,135 --> 01:02:01,040
kinds of domains, uh, that,

1276
01:02:01,040 --> 01:02:03,290
uh, these entailment questions are asked over.

1277
01:02:03,290 --> 01:02:06,980
Uh, and here, out of the box it achieved 62.

1278
01:02:06,980 --> 01:02:10,200
Uh, but then, uh, once you fine tuned it and

1279
01:02:10,200 --> 01:02:14,230
similar to these experiments here continue to actually train on this data set,

1280
01:02:14,230 --> 01:02:17,680
it quickly uh, converged to 87 which was

1281
01:02:17,680 --> 01:02:21,625
still two percent gain over a randomlyor initialized McCann model. Yeah.

1282
01:02:21,625 --> 01:02:29,075
In that experiment, did you evaluate how much less data you can get away with?

1283
01:02:29,075 --> 01:02:32,900
Did we evaluate how much less data we can get away with? We didn't.

1284
01:02:32,900 --> 01:02:35,510
And in some ways, whenever you would run this experiment,

1285
01:02:35,510 --> 01:02:38,000
you'd basically be like, you'd still not do as well.

1286
01:02:38,000 --> 01:02:41,555
Like, everything- all these models will still do better with more training data.

1287
01:02:41,555 --> 01:02:43,640
So you just kind of, it would be a fuzzy kind of say,

1288
01:02:43,640 --> 01:02:46,220
like, cut- fuzzy sort of result, right?

1289
01:02:46,220 --> 01:02:48,140
Where you say, well, with one-tenth we might get

1290
01:02:48,140 --> 01:02:50,885
to 50 and the other model might get only to 40,

1291
01:02:50,885 --> 01:02:52,160
doing something like that.

1292
01:02:52,160 --> 01:02:54,830
Um, we don't- I don't have those numbers.

1293
01:02:54,830 --> 01:02:57,380
It would be kind of actually also a neat, neat, uh,

1294
01:02:57,380 --> 01:02:59,750
analysis to do. Yeah.

1295
01:02:59,750 --> 01:03:06,835
So if you wanted to like train on a new task [inaudible].

1296
01:03:06,835 --> 01:03:07,935
Yeah.

1297
01:03:07,935 --> 01:03:10,165
[inaudible] .

1298
01:03:10,165 --> 01:03:13,105
So, do we have the code to train a new task? Yes, we do.

1299
01:03:13,105 --> 01:03:14,695
Um, you can just, uh, edit,

1300
01:03:14,695 --> 01:03:16,795
make it into this format using context.

1301
01:03:16,795 --> 01:03:19,465
Here's a question, simple like CSV type format,

1302
01:03:19,465 --> 01:03:24,160
and then you add it and you can both like train the pre-trained model yourself.

1303
01:03:24,160 --> 01:03:28,690
You can download a pre-trained model and just add it. So I'll look it up, yeah.

1304
01:03:28,690 --> 01:03:34,795
Do you know how this compares to using other kinds of pre-trained representations like, say BERT?

1305
01:03:34,795 --> 01:03:37,330
So, um, it's a great question.

1306
01:03:37,330 --> 01:03:40,120
So how does this compare to other pre-trained representations like BERT?

1307
01:03:40,120 --> 01:03:41,935
So, in some ways,

1308
01:03:41,935 --> 01:03:44,200
people say BERT is kind of this model that does everything,

1309
01:03:44,200 --> 01:03:46,690
but when you actually read the paper, you realize, well,

1310
01:03:46,690 --> 01:03:49,930
it's a separate model for these different tasks, right?

1311
01:03:49,930 --> 01:03:52,375
If you wanted to have a classification task,

1312
01:03:52,375 --> 01:03:54,070
you have a little token in the beginning,

1313
01:03:54,070 --> 01:03:55,330
and you have a different top layer.

1314
01:03:55,330 --> 01:03:57,400
If you wanna do a sequence labeling task,

1315
01:03:57,400 --> 01:03:58,450
you have a different top layer.

1316
01:03:58,450 --> 01:04:00,400
If you wanted to do a sequence extraction task,

1317
01:04:00,400 --> 01:04:01,765
you have a different top layer.

1318
01:04:01,765 --> 01:04:06,220
So, BERT isn't actually a single model for all of these different tasks.

1319
01:04:06,220 --> 01:04:08,410
Ah, and then, on all the results,

1320
01:04:08,410 --> 01:04:11,800
there's a lot of extra tuning for each of the data sets,

1321
01:04:11,800 --> 01:04:13,765
and tasks, uh, that, you know,

1322
01:04:13,765 --> 01:04:16,030
different learning rate for this task, uh,

1323
01:04:16,030 --> 01:04:19,120
different size, or different sets of BERT, and so on.

1324
01:04:19,120 --> 01:04:21,670
So, we're also super excited, we're like maybe this is it,

1325
01:04:21,670 --> 01:04:23,590
we'll just run everything on BERT,

1326
01:04:23,590 --> 01:04:25,180
and then we looked into all the details,

1327
01:04:25,180 --> 01:04:26,920
and there's so much excitement in the beginning.

1328
01:04:26,920 --> 01:04:29,020
And then the more we dug through the details,

1329
01:04:29,020 --> 01:04:31,795
the less excited we became as this being like sort of the answer,

1330
01:04:31,795 --> 01:04:33,580
because it is not a single model.

1331
01:04:33,580 --> 01:04:36,880
Uh, in some ways, it's probably better to- for pre-training.

1332
01:04:36,880 --> 01:04:38,290
So instead of CoVe,

1333
01:04:38,290 --> 01:04:41,140
you can have kind of BERT at the very beginning,

1334
01:04:41,140 --> 01:04:43,450
and my hunch is everything will get slightly better,

1335
01:04:43,450 --> 01:04:46,075
but you still need to have, um,

1336
01:04:46,075 --> 01:04:52,120
a lot of the- a lot of the other sort of modeling architecture on top of it.

1337
01:04:52,120 --> 01:04:56,050
Uh, and then the sad thing is to really get the state of the art results,

1338
01:04:56,050 --> 01:05:00,355
there's a lot of very spec- task-specific tuning of those last top layers.

1339
01:05:00,355 --> 01:05:04,525
So, if you try to unify that task-specific tuning,

1340
01:05:04,525 --> 01:05:06,700
you lose a lot of the good performance of BERT.

1341
01:05:06,700 --> 01:05:10,495
Um, so, unfortunately, it's not quite the sort of,

1342
01:05:10,495 --> 01:05:12,175
"Oh, just use BERT for it,

1343
01:05:12,175 --> 01:05:15,220
and you'll just have state-of-the-art numbers and all the things."

1344
01:05:15,220 --> 01:05:18,565
Um, I could probably go like talk about it a lot more, but, uh,

1345
01:05:18,565 --> 01:05:21,295
I think it still makes sense to think about, um,

1346
01:05:21,295 --> 01:05:23,065
some of the ideas from BERT,

1347
01:05:23,065 --> 01:05:26,355
like basically, add as one of the tasks language modeling.

1348
01:05:26,355 --> 01:05:30,990
That would be very likely the task  that helps the most for all the other tasks,

1349
01:05:30,990 --> 01:05:33,480
and we should include that, uh,

1350
01:05:33,480 --> 01:05:37,535
it also would be nice to have a faster model right now.

1351
01:05:37,535 --> 01:05:40,270
Um, it's hard to do language modeling is very, very large,

1352
01:05:40,270 --> 01:05:41,740
it benefits even more from,

1353
01:05:41,740 --> 01:05:43,840
you know, billions and billions of words.

1354
01:05:43,840 --> 01:05:45,670
It's hard to train the McCann model,

1355
01:05:45,670 --> 01:05:48,940
this current question answering model of the co-attention mechanism of the question

1356
01:05:48,940 --> 01:05:52,029
with like an increasingly large context.

1357
01:05:52,029 --> 01:05:54,970
So you'd have to kind of split it also like BERT,

1358
01:05:54,970 --> 01:05:59,020
works also reasonably well only for like at most I think 500 words or so,

1359
01:05:59,020 --> 01:06:02,050
and if you wanted to do summarization you'd basically have to cut

1360
01:06:02,050 --> 01:06:06,490
the original document to only 500 words, and then try to summarize it.

1361
01:06:06,490 --> 01:06:09,820
So, there are a lot of like devil in the details that they didn't have to figure out,

1362
01:06:09,820 --> 01:06:12,520
because they said, "Well, we'll just sort of just like word vectors,

1363
01:06:12,520 --> 01:06:16,420
we can take them in, and then we do a lot of other stuff that is task-specific,

1364
01:06:16,420 --> 01:06:18,775
um, with those- those word vectors,

1365
01:06:18,775 --> 01:06:20,350
or with the BERT architecture."

1366
01:06:20,350 --> 01:06:22,720
I still- I don't want to- this BERT is obviously amazing,

1367
01:06:22,720 --> 01:06:25,120
and we are looking into trying to use ideas from it.

1368
01:06:25,120 --> 01:06:27,400
But unfortunately, it wasn't just sort of a silver bullet to

1369
01:06:27,400 --> 01:06:33,355
solve multi-task learning. Mm-hmm?

1370
01:06:33,355 --> 01:06:35,515
Pre-training process to be considered, uh,

1371
01:06:35,515 --> 01:06:40,990
prioritized sampling based off of how much fewer group, how much loss there is?

1372
01:06:40,990 --> 01:06:42,670
Sorry, did we- say again?

1373
01:06:42,670 --> 01:06:46,390
Would you consider prioritizing sampling [inaudible]?

1374
01:06:46,390 --> 01:06:48,370
So, did we consider prioritizing the sampling?

1375
01:06:48,370 --> 01:06:51,760
So in some ways with this pre-trained strategy here, um,

1376
01:06:51,760 --> 01:06:56,500
that's kind of what we did by basically focusing on these really hard tasks.

1377
01:06:56,500 --> 01:07:02,140
And, uh, a lot of like the gap in the end was improved by really waiting for,

1378
01:07:02,140 --> 01:07:04,555
like four of the tasks at the very end,

1379
01:07:04,555 --> 01:07:05,995
uh, bef- unti- you know, uh,

1380
01:07:05,995 --> 01:07:08,560
until after you're gone through, uh,

1381
01:07:08,560 --> 01:07:10,750
sort of oversampling all of these,

1382
01:07:10,750 --> 01:07:11,800
uh, really hard tasks.

1383
01:07:11,800 --> 01:07:16,375
In the last 10 minutes, uh, basically, uh,

1384
01:07:16,375 --> 01:07:18,400
th- the most exciting thing, uh,

1385
01:07:18,400 --> 01:07:22,540
for- for last though I think you could also do a lot more work in this direction.

1386
01:07:22,540 --> 01:07:24,460
Uh, I mentioned the sole question pointer

1387
01:07:24,460 --> 01:07:26,380
and zero short learning in the beginning, and, uh,

1388
01:07:26,380 --> 01:07:29,965
we basically just tried to play around with that a little bit, um,

1389
01:07:29,965 --> 01:07:32,185
and found that in some cases,

1390
01:07:32,185 --> 01:07:35,080
it actually kind of magically works.

1391
01:07:35,080 --> 01:07:37,060
Uh, so here, we tried, uh,

1392
01:07:37,060 --> 01:07:38,725
a sentence John had a party,

1393
01:07:38,725 --> 01:07:40,855
but no one came, and he was all alone.

1394
01:07:40,855 --> 01:07:43,960
And then we asked, "Is this story sad, or happy?"

1395
01:07:43,960 --> 01:07:46,120
And while the model could've, you know,

1396
01:07:46,120 --> 01:07:47,920
generate some random German words,

1397
01:07:47,920 --> 01:07:49,570
or some random SQL words,

1398
01:07:49,570 --> 01:07:51,235
or it's just said whatever,

1399
01:07:51,235 --> 01:07:54,490
it actually pointed to, of all the words,

1400
01:07:54,490 --> 01:07:56,440
you could've pointed to in the context or the question that

1401
01:07:56,440 --> 01:07:58,825
pointed to "Sad", which is pretty cool.

1402
01:07:58,825 --> 01:08:01,750
Like- and it's just one small sample,

1403
01:08:01,750 --> 01:08:03,580
and, you know, you could do a lot more,

1404
01:08:03,580 --> 01:08:08,905
you could try to come up with a very large zero-shot kind of classification data set,

1405
01:08:08,905 --> 01:08:10,300
which is actually kind of hard too.

1406
01:08:10,300 --> 01:08:12,550
You have to be quite creative, it's not like you can just say, "Oh,

1407
01:08:12,550 --> 01:08:13,750
it would just take all these reviews,

1408
01:08:13,750 --> 01:08:15,700
and label them as these, you know, positive negative.

1409
01:08:15,700 --> 01:08:19,810
Ah, but so, I think we- we need to do more work in that direction.

1410
01:08:19,810 --> 01:08:23,230
Somebody will hopefully create a zero-shot kind of task data set,

1411
01:08:23,230 --> 01:08:25,570
that is not just zero-shot for, you know,

1412
01:08:25,570 --> 01:08:29,050
kind of new distributions or something with completely different, uh, outputs.

1413
01:08:29,050 --> 01:08:31,810
Uh, but we- we tried a couple,

1414
01:08:31,810 --> 01:08:32,950
and it doesn't always work, right.

1415
01:08:32,950 --> 01:08:34,510
You can be adversarial about it,

1416
01:08:34,510 --> 01:08:38,470
you can make this basically looks most similar to,

1417
01:08:38,470 --> 01:08:40,510
is the sentiment positive or negative?

1418
01:08:40,510 --> 01:08:42,805
Uh, is this sen- is this sentence positive or negative?

1419
01:08:42,805 --> 01:08:45,955
That was the formalism we had for sentiment analysis.

1420
01:08:45,955 --> 01:08:47,665
And so you could,

1421
01:08:47,665 --> 01:08:50,380
if you make the question more and more different,

1422
01:08:50,380 --> 01:08:52,000
eventually, it'll kinda get tripped up.

1423
01:08:52,000 --> 01:08:55,015
Ah, and it's clear that it's benefited, uh,

1424
01:08:55,015 --> 01:08:57,010
from the word vectors,

1425
01:08:57,010 --> 01:08:59,020
of sad being closer to negative,

1426
01:08:59,020 --> 01:09:01,360
and then understanding sort of through all these,

1427
01:09:01,360 --> 01:09:03,715
uh, correlations, and- and, uh,

1428
01:09:03,715 --> 01:09:08,920
deep representations that there are other sort of sad words in this context,

1429
01:09:08,920 --> 01:09:10,120
or- or whatever it is.

1430
01:09:10,120 --> 01:09:12,370
Uh, and so, it was able to point to this.

1431
01:09:12,370 --> 01:09:14,740
But you can be adversarial, it doesn't always work.

1432
01:09:14,740 --> 01:09:16,780
But even the fact that, uh,

1433
01:09:16,780 --> 01:09:20,335
it was sort of zero-shot classification based on word vectors, uh,

1434
01:09:20,335 --> 01:09:22,150
for new kinds of questions,

1435
01:09:22,150 --> 01:09:24,070
uh, personally, it was very exciting to me.

1436
01:09:24,070 --> 01:09:26,170
And we tried a couple of other things like,

1437
01:09:26,170 --> 01:09:28,615
uh, Bryan gave a talk and nobody clapped.

1438
01:09:28,615 --> 01:09:29,650
Was Bryan happy, or sad?

1439
01:09:29,650 --> 01:09:30,670
And it also got it right.

1440
01:09:30,670 --> 01:09:33,295
So, um, there are a couple- a couple of the,

1441
01:09:33,295 --> 01:09:36,190
the examples were, were at least as happy or sad thing worked.

1442
01:09:36,190 --> 01:09:39,295
And then, uh, a couple of other sort of adjective questions that we,

1443
01:09:39,295 --> 01:09:40,780
we tried but, um,

1444
01:09:40,780 --> 01:09:43,690
what I'm- what I would be most excited about is eventually actually

1445
01:09:43,690 --> 01:09:47,755
trying to have a zero-shot classification task,

1446
01:09:47,755 --> 01:09:49,675
uh, that combines the different tasks too.

1447
01:09:49,675 --> 01:09:52,540
So, uh, unfortunately, there's no data set for that,

1448
01:09:52,540 --> 01:09:54,460
so we didn't train it, so it doesn't happen with the model.

1449
01:09:54,460 --> 01:09:57,729
But in theory, if you ask what is the sum- you can summarize,

1450
01:09:57,729 --> 01:09:59,995
and you can translate from English into German,

1451
01:09:59,995 --> 01:10:02,515
why couldn't you ask the model for a German summary?

1452
01:10:02,515 --> 01:10:04,240
And if that worked, eventually,

1453
01:10:04,240 --> 01:10:05,650
that would be even more amazing,

1454
01:10:05,650 --> 01:10:07,390
but it, it doesn't work right now,

1455
01:10:07,390 --> 01:10:09,190
because we never ask it sort of for these

1456
01:10:09,190 --> 01:10:12,310
compositional task- these compositional task questions.

1457
01:10:12,310 --> 01:10:15,490
But is yet another interesting line of research that I think could spawn from this.

1458
01:10:15,490 --> 01:10:16,675
Uh, all right.

1459
01:10:16,675 --> 01:10:19,150
So, I hope I could show you that this sort of

1460
01:10:19,150 --> 01:10:24,130
decaNLP framework is an interesting new benchmark for generalized NLP.

1461
01:10:24,130 --> 01:10:27,160
Uh, I do think it's a reasonably good framework

1462
01:10:27,160 --> 01:10:30,310
for tackling a bunch of the really hard questions in the field.

1463
01:10:30,310 --> 01:10:32,259
Uh, more general language understanding,

1464
01:10:32,259 --> 01:10:33,550
and question answering of course,

1465
01:10:33,550 --> 01:10:37,180
uh, multitask learning, domain adaptation, uh,

1466
01:10:37,180 --> 01:10:39,790
which we sort of analyzed a little bit with the sentiment,

1467
01:10:39,790 --> 01:10:41,815
and SNLI versus multi NLI,

1468
01:10:41,815 --> 01:10:44,710
um, transfer learning, and then weight sharing.

1469
01:10:44,710 --> 01:10:46,780
I think it's clear, everybody loves weight sharing,

1470
01:10:46,780 --> 01:10:48,850
you wanna share as many weights as possible.

1471
01:10:48,850 --> 01:10:52,375
Uh, word vector started at, uh, ELMo,

1472
01:10:52,375 --> 01:10:55,300
CoVe, and now BERT basically share more and more,

1473
01:10:55,300 --> 01:10:56,545
deeper and deeper layers.

1474
01:10:56,545 --> 01:10:59,560
It would be great if we can unify that last bit also, uh,

1475
01:10:59,560 --> 01:11:02,575
and then share basically the entirety of the networks,

1476
01:11:02,575 --> 01:11:05,200
and then eventually hopefully get to zero-shot learning.

1477
01:11:05,200 --> 01:11:07,330
Now, there's a bunch of related work.

1478
01:11:07,330 --> 01:11:09,220
The original paper has over 100,

1479
01:11:09,220 --> 01:11:11,725
um, citations in it, uh, of,

1480
01:11:11,725 --> 01:11:13,525
of, you know, papers to other,

1481
01:11:13,525 --> 01:11:16,405
other, um, lines of, uh, work.

1482
01:11:16,405 --> 01:11:18,490
But, uh, this is actually zero- at least some of

1483
01:11:18,490 --> 01:11:21,670
the models and papers that influenced us the most,

1484
01:11:21,670 --> 01:11:23,920
uh, in, in our thinking and modelling.

1485
01:11:23,920 --> 01:11:25,465
Uh, one of them actually comes from,

1486
01:11:25,465 --> 01:11:27,550
uh, the two instructors of the class.

1487
01:11:27,550 --> 01:11:31,165
And so, um, hopefully, uh, we can,

1488
01:11:31,165 --> 01:11:35,050
you know, sort of think about what- what's next after all this architecture engineering.

1489
01:11:35,050 --> 01:11:38,125
And, uh, I think one potential answer to that, uh,

1490
01:11:38,125 --> 01:11:42,400
is single multitask learning for more generalized NLP models.

1491
01:11:42,400 --> 01:11:53,620
[NOISE] All right. Thank you. [APPLAUSE]

