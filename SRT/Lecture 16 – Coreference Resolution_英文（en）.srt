1
00:00:04,790 --> 00:00:09,660
Hi everybody, time to get started.

2
00:00:09,660 --> 00:00:18,755
Okay. Um, so, so today what we're gonna talk about is a topic that's, um,

3
00:00:18,755 --> 00:00:23,745
coreference resolution and I'll explain in just a minute what that is,

4
00:00:23,745 --> 00:00:27,060
um, but before getting on to that just a,

5
00:00:27,060 --> 00:00:29,565
uh, couple of words on the announcements.

6
00:00:29,565 --> 00:00:35,770
Um, so the TAs are feverishly working on getting homework five grades worked out,

7
00:00:35,770 --> 00:00:39,170
so we hope that we can deliver those to you, um,

8
00:00:39,170 --> 00:00:41,600
tomorrow just in case you're anxious to know

9
00:00:41,600 --> 00:00:44,750
them before you make your final decisions about things.

10
00:00:44,750 --> 00:00:48,290
And then, the other thing that you should be remembering

11
00:00:48,290 --> 00:00:52,580
is that the milestone for the final project is this Tuesday.

12
00:00:52,580 --> 00:00:55,780
Now, I will confess that even to me it seems like,

13
00:00:55,780 --> 00:00:58,925
"Boy, boy this milestone came around really quickly."

14
00:00:58,925 --> 00:01:01,680
So you probably feel that doubly, I realize.

15
00:01:01,680 --> 00:01:06,530
And so you know, I do apologize for that a little bit,

16
00:01:06,530 --> 00:01:11,435
but you know, really our hope was that we could actually use this to be helpful,

17
00:01:11,435 --> 00:01:15,050
and to give you feedback on what you're doing and suggestions,

18
00:01:15,050 --> 00:01:17,120
and it just really seemed like, well,

19
00:01:17,120 --> 00:01:19,880
the only chance in which we can kind of, um,

20
00:01:19,880 --> 00:01:23,900
turn around giving more feedback on the projects, um,

21
00:01:23,900 --> 00:01:28,825
before it goes into the final week of the quarter is if we can kind of get stuff,

22
00:01:28,825 --> 00:01:30,710
um, Tuesday, and hope to be then,

23
00:01:30,710 --> 00:01:32,810
sort of turning it around again by the end of the week.

24
00:01:32,810 --> 00:01:36,040
So the hope is to help you not to just,

25
00:01:36,040 --> 00:01:39,685
um, create obstacles and roadblocks in your life.

26
00:01:39,685 --> 00:01:44,420
Okay. So today what we're gonna do, um, is, uh,

27
00:01:44,420 --> 00:01:47,750
learn more about a linguistic topic for a change and learn

28
00:01:47,750 --> 00:01:51,920
some more stuff about what goes on in coreference resolution.

29
00:01:51,920 --> 00:01:54,185
So first of all, I'm gonna talk about the task,

30
00:01:54,185 --> 00:01:57,380
and then go on to some of the kinds of models that people,

31
00:01:57,380 --> 00:01:59,990
um, do for coreference resolution.

32
00:01:59,990 --> 00:02:03,035
So first of all, what is it?

33
00:02:03,035 --> 00:02:08,850
Um, so the idea of coreference resolution is what we do, which we have a text,

34
00:02:08,850 --> 00:02:13,175
"Barack Obama nominated Hillary Rodham Clinton as his Secretary of State on

35
00:02:13,175 --> 00:02:18,230
Monday," and this text like most texts are about entities,

36
00:02:18,230 --> 00:02:21,200
where entities are commonly human beings,

37
00:02:21,200 --> 00:02:26,000
but they can be other things like God saw talking giraffes or whatever it is.

38
00:02:26,000 --> 00:02:28,580
So it seems like we want to make,

39
00:02:28,580 --> 00:02:32,225
find where entities are mentioned.

40
00:02:32,225 --> 00:02:34,040
So my entities are mentioned,

41
00:02:34,040 --> 00:02:35,990
they're referred to as mentions.

42
00:02:35,990 --> 00:02:39,500
So things like Barack Obama and Secretary of State,

43
00:02:39,500 --> 00:02:43,265
he, her, they are mentions of entities.

44
00:02:43,265 --> 00:02:46,990
And then, when we talk about coreference resolution,

45
00:02:46,990 --> 00:02:50,270
the task that we're wanting to do is say,

46
00:02:50,270 --> 00:02:54,680
which of these mentions refer to the same entity,

47
00:02:54,680 --> 00:02:57,170
the same real thing in the world.

48
00:02:57,170 --> 00:03:02,330
So well, one entity that's mentioned in this text is Barack Obama,

49
00:03:02,330 --> 00:03:06,919
and then he's referred to later in the text as his and he,

50
00:03:06,919 --> 00:03:12,425
and so these three red noun phrases are all coreferent to each other.

51
00:03:12,425 --> 00:03:17,270
And that then, refers to this real-world entity.

52
00:03:17,270 --> 00:03:21,110
Um, and then, we have these references Hillary Rodham Clinton,

53
00:03:21,110 --> 00:03:22,700
Secretary of State, her,

54
00:03:22,700 --> 00:03:28,445
she, First Lady, they're all references to a different entity.

55
00:03:28,445 --> 00:03:31,520
And so they all refer to this person.

56
00:03:31,520 --> 00:03:34,390
And so those are examples of our coreference.

57
00:03:34,390 --> 00:03:40,745
Um, in a way this is triv- sort of seems obvious to a human being,

58
00:03:40,745 --> 00:03:42,635
um, looking at things, um,

59
00:03:42,635 --> 00:03:45,590
but it can actually be kind of tricky and hard.

60
00:03:45,590 --> 00:03:50,480
Um, so, um, I thought we could spend a few minutes doing

61
00:03:50,480 --> 00:03:55,655
interactive working out coreferents together so that you guys can,

62
00:03:55,655 --> 00:03:57,980
um, think about it all for a few minutes.

63
00:03:57,980 --> 00:04:01,010
Um, so here's part of a little story.

64
00:04:01,010 --> 00:04:04,270
Um, it's a story by Shruthi Rao called The Star.

65
00:04:04,270 --> 00:04:08,089
Um, now, I confess that since this is a CS class,

66
00:04:08,089 --> 00:04:10,250
um, not a literature class,

67
00:04:10,250 --> 00:04:11,840
I did a little bit of, um,

68
00:04:11,840 --> 00:04:15,049
helpful editing of this text to make it shorter,

69
00:04:15,049 --> 00:04:16,630
so I could fit more of,

70
00:04:16,630 --> 00:04:18,800
what was going on, um,

71
00:04:18,800 --> 00:04:22,145
onto the page, um, but, um,

72
00:04:22,145 --> 00:04:24,439
everything that is a sort of a linguistic

73
00:04:24,439 --> 00:04:27,300
[inaudible] is something that comes from the original text.

74
00:04:27,300 --> 00:04:31,605
Okay. So, um, in this text,

75
00:04:31,605 --> 00:04:36,400
um, who is the first entity that's mentioned?

76
00:04:37,970 --> 00:04:41,100
Vanaja, okay.

77
00:04:41,100 --> 00:04:42,570
Okay. So it's Vanaja.

78
00:04:42,570 --> 00:04:45,255
Now, where, let's do it forward.

79
00:04:45,255 --> 00:04:51,040
Where else is Vanaja mentioned in this text?

80
00:04:52,580 --> 00:04:54,720
Her son, right?

81
00:04:54,720 --> 00:04:56,460
So this her not the son,

82
00:04:56,460 --> 00:05:00,400
but this her is a reference of Vanaja, right?

83
00:05:05,120 --> 00:05:08,520
Um, she resigned.

84
00:05:08,520 --> 00:05:12,090
Okay. After that?

85
00:05:12,090 --> 00:05:15,810
She bought.

86
00:05:15,810 --> 00:05:17,535
Okay. So there's another she.

87
00:05:17,535 --> 00:05:20,205
Was there another reference before that?

88
00:05:20,205 --> 00:05:24,750
Herself, right? So herself is also a reference to Vanaja.

89
00:05:25,280 --> 00:05:27,770
Um, okay. So then, it's again,

90
00:05:27,770 --> 00:05:32,750
she made this, she, okay.

91
00:05:32,750 --> 00:05:33,980
So we've done Vanaja.

92
00:05:33,980 --> 00:05:36,140
Okay, that's a good start.

93
00:05:36,140 --> 00:05:40,270
Okay. So then, um, we've got Akhila.

94
00:05:40,270 --> 00:05:45,495
Okay. Um, where's Akhila next referred to?

95
00:05:45,495 --> 00:05:48,450
As Akhila. Okay, there we go.

96
00:05:48,450 --> 00:05:54,940
Um, are there other references, um, to Akhila?

97
00:05:58,280 --> 00:06:04,390
Maybe not. Okay. What's the next entity that's mentioned?

98
00:06:07,820 --> 00:06:10,200
Prajwal.

99
00:06:10,200 --> 00:06:15,150
Okay. So what other references are there to Prajwal?

100
00:06:19,060 --> 00:06:20,180
They.

101
00:06:20,180 --> 00:06:24,330
They? Okay. So here's a tricky one, right?

102
00:06:24,330 --> 00:06:26,430
So this they, I mean,

103
00:06:26,430 --> 00:06:29,020
who does that refer to?

104
00:06:30,440 --> 00:06:35,790
It occ- refers to Prajwal and Akash.

105
00:06:35,790 --> 00:06:40,500
Yeah, so this they refers both to Prajwal and this Akash.

106
00:06:40,500 --> 00:06:44,180
So that's, that's something that happens in human languages.

107
00:06:44,180 --> 00:06:46,790
This is referred to as split antecedents,

108
00:06:46,790 --> 00:06:49,220
where you have one thing that they,

109
00:06:49,220 --> 00:06:54,245
that's sort of referring to two distributed things that came before it.

110
00:06:54,245 --> 00:07:01,085
Um, so here's one of my first sad admissions of natural language processing technology.

111
00:07:01,085 --> 00:07:04,940
None of the NLP systems that we're gonna talk about later

112
00:07:04,940 --> 00:07:09,920
today or in general that have been built deal with split antecedents.

113
00:07:09,920 --> 00:07:13,790
They automatically lose as soon as there's split antecedents.

114
00:07:13,790 --> 00:07:15,435
Um, so that's a bit sad,

115
00:07:15,435 --> 00:07:17,300
um, but that's the state of technology.

116
00:07:17,300 --> 00:07:18,710
So it's something, um,

117
00:07:18,710 --> 00:07:20,390
we could still work to improve,

118
00:07:20,390 --> 00:07:26,030
but okay there's this sort of they that's kind of half Prajwal. Um, okay.

119
00:07:26,030 --> 00:07:29,195
So there's directly Prajwal here,

120
00:07:29,195 --> 00:07:36,580
but was there another place early in the text that Prajwal is effectively mentioned?

121
00:07:40,190 --> 00:07:46,565
Yeah. So Akhila's son is really another mention of Prajwal, right?

122
00:07:46,565 --> 00:07:52,590
Okay. Um, okay.

123
00:07:52,590 --> 00:07:56,130
Um, any other mentions of Prajwal? Maybe not.

124
00:07:56,130 --> 00:07:57,765
Okay. Then we go on.

125
00:07:57,765 --> 00:08:00,315
Okay. Who's the next entity?

126
00:08:00,315 --> 00:08:04,305
Akash. So we have Akash here,

127
00:08:04,305 --> 00:08:05,700
and that then again,

128
00:08:05,700 --> 00:08:09,150
we have that her son referring to Akash.

129
00:08:09,150 --> 00:08:12,435
Um, and here was Akash.

130
00:08:12,435 --> 00:08:17,530
Okay. What other, what other mentions of Akash are there?

131
00:08:20,450 --> 00:08:29,010
Okay so there's another Akash here, um, fourth him.

132
00:08:29,010 --> 00:08:35,340
Okay. Uh, there's another Akash.

133
00:08:35,340 --> 00:08:42,555
Okay, um, but, so, um, here.

134
00:08:42,555 --> 00:08:45,330
Okay. So are the obvious Akash's.

135
00:08:45,330 --> 00:08:47,760
There's sort of a tricky case here which

136
00:08:47,760 --> 00:08:50,280
you could wonder what the right treatment of this, right?

137
00:08:50,280 --> 00:08:55,800
You know, it's sort of says Akash was to be a tree, all right.

138
00:08:55,800 --> 00:09:00,195
So in some sense the tree is Akash.

139
00:09:00,195 --> 00:09:05,430
Um, so really in terms of reference in this story,

140
00:09:05,430 --> 00:09:09,990
the reference of the tree is the same as Akash.

141
00:09:09,990 --> 00:09:12,795
And you could think, um,

142
00:09:12,795 --> 00:09:16,440
that means you should treat the instances of,

143
00:09:16,440 --> 00:09:18,645
um, the tree, the,

144
00:09:18,645 --> 00:09:20,925
the instances here of the tree,

145
00:09:20,925 --> 00:09:25,500
and later on when the nicest tree right that really,

146
00:09:25,500 --> 00:09:28,215
that's sort of this Akash as well.

147
00:09:28,215 --> 00:09:30,255
That doesn't quite feel right,

148
00:09:30,255 --> 00:09:33,030
but this is something that comes up in coreference, right?

149
00:09:33,030 --> 00:09:38,145
So here we have a sort of a predictive construction um,

150
00:09:38,145 --> 00:09:39,770
with, you know, B.

151
00:09:39,770 --> 00:09:41,810
And when you set,

152
00:09:41,810 --> 00:09:46,760
when you have sentences such as like, um, you know,

153
00:09:46,760 --> 00:09:52,270
my child is the smartest kid in the class or something like that, in some sense,

154
00:09:52,270 --> 00:09:54,960
you're sort of saying that the smartest kid in

155
00:09:54,960 --> 00:09:58,560
the class has the same reference as my child.

156
00:09:58,560 --> 00:10:04,635
And some systems count links over that kind of predication,

157
00:10:04,635 --> 00:10:07,215
and say that is coreference whereas

158
00:10:07,215 --> 00:10:10,890
other ones don't and think that that's not quite reasonable.

159
00:10:10,890 --> 00:10:12,780
So different things go on.

160
00:10:12,780 --> 00:10:14,985
Okay. So, um, those,

161
00:10:14,985 --> 00:10:17,925
those are fair number of entities.

162
00:10:17,925 --> 00:10:23,025
I mean, so there are obviously lots of other things that are mentioned,

163
00:10:23,025 --> 00:10:25,440
um that sort of, um, right?

164
00:10:25,440 --> 00:10:27,660
So there's the local park, right,

165
00:10:27,660 --> 00:10:30,450
that's a mention of some entity.

166
00:10:30,450 --> 00:10:36,360
Um, there's, um, the school, um right?

167
00:10:36,360 --> 00:10:44,475
So there's this school here and so that the school is coreferent with pre,

168
00:10:44,475 --> 00:10:47,370
the preschool right here, right?

169
00:10:47,370 --> 00:10:49,380
Um, and then there's,

170
00:10:49,380 --> 00:10:52,155
um, again this sort of tricky one,

171
00:10:52,155 --> 00:10:56,090
of how to treat the naughty child Lord Krishna because,

172
00:10:56,090 --> 00:10:59,525
you know, in some sense Prajwal is representing that.

173
00:10:59,525 --> 00:11:02,510
And then there are lots of other entities that are mentioned, right?

174
00:11:02,510 --> 00:11:05,195
There's a t-shirt, and there's trousers,

175
00:11:05,195 --> 00:11:08,355
um, and, um, things like that.

176
00:11:08,355 --> 00:11:12,060
Another tricky thing that turns up here when you get later on into

177
00:11:12,060 --> 00:11:16,950
the story is you can have entities that have parts.

178
00:11:16,950 --> 00:11:19,425
So we not only have a tree,

179
00:11:19,425 --> 00:11:21,840
but that tree then has a lot of parts, right?

180
00:11:21,840 --> 00:11:23,370
So the tree has a trunk,

181
00:11:23,370 --> 00:11:25,169
and the tree has foliage,

182
00:11:25,169 --> 00:11:28,275
um, and things like that.

183
00:11:28,275 --> 00:11:31,170
And there are these red balls that are representing fruits, right?

184
00:11:31,170 --> 00:11:35,400
So there's a lot of stuff that's somehow connected together and somehow separate.

185
00:11:35,400 --> 00:11:39,330
And that sort of, that doesn't fit terribly well with the kind of models we

186
00:11:39,330 --> 00:11:43,425
use with coreference either because really we make our coreference,

187
00:11:43,425 --> 00:11:48,015
um, reference models basically out of this notion of entities.

188
00:11:48,015 --> 00:11:50,325
Um, but somehow there's this complexity that,

189
00:11:50,325 --> 00:11:52,290
you know, human beings have parts too, right?

190
00:11:52,290 --> 00:11:53,744
We have hands and faces,

191
00:11:53,744 --> 00:11:56,145
and we can't say, oh, that's a separate entity,

192
00:11:56,145 --> 00:11:59,895
but they're somehow in, um, involved with the other entity.

193
00:11:59,895 --> 00:12:04,740
Okay. Um, hope that's sort of useful to give some idea.

194
00:12:04,740 --> 00:12:07,665
Why is coreference resolution useful?

195
00:12:07,665 --> 00:12:11,895
Um, so there are all kinds of things that we'd like to do well

196
00:12:11,895 --> 00:12:16,379
in natural language processing that you really can't do well unless,

197
00:12:16,379 --> 00:12:19,845
uh, you know how to do coreference resolution.

198
00:12:19,845 --> 00:12:25,410
So anything that we want to do in terms of question-answering, summarization,

199
00:12:25,410 --> 00:12:28,500
extracting facts from texts or anything like that,

200
00:12:28,500 --> 00:12:32,985
there are places we are gonna fail unless we can do coreference resolution.

201
00:12:32,985 --> 00:12:35,025
Because if we're reading a piece of text,

202
00:12:35,025 --> 00:12:38,640
and it says he was born in 1961, um,

203
00:12:38,640 --> 00:12:41,610
we can get a fact out or answer a question,

204
00:12:41,610 --> 00:12:43,845
if we can work out who he was,

205
00:12:43,845 --> 00:12:46,780
but we probably can't otherwise.

206
00:12:49,040 --> 00:12:53,580
Um, there are, there's sort of another place that where

207
00:12:53,580 --> 00:12:57,480
this is very useful is in machine translation,

208
00:12:57,480 --> 00:13:02,130
so that lots of languages drop pronouns.

209
00:13:02,130 --> 00:13:04,935
So you don't have to give explicit pronouns,

210
00:13:04,935 --> 00:13:09,105
but you need to be able to work out how to fill them in.

211
00:13:09,105 --> 00:13:12,765
And this is making coreference decisions about,

212
00:13:12,765 --> 00:13:15,075
um, arguments of verbs.

213
00:13:15,075 --> 00:13:18,030
And so here are a couple of examples,

214
00:13:18,030 --> 00:13:22,545
um, that, um, covering from Spanish to English.

215
00:13:22,545 --> 00:13:27,660
So in Spanish, you can freely drop the subjects of verbs and in these sentences,

216
00:13:27,660 --> 00:13:29,520
in the because clause,

217
00:13:29,520 --> 00:13:31,725
there's no overt subject.

218
00:13:31,725 --> 00:13:35,625
And so he gets Alicia likes Juan because he's smart.

219
00:13:35,625 --> 00:13:40,620
And so Google Translate is stuck in a he and that is right.

220
00:13:40,620 --> 00:13:42,600
And to stick in that he,

221
00:13:42,600 --> 00:13:47,700
it's implicitly making a coreference decision and saying, "Okay well,

222
00:13:47,700 --> 00:13:49,605
the subject of this, um,

223
00:13:49,605 --> 00:13:54,224
adjective smart should be Juan who's male,

224
00:13:54,224 --> 00:13:56,985
and therefore, I should say he."

225
00:13:56,985 --> 00:14:00,060
But, you know, the reality is Google Translate knows

226
00:14:00,060 --> 00:14:03,615
nothing about coreference and making these coreference decisions.

227
00:14:03,615 --> 00:14:05,280
And as has been um,

228
00:14:05,280 --> 00:14:10,365
covered quite a bit in the media now and I think came up earlier in an earlier class,

229
00:14:10,365 --> 00:14:15,240
that, um, Google Translate mainly just defaults to male default.

230
00:14:15,240 --> 00:14:18,075
Um, so if you sort of swap- sweep it, uh,

231
00:14:18,075 --> 00:14:19,770
if you flip it around and say,

232
00:14:19,770 --> 00:14:23,370
Juan likes Alicia, it also says because he's smart.

233
00:14:23,370 --> 00:14:28,005
Uh, whereas probably it should be because she's smart in that case.

234
00:14:28,005 --> 00:14:31,755
And indeed you notice the bad effects of that everywhere.

235
00:14:31,755 --> 00:14:36,225
So many languages, um, Turkish, Indonesian, um,

236
00:14:36,225 --> 00:14:38,070
don't actually have gender,

237
00:14:38,070 --> 00:14:41,910
so that they're much less sexist languages than English,

238
00:14:41,910 --> 00:14:43,350
French or Germany is.

239
00:14:43,350 --> 00:14:45,495
But what happens, um,

240
00:14:45,495 --> 00:14:48,060
when you then translate where you just have

241
00:14:48,060 --> 00:14:52,275
a generic pronoun that means third person pronoun, um,

242
00:14:52,275 --> 00:14:56,460
that Google Translate is essentially using its language model,

243
00:14:56,460 --> 00:14:59,055
which means that reconstructs, um,

244
00:14:59,055 --> 00:15:01,635
the worst of stereotypes of she is a cook,

245
00:15:01,635 --> 00:15:03,855
and he is an engineer, he is a doctor.

246
00:15:03,855 --> 00:15:07,500
And well, in a connected piece of this course,

247
00:15:07,500 --> 00:15:10,905
if you'd like Google Translate to be able to do better than that,

248
00:15:10,905 --> 00:15:14,160
well again, what would be required is that you could actually do

249
00:15:14,160 --> 00:15:19,755
coreference resolution and track along the actors in the text as you go along.

250
00:15:19,755 --> 00:15:23,970
Um, one final example we haven't really talked about yet,

251
00:15:23,970 --> 00:15:27,480
but we'll get back to soon now because the class is almost over

252
00:15:27,480 --> 00:15:31,440
is doing things with dialogue agents or chat systems.

253
00:15:31,440 --> 00:15:36,240
That, as soon as you are going to do anything more than a single turn,

254
00:15:36,240 --> 00:15:39,840
um, dialog, that you need to start dealing with reference.

255
00:15:39,840 --> 00:15:41,655
So if you've got something like, um,

256
00:15:41,655 --> 00:15:44,115
booked tickets to see James Bond,

257
00:15:44,115 --> 00:15:46,200
um, then you want to say something like,

258
00:15:46,200 --> 00:15:49,050
"Spectre is playing near you at 2:00 and 3:00 today.

259
00:15:49,050 --> 00:15:51,030
How many tickets would you like?"

260
00:15:51,030 --> 00:15:54,270
Um, two tickets for the showing at three.

261
00:15:54,270 --> 00:15:56,265
That as shown in the color,

262
00:15:56,265 --> 00:16:02,340
there are various kinds of reference going on here where things have related reference,

263
00:16:02,340 --> 00:16:04,680
but it's kind of complicated here.

264
00:16:04,680 --> 00:16:07,470
And this is something that we'll come back to in a moment.

265
00:16:07,470 --> 00:16:13,035
So James Bond and Spectre aren't obviously the same thing,

266
00:16:13,035 --> 00:16:16,530
but in a context like, um, booking movies,

267
00:16:16,530 --> 00:16:23,460
they are the same thing because one is the name of a character in a movie series,

268
00:16:23,460 --> 00:16:28,530
and the other is the name of a movie that's currently showing that belongs to that,

269
00:16:28,530 --> 00:16:30,960
so that they're sort of associated, um,

270
00:16:30,960 --> 00:16:33,900
in a sort of subtle way that isn't exact identity,

271
00:16:33,900 --> 00:16:37,125
but is relevant to a lot of the things that we want to do.

272
00:16:37,125 --> 00:16:39,480
I'll come back to that in a little bit when we

273
00:16:39,480 --> 00:16:41,655
talk a bit more about the linguistics of this.

274
00:16:41,655 --> 00:16:45,760
Okay. So if we want to do the task of coreference resolution,

275
00:16:45,760 --> 00:16:47,660
there are essentially two steps.

276
00:16:47,660 --> 00:16:51,425
So the first step is gee, we want to work out

277
00:16:51,425 --> 00:16:55,835
what mentions there are in the text that we should be doing something with.

278
00:16:55,835 --> 00:16:58,640
And this one is effectively pretty easy,

279
00:16:58,640 --> 00:17:01,430
but I'll have just a few slides on that immediately.

280
00:17:01,430 --> 00:17:04,835
And then what the bulk of the class is gonna be on is,

281
00:17:04,835 --> 00:17:08,810
um, working out coreference between mentions.

282
00:17:08,810 --> 00:17:10,400
And if you think about this,

283
00:17:10,400 --> 00:17:13,355
coreference is essentially a clustering task.

284
00:17:13,355 --> 00:17:15,275
Because if you do the first task,

285
00:17:15,275 --> 00:17:18,770
you have a set of mentions and then you want to be saying well,

286
00:17:18,770 --> 00:17:22,710
how can I group these into clusters that have the same reference?

287
00:17:22,710 --> 00:17:25,750
And so that's what we're going to look more at doing.

288
00:17:25,750 --> 00:17:28,080
So quickly on mention detection.

289
00:17:28,080 --> 00:17:29,730
So, um, for mention,

290
00:17:29,730 --> 00:17:34,085
we wanna find all the spans that are candidates for,

291
00:17:34,085 --> 00:17:36,110
um, referring to some entity.

292
00:17:36,110 --> 00:17:38,725
And the answer to what these, um,

293
00:17:38,725 --> 00:17:43,620
candidates are is basically they're all the noun phrases in the text.

294
00:17:43,620 --> 00:17:48,125
And so normally people think of there being three types of mentions that we identify.

295
00:17:48,125 --> 00:17:49,730
There are pronouns, I,

296
00:17:49,730 --> 00:17:50,990
you, he, she, it,

297
00:17:50,990 --> 00:17:52,790
etc., that are, um,

298
00:17:52,790 --> 00:17:54,605
referring to different entities.

299
00:17:54,605 --> 00:17:57,170
They're explicit names of people like that was

300
00:17:57,170 --> 00:17:59,960
that Barack Obama and Hillary Clinton examples.

301
00:17:59,960 --> 00:18:02,210
And then many of the tricky examples,

302
00:18:02,210 --> 00:18:04,685
and then when we have common noun phrases

303
00:18:04,685 --> 00:18:07,670
like a dog or the big fluffy cat stuck in the tree.

304
00:18:07,670 --> 00:18:11,690
That the big fluffy cat stuck in the tree is a mention.

305
00:18:11,690 --> 00:18:14,405
Um, it's actually a complex mention because it

306
00:18:14,405 --> 00:18:17,975
also has embedded inside it other mentions.

307
00:18:17,975 --> 00:18:22,230
Um, so the tree is also a mention.

308
00:18:22,500 --> 00:18:26,560
Okay. So how can we detect mentions?

309
00:18:26,560 --> 00:18:28,180
Well, one answer is to say,

310
00:18:28,180 --> 00:18:30,055
well we've looked at, um,

311
00:18:30,055 --> 00:18:32,740
various other NLP systems on and off.

312
00:18:32,740 --> 00:18:39,250
And we can just use those NLP systems as preprocessing systems to find mentions.

313
00:18:39,250 --> 00:18:43,900
So for pronouns, they're part of speech taggers that say what's a noun,

314
00:18:43,900 --> 00:18:45,475
or a verb, or a pronoun,

315
00:18:45,475 --> 00:18:48,910
and so we can run those and find all the pronouns and we're done.

316
00:18:48,910 --> 00:18:52,180
From- for, um, the names of things like Barack Obama.

317
00:18:52,180 --> 00:18:54,970
We've talked a couple of times about named entity recognizers,

318
00:18:54,970 --> 00:18:57,730
so we can run those and find all the named entities.

319
00:18:57,730 --> 00:19:00,475
Um, then for common noun phrases,

320
00:19:00,475 --> 00:19:03,400
that's sort of where we need parsers to find

321
00:19:03,400 --> 00:19:06,925
the structure of the sentence and find where the noun phrases are.

322
00:19:06,925 --> 00:19:09,745
And we have talked about dependency parsers and well,

323
00:19:09,745 --> 00:19:14,380
one choice is you can use a dependency parser to find the sort of nominal arguments,

324
00:19:14,380 --> 00:19:15,670
and work with them.

325
00:19:15,670 --> 00:19:19,180
That's sort of actually a little bit subtler than just sort of wanting to pick

326
00:19:19,180 --> 00:19:22,525
out spans that refer to common noun phrases.

327
00:19:22,525 --> 00:19:25,855
So the other notion of parsing which we come back to,

328
00:19:25,855 --> 00:19:28,150
um, next week is constituency parsing.

329
00:19:28,150 --> 00:19:30,280
In some sense, constituency parsers are

330
00:19:30,280 --> 00:19:34,465
the simplest way to find mentions for this process.

331
00:19:34,465 --> 00:19:39,310
Um, most of it seems and is easy,

332
00:19:39,310 --> 00:19:44,200
um, there are sort of tricky cases as to what counts as a mention or not.

333
00:19:44,200 --> 00:19:47,785
So, um, if it's kind of it is sunny,

334
00:19:47,785 --> 00:19:49,975
I mean, is it a mention of something?

335
00:19:49,975 --> 00:19:51,880
It's sort of seems like it's not really,

336
00:19:51,880 --> 00:19:55,870
it's just it seems like it's an it that you stick at the start of the sentence,

337
00:19:55,870 --> 00:19:57,340
um, that doesn't mean anything.

338
00:19:57,340 --> 00:19:59,215
So that's maybe not a mention.

339
00:19:59,215 --> 00:20:00,730
Um, every student.

340
00:20:00,730 --> 00:20:02,695
Is every student a mention?

341
00:20:02,695 --> 00:20:07,810
I mean, it's certainly, at best it's some kind of collective,

342
00:20:07,810 --> 00:20:11,965
um, but it's not sort of a very clear concrete reference, um.

343
00:20:11,965 --> 00:20:15,520
That goes further, if I sort of use different quantifiers,

344
00:20:15,520 --> 00:20:17,920
so if it was like, every and no are called quantifiers.

345
00:20:17,920 --> 00:20:21,070
I mean no student definitely doesn't have reference,

346
00:20:21,070 --> 00:20:23,560
because it's not pointing at anything, right?

347
00:20:23,560 --> 00:20:25,990
It's asserting a claim of nonexistence.

348
00:20:25,990 --> 00:20:28,015
So that there's definitely, um,

349
00:20:28,015 --> 00:20:31,090
no- it isn't a mention of anything.

350
00:20:31,090 --> 00:20:34,420
Um, yeah, the best donut in the world.

351
00:20:34,420 --> 00:20:37,675
Um, does that have reference?

352
00:20:37,675 --> 00:20:40,125
Um, that's unclear.

353
00:20:40,125 --> 00:20:44,460
This is the kind of thing that actual philosophers of language debate over, right?

354
00:20:44,460 --> 00:20:48,315
So if there was agreement on what the best donut in the world is,

355
00:20:48,315 --> 00:20:50,535
then maybe it has reference, um,

356
00:20:50,535 --> 00:20:52,815
but I can say sentences like,

357
00:20:52,815 --> 00:20:56,130
I'm searching everywhere to find the best donut in the world.

358
00:20:56,130 --> 00:20:57,960
And then in that sentence,

359
00:20:57,960 --> 00:20:59,340
it doesn't have any reference, right?

360
00:20:59,340 --> 00:21:03,975
It's sort of an intentional description of what I'm hoping to find,

361
00:21:03,975 --> 00:21:06,995
that there's no concrete thing it refers to.

362
00:21:06,995 --> 00:21:11,605
Um, things like quantities, 100 miles.

363
00:21:11,605 --> 00:21:14,170
That sort of behaves like a noun phrase,

364
00:21:14,170 --> 00:21:17,995
but it is in- it's sort of really a quantity that doesn't really have reference.

365
00:21:17,995 --> 00:21:23,200
Um, and so then there's the question of how can you deal with this stuff?

366
00:21:23,200 --> 00:21:27,355
Um, well, um, our tool whenever we want to deal with stuff,

367
00:21:27,355 --> 00:21:30,265
is we train classifiers,

368
00:21:30,265 --> 00:21:34,150
as in they pick out things that are mentioned and things that aren't.

369
00:21:34,150 --> 00:21:38,800
And so that's something that you could do is write a classifier that filters out,

370
00:21:38,800 --> 00:21:42,940
um, these spurious things that you want to say aren't really mentions.

371
00:21:42,940 --> 00:21:44,980
And people absolutely have done that.

372
00:21:44,980 --> 00:21:47,650
But commonly actually people skip that step,

373
00:21:47,650 --> 00:21:54,295
and you just sort of instead have your mention detector find all candidate mentions.

374
00:21:54,295 --> 00:21:57,610
Because it turns out that that tends to work pretty well.

375
00:21:57,610 --> 00:22:01,510
Because after we found all of our mentions, um,

376
00:22:01,510 --> 00:22:05,815
we're then going to be doing this clustering process to find coreferent mentions.

377
00:22:05,815 --> 00:22:08,440
And if there are just a few stray mentions like

378
00:22:08,440 --> 00:22:12,655
no student and we don't cluster them wrongly with anything else,

379
00:22:12,655 --> 00:22:18,490
it kind of doesn't do any harm because we are mainly involved in this clustering process.

380
00:22:18,490 --> 00:22:23,440
Okay. Um, something you might be wondering is,

381
00:22:23,440 --> 00:22:25,090
well I've sort of implied now,

382
00:22:25,090 --> 00:22:26,605
we have a pipeline.

383
00:22:26,605 --> 00:22:29,770
I'm saying we're going to run a part of speech tagger,

384
00:22:29,770 --> 00:22:31,810
and we're going to run a named entity recognizer,

385
00:22:31,810 --> 00:22:33,265
and we're going to run a parser.

386
00:22:33,265 --> 00:22:35,455
And we're going to run a, um,

387
00:22:35,455 --> 00:22:38,185
a named mention detector.

388
00:22:38,185 --> 00:22:41,530
And then eventually, we're going to run this coref clustering system,

389
00:22:41,530 --> 00:22:44,290
so we have a sort of a five-step pipeline.

390
00:22:44,290 --> 00:22:50,950
Um, is that the only way you can do, um, coreference resolution?

391
00:22:50,950 --> 00:22:53,020
And the traditional answer was yup,

392
00:22:53,020 --> 00:22:55,030
that's the way you did coreference resolution.

393
00:22:55,030 --> 00:22:59,275
That essentially, all systems for coreference resolution,

394
00:22:59,275 --> 00:23:05,800
until approximately 2016 where a pipeline that went through about those stages.

395
00:23:05,800 --> 00:23:09,735
Um, but just recently and I will dico- cover one such system,

396
00:23:09,735 --> 00:23:12,180
um, later in the class, um,

397
00:23:12,180 --> 00:23:15,570
that people in the neural world have started doing what's been

398
00:23:15,570 --> 00:23:19,560
effective in a lot of places in the neural network world of saying,

399
00:23:19,560 --> 00:23:22,815
can we just build an end-to-end coreference system

400
00:23:22,815 --> 00:23:26,385
that starts with just plain text of a paragraph,

401
00:23:26,385 --> 00:23:33,685
and feeds out coreference clusters without there being any intervening pipeline steps?

402
00:23:33,685 --> 00:23:36,625
And I'll show you a bit more about how that works.

403
00:23:36,625 --> 00:23:40,090
Um, but before we get into systems,

404
00:23:40,090 --> 00:23:45,220
I just wanted to say a little bit more about the linguistics of coreference.

405
00:23:45,220 --> 00:23:49,570
Um, there's actually quite a lot of interesting stuff here,

406
00:23:49,570 --> 00:23:52,360
and to a fair degree,

407
00:23:52,360 --> 00:23:55,810
it's not actually stuff that's been thought about

408
00:23:55,810 --> 00:23:59,065
very much by people who build NLP systems, right?

409
00:23:59,065 --> 00:24:01,150
I already mentioned, um,

410
00:24:01,150 --> 00:24:03,505
from the Shruthi Rao story, um,

411
00:24:03,505 --> 00:24:06,250
the example of split antecedents, right?

412
00:24:06,250 --> 00:24:09,969
That that's just a clear linguistic phenomenon that happens,

413
00:24:09,969 --> 00:24:12,160
and it's not even incredibly rare, right?

414
00:24:12,160 --> 00:24:14,095
Um, that, you know, um,

415
00:24:14,095 --> 00:24:19,585
people build these simple machine learning models that just can't deal with that.

416
00:24:19,585 --> 00:24:22,630
And there's really quite a bit more structure

417
00:24:22,630 --> 00:24:25,750
to what happens in the linguistics of coreference,

418
00:24:25,750 --> 00:24:30,280
it isn't really being exploited in most of the systems people bui- build.

419
00:24:30,280 --> 00:24:33,145
So I just wanted to show people a bit more of that.

420
00:24:33,145 --> 00:24:38,200
And essentially, to sort of understanding, um,

421
00:24:38,200 --> 00:24:41,095
more about how people see things linguistically,

422
00:24:41,095 --> 00:24:46,945
there are two concepts that are related and commonly confused,

423
00:24:46,945 --> 00:24:48,370
that are really different.

424
00:24:48,370 --> 00:24:50,635
So one is coreference.

425
00:24:50,635 --> 00:24:54,700
So we say that things are coreferent when there are

426
00:24:54,700 --> 00:24:59,230
two mentions and they refer to the same entity in the world.

427
00:24:59,230 --> 00:25:00,970
So if it's sort of,

428
00:25:00,970 --> 00:25:05,140
um, Donald Trump and the current president, right?

429
00:25:05,140 --> 00:25:09,205
They're two mentions and they refer to the same person in the world.

430
00:25:09,205 --> 00:25:12,190
And so that is a relationship of coreference.

431
00:25:12,190 --> 00:25:16,345
Um, and that's then contrasted, um, with anaphora.

432
00:25:16,345 --> 00:25:24,595
And so the idea of anaphora is some terms in text don't have independent reference,

433
00:25:24,595 --> 00:25:30,835
and you work out their reference by relating them back to another thing in the text.

434
00:25:30,835 --> 00:25:32,800
So if we have the sentence,

435
00:25:32,800 --> 00:25:35,500
Barack Obama said he would sign the bill.

436
00:25:35,500 --> 00:25:37,225
He is an anaphor.

437
00:25:37,225 --> 00:25:39,430
And if I just say, he,

438
00:25:39,430 --> 00:25:41,905
what does he refer to in the abstract?

439
00:25:41,905 --> 00:25:45,775
Well, you know, apart from saying something male, right?

440
00:25:45,775 --> 00:25:47,050
You've got no idea, right?

441
00:25:47,050 --> 00:25:50,110
Because you can't work out what he means just by knowing he.

442
00:25:50,110 --> 00:25:54,370
You have to be looking at a text and interpreting it relative to the text.

443
00:25:54,370 --> 00:25:56,515
And then if you're interpreting it,

444
00:25:56,515 --> 00:25:58,795
um, relative to the text,

445
00:25:58,795 --> 00:26:00,370
you're then in this situation of,

446
00:26:00,370 --> 00:26:03,880
okay I see, this refers back to Barack Obama.

447
00:26:03,880 --> 00:26:07,600
So he is another mention of Barack Obama,

448
00:26:07,600 --> 00:26:10,915
then- and this then is this concept of anaphora.

449
00:26:10,915 --> 00:26:13,615
So the picture we have is sort of like this,

450
00:26:13,615 --> 00:26:17,635
that you can either have these independent mentions,

451
00:26:17,635 --> 00:26:19,690
which do refer, um,

452
00:26:19,690 --> 00:26:21,310
to the same thing in the world.

453
00:26:21,310 --> 00:26:22,765
They're coreferent.

454
00:26:22,765 --> 00:26:24,580
But in many cases,

455
00:26:24,580 --> 00:26:27,969
such as when they're full mentions like President Obama,

456
00:26:27,969 --> 00:26:31,990
versus Barack Obama, they don't have any textual relationship.

457
00:26:31,990 --> 00:26:35,455
It's just they happen to refer to the same thing in the world.

458
00:26:35,455 --> 00:26:40,825
And that then contrast with cases like Barack Obama said he would do something,

459
00:26:40,825 --> 00:26:45,265
where the he has a textual relationship back to Barack Obama.

460
00:26:45,265 --> 00:26:47,530
And that's an example of anaphora.

461
00:26:47,530 --> 00:26:54,985
Um, this might up until now feel like an almost meaningless distinction.

462
00:26:54,985 --> 00:26:59,425
But something that maybe gives you more of a sense that there's something useful here is,

463
00:26:59,425 --> 00:27:04,420
um, these textual relationships exist even when there isn't coreference.

464
00:27:04,420 --> 00:27:06,790
So we sort of mentioned before,

465
00:27:06,790 --> 00:27:09,430
these cases like no dancer, right?

466
00:27:09,430 --> 00:27:12,610
So no dancer doesn't have reference, right?

467
00:27:12,610 --> 00:27:14,305
It refers to nothing.

468
00:27:14,305 --> 00:27:16,810
Um, but if you have a sentence like,

469
00:27:16,810 --> 00:27:22,615
"no dancer twisted her knee," well we have an anaphor here.

470
00:27:22,615 --> 00:27:26,260
And that anaphor is referring back to "no

471
00:27:26,260 --> 00:27:30,520
dancer" despite the fact that "no dancer" doesn't have reference.

472
00:27:30,520 --> 00:27:34,150
So we can still have the anaphoric textual relationship.

473
00:27:34,150 --> 00:27:36,070
And indeed, you know,

474
00:27:36,070 --> 00:27:39,280
her knee is then a part of her.

475
00:27:39,280 --> 00:27:42,040
And so these are the sort of part relationships again.

476
00:27:42,040 --> 00:27:45,370
But her knee, in a sense that I'll just come back to,

477
00:27:45,370 --> 00:27:51,325
is also an anaphor which is interpreted with respect, um, to the dancer.

478
00:27:51,325 --> 00:27:54,370
So we have two anaphoric relationships here,

479
00:27:54,370 --> 00:27:57,250
even though we have no reference.

480
00:27:57,250 --> 00:28:00,865
There's another interesting case of

481
00:28:00,865 --> 00:28:04,795
anaphoric relationships which aren't the same as reference,

482
00:28:04,795 --> 00:28:08,380
which is you could have looser forms of anaphoric relationships.

483
00:28:08,380 --> 00:28:10,810
So you get lots of sentences like this.

484
00:28:10,810 --> 00:28:13,135
"We went to see a concert last night,

485
00:28:13,135 --> 00:28:15,250
the tickets were really expensive."

486
00:28:15,250 --> 00:28:18,925
So we have this mentioned here of the tickets.

487
00:28:18,925 --> 00:28:22,465
Um, but really to interpret the tickets,

488
00:28:22,465 --> 00:28:26,995
we have to interpret them with respect to this,

489
00:28:26,995 --> 00:28:28,735
um, mention back here,

490
00:28:28,735 --> 00:28:31,465
a concept, because really what this is saying,

491
00:28:31,465 --> 00:28:35,095
the tickets for the concert were really expensive.

492
00:28:35,095 --> 00:28:39,205
So this is also referred to as an anaphoric relationship,

493
00:28:39,205 --> 00:28:42,190
where the meaning of the tickets has to be interpreted

494
00:28:42,190 --> 00:28:46,600
textually based on another, um, noun phrase.

495
00:28:46,600 --> 00:28:49,690
But it's not a coreference relationship that

496
00:28:49,690 --> 00:28:53,335
the concert and the tickets are clearly two different entities.

497
00:28:53,335 --> 00:28:56,979
So these kinda looser cases are referred to as bridging anaphora,

498
00:28:56,979 --> 00:29:00,805
because you sort of have to supply for yourself the bridge,

499
00:29:00,805 --> 00:29:07,105
the relation that connects together the antecedent and the anaphor.

500
00:29:07,105 --> 00:29:10,780
Okay. So that's how- we then have these pictures,

501
00:29:10,780 --> 00:29:14,890
that we have this sort of not in- not complete crossovers

502
00:29:14,890 --> 00:29:19,510
between coreference and anaphora that we've sort of talked about.

503
00:29:19,510 --> 00:29:24,200
Um, I have one other note on anaphora. Um,

504
00:29:24,200 --> 00:29:28,825
Who- has anyone here ever done any Ancient Greek?

505
00:29:28,825 --> 00:29:34,020
Any Ancient Greek? [LAUGHTER] Yes.

506
00:29:34,020 --> 00:29:36,035
Okay. Um, so, um,

507
00:29:36,035 --> 00:29:40,745
from- from the origins of the words anaphora,

508
00:29:40,745 --> 00:29:47,135
anaphora is meant to be that you're finding your textual reference before you.

509
00:29:47,135 --> 00:29:53,300
Um, and so there's actually a- a complementary, um,

510
00:29:53,300 --> 00:29:56,940
term of art which is referred to as

511
00:29:56,940 --> 00:30:02,140
cataphora where you're finding your reference after you.

512
00:30:02,140 --> 00:30:05,700
Um, so here is a beautiful example of cataphora.

513
00:30:05,700 --> 00:30:07,380
So this is from Oscar Wilde's,

514
00:30:07,380 --> 00:30:09,330
The Picture of Dorian Gray.

515
00:30:09,330 --> 00:30:14,339
"From the corner of the divan of Persian saddle-bags on which he was lying,

516
00:30:14,339 --> 00:30:16,620
smoking, as was his custom,

517
00:30:16,620 --> 00:30:20,690
innumerable cigarettes, Lord Henry Wotton could just catch

518
00:30:20,690 --> 00:30:25,470
the gleam of the honey-sweet and honey-colored blossoms of a laburnum."

519
00:30:25,470 --> 00:30:28,865
Um, right. So here we have this, um, mentioned,

520
00:30:28,865 --> 00:30:33,060
Lord Henry Wotton and there are two anaphors,

521
00:30:33,060 --> 00:30:35,940
um, that refer to Lord Henry Wotton.

522
00:30:35,940 --> 00:30:39,300
Um, he and his,

523
00:30:39,300 --> 00:30:41,730
and that they both come before,

524
00:30:41,730 --> 00:30:43,995
um, Lord Henry Wotton.

525
00:30:43,995 --> 00:30:47,010
And so these are referred to, um,

526
00:30:47,010 --> 00:30:53,670
as instances of cataphora among a certain kind of classical scholar.

527
00:30:53,670 --> 00:30:56,460
Um, and in case you don't know what a laburnum is,

528
00:30:56,460 --> 00:30:58,680
um, this is a laburnum.

529
00:30:58,680 --> 00:31:01,890
[LAUGHTER] Right. But, yeah,

530
00:31:01,890 --> 00:31:03,365
so thi- this is cataphora.

531
00:31:03,365 --> 00:31:05,940
Now- now there are two sad things to say.

532
00:31:05,940 --> 00:31:09,630
Um, the first sad thing is in modern linguistics,

533
00:31:09,630 --> 00:31:12,375
the term cataphora is completely disused.

534
00:31:12,375 --> 00:31:18,185
And we mean- we just used the word um, anaphors everywhere as meaning

535
00:31:18,185 --> 00:31:21,300
a word that gets referenced from some other mention in

536
00:31:21,300 --> 00:31:24,525
the text and it doesn't matter what side it's on.

537
00:31:24,525 --> 00:31:28,560
Um, so, um, that we go downhill one stage to

538
00:31:28,560 --> 00:31:34,055
linguistics but then we get to NLP and we go downhill a second stage.

539
00:31:34,055 --> 00:31:38,160
Because what you'll see is that in general,

540
00:31:38,160 --> 00:31:40,485
the systems that people are building for,

541
00:31:40,485 --> 00:31:47,355
um, reference resolution, they don't make any distinction of direction at all.

542
00:31:47,355 --> 00:31:49,155
That once you find a mention,

543
00:31:49,155 --> 00:31:52,230
you're always looking backwards for its reference.

544
00:31:52,230 --> 00:31:54,875
Um, and you've got no idea that,

545
00:31:54,875 --> 00:31:57,630
well, maybe sometimes you could look forwards.

546
00:31:57,630 --> 00:31:59,280
So effectively, what it means,

547
00:31:59,280 --> 00:32:01,370
that the systems end up doing is saying,

548
00:32:01,370 --> 00:32:02,930
well, there's a he here,

549
00:32:02,930 --> 00:32:05,955
there are various other things, there's a his, etc.,

550
00:32:05,955 --> 00:32:09,650
and you'll eventually get to Lord Henry Wotton and you'll be able to

551
00:32:09,650 --> 00:32:13,995
be trying to find its reference by looking backwards,

552
00:32:13,995 --> 00:32:17,760
even though that's sort of ill-formed from any kind of linguistic sense

553
00:32:17,760 --> 00:32:22,305
whereas really he and his that should have been looking for their reference forward.

554
00:32:22,305 --> 00:32:29,140
Okay. Um, is everyone good up to there, any questions?

555
00:32:29,840 --> 00:32:34,110
Okay. We'll move ahead and, um,

556
00:32:34,110 --> 00:32:38,610
try and move on to kinds of coreference, um, models.

557
00:32:38,610 --> 00:32:41,715
So I wanted to, um, tell you, um,

558
00:32:41,715 --> 00:32:45,300
as much as I can and I have 45 minutes, um,

559
00:32:45,300 --> 00:32:49,055
left about, so the kinda models people build with coreference.

560
00:32:49,055 --> 00:32:53,670
And I hope to mention quickly four different ways that people have looked at coreference.

561
00:32:53,670 --> 00:32:57,800
I wanna tell you a teeny bit about classical rule-based coreference.

562
00:32:57,800 --> 00:33:02,010
Um, then, um, mention- mention pair coreference.

563
00:33:02,010 --> 00:33:04,930
Spend the most time on mention ranking systems which have

564
00:33:04,930 --> 00:33:07,995
tended to be the easiest simple systems.

565
00:33:07,995 --> 00:33:09,580
And then just say a little bit about

566
00:33:09,580 --> 00:33:12,780
clustering systems which should be the right way to do

567
00:33:12,780 --> 00:33:18,150
it but in practice has been a way that's been hard to get the best performance from.

568
00:33:18,150 --> 00:33:20,900
Okay. So here's a bit of history.

569
00:33:20,900 --> 00:33:22,980
Um, this guy here is Jerry Hobbs.

570
00:33:22,980 --> 00:33:28,320
He just had his retirement party from University of Southern California last month.

571
00:33:28,320 --> 00:33:29,670
Um, so Jerry Hobbs,

572
00:33:29,670 --> 00:33:31,815
way back when, um,

573
00:33:31,815 --> 00:33:33,660
wrote a famous paper,

574
00:33:33,660 --> 00:33:37,905
it was in 1976 on coreference resolution.

575
00:33:37,905 --> 00:33:41,520
And in that paper, um, he proposed,

576
00:33:41,520 --> 00:33:45,600
um, what's normally now referred to as the Hobbs Algorithm.

577
00:33:45,600 --> 00:33:47,895
But actually, um, in his paper,

578
00:33:47,895 --> 00:33:51,180
he refers to it as a naive algorithm.

579
00:33:51,180 --> 00:33:54,680
Um, and I'll come back to that distinction in just a moment.

580
00:33:54,680 --> 00:33:57,630
Um, but what the Hobbs algorithm was,

581
00:33:57,630 --> 00:34:01,980
is if you have a sentence- so actually I should say this,

582
00:34:01,980 --> 00:34:05,430
this algorithm is just for finding the reference of pronouns.

583
00:34:05,430 --> 00:34:08,500
So one can extend out to other cases but the part I'm gonna show

584
00:34:08,500 --> 00:34:11,760
you is just the part for doing the reference of pronouns.

585
00:34:11,760 --> 00:34:12,935
So when you find out,

586
00:34:12,935 --> 00:34:17,925
find a pronoun and you wanna say what is it, um, coreferent with?

587
00:34:17,925 --> 00:34:22,700
What you're going to do is run this mechanical algorithm

588
00:34:22,700 --> 00:34:27,945
that's looking at a parse of a sentence and is working out what to do with it.

589
00:34:27,945 --> 00:34:30,824
Begin at the NP immediately dominating the pronoun,

590
00:34:30,824 --> 00:34:35,325
go up the trees or the first NP or S. Call this X and the path p,

591
00:34:35,325 --> 00:34:37,760
traverse along, ah, it goes on and on.

592
00:34:37,760 --> 00:34:39,015
Um, there's more of it.

593
00:34:39,015 --> 00:34:40,110
That was only the beginning of it.

594
00:34:40,110 --> 00:34:41,475
There are a lot more stages.

595
00:34:41,475 --> 00:34:43,230
Um, but, you know,

596
00:34:43,230 --> 00:34:46,890
I'm not- I don't really wanna go into the details of this.

597
00:34:46,890 --> 00:34:50,070
Um, but, you know, to try and explain the flavor of it,

598
00:34:50,070 --> 00:34:51,665
here's a piece of text.

599
00:34:51,665 --> 00:34:53,980
"Niall Ferguson is prolific,

600
00:34:53,980 --> 00:34:56,220
well-paid, and a snappy dresser.

601
00:34:56,220 --> 00:34:58,395
Stephen Moss hated him."

602
00:34:58,395 --> 00:35:02,854
Um, and so if you can remember any of the steps of that algorithm,

603
00:35:02,854 --> 00:35:06,275
here's our, um, pronoun him.

604
00:35:06,275 --> 00:35:11,535
Um, and then, what it said to do was begin at the NP,

605
00:35:11,535 --> 00:35:14,210
the noun phrase above the pronoun.

606
00:35:14,210 --> 00:35:19,010
And then it said, to go up to the first noun phrase or S above that,

607
00:35:19,010 --> 00:35:21,370
um, here is the S above that.

608
00:35:21,370 --> 00:35:24,765
Um, and then what you're meant to do is, from there,

609
00:35:24,765 --> 00:35:30,185
you're meant to go left to right through stuff that came before that.

610
00:35:30,185 --> 00:35:32,980
So there's a lot of cleverness in this handwritten algorithm.

611
00:35:32,980 --> 00:35:36,440
You know, this is in the space of clever handwritten algorithms.

612
00:35:36,440 --> 00:35:40,140
And so what this is reflecting is that you might just think you

613
00:35:40,140 --> 00:35:44,045
should go to the closest thing to find reference,

614
00:35:44,045 --> 00:35:48,735
but actually if you have reference within the same sentence,

615
00:35:48,735 --> 00:35:51,730
it's much more common for the sort of

616
00:35:51,730 --> 00:35:56,085
highest syntactic roles to be what you're coreferent with.

617
00:35:56,085 --> 00:35:59,780
So you're more likely to be coreferent with a subject than an object,

618
00:35:59,780 --> 00:36:03,760
and you're more likely to be coreferent with an object than something like

619
00:36:03,760 --> 00:36:08,935
a noun phrase and that's inside a prepositional phrase that follows the object.

620
00:36:08,935 --> 00:36:11,855
So we're gonna start from the left here and we're gonna

621
00:36:11,855 --> 00:36:14,710
say here's a noun phrase, Stephen Moss.

622
00:36:14,710 --> 00:36:16,520
That's the first one we come to.

623
00:36:16,520 --> 00:36:20,265
And then there's this clever bit of text that says,

624
00:36:20,265 --> 00:36:24,820
um, traversal branches, um, below X,

625
00:36:24,820 --> 00:36:27,080
that are to the left- left to right,

626
00:36:27,080 --> 00:36:31,040
propose as antecedent and noun phrase, um,

627
00:36:31,040 --> 00:36:37,220
that has a noun phrase or sentence between it's an ec- in the S. So it was saying,

628
00:36:37,220 --> 00:36:38,990
this will be a candidate,

629
00:36:38,990 --> 00:36:40,520
if and only if,

630
00:36:40,520 --> 00:36:44,345
there's some other noun phrase or S in-between.

631
00:36:44,345 --> 00:36:48,870
Um, and so what that's saying is Stephen Moss hated him.

632
00:36:48,870 --> 00:36:52,210
It- this him cannot refer back to

633
00:36:52,210 --> 00:36:55,830
Stephen Moss and that sort of pretty much a fact of English syntax.

634
00:36:55,830 --> 00:37:00,090
But what it's wanting to do is distinguish between,

635
00:37:00,090 --> 00:37:02,940
another thing that we could have had here was

636
00:37:02,940 --> 00:37:09,100
a noun phrase that had another possessive noun phrase inside it.

637
00:37:09,100 --> 00:37:17,595
Um, so if we had something like Stephen Moss's mother hated him, right?

638
00:37:17,595 --> 00:37:22,620
Then the Stephen mother- Moss's mother hated him, then that would,

639
00:37:22,620 --> 00:37:28,055
in that case, it would be perfectly okay for him to be coreferent with Stephen Moss.

640
00:37:28,055 --> 00:37:31,120
And the algorithm allows that because relative to

641
00:37:31,120 --> 00:37:35,760
this noun phrase is another noun phrase above it and between.

642
00:37:35,760 --> 00:37:38,205
Okay. So that didn't work, um,

643
00:37:38,205 --> 00:37:40,740
as an antece- as an antecedent,

644
00:37:40,740 --> 00:37:43,415
so then we go onto the next step of the algorithm.

645
00:37:43,415 --> 00:37:44,920
And then, the next step says,

646
00:37:44,920 --> 00:37:49,095
we should proceed backwards through preceding sentences,

647
00:37:49,095 --> 00:37:50,595
um, right to left.

648
00:37:50,595 --> 00:37:55,085
And so that captures an important heuristic that proximity is actually

649
00:37:55,085 --> 00:37:58,085
a good heuristic to find coreference

650
00:37:58,085 --> 00:38:02,115
because coreference for pronouns is usually close by overall.

651
00:38:02,115 --> 00:38:05,055
And so we go to the first sentence back.

652
00:38:05,055 --> 00:38:07,885
And then in this sentence, again,

653
00:38:07,885 --> 00:38:09,610
we go into within the sentence,

654
00:38:09,610 --> 00:38:13,335
go left to right because there's the same kind of subject prominence role.

655
00:38:13,335 --> 00:38:15,685
And so we're gonna start in this sentence,

656
00:38:15,685 --> 00:38:16,990
and we're gonna say okay,

657
00:38:16,990 --> 00:38:18,710
here's a noun phrase.

658
00:38:18,710 --> 00:38:21,315
And now because we're in a different sentence,

659
00:38:21,315 --> 00:38:23,360
there's nothing wrong with this one.

660
00:38:23,360 --> 00:38:24,725
So we say, aha,

661
00:38:24,725 --> 00:38:27,340
we have a candidate, Niall Ferguson,

662
00:38:27,340 --> 00:38:32,385
um, is a possible antecedent and it's the first one we found.

663
00:38:32,385 --> 00:38:35,700
And therefore, we say that him refers back to Niall Ferguson.

664
00:38:35,700 --> 00:38:38,640
And this algorithm actually gives the right answer,

665
00:38:38,640 --> 00:38:40,750
if you could follow along all of that.

666
00:38:40,750 --> 00:38:43,320
Um, though that sounds like, um,

667
00:38:43,320 --> 00:38:47,020
horrible handwritten stuff.

668
00:38:47,020 --> 00:38:56,810
But, um, so Jerry Hobbs was aware of that this was horrible handwritten stuff,

669
00:38:56,810 --> 00:39:01,805
but he was interested in this algorithm for a couple of reasons.

670
00:39:01,805 --> 00:39:04,970
I mean, reason one is, you know,

671
00:39:04,970 --> 00:39:09,980
this is actually one of the first places in natural language processing,

672
00:39:09,980 --> 00:39:12,740
that someone produced the baseline, right.

673
00:39:12,740 --> 00:39:15,620
In for final projects and elsewhere,

674
00:39:15,620 --> 00:39:17,750
um, and stuff we gave you, right,

675
00:39:17,750 --> 00:39:21,260
it's seen now in NLP and other areas,

676
00:39:21,260 --> 00:39:22,490
that anything you are doing,

677
00:39:22,490 --> 00:39:24,620
the first thing you should do is have a baseline,

678
00:39:24,620 --> 00:39:27,440
a simple system and see how well it works.

679
00:39:27,440 --> 00:39:31,955
And this was his simple rule-based system for doing coreference,

680
00:39:31,955 --> 00:39:36,575
um, and he wanted to observe that actually this baseline was pretty good.

681
00:39:36,575 --> 00:39:40,925
It actually gave the right answer a lot of the time.

682
00:39:40,925 --> 00:39:47,360
And so the challenge was how to build a system that did better than this baseline.

683
00:39:47,360 --> 00:39:49,220
And so he was well aware of it,

684
00:39:49,220 --> 00:39:50,780
you know, it was a dumb algorithm,

685
00:39:50,780 --> 00:39:55,580
but he proposed that as a good baseline for doing coreference resolution.

686
00:39:55,580 --> 00:39:57,920
So what he was interested in,

687
00:39:57,920 --> 00:40:00,980
um, remember that we're back in the 1970s here,

688
00:40:00,980 --> 00:40:06,860
was how to do knowledge-based pronominal coreference resolution.

689
00:40:06,860 --> 00:40:12,250
And so, um, essentially what he was noticing is well,

690
00:40:12,250 --> 00:40:16,360
these kinds of syntactic factors that I was mentioning prefer subjects,

691
00:40:16,360 --> 00:40:18,430
prefer close by, etc,

692
00:40:18,430 --> 00:40:20,785
they're all useful predictors.

693
00:40:20,785 --> 00:40:23,830
But there are lots of cases where they don't give the right answer,

694
00:40:23,830 --> 00:40:25,750
and to know when they give, when,

695
00:40:25,750 --> 00:40:29,005
to know what's really the coreferent thing,

696
00:40:29,005 --> 00:40:33,000
you have to actually understand what's being described in the world.

697
00:40:33,000 --> 00:40:35,105
So if I have this sentence,

698
00:40:35,105 --> 00:40:39,005
she poured water from the pitcher into the cup until it was full.

699
00:40:39,005 --> 00:40:41,700
What is it coreferent with?

700
00:40:44,530 --> 00:40:45,740
Cup.

701
00:40:45,740 --> 00:40:46,700
[NOISE] The cup.

702
00:40:46,700 --> 00:40:48,350
Thank you. [LAUGHTER] Okay.

703
00:40:48,350 --> 00:40:50,870
So that, it refers to the cup.

704
00:40:50,870 --> 00:40:53,225
But then let's look at this example.

705
00:40:53,225 --> 00:40:57,530
She poured water from the pitcher into the cup until it was empty.

706
00:40:57,530 --> 00:40:59,375
What does it refer to?

707
00:40:59,375 --> 00:40:59,900
The [OVERLAPPING].

708
00:40:59,900 --> 00:41:01,775
The pitcher. [LAUGHTER] Okay.

709
00:41:01,775 --> 00:41:06,125
So the crucial thing to notice in these two sentences is,

710
00:41:06,125 --> 00:41:10,940
these sentences have identical syntactic structure, right.

711
00:41:10,940 --> 00:41:15,695
So Jerry Hobbs's algorithm can't possibly work,

712
00:41:15,695 --> 00:41:18,365
um, for both of these sentences.

713
00:41:18,365 --> 00:41:20,615
It's gonna work for one of them,

714
00:41:20,615 --> 00:41:22,550
but not the other one.

715
00:41:22,550 --> 00:41:26,030
Um, since it's working from left to right within a sentence,

716
00:41:26,030 --> 00:41:28,865
it's gonna say the pitcher both times actually, right.

717
00:41:28,865 --> 00:41:35,360
So you can't get the answer right by Jerry Hobbs' algorithm and Jerry believed,

718
00:41:35,360 --> 00:41:37,415
and still believes, um,

719
00:41:37,415 --> 00:41:40,475
that the only way to get these kind of examples right,

720
00:41:40,475 --> 00:41:43,415
is actually if you understand the world,

721
00:41:43,415 --> 00:41:46,640
and you actually know what's going on in the world,

722
00:41:46,640 --> 00:41:49,040
so you can see what, what this is talking about.

723
00:41:49,040 --> 00:41:51,020
And there are lots of examples like this.

724
00:41:51,020 --> 00:41:53,765
Um, this is another very famous example.

725
00:41:53,765 --> 00:41:58,310
The city council refused the women a permit because they feared violence.

726
00:41:58,310 --> 00:42:00,140
Um, who does that they refer to?

727
00:42:00,140 --> 00:42:01,550
[inaudible].

728
00:42:01,550 --> 00:42:03,125
The city councilors.

729
00:42:03,125 --> 00:42:05,360
Um, but here's another sentence.

730
00:42:05,360 --> 00:42:10,415
The city council refused the women a permit because they advocated violence.

731
00:42:10,415 --> 00:42:12,785
Who does that they refer to?

732
00:42:12,785 --> 00:42:14,000
The women.

733
00:42:14,000 --> 00:42:16,340
The women. Okay. So this time it refers to the women.

734
00:42:16,340 --> 00:42:18,245
Um, and again, you know,

735
00:42:18,245 --> 00:42:24,185
identical syntactic structure, it couldn't possibly be done right by the Hobbs algorithm.

736
00:42:24,185 --> 00:42:27,365
Um, so this particular pair of examples,

737
00:42:27,365 --> 00:42:29,270
um, comes from Terry Winograd.

738
00:42:29,270 --> 00:42:31,610
Um, how long ti- uh,

739
00:42:31,610 --> 00:42:35,150
so Terry Winograd was originally an NLP faculty, um,

740
00:42:35,150 --> 00:42:39,410
he sort of got disillusioned with NLP because there wasn't making much progress, um,

741
00:42:39,410 --> 00:42:42,065
and ventured off into the land of HCI,

742
00:42:42,065 --> 00:42:43,880
um, that became his career.

743
00:42:43,880 --> 00:42:46,250
Um, but in his early work, um,

744
00:42:46,250 --> 00:42:48,109
he was interested in these phenomena,

745
00:42:48,109 --> 00:42:50,150
and came up with this example.

746
00:42:50,150 --> 00:42:52,955
And so this example really stuck with people.

747
00:42:52,955 --> 00:42:56,030
And so these kind of contrasts are referred to by

748
00:42:56,030 --> 00:42:59,615
other people as Winograd sentences or Winograd schema.

749
00:42:59,615 --> 00:43:04,505
And so this is actually something that's interesting that's revived recently.

750
00:43:04,505 --> 00:43:07,190
Um, so Hector Le- Levesque, um,

751
00:43:07,190 --> 00:43:09,875
wrote a paper, I guess five years ago now,

752
00:43:09,875 --> 00:43:14,030
where he was trying to advocate for return to doing

753
00:43:14,030 --> 00:43:18,515
more in the way of knowledge and world modeling and artificial intelligence,

754
00:43:18,515 --> 00:43:21,710
and arguing that there are lots of problems that you just

755
00:43:21,710 --> 00:43:25,595
can't solve by the kind of crude statistical methods,

756
00:43:25,595 --> 00:43:28,250
that our machine learning systems are using.

757
00:43:28,250 --> 00:43:31,370
And that you really needed to do more world understanding.

758
00:43:31,370 --> 00:43:33,080
And so he proposed that

759
00:43:33,080 --> 00:43:38,240
these Winograd schema would be a good te- alternative to the Turing test,

760
00:43:38,240 --> 00:43:40,760
as a way of measuring intelligence.

761
00:43:40,760 --> 00:43:43,850
And actually they're just coreference decisions, right.

762
00:43:43,850 --> 00:43:47,165
So, um, so there's sort of a claim here that,

763
00:43:47,165 --> 00:43:50,675
if you can do a coreference right 100 percent of the time,

764
00:43:50,675 --> 00:43:54,110
you've solved artificial intelligence in that you're, sort of you can,

765
00:43:54,110 --> 00:43:58,745
can code knowledge of the world into coreference problems.

766
00:43:58,745 --> 00:44:04,010
Um, yes so people have then tried to work on these Winograd schemas,

767
00:44:04,010 --> 00:44:07,070
and Levesque's feeling was, you know,

768
00:44:07,070 --> 00:44:09,664
you just couldn't do these,

769
00:44:09,664 --> 00:44:11,240
um, using kind of,

770
00:44:11,240 --> 00:44:14,360
the kind of statistical factors, um,

771
00:44:14,360 --> 00:44:17,870
that people put into their machine learning systems.

772
00:44:17,870 --> 00:44:22,520
He was partly wrong about that because subsequent work, um,

773
00:44:22,520 --> 00:44:27,050
both neural systems and otherwise has shown that actually you can

774
00:44:27,050 --> 00:44:32,165
get f- a nontrivial distance with these kind of problems because, you know,

775
00:44:32,165 --> 00:44:33,709
if it is the case,

776
00:44:33,709 --> 00:44:35,270
um, that, you know,

777
00:44:35,270 --> 00:44:38,120
you can somehow see enough examples,

778
00:44:38,120 --> 00:44:41,120
where the city council refuses permits,

779
00:44:41,120 --> 00:44:42,755
fearing violence, you know.

780
00:44:42,755 --> 00:44:44,390
If you've go- if you're collecting

781
00:44:44,390 --> 00:44:48,560
your neural language model over tens of billions of words,

782
00:44:48,560 --> 00:44:51,530
you might have seen some instances of things like that,

783
00:44:51,530 --> 00:44:54,845
and you could sort of predict it just on statistical patterning.

784
00:44:54,845 --> 00:44:56,015
But the question is, you know,

785
00:44:56,015 --> 00:44:58,340
how far can you actually get doing that,

786
00:44:58,340 --> 00:45:00,605
without having a bit more of a world model?

787
00:45:00,605 --> 00:45:02,030
And so that was, you know,

788
00:45:02,030 --> 00:45:05,705
what Hobbs was interested in way back in 1978.

789
00:45:05,705 --> 00:45:10,010
So he wrote, the naive approach is quite good,

790
00:45:10,010 --> 00:45:14,809
computationally speaking it will be a long time before a semantically based algorithm,

791
00:45:14,809 --> 00:45:17,585
is sophisticated enough to perform as well.

792
00:45:17,585 --> 00:45:21,650
And these results set a very high standard for any other approach to aim for.

793
00:45:21,650 --> 00:45:23,990
He was totally right about that, um,

794
00:45:23,990 --> 00:45:28,655
that it really wasn't until the 2010s that anybody

795
00:45:28,655 --> 00:45:33,830
managed to produce an algorithm for pronominal anaphora resolution,

796
00:45:33,830 --> 00:45:35,900
that outperformed the Hobbs algorithm.

797
00:45:35,900 --> 00:45:38,420
Even though it was just, uh,

798
00:45:38,420 --> 00:45:40,535
what he called a naive algorithm,

799
00:45:40,535 --> 00:45:44,000
or he might call a crude set of linguistic rules.

800
00:45:44,000 --> 00:45:46,325
Um, but he says,

801
00:45:46,325 --> 00:45:50,090
yet there is every reason to pursue a semantically based approach,

802
00:45:50,090 --> 00:45:52,325
the naive algorithm does not work.

803
00:45:52,325 --> 00:45:55,055
Anyone can think of examples where it fails.

804
00:45:55,055 --> 00:45:57,409
In these cases it not only fails,

805
00:45:57,409 --> 00:45:59,660
it gives no indication that it has failed,

806
00:45:59,660 --> 00:46:03,170
and offers no help in finding the real antecedent.

807
00:46:03,170 --> 00:46:05,510
Um, so food for thought there.

808
00:46:05,510 --> 00:46:08,135
Um, but, um, notwithstanding that,

809
00:46:08,135 --> 00:46:10,400
I'm gonna just rush ahead at this point,

810
00:46:10,400 --> 00:46:12,350
and tell you about some of the, um,

811
00:46:12,350 --> 00:46:14,450
statistical and neural algorithms,

812
00:46:14,450 --> 00:46:17,120
um, that have been used for coreference resolution.

813
00:46:17,120 --> 00:46:20,945
So the simplest form of algorithm that's commonly used,

814
00:46:20,945 --> 00:46:24,215
is what is called mention pair models.

815
00:46:24,215 --> 00:46:29,135
So what we mean by mention pair models is, um,

816
00:46:29,135 --> 00:46:32,030
we are gonna take pairs of mentions,

817
00:46:32,030 --> 00:46:36,110
and we're gonna train a binary classifier that says,

818
00:46:36,110 --> 00:46:39,230
is coreferent or isn't coreferent.

819
00:46:39,230 --> 00:46:44,105
And so then we're gonna proceed left to right through the text.

820
00:46:44,105 --> 00:46:49,325
And every time we get to a new mention,

821
00:46:49,325 --> 00:46:55,895
we're gonna then evaluate our classifier with respect to every preceding mention,

822
00:46:55,895 --> 00:46:58,835
and we're gonna say, are they coreferent?

823
00:46:58,835 --> 00:47:01,235
And it's gonna say yes or no.

824
00:47:01,235 --> 00:47:03,500
And we're gonna find out that some of them.

825
00:47:03,500 --> 00:47:06,155
It says yes for, um,

826
00:47:06,155 --> 00:47:10,160
I voted for Nader because he was like, most aligned with my value.

827
00:47:10,160 --> 00:47:13,100
She said, if we have a good classifier,

828
00:47:13,100 --> 00:47:18,470
it will say yes to the two bu- blue ones and not to the rest of them.

829
00:47:18,470 --> 00:47:22,190
Um, and so then we'll have at training time,

830
00:47:22,190 --> 00:47:26,540
negative examples that Nader and he are negative examples.

831
00:47:26,540 --> 00:47:30,860
[NOISE] So if you have data marked for coreference,

832
00:47:30,860 --> 00:47:33,530
we have the sort of positive and negative examples,

833
00:47:33,530 --> 00:47:35,120
and we can train a model.

834
00:47:35,120 --> 00:47:36,980
And so for training a model,

835
00:47:36,980 --> 00:47:42,230
we have a sort of the classifier outcome is one or zero,

836
00:47:42,230 --> 00:47:45,665
based on whether two mentions are coreferent.

837
00:47:45,665 --> 00:47:47,960
We're gonna have a coreference model that

838
00:47:47,960 --> 00:47:50,960
predicts the probability of them being coreferent.

839
00:47:50,960 --> 00:47:54,350
And we're gonna train it with the same kind of cross entropy loss,

840
00:47:54,350 --> 00:47:57,275
we've used other places and, um,

841
00:47:57,275 --> 00:48:01,160
try and learn a model that predicts coreference.

842
00:48:01,160 --> 00:48:04,550
And so then when we get to test time, um,

843
00:48:04,550 --> 00:48:08,495
and we have a piece of text with mentions, um,

844
00:48:08,495 --> 00:48:12,230
we're gonna run this classifier and it's gonna say,

845
00:48:12,230 --> 00:48:16,400
um, yes or no, with some probability.

846
00:48:16,400 --> 00:48:19,205
And if we pick a threshold like 0,5,

847
00:48:19,205 --> 00:48:22,565
we'll add certain coreference links.

848
00:48:22,565 --> 00:48:25,490
And that sort of looks pretty good.

849
00:48:25,490 --> 00:48:29,480
Um, but we're gonna sort of complete it off by saying well,

850
00:48:29,480 --> 00:48:34,280
if A is coreferent to B and B is K coreferent to C. Then really

851
00:48:34,280 --> 00:48:40,040
also A is coreferent to C. So we're gonna do a transitive closure,

852
00:48:40,040 --> 00:48:42,410
and that will give us our clustering.

853
00:48:42,410 --> 00:48:46,160
Um, note here that there's a certain danger in this.

854
00:48:46,160 --> 00:48:48,650
Because this means, if we make,

855
00:48:48,650 --> 00:48:51,755
since we're sor- with the transitive closure,

856
00:48:51,755 --> 00:48:54,425
that's always adding clustering links.

857
00:48:54,425 --> 00:48:58,310
And so that means the danger is that we're gonna over cluster,

858
00:48:58,310 --> 00:49:04,400
because if we make a single mistake and we link things that should be kept separate.

859
00:49:04,400 --> 00:49:06,920
So for example, if we wrongly said,

860
00:49:06,920 --> 00:49:08,870
he and my are coreferent,

861
00:49:08,870 --> 00:49:10,565
then everything of this, um,

862
00:49:10,565 --> 00:49:13,774
discourse would collapse together into one cluster,

863
00:49:13,774 --> 00:49:16,920
and everything would be deemed coreferent.

864
00:49:16,920 --> 00:49:20,995
Okay, um, and this,

865
00:49:20,995 --> 00:49:25,480
something that I haven't really emphasized, but comes up,

866
00:49:25,480 --> 00:49:30,070
is well, there's some mentions that are coreferent to nothing, right.

867
00:49:30,070 --> 00:49:32,890
In the Shruthi Rao story, there was a park,

868
00:49:32,890 --> 00:49:35,875
which was just mentioned once in the text, and so on,

869
00:49:35,875 --> 00:49:38,190
in this form of algorithm,

870
00:49:38,190 --> 00:49:41,540
what we'd like the classifier to say is, no,

871
00:49:41,540 --> 00:49:42,755
no, no, no, no,

872
00:49:42,755 --> 00:49:44,615
for all of the decisions.

873
00:49:44,615 --> 00:49:46,970
And so it's deemed coreferent to nothing.

874
00:49:46,970 --> 00:49:49,915
And then it's just a singleton mention.

875
00:49:49,915 --> 00:49:52,360
This sort of works,

876
00:49:52,360 --> 00:49:58,660
but it hasn't proven to be the best way of doing coreference.

877
00:49:58,660 --> 00:50:03,585
And a lot of the reason why it's not the best way to do coreference

878
00:50:03,585 --> 00:50:09,410
is because we have this phenomenon of anaphora where we have textural dependence.

879
00:50:09,410 --> 00:50:11,065
A lot of the time,

880
00:50:11,065 --> 00:50:14,535
it seems that we're not really,

881
00:50:14,535 --> 00:50:19,815
um, what- sort of wanting to make this all coreference decisions.

882
00:50:19,815 --> 00:50:24,185
We'd like to make the anaphora decisions of textural dependence.

883
00:50:24,185 --> 00:50:27,410
So we'd like to say that he is,

884
00:50:27,410 --> 00:50:33,140
um, dependent on Nader and my is dependent on I.

885
00:50:33,140 --> 00:50:35,115
These are anaphora relationships.

886
00:50:35,115 --> 00:50:41,285
So we'd like to just choose one example of what is this anaphora relationship.

887
00:50:41,285 --> 00:50:44,870
And so that's led to people then looking at what is called,

888
00:50:44,870 --> 00:50:47,475
um, Mention Pair Models, right?

889
00:50:47,475 --> 00:50:52,100
That the problem is that if we have a long document with lots of mentions,

890
00:50:52,100 --> 00:50:57,050
um, that we want to not be saying- trying to find all of them and say, yes.

891
00:50:57,050 --> 00:51:00,160
We just want to be saying there's a particular- we

892
00:51:00,160 --> 00:51:03,695
just want to be saying that there's a particular one.

893
00:51:03,695 --> 00:51:05,770
So for the he at the end here,

894
00:51:05,770 --> 00:51:11,350
its anaphor relationship is back to Nader and you don't wanna be trying to say this

895
00:51:11,350 --> 00:51:17,600
he is also coreferent back to all of these other things that are earlier in the text.

896
00:51:17,600 --> 00:51:22,355
So it's not something that's been explored much.

897
00:51:22,355 --> 00:51:24,940
But arguably, this is a case again,

898
00:51:24,940 --> 00:51:30,905
where you should be separating coreference from anaphors because for anaphors it seems like

899
00:51:30,905 --> 00:51:32,980
the right way to think is that they have

900
00:51:32,980 --> 00:51:37,630
one prior thing in the text that they're textually dependent on.

901
00:51:37,630 --> 00:51:43,345
Whereas true coreferents, when you just have various mentions in the text of Ralph Nader,

902
00:51:43,345 --> 00:51:44,650
this Ralph Nader that,

903
00:51:44,650 --> 00:51:48,050
Nader did that, those aren't textually dependent

904
00:51:48,050 --> 00:51:52,070
and they should all be being grouped together as coreferents.

905
00:51:52,070 --> 00:51:58,520
Um, but our models sort of don't normally try and do some one way and some the other way,

906
00:51:58,520 --> 00:52:00,725
but you choose one of the models.

907
00:52:00,725 --> 00:52:02,815
So in the other one,

908
00:52:02,815 --> 00:52:06,010
we do it for- to do the other way,

909
00:52:06,010 --> 00:52:08,160
you do what's mention rankings.

910
00:52:08,160 --> 00:52:09,695
So for mention ranking,

911
00:52:09,695 --> 00:52:13,400
the idea is for each mention,

912
00:52:13,400 --> 00:52:15,545
we're going to find- try and find it

913
00:52:15,545 --> 00:52:20,640
an antecedent that comes before- before it in the text,

914
00:52:20,640 --> 00:52:22,650
that is- that it is, um,

915
00:52:22,650 --> 00:52:26,810
coreferent with, and we're going to make a one of N decision.

916
00:52:26,810 --> 00:52:29,509
So that when we see she here,

917
00:52:29,509 --> 00:52:30,815
we're going to say,

918
00:52:30,815 --> 00:52:35,400
"Okay, um, what is this coreferent with?"

919
00:52:35,400 --> 00:52:37,660
And we're going to pick one thing that it's coreferent

920
00:52:37,660 --> 00:52:41,130
with even though there might be others in the text.

921
00:52:41,130 --> 00:52:44,155
Um, so if we're doing that,

922
00:52:44,155 --> 00:52:47,490
we then have a problem with singleton mentions because if

923
00:52:47,490 --> 00:52:51,160
we're trying to- for every mention we find say,

924
00:52:51,160 --> 00:52:55,430
choose the thing that came before it in the text with which it's coreferent,

925
00:52:55,430 --> 00:52:58,575
the right answer might be that there's no such thing.

926
00:52:58,575 --> 00:53:00,580
So what we do is we add

927
00:53:00,580 --> 00:53:06,410
one additional dummy mention right at the front here, the NA mention.

928
00:53:06,410 --> 00:53:11,340
So one choice is you're gonna say there isn't anything preceding.

929
00:53:11,340 --> 00:53:13,935
So effectively, when you get to I,

930
00:53:13,935 --> 00:53:17,525
since this is, um, the first, um,

931
00:53:17,525 --> 00:53:19,265
real mention in the text,

932
00:53:19,265 --> 00:53:21,775
you're necessarily gonna choose as,

933
00:53:21,775 --> 00:53:24,260
um, its antecedent NA.

934
00:53:24,260 --> 00:53:27,815
You then go on to Nader and you have two choices.

935
00:53:27,815 --> 00:53:34,110
You can either say it's coreferent to I or it's coreferent to NA.

936
00:53:34,110 --> 00:53:38,410
I, it's a new mention- a new entity that's being mentioned in the text and

937
00:53:38,410 --> 00:53:43,280
the right answer is it's a new mention in- a new entity being mentioned in the text.

938
00:53:43,280 --> 00:53:46,935
Then you get to he and now you have three choices,

939
00:53:46,935 --> 00:53:51,100
and the right thing is to say that it's coreferent to Nader.

940
00:53:51,110 --> 00:53:55,199
Okay. Um, so this time,

941
00:53:55,199 --> 00:53:57,640
it's- for training our models,

942
00:53:57,640 --> 00:54:00,525
it's sort of the same, um,

943
00:54:00,525 --> 00:54:04,510
apart from this, sort of this different one of semantics.

944
00:54:04,510 --> 00:54:09,820
So now- previously, we wanted to say that for our, um,

945
00:54:09,820 --> 00:54:15,030
mention pair classifier that is going to try and classify I and she,

946
00:54:15,030 --> 00:54:16,355
and my and she,

947
00:54:16,355 --> 00:54:19,220
and both of them had to get a high score,

948
00:54:19,220 --> 00:54:22,030
where now it's sufficient that just one of them gets

949
00:54:22,030 --> 00:54:26,150
a high score because that's sort of enough for us to do.

950
00:54:26,150 --> 00:54:30,605
So what we're gonna use is our good old softmax and so for she,

951
00:54:30,605 --> 00:54:34,550
we're gonna put a softmax over the antecedents.

952
00:54:34,550 --> 00:54:39,660
And our hope is simply that we get a high probability with one of the antecedents,

953
00:54:39,660 --> 00:54:43,700
if it has an antecedent or a high score with NA,

954
00:54:43,700 --> 00:54:46,755
if it doesn't have any prior referents.

955
00:54:46,755 --> 00:54:51,365
And so then when we're doing classification at run-time,

956
00:54:51,365 --> 00:54:56,355
we're going to sort of add only the highest scoring coreference link.

957
00:54:56,355 --> 00:54:58,990
So that means we train it just slightly

958
00:54:58,990 --> 00:55:03,180
differently because now what we're going to do is that,

959
00:55:03,180 --> 00:55:06,200
when we're- what we're wanting to say is,

960
00:55:06,200 --> 00:55:13,025
we want a high score of coreference between at least one of the antecedents.

961
00:55:13,025 --> 00:55:15,010
And so one possible model is,

962
00:55:15,010 --> 00:55:17,300
we can maximize this probability.

963
00:55:17,300 --> 00:55:21,105
So for the ones that are coreferent in the gold standard data,

964
00:55:21,105 --> 00:55:24,885
we want the sum of their assigned probabilities to be high.

965
00:55:24,885 --> 00:55:31,135
And so what that means is that it's sort of sufficient if we have,

966
00:55:31,135 --> 00:55:34,300
um, one of them giving

967
00:55:34,300 --> 00:55:38,375
a high probability and they don't all have to give a high probability.

968
00:55:38,375 --> 00:55:41,219
So providing it's giving 0,9 probability,

969
00:55:41,219 --> 00:55:43,535
say it a one of the correct antecedents,

970
00:55:43,535 --> 00:55:45,695
we're getting a high score.

971
00:55:45,695 --> 00:55:49,660
Okay. So we're gonna turn that into a loss function in the kind of

972
00:55:49,660 --> 00:55:53,585
standard way we do in which we take log probabilities,

973
00:55:53,585 --> 00:55:56,260
um, and then we want to, um,

974
00:55:56,260 --> 00:55:58,590
or negative log probabilities to give us

975
00:55:58,590 --> 00:56:02,150
a loss and then we're wanting to minimize that loss.

976
00:56:02,150 --> 00:56:05,909
So with the mention ranking model,

977
00:56:05,909 --> 00:56:07,870
um, at test time,

978
00:56:07,870 --> 00:56:09,360
it's pretty much the same,

979
00:56:09,360 --> 00:56:16,280
but our softmax classifier is just going to assign one antecedent for each mention.

980
00:56:16,280 --> 00:56:20,470
And so we're then gonna hope that those sort of give us the kind

981
00:56:20,470 --> 00:56:25,640
of clusters that we want and there's no subsequent clustering phase.

982
00:56:25,820 --> 00:56:30,875
So there's a big part of this that I left out which was,

983
00:56:30,875 --> 00:56:32,510
I've just said, "Okay,

984
00:56:32,510 --> 00:56:38,590
we have this probability of MI and MJ as the- are they coreferent?"

985
00:56:38,590 --> 00:56:41,050
But I've sort of said, zero as to

986
00:56:41,050 --> 00:56:43,760
how you can determine whether they're coreferent or not.

987
00:56:43,760 --> 00:56:46,640
Um, so briefly, um,

988
00:56:46,640 --> 00:56:49,775
here- here's the classical way of doing it.

989
00:56:49,775 --> 00:56:51,895
The classical way of doing it is,

990
00:56:51,895 --> 00:56:56,090
you had a whole bunch of features and you had

991
00:56:56,090 --> 00:57:00,480
a feature based statistical classifier which gave a score.

992
00:57:00,480 --> 00:57:02,650
And these are the kind of features you could use.

993
00:57:02,650 --> 00:57:06,490
So there are sort of strong features of person, number, gender agreement.

994
00:57:06,490 --> 00:57:09,720
So if you have a masculine or feminine pronoun,

995
00:57:09,720 --> 00:57:12,290
you wanna find an appropriate antecedent for it.

996
00:57:12,290 --> 00:57:16,630
There are weaker, um, semantic compatibility features.

997
00:57:16,630 --> 00:57:18,980
So the mining conglomerate, the company,

998
00:57:18,980 --> 00:57:21,755
the conglomerate might be sort of similar to a company.

999
00:57:21,755 --> 00:57:25,700
You could use something like word2vec similarity and assess that.

1000
00:57:25,700 --> 00:57:28,120
There are syntactic constraints.

1001
00:57:28,120 --> 00:57:30,730
So this is then kind of like, um,

1002
00:57:30,730 --> 00:57:34,570
what Hobbs's algorithm was all about us working out

1003
00:57:34,570 --> 00:57:38,700
how likely different syntactic configurations are gonna mean coreference.

1004
00:57:38,700 --> 00:57:40,950
And indeed it is the case, you know,

1005
00:57:40,950 --> 00:57:46,150
that a lot of these feature-based systems used Hobbs' algorithm as a feature inside

1006
00:57:46,150 --> 00:57:51,860
the system that was weighted and was normally a very strong feature to decide coreference.

1007
00:57:51,860 --> 00:57:55,425
Um, there are lots of other things you can put in as features.

1008
00:57:55,425 --> 00:57:56,860
Um, recency.

1009
00:57:56,860 --> 00:57:58,240
So John went to a movie,

1010
00:57:58,240 --> 00:57:59,290
Jack went as well,

1011
00:57:59,290 --> 00:58:00,600
he was not busy.

1012
00:58:00,600 --> 00:58:05,180
The most likely referent for he is the closer candidate Jack.

1013
00:58:05,180 --> 00:58:10,120
Um, I've mentioned subjects are more likely to be, um, the antecedent.

1014
00:58:10,120 --> 00:58:11,550
John went to a movie with Jack,

1015
00:58:11,550 --> 00:58:12,930
he was not busy.

1016
00:58:12,930 --> 00:58:15,990
Um, John seems a more likely antecedent.

1017
00:58:15,990 --> 00:58:18,155
So that's the sort of subject preference.

1018
00:58:18,155 --> 00:58:20,395
There's also a parallelism preference.

1019
00:58:20,395 --> 00:58:22,295
So John went with Jack to a movie,

1020
00:58:22,295 --> 00:58:24,170
Joe went with him to a bar.

1021
00:58:24,170 --> 00:58:28,405
I think it's sort of reasonable to think that him there is probably Jack,

1022
00:58:28,405 --> 00:58:32,825
and that's sort of for parallelism reasons as opposed to going with the subject.

1023
00:58:32,825 --> 00:58:36,480
So there are various kind of linguistic features and constraints and so on,

1024
00:58:36,480 --> 00:58:39,970
and you can throw these all into a statistical classifier and that's

1025
00:58:39,970 --> 00:58:44,910
sort of 2000s decade coref systems as to how they're built.

1026
00:58:44,910 --> 00:58:49,350
Um, more recently, people have built neural systems.

1027
00:58:49,350 --> 00:58:50,560
And so for these,

1028
00:58:50,560 --> 00:58:53,810
we are kind of normally using the same kind of embeddings.

1029
00:58:53,810 --> 00:58:58,250
So we'll have a candidate antecedent that will have embeddings,

1030
00:58:58,250 --> 00:59:00,510
we'll have a mention that has embeddings.

1031
00:59:00,510 --> 00:59:01,785
And this will be something like

1032
00:59:01,785 --> 00:59:05,595
average word vectors or something like that for the mention.

1033
00:59:05,595 --> 00:59:09,935
And we're gonna feed these into a neural network that will give us our score.

1034
00:59:09,935 --> 00:59:13,070
But what you find is that

1035
00:59:13,070 --> 00:59:16,995
most of these systems as well as having something like word vectors,

1036
00:59:16,995 --> 00:59:20,610
they also have additional features, um,

1037
00:59:20,610 --> 00:59:23,910
and these features still capture some of

1038
00:59:23,910 --> 00:59:28,265
the things that were in the feature-based statistical classifiers.

1039
00:59:28,265 --> 00:59:31,865
So there will be often features that reflect things like,

1040
00:59:31,865 --> 00:59:37,270
what grammatical relation does this mention have? Is it a subject?

1041
00:59:37,270 --> 00:59:38,515
Is it an object?

1042
00:59:38,515 --> 00:59:42,825
That's something you could put into the features of a mention.

1043
00:59:42,825 --> 00:59:46,565
But then, closer things are more likely to be coreferent.

1044
00:59:46,565 --> 00:59:51,270
So you might have additional features here which record how far apart dimensions are,

1045
00:59:51,270 --> 00:59:54,180
and those things get thrown in as well.

1046
00:59:54,180 --> 01:00:00,935
Um, and so these kind of features are still important even in neural systems.

1047
01:00:00,935 --> 01:00:07,165
And so I'll skip ahead now and show you a bit about, um,

1048
01:00:07,165 --> 01:00:11,450
what is the kind of current state of the art for coreference resolution,

1049
01:00:11,450 --> 01:00:14,950
and this was a system that was done at the University of Washington in

1050
01:00:14,950 --> 01:00:20,495
2017 by Kenton Lee and assorted other, um, authors.

1051
01:00:20,495 --> 01:00:26,910
Um, so the goal here was to produce an end-to-end coreference system that it was text in,

1052
01:00:26,910 --> 01:00:30,715
um, mention clusters that are coreferent out.

1053
01:00:30,715 --> 01:00:35,335
Um, and so they're wanting to use sort of a more complex

1054
01:00:35,335 --> 01:00:40,420
neural network that can do the whole thing end-to-end. So I'll go through,

1055
01:00:40,420 --> 01:00:41,730
um, the steps of that.

1056
01:00:41,730 --> 01:00:45,985
So the first step is we just start off with words.

1057
01:00:45,985 --> 01:00:47,705
And so for each word,

1058
01:00:47,705 --> 01:00:53,020
we're going to look up a word embedding for it and that's in other stuff we've seen.

1059
01:00:53,020 --> 01:00:55,795
We're also going to put in a character level CNN,

1060
01:00:55,795 --> 01:01:00,885
and the two of those concatenated are going to give the representation of each token.

1061
01:01:00,885 --> 01:01:02,600
That much should look familiar.

1062
01:01:02,600 --> 01:01:05,065
Okay. Then after that,

1063
01:01:05,065 --> 01:01:11,255
we're going to run a deep bidirectional LSTM back and forth across the sentence.

1064
01:01:11,255 --> 01:01:15,440
Again, that should look familiar from stuff that we've seen before.

1065
01:01:15,440 --> 01:01:21,700
Um, the next step gets us a bit into doing something more special, um,

1066
01:01:21,700 --> 01:01:24,115
For coreference.

1067
01:01:24,115 --> 01:01:30,790
So what they wanted to do after that is have a representation for spans.

1068
01:01:30,790 --> 01:01:32,635
And so by span,

1069
01:01:32,635 --> 01:01:38,050
we mean any contiguous subphrase of the word, of the sentence.

1070
01:01:38,050 --> 01:01:40,030
So this is a span.

1071
01:01:40,030 --> 01:01:41,380
This is a span.

1072
01:01:41,380 --> 01:01:42,670
This is a span.

1073
01:01:42,670 --> 01:01:46,315
Electric said the postal is a span, every sub-sequence.

1074
01:01:46,315 --> 01:01:48,250
Um, so I'll come back to that.

1075
01:01:48,250 --> 01:01:50,215
But, you know, they'll- in principle,

1076
01:01:50,215 --> 01:01:53,155
you're working this out for every sub-sequence.

1077
01:01:53,155 --> 01:01:55,360
So for every sub-sequence,

1078
01:01:55,360 --> 01:01:58,675
they want to come up with a span representation.

1079
01:01:58,675 --> 01:02:04,975
And so this span representation is going to be in three parts,

1080
01:02:04,975 --> 01:02:09,325
um, that represent one of these sub-sequences.

1081
01:02:09,325 --> 01:02:13,660
Um, so each of these will get its own representation.

1082
01:02:13,660 --> 01:02:15,640
And so the question is, what?

1083
01:02:15,640 --> 01:02:18,910
And so we have this span representation,

1084
01:02:18,910 --> 01:02:22,375
and it's gonna be in these three parts here.

1085
01:02:22,375 --> 01:02:26,770
Um, so what these parts are is,

1086
01:02:26,770 --> 01:02:28,420
well, first of all,

1087
01:02:28,420 --> 01:02:31,450
we're going to have a representation, um,

1088
01:02:31,450 --> 01:02:35,035
which is just looking at the first word of

1089
01:02:35,035 --> 01:02:40,450
the span and the last word of the span according to the BiLSTM.

1090
01:02:40,450 --> 01:02:43,135
So if we're looking at the span, the postal service,

1091
01:02:43,135 --> 01:02:45,670
we're going to take this BiLSTM and

1092
01:02:45,670 --> 01:02:50,245
this BiLSTM and use them as part of the representation of the span.

1093
01:02:50,245 --> 01:02:52,225
Um, that's a good start,

1094
01:02:52,225 --> 01:02:54,730
but then they actually do something a little tricky.

1095
01:02:54,730 --> 01:02:59,710
So kind of like when we're doing dependency parsing, the idea was,

1096
01:02:59,710 --> 01:03:02,815
well, phrases are going to have a headword,

1097
01:03:02,815 --> 01:03:05,050
um, so that if it's,

1098
01:03:05,050 --> 01:03:10,420
um, you know, my younger sister that the headword of that is sister,

1099
01:03:10,420 --> 01:03:14,905
and there- if it's something like the goat in the corner of the field,

1100
01:03:14,905 --> 01:03:16,960
the headword of that is going to be goat.

1101
01:03:16,960 --> 01:03:21,520
So they want to find a way of capturing headwords out of the text.

1102
01:03:21,520 --> 01:03:26,200
Um, and so what they're going to do for that is use attention.

1103
01:03:26,200 --> 01:03:30,820
So they're going to say we have this span, the postal service,

1104
01:03:30,820 --> 01:03:33,640
and we're going to use attention as

1105
01:03:33,640 --> 01:03:38,050
a span internal mechanism to sort of approximate a head.

1106
01:03:38,050 --> 01:03:42,205
So what we're going to do, uh, here,

1107
01:03:42,205 --> 01:03:45,520
what we're going to do is we're going to want to

1108
01:03:45,520 --> 01:03:50,020
learn attention weights, I'm just gonna, yeah.

1109
01:03:50,020 --> 01:03:54,220
Um, what we're gonna do is for this span, um,

1110
01:03:54,220 --> 01:03:58,810
we're going to be learning based on the hope,

1111
01:03:58,810 --> 01:04:03,250
the ends of the span which words to pay how much attention to.

1112
01:04:03,250 --> 01:04:06,970
So we're gonna put attention weights on the different words,

1113
01:04:06,970 --> 01:04:09,730
and then we're going to, in the usual attention way,

1114
01:04:09,730 --> 01:04:15,220
make this weighted sum of having put the word pair-

1115
01:04:15,220 --> 01:04:19,360
the bidirectional LSTM pairs through a feed-forward network and end

1116
01:04:19,360 --> 01:04:23,695
up with this new representation of a weighted representation.

1117
01:04:23,695 --> 01:04:25,450
And the hope is that in this case,

1118
01:04:25,450 --> 01:04:28,630
most of the weight will go on this final servers,

1119
01:04:28,630 --> 01:04:30,445
which will be the headword.

1120
01:04:30,445 --> 01:04:32,755
But there'll be sort of distributed across it.

1121
01:04:32,755 --> 01:04:36,055
And so that gives them a model of

1122
01:04:36,055 --> 01:04:41,875
sort of mentions that use both ends and hope to find the key word of the mention.

1123
01:04:41,875 --> 01:04:46,015
Okay. Um, so, um,

1124
01:04:46,015 --> 01:04:48,010
that's two-thirds of the span,

1125
01:04:48,010 --> 01:04:51,235
but they still have over here these additional features.

1126
01:04:51,235 --> 01:04:54,235
And so they still have some additional features.

1127
01:04:54,235 --> 01:04:58,195
They want to be able to mark speakers and addressees.

1128
01:04:58,195 --> 01:05:02,200
Um, they want to mark other things like the grammatical role.

1129
01:05:02,200 --> 01:05:03,970
But if things occur, you know,

1130
01:05:03,970 --> 01:05:07,240
it is still useful to have some additional features.

1131
01:05:07,240 --> 01:05:08,620
And so what they do is,

1132
01:05:08,620 --> 01:05:11,860
this is a representation of each span,

1133
01:05:11,860 --> 01:05:16,390
and then they're going to want to say are two spans coreferent.

1134
01:05:16,390 --> 01:05:22,120
And so they're going to have one score for the two, two split, each of two spans,

1135
01:05:22,120 --> 01:05:23,365
which is essentially saying,

1136
01:05:23,365 --> 01:05:24,850
is that a good mention?

1137
01:05:24,850 --> 01:05:26,920
And then you're going to have scores of,

1138
01:05:26,920 --> 01:05:29,170
do they look coreferent?

1139
01:05:29,170 --> 01:05:34,870
And so having calculated these representations for each span,

1140
01:05:34,870 --> 01:05:39,220
you're running three- through things through a fully connected feed-forward network,

1141
01:05:39,220 --> 01:05:41,245
multiplying by a weight factor,

1142
01:05:41,245 --> 01:05:42,490
and that's giving you, uh,

1143
01:05:42,490 --> 01:05:44,635
is that a good mention score?

1144
01:05:44,635 --> 01:05:46,960
And then for are they coreferent,

1145
01:05:46,960 --> 01:05:49,330
you're taking two spans,

1146
01:05:49,330 --> 01:05:53,650
the pointwise Hadamard product of two spans and

1147
01:05:53,650 --> 01:05:56,140
some extra features like distance apart in

1148
01:05:56,140 --> 01:05:59,469
the text and putting them through another neural network,

1149
01:05:59,469 --> 01:06:01,285
and that's then giving you, are

1150
01:06:01,285 --> 01:06:03,475
these two spans coreferent?

1151
01:06:03,475 --> 01:06:05,590
But all of these pieces,

1152
01:06:05,590 --> 01:06:10,480
um, give you an overall loss function.

1153
01:06:10,480 --> 01:06:14,815
So you can say that your model is, um, okay.

1154
01:06:14,815 --> 01:06:16,885
We're going to run these LSTMs,

1155
01:06:16,885 --> 01:06:18,880
we're going to take all spans,

1156
01:06:18,880 --> 01:06:20,829
we're going to score this,

1157
01:06:20,829 --> 01:06:24,370
and we know the gold answer for our coreference system.

1158
01:06:24,370 --> 01:06:29,200
And so we want to be predicting things that are coreferent and have

1159
01:06:29,200 --> 01:06:34,375
a loss based on the probability that we calculate with these scores,

1160
01:06:34,375 --> 01:06:35,770
um, as I had mentioned,

1161
01:06:35,770 --> 01:06:39,205
ranking model using a softmax loss like before.

1162
01:06:39,205 --> 01:06:42,775
So if you put all of this together and train it end to end,

1163
01:06:42,775 --> 01:06:48,685
you've got a whole coreference system that goes from words to coreference decisions.

1164
01:06:48,685 --> 01:06:52,045
Um, there's a huge problem with that,

1165
01:06:52,045 --> 01:06:55,810
um, which is if you actually applied this naively, well,

1166
01:06:55,810 --> 01:06:58,930
the problem is the number of spans in a piece of

1167
01:06:58,930 --> 01:07:03,055
text is the square of the length of the text in words.

1168
01:07:03,055 --> 01:07:06,400
And so therefore, if you're making coreference decisions,

1169
01:07:06,400 --> 01:07:10,060
which are between, um, pairs of spans,

1170
01:07:10,060 --> 01:07:13,060
you've then got an algorithm that's, um,

1171
01:07:13,060 --> 01:07:15,415
O- OT to the fourth,

1172
01:07:15,415 --> 01:07:18,025
where the length of the text is T words.

1173
01:07:18,025 --> 01:07:22,375
So that's sort of really, really computationally impractical.

1174
01:07:22,375 --> 01:07:23,515
So at this point,

1175
01:07:23,515 --> 01:07:26,350
they sort of say, well, actually,

1176
01:07:26,350 --> 01:07:29,905
we do want to use our mouths a little and we want to work out

1177
01:07:29,905 --> 01:07:34,090
how likely different things are to be mentions.

1178
01:07:34,090 --> 01:07:38,650
So effectively, um, then they're putting in a lot of pruning to

1179
01:07:38,650 --> 01:07:43,765
decide which spans are actually things that they want to consider in their model.

1180
01:07:43,765 --> 01:07:45,730
And so at this point, in some sense,

1181
01:07:45,730 --> 01:07:47,170
it's a little bit of a cheat, right?

1182
01:07:47,170 --> 01:07:50,440
Because really this pruning step here is okay,

1183
01:07:50,440 --> 01:07:51,760
we're going to stick in

1184
01:07:51,760 --> 01:07:54,205
a mention detection module,

1185
01:07:54,205 --> 01:07:57,040
um, just like a conventional system.

1186
01:07:57,040 --> 01:08:01,225
Um, but the prettiness of it is in terms of

1187
01:08:01,225 --> 01:08:05,440
the algor- in terms of the loss function that's defined.

1188
01:08:05,440 --> 01:08:09,730
The loss function is really defined end to end from just a sequence of

1189
01:08:09,730 --> 01:08:14,380
tokens through to the mention ranking decisions.

1190
01:08:14,380 --> 01:08:18,204
And so it is an end-to-end model,

1191
01:08:18,204 --> 01:08:20,320
even though in practice to make it practical,

1192
01:08:20,320 --> 01:08:24,680
you have to have something like a mention detector to get it to work.

1193
01:08:26,100 --> 01:08:30,520
Okay. Pause for breath. Um, yeah,

1194
01:08:30,520 --> 01:08:34,510
so there's one last.

1195
01:08:34,510 --> 01:08:40,180
So we've done sort of mention pair model and mention ranking model.

1196
01:08:40,180 --> 01:08:42,370
Um, and so for both of those,

1197
01:08:42,370 --> 01:08:44,830
you're just taking individual mentions and saying,

1198
01:08:44,830 --> 01:08:46,870
here's another mention, what,

1199
01:08:46,870 --> 01:08:48,550
what shall I do with it?

1200
01:08:48,550 --> 01:08:53,125
Let's look at mentions and see if we're coreferent to each other.

1201
01:08:53,125 --> 01:09:00,280
And that there's no real concept of entities which are clusters of mentions.

1202
01:09:00,280 --> 01:09:02,680
You're just making these sort of one-off decisions

1203
01:09:02,680 --> 01:09:06,040
between pairs of mentions, and somehow,

1204
01:09:06,040 --> 01:09:09,610
sort of the entities as clusters just

1205
01:09:09,610 --> 01:09:14,050
emerge as a consequence of those mention pair decisions.

1206
01:09:14,050 --> 01:09:19,555
So there's been this sort of long-standing feeling that,

1207
01:09:19,555 --> 01:09:22,570
oh that can't really be right,

1208
01:09:22,570 --> 01:09:28,060
the right way to do coreference must be really to do it as a clustering task,

1209
01:09:28,060 --> 01:09:30,040
and people often refer to this as saying,

1210
01:09:30,040 --> 01:09:33,235
we want entities as first-class citizens.

1211
01:09:33,235 --> 01:09:34,600
So we want to be,

1212
01:09:34,600 --> 01:09:40,300
sort of putting together mentions into clusters that represent the entities.

1213
01:09:40,300 --> 01:09:45,010
And the obvious way to do that is to do a kind of bottom-up agglomerative clustering.

1214
01:09:45,010 --> 01:09:46,900
So you start off by saying,

1215
01:09:46,900 --> 01:09:49,855
each mention is its own singleton cluster,

1216
01:09:49,855 --> 01:09:55,585
and then you're making decisions to merge clu- clusters which is initially,

1217
01:09:55,585 --> 01:09:58,150
um, saying two mentions are coreferent.

1218
01:09:58,150 --> 01:09:59,695
But as you go on with it,

1219
01:09:59,695 --> 01:10:04,285
you're then making decisions that two clusters are coreferent or not.

1220
01:10:04,285 --> 01:10:07,120
So the idea here is you'll have a piece of text,

1221
01:10:07,120 --> 01:10:09,265
Google recently blah blah blah blah,

1222
01:10:09,265 --> 01:10:12,055
the company announced Google Plus, blah blah blah blah,

1223
01:10:12,055 --> 01:10:14,380
the product features blah blah blah blah.

1224
01:10:14,380 --> 01:10:17,170
And so you have here some mentions.

1225
01:10:17,170 --> 01:10:20,950
And so what you're going to do is start off saying that okay,

1226
01:10:20,950 --> 01:10:24,445
there are these four mentions that each their own cluster.

1227
01:10:24,445 --> 01:10:26,170
And then what we're gonna do,

1228
01:10:26,170 --> 01:10:28,525
is we're going to make some decisions.

1229
01:10:28,525 --> 01:10:32,740
Um, so we might decide that these two clusters

1230
01:10:32,740 --> 01:10:37,375
are coreferent and merge them into one cluster.

1231
01:10:37,375 --> 01:10:41,965
And then we might decide that these two,

1232
01:10:41,965 --> 01:10:48,095
um, clusters are coreferent and merge them into one cluster.

1233
01:10:48,095 --> 01:10:51,255
And so we're progressively clustering.

1234
01:10:51,255 --> 01:10:54,030
And so then, we're going to look at these two clusters,

1235
01:10:54,030 --> 01:10:56,835
cluster one and cluster two, and say,

1236
01:10:56,835 --> 01:11:00,585
no we don't think those ones are coreferent,

1237
01:11:00,585 --> 01:11:03,030
and therefore we're going to keep them apart.

1238
01:11:03,030 --> 01:11:10,645
And so your, your coreference algorithm stops when there's nothing left to merge.

1239
01:11:10,645 --> 01:11:15,430
And the reason why people think that this is the right thing to do is,

1240
01:11:15,430 --> 01:11:19,930
the feeling is that if we sort of build partial clusters like this,

1241
01:11:19,930 --> 01:11:22,405
that you'll be able to do a better job.

1242
01:11:22,405 --> 01:11:24,040
Because if I just sort of say,

1243
01:11:24,040 --> 01:11:25,615
well here are two mentions,

1244
01:11:25,615 --> 01:11:27,459
Google and Google Plus,

1245
01:11:27,459 --> 01:11:32,020
should they be regarded as co- coreferent or not?

1246
01:11:32,020 --> 01:11:34,450
Um, well, since you're smart human beings,

1247
01:11:34,450 --> 01:11:36,640
and know what Google is and know what Google Plus is,

1248
01:11:36,640 --> 01:11:39,070
of course you'll answer no, of course not.

1249
01:11:39,070 --> 01:11:40,495
Um, but, you know,

1250
01:11:40,495 --> 01:11:43,180
if you're just a computer trying to make a decision,

1251
01:11:43,180 --> 01:11:45,325
it's sort of hard to know the right answer,

1252
01:11:45,325 --> 01:11:49,240
because there are lots of other cases when there are shortenings,

1253
01:11:49,240 --> 01:11:52,030
where the right answer is that they're coreferent, right.

1254
01:11:52,030 --> 01:11:56,199
Because if this is being Google and Google Corp,

1255
01:11:56,199 --> 01:11:59,200
then it would have been right to regard them as coreferent.

1256
01:11:59,200 --> 01:12:01,435
Or if it was sort of, um,

1257
01:12:01,435 --> 01:12:04,210
something like Hillary Clinton and Hillary,

1258
01:12:04,210 --> 01:12:06,610
it would have been right to regard them as coreferent.

1259
01:12:06,610 --> 01:12:09,895
So it can often be kind of hard to tell what's coreferent.

1260
01:12:09,895 --> 01:12:11,830
Um, but the hope is that,

1261
01:12:11,830 --> 01:12:14,980
if you've made some of the easy decisions first,

1262
01:12:14,980 --> 01:12:17,830
so if you decide Google and the company are coreferent

1263
01:12:17,830 --> 01:12:20,950
and Google Plus and the product are coreferent,

1264
01:12:20,950 --> 01:12:24,580
then it should be much easier to tell and to say,

1265
01:12:24,580 --> 01:12:25,990
well product and company,

1266
01:12:25,990 --> 01:12:27,865
they're definitely different things.

1267
01:12:27,865 --> 01:12:31,510
And therefore we should keep these things separate.

1268
01:12:31,510 --> 01:12:34,405
Um, and so that is the goal,

1269
01:12:34,405 --> 01:12:36,955
and so to follow that goal,

1270
01:12:36,955 --> 01:12:39,160
the kind of models people build.

1271
01:12:39,160 --> 01:12:43,675
And this was actually a model that Kevin Clark is one of the PhD students here,

1272
01:12:43,675 --> 01:12:46,080
um, and we did a couple of years ago.

1273
01:12:46,080 --> 01:12:47,415
The idea was well,

1274
01:12:47,415 --> 01:12:49,350
what we're going to do is,

1275
01:12:49,350 --> 01:12:52,860
we're initially going to consider mentioned pairs,

1276
01:12:52,860 --> 01:12:57,180
and build some kind of distributed, mention pair representation,

1277
01:12:57,180 --> 01:13:01,815
which is kind of similar to what we were doing previously with the previous models.

1278
01:13:01,815 --> 01:13:07,900
But we're then going to go beyond that and come up with cluster representations.

1279
01:13:07,900 --> 01:13:11,095
And then we can look at cluster pair representations.

1280
01:13:11,095 --> 01:13:16,045
And we would hope that by looking at these cluster representations,

1281
01:13:16,045 --> 01:13:21,760
we'll be able to make better decisions of what to merge or what next to merge.

1282
01:13:21,760 --> 01:13:27,760
Um, I have a few more slides that go through the Clark and Manning algorithm.

1283
01:13:27,760 --> 01:13:30,490
Um, but I also have just a few minutes left.

1284
01:13:30,490 --> 01:13:33,680
And so I think I'll skip the details.

1285
01:13:33,680 --> 01:13:37,120
Um, I think the main thing that's interesting here,

1286
01:13:37,120 --> 01:13:41,740
is the idea of clustering based coreference algorithms,

1287
01:13:41,740 --> 01:13:43,465
and why in principle,

1288
01:13:43,465 --> 01:13:45,490
it should give you extra oomph.

1289
01:13:45,490 --> 01:13:49,135
Um, and that's sort of the main useful thing to get through.

1290
01:13:49,135 --> 01:13:51,280
Because what I want to make sure we have covered in

1291
01:13:51,280 --> 01:13:54,115
the last few minutes that I've said nothing at all about,

1292
01:13:54,115 --> 01:13:58,630
is how do you evaluate coreference resolution and how well does it work?

1293
01:13:58,630 --> 01:14:01,330
So let me skip ahead to that.

1294
01:14:01,330 --> 01:14:07,120
Um, so if you look at coreference resolution papers,

1295
01:14:07,120 --> 01:14:08,680
or something like that,

1296
01:14:08,680 --> 01:14:15,250
um, there are many metrics that people have used to evaluate coreference,

1297
01:14:15,250 --> 01:14:17,680
and they have a long alphabet soup of names.

1298
01:14:17,680 --> 01:14:20,140
So there's MUC, and CEAF, and LEA,

1299
01:14:20,140 --> 01:14:22,510
and B- CUBED, and BLANC and,

1300
01:14:22,510 --> 01:14:24,010
um, things like that.

1301
01:14:24,010 --> 01:14:29,080
Um, so effectively part of it is that if you look in the clustering literature,

1302
01:14:29,080 --> 01:14:32,215
there are lots of ways that people try and evaluate clustering,

1303
01:14:32,215 --> 01:14:36,610
and essentially any of those metrics and some other ones, you can, um,

1304
01:14:36,610 --> 01:14:41,590
port over, um, to, um, coreference evaluation.

1305
01:14:41,590 --> 01:14:45,895
I mean, why it's kind of difficult is the situation you have,

1306
01:14:45,895 --> 01:14:49,900
is that you have a gold standard which picks out certain clusters,

1307
01:14:49,900 --> 01:14:52,885
and the system picks out certain clusters,

1308
01:14:52,885 --> 01:14:58,000
and you get some result like this and you have to decide how good it is.

1309
01:14:58,000 --> 01:15:01,375
So I'm going to show you just quickly one particular algorithm.

1310
01:15:01,375 --> 01:15:04,630
So the B-CUBED algorithm uses

1311
01:15:04,630 --> 01:15:08,725
precision and recall and F-measure like we thought of before.

1312
01:15:08,725 --> 01:15:11,290
So it looks at, uh,

1313
01:15:11,290 --> 01:15:13,870
cluster identified by the system.

1314
01:15:13,870 --> 01:15:19,105
And it says, well this cluster is four-fifths,

1315
01:15:19,105 --> 01:15:20,890
um, gold cluster one,

1316
01:15:20,890 --> 01:15:23,470
so the precision is four-fifths.

1317
01:15:23,470 --> 01:15:28,240
But actually, um, there are six things in gold cluster one.

1318
01:15:28,240 --> 01:15:33,760
So it only has a recall of four-sixth of that cluster.

1319
01:15:33,760 --> 01:15:36,985
And then it similarly does for the other one,

1320
01:15:36,985 --> 01:15:39,445
the same kind of calculation.

1321
01:15:39,445 --> 01:15:43,990
And then it's going to average across the precisions and recalls,

1322
01:15:43,990 --> 01:15:49,045
um, and it's going to come up with an overall, um, B-CUBED score.

1323
01:15:49,045 --> 01:15:54,400
Um, in- if you think about this from an algorithm's perspective,

1324
01:15:54,400 --> 01:15:57,205
this is actually tricky because I sort of said,

1325
01:15:57,205 --> 01:16:00,460
um, okay, this cluster is mainly gold cluster one.

1326
01:16:00,460 --> 01:16:03,730
So use that as its reference,

1327
01:16:03,730 --> 01:16:06,820
but that means you have to do a bipartite graph alignment

1328
01:16:06,820 --> 01:16:09,520
between system clusters, and gold clusters.

1329
01:16:09,520 --> 01:16:12,955
So hidden in- hidden inside this evaluation,

1330
01:16:12,955 --> 01:16:16,210
um, system is actually an NP-complete problem.

1331
01:16:16,210 --> 01:16:19,525
But in practice you can normally do it heuristically well enough,

1332
01:16:19,525 --> 01:16:21,610
that the evaluation method, um,

1333
01:16:21,610 --> 01:16:23,245
runs and works.

1334
01:16:23,245 --> 01:16:25,435
Um, okay.

1335
01:16:25,435 --> 01:16:28,210
And so the kind of thing to notice is that,

1336
01:16:28,210 --> 01:16:29,965
if you under cluster,

1337
01:16:29,965 --> 01:16:32,350
you automatically get great precision,

1338
01:16:32,350 --> 01:16:34,135
but you get bad recall.

1339
01:16:34,135 --> 01:16:35,815
And if you over cluster,

1340
01:16:35,815 --> 01:16:40,405
you get- get great recall because everything that should be in the same cluster is,

1341
01:16:40,405 --> 01:16:42,910
um, but you get terrible precision.

1342
01:16:42,910 --> 01:16:48,175
And so what you want to be doing is balancing those two things.

1343
01:16:48,175 --> 01:16:51,280
Okay. Last two minutes,

1344
01:16:51,280 --> 01:16:53,380
just to give you some idea of performance.

1345
01:16:53,380 --> 01:16:58,285
So these are results from the OntoNotes dataset which is about 3,000 documents.

1346
01:16:58,285 --> 01:17:01,495
Chinese, English, labeled for coreference.

1347
01:17:01,495 --> 01:17:05,980
Um, the scores I'm reporting is actually an average over three metrics.

1348
01:17:05,980 --> 01:17:09,295
One of which is the one I just showed you for B-CUBED,

1349
01:17:09,295 --> 01:17:11,965
um, here are some numbers.

1350
01:17:11,965 --> 01:17:17,245
Um, so Lee et al 2010 was the Stanford system.

1351
01:17:17,245 --> 01:17:21,400
So there- there was this shared task evaluation of coreference systems.

1352
01:17:21,400 --> 01:17:24,265
And we believe that Jerry Hobbs, um,

1353
01:17:24,265 --> 01:17:28,885
was still right, and you could do fine with rule-based coreference.

1354
01:17:28,885 --> 01:17:30,890
And so in 2010,

1355
01:17:30,890 --> 01:17:36,030
we managed to beat all machine learning systems with a rule-based coreference system,

1356
01:17:36,030 --> 01:17:37,725
and we were proud of it.

1357
01:17:37,725 --> 01:17:40,455
Um, and that's its performance right here.

1358
01:17:40,455 --> 01:17:42,240
Um, in subsequent years,

1359
01:17:42,240 --> 01:17:45,150
people did start to do a bit better, um,

1360
01:17:45,150 --> 01:17:50,110
with, um, with, uh, machine learning systems.

1361
01:17:50,110 --> 01:17:51,820
But as you see, not very much,

1362
01:17:51,820 --> 01:17:57,040
right for these 2012 systems that this one's somewhat,

1363
01:17:57,040 --> 01:17:58,930
better this one really wasn't better,

1364
01:17:58,930 --> 01:18:02,335
um, this, um, but making a bit of progress.

1365
01:18:02,335 --> 01:18:07,975
Starting in 2015, there started to be neural systems.

1366
01:18:07,975 --> 01:18:11,200
Um, so Wiseman et al was sort of the first neural system,

1367
01:18:11,200 --> 01:18:14,245
I vaguely mentioned this Clark &amp; Manning system,

1368
01:18:14,245 --> 01:18:16,945
and the numbers are going up into the mid-sixties.

1369
01:18:16,945 --> 01:18:21,355
And this is the Kenton Lee system that has the end-to-end neural coreference,

1370
01:18:21,355 --> 01:18:23,815
and on English is getting about 67.

1371
01:18:23,815 --> 01:18:25,795
So something you'll notice from this,

1372
01:18:25,795 --> 01:18:28,075
is the numbers aren't great.

1373
01:18:28,075 --> 01:18:31,420
So coreference is still far from a solved problem.

1374
01:18:31,420 --> 01:18:33,790
Um, so if you want to have a bit of fun, um,

1375
01:18:33,790 --> 01:18:37,450
you can go out and try coreference systems for yourself.

1376
01:18:37,450 --> 01:18:41,470
Um, there's a Stanford one on the first link or the one from Hugging Face

1377
01:18:41,470 --> 01:18:44,965
is a good modern coreference system as well.

1378
01:18:44,965 --> 01:18:47,740
And if you just try these out with some pieces of text,

1379
01:18:47,740 --> 01:18:50,380
you'll notice they still get lots of things wrong.

1380
01:18:50,380 --> 01:18:52,480
Um, so there's still more work to do,

1381
01:18:52,480 --> 01:18:55,270
because this is just a harder language understanding task,

1382
01:18:55,270 --> 01:18:57,535
[NOISE] which is just kind of like, um,

1383
01:18:57,535 --> 01:19:01,270
Jerry Hobbs and Terry- Terry Winograd earlier observed.

1384
01:19:01,270 --> 01:19:04,675
Okay, um, but I'll stop there for now. Thanks a lot.

1385
01:19:04,675 --> 01:19:09,745
Um, oh yeah, I should have a reminder, invited speaker next Tuesday.

1386
01:19:09,745 --> 01:19:11,575
Um, so I'll be taking,

1387
01:19:11,575 --> 01:19:14,720
um, attendance for invited speakers.

