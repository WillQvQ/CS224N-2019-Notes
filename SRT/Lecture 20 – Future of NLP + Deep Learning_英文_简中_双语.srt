1
00:00:04,760 --> 00:00:09,570
Let's get started. So welcome to the very final lecture of the class.
让我们开始吧。欢迎来到课堂的最后一堂课。

2
00:00:09,570 --> 00:00:11,475
I hope you're all surviving the last week and,
我希望你们在上周幸存下来，

3
00:00:11,475 --> 00:00:13,830
uh, wrapping up your projects.
呃，结束你的项目。

4
00:00:13,830 --> 00:00:18,540
So today we're going to be hearing about the future of NLP and deep learning.
所以今天我们将会听到NLP和深度学习的未来。

5
00:00:18,540 --> 00:00:22,440
Uh, so Chris is still traveling and today we're going to be having Kevin Clark,
呃，所以克里斯还在旅行，今天我们将要有凯文克拉克，

6
00:00:22,440 --> 00:00:24,825
who's one of the PhD students in the lab, uh,
谁是实验室的博士生之一，呃，

7
00:00:24,825 --> 00:00:26,580
in the NLP lab,
在NLP实验室，

8
00:00:26,580 --> 00:00:29,610
and he was also one of the head TAs for the class last year.
去年他也是该班的首席助教之一。

9
00:00:29,610 --> 00:00:31,785
So he's very familiar with the class as a whole.
所以他对整个班级非常熟悉。

10
00:00:31,785 --> 00:00:33,765
Um, so, take it away Kevin.
嗯，把它带走凯文。

11
00:00:33,765 --> 00:00:37,830
Okay. Thanks, Abby. Um, yeah,
好的。谢谢，艾比。嗯，是的，

12
00:00:37,830 --> 00:00:40,440
it's great to be back after being a TA last year.
去年成为TA后回来真好。

13
00:00:40,440 --> 00:00:45,350
Um, I'm really excited today to be talking about the future of deep learning and NLP.
嗯，今天我真的很兴奋谈论深度学习和NLP的未来。

14
00:00:45,350 --> 00:00:49,090
Um, obviously, trying to forecast the future, um,
嗯，显然，试图预测未来，嗯，

15
00:00:49,090 --> 00:00:51,800
for deep learning or anything in that space is really
深度学习或那个空间中的任何东西都是真的

16
00:00:51,800 --> 00:00:54,800
difficult because the field is changing super quickly.
困难，因为该领域正在快速变化。

17
00:00:54,800 --> 00:00:57,080
Um, so as one reference point, um,
嗯，作为一个参考点，嗯，

18
00:00:57,080 --> 00:01:00,050
let's look at what did deep learning for NLP,
让我们来看看NLP的深度学习，

19
00:01:00,050 --> 00:01:02,290
um, look like about five years ago.
嗯，看起来大约五年前。

20
00:01:02,290 --> 00:01:08,300
And really, a lot of ideas that are now considered to be pretty core techniques,
实际上，很多想法现在被认为是非常核心的技术，

21
00:01:08,300 --> 00:01:10,445
um, when we think of deep learning and NLP,
嗯，当我们想到深度学习和NLP时，

22
00:01:10,445 --> 00:01:12,170
um, didn't even exist back then.
嗯，那时候甚至不存在。

23
00:01:12,170 --> 00:01:14,870
Um, so things you learned in this class like Seq2Seq,
嗯，你在这堂课上学到的东西就像Seq2Seq，

24
00:01:14,870 --> 00:01:17,180
attention mechanism, um, large-scale,
注意机制，嗯，大规模，

25
00:01:17,180 --> 00:01:20,105
reading comprehension, uh, even frameworks
阅读理解，呃，甚至是框架

26
00:01:20,105 --> 00:01:23,300
such as TensorFlow or Pytorch, um, didn't exist.
比如TensorFlow或Pytorch，嗯，不存在。

27
00:01:23,300 --> 00:01:27,145
And, uh, the point I want to make with this is that, um,
而且，呃，我想用这个做点是，嗯，

28
00:01:27,145 --> 00:01:31,205
because of this it's really difficult to, to look into the future and say,
因为这样，展望未来真的很难说，

29
00:01:31,205 --> 00:01:33,665
okay, what are things going to be like?
好吧，会发生什么事情？

30
00:01:33,665 --> 00:01:38,065
Um, what I think we can do though is look at, um,
嗯，我认为我们可以做的就是看，嗯，

31
00:01:38,065 --> 00:01:41,870
areas that right now are really sort of taking off, um,
现在的地方真的有点起飞，嗯，

32
00:01:41,870 --> 00:01:43,640
so areas in which, um,
那些领域，嗯，

33
00:01:43,640 --> 00:01:46,370
there's a lot, been a lot of recent success and kind of, uh,
有很多，最近有很多成功，有点，呃，

34
00:01:46,370 --> 00:01:48,100
project from that, that,
那个项目，那个，

35
00:01:48,100 --> 00:01:50,900
those same areas will likely be important in the future.
这些相同的领域将来可能很重要。

36
00:01:50,900 --> 00:01:55,820
Um, and in this talk I'm going to be mostly focusing on one key idea of
嗯，在这次演讲中，我将主要关注一个关键的想法

37
00:01:55,820 --> 00:01:58,970
wh- key idea which is the idea of leveraging
什么是关键的想法，这是利用的想法

38
00:01:58,970 --> 00:02:02,915
unlabeled examples when training our NLP systems.
训练我们的NLP系统时没有标记的例子。

39
00:02:02,915 --> 00:02:07,490
So I'll be talking a bit about doing that for machine translation, um,
所以我会谈谈为机器翻译做这件事，嗯，

40
00:02:07,490 --> 00:02:10,820
both in improving the quality of translation and even
两者都在提高翻译质量甚至

41
00:02:10,820 --> 00:02:14,315
in doing a translation in an unsupervised way.
以无人监督的方式进行翻译。

42
00:02:14,315 --> 00:02:16,170
So that means you don't have, um,
这意味着你没有，嗯，

43
00:02:16,170 --> 00:02:19,230
paired sentences, uh, with, with their translations.
成对的句子，呃，用，和他们的翻译。

44
00:02:19,230 --> 00:02:23,365
Um, you try to learn a translation model only from a monolingual corpus.
嗯，你试图只从单语语料库中学习翻译模型。

45
00:02:23,365 --> 00:02:27,115
Um, the second thing I'll be talking a little bit about is, uh,
嗯，我要谈的第二件事是，呃，

46
00:02:27,115 --> 00:02:29,330
OpenAI's GPT-2, um,
OpenAI的GPT-2，嗯，

47
00:02:29,330 --> 00:02:32,435
and in general this phenomenon of really scaling up,
而且一般来说这种现象非常大，

48
00:02:32,435 --> 00:02:34,045
um, deep learning models.
嗯，深度学习模型。

49
00:02:34,045 --> 00:02:38,330
Um, I know you saw a little bit of this in the lecture on contextual representations,
嗯，我知道你在关于语境表征的讲座中看到了一点点，

50
00:02:38,330 --> 00:02:40,340
but this, but this will be a little bit more in depth.
但是这个，但这会更深入一点。

51
00:02:40,340 --> 00:02:42,335
Um, and I think, um,
嗯，我想，嗯，

52
00:02:42,335 --> 00:02:46,655
these new developments in NLP have had some,
NLP的这些新发展有一些，

53
00:02:46,655 --> 00:02:48,600
um, pretty big, uh,
嗯，很大，呃，

54
00:02:48,600 --> 00:02:50,595
impacts in terms of,
影响，

55
00:02:50,595 --> 00:02:53,755
uh, more broadly kind of beyond even the technology we're using,
呃，更广泛地说，甚至超越了我们正在使用的技术，

56
00:02:53,755 --> 00:02:55,070
and in particular, I mean,
特别是，我的意思是，

57
00:02:55,070 --> 00:03:00,555
starting to raise more and more concerns about the social impact of NLP, um,
开始对NLP的社会影响提出越来越多的担忧，嗯，

58
00:03:00,555 --> 00:03:03,520
both, um, in what our models can do and also in kind
嗯，我们的模型可以做什么，也可以做

59
00:03:03,520 --> 00:03:06,590
of plans of what, where people are looking to apply these models, um,
人们希望应用这些模型的计划，嗯，

60
00:03:06,590 --> 00:03:09,755
and I think that really has some risks associated with it, um,
而且我认为这确实存在一些与之相关的风险，嗯，

61
00:03:09,755 --> 00:03:13,160
in terms of security also in terms of areas like bias.
在安全方面也是偏见等方面。

62
00:03:13,160 --> 00:03:16,465
Um, I'm also gonna talk a bit about future areas of research,
嗯，我还要谈谈未来的研究领域，

63
00:03:16,465 --> 00:03:19,140
um, these are mostly research areas now that are, um,
嗯，现在这些都是研究领域，嗯，

64
00:03:19,140 --> 00:03:22,060
over the past year have really kind of developed into
在过去的一年里真的有点发展成了

65
00:03:22,060 --> 00:03:27,180
promising areas and I expect they will continue to be important in the future.
有希望的领域，我希望它们在未来继续发挥重要作用。

66
00:03:27,190 --> 00:03:29,700
Okay, um, to start with,
好的，嗯，首先，

67
00:03:29,700 --> 00:03:33,305
I wanna ask this question, why has deep learning been so successful recently?
我想问这个问题，为什么最近深度学习如此成功？

68
00:03:33,305 --> 00:03:35,510
Um, I like this comic, um,
嗯，我喜欢这个漫画，嗯，

69
00:03:35,510 --> 00:03:38,045
here there's a statistical learning person,
这里有一个统计学习者，

70
00:03:38,045 --> 00:03:41,030
um, and they've got some really complicated,
嗯，他们有一些非常复杂的，

71
00:03:41,030 --> 00:03:44,015
um, well-motivated, uh, method for doing, um,
嗯，精力充沛，呃，做的方法，嗯，

72
00:03:44,015 --> 00:03:45,510
the task they care about,
他们关心的任务，

73
00:03:45,510 --> 00:03:47,460
and then the neural net person just says,
然后神经网络的人说，

74
00:03:47,460 --> 00:03:49,110
er, stack more layers.
呃，堆叠更多层。

75
00:03:49,110 --> 00:03:52,020
Um, so, so the point I want to make here is that, um,
嗯，所以我想在这里说的是，嗯，

76
00:03:52,020 --> 00:03:56,075
deep learning has not been successful recently because it's more
深度学习最近并没有成功，因为它更多

77
00:03:56,075 --> 00:04:01,530
theoretically motivated or it's more sophisticated than previous techniques, um.
嗯，理论上有动力或比以前的技术更复杂。

78
00:04:01,530 --> 00:04:04,245
In fact I would say that actually a lot of, um,
事实上，我会说实际上很多，嗯，

79
00:04:04,245 --> 00:04:06,635
older statistical methods have more of
较旧的统计方法有更多

80
00:04:06,635 --> 00:04:10,175
a theoretical underpinning than some of the tricks we do in deep learning.
理论基础而不是我们在深度学习中所做的一些技巧。

81
00:04:10,175 --> 00:04:14,050
Um, really the thing that makes deep learning so
嗯，真的是深入学习的东西

82
00:04:14,050 --> 00:04:17,660
successful in recent years has been its ability to scale, right.
近年来成功的是它的扩展能力，对吧。

83
00:04:17,660 --> 00:04:22,310
So neural nets, as we increase the size of the data,
所以神经网络，随着我们增加数据的大小，

84
00:04:22,310 --> 00:04:24,265
as we increase the size of the models, um,
当我们增加模型的大小时，嗯，

85
00:04:24,265 --> 00:04:26,170
they get a really big boost in accuracy,
他们的准确性得到了极大的提升，

86
00:04:26,170 --> 00:04:28,565
in ways other approaches do not.
其他方法没有。

87
00:04:28,565 --> 00:04:32,210
And, um, if you look to the '80s and '90s, um,
而且，嗯，如果你看看80年代和90年代，嗯，

88
00:04:32,210 --> 00:04:36,190
there was actually plenty of research in neural nets going on, um.
实际上有很多关于神经网络的研究，嗯。

89
00:04:36,190 --> 00:04:39,130
But it hadn't, doesn't have a hype around it that it does
但它没有，它没有围绕它做炒作

90
00:04:39,130 --> 00:04:42,205
now and that seems likely to be because,
现在，这似乎可能是因为，

91
00:04:42,205 --> 00:04:45,020
um, in the past there wasn't, um,
嗯，过去没有，嗯，

92
00:04:45,020 --> 00:04:47,180
the same resources in terms of computers,
计算机方面的资源相同，

93
00:04:47,180 --> 00:04:49,235
in terms of data and, um,
就数据而言，嗯，

94
00:04:49,235 --> 00:04:53,120
only now after we've reached sort of an inflection point where we can
直到我们达到了一个拐点，我们才能做到

95
00:04:53,120 --> 00:04:55,220
really take advantage of scale in
真正利用规模

96
00:04:55,220 --> 00:04:57,965
our deep learning models and we started to see it become,
我们的深度学习模式，我们开始看到它，

97
00:04:57,965 --> 00:05:01,520
um, a really successful paradigm for machine learning.
嗯，这是一个非常成功的机器学习范例。

98
00:05:01,520 --> 00:05:04,080
Um, if we look at big, uh,
嗯，如果我们看一下大，呃，

99
00:05:04,080 --> 00:05:06,075
deep learning success stories, um,
深度学习成功的故事，嗯，

100
00:05:06,075 --> 00:05:10,200
I think, uh, you can see kind of this idea play out, right?
我想，呃，你可以看到这种想法发挥出来，对吧？

101
00:05:10,200 --> 00:05:16,490
So here are three of what are arguably the most famous successes of deep learning, right.
所以这里有三个可以说是深度学习中最着名的成功，对吧。

102
00:05:16,490 --> 00:05:18,620
So there's image recognition, where before,
所以有图像识别，以前，

103
00:05:18,620 --> 00:05:20,870
people used very highly engineered, um,
人们使用非常高度工程，嗯，

104
00:05:20,870 --> 00:05:25,870
features to classify images and now neural nets are much superior, um, to those methods.
对图像进行分类的功能现在神经网络对这些方法来说是非常优越的。

105
00:05:25,870 --> 00:05:29,785
Um, machine translation has really closed the gap between, um,
嗯，机器翻译真的缩小了，嗯，

106
00:05:29,785 --> 00:05:33,020
phrase-based systems and human quality translation,
基于短语的系统和人类素质翻译，

107
00:05:33,020 --> 00:05:35,730
so this is widely used in things like Google Translate
所以这被广泛用于谷歌翻译

108
00:05:35,730 --> 00:05:39,115
and the quality has actually gotten a lot better over the past five years.
在过去的五年里，质量实际上已经变得更好了。

109
00:05:39,115 --> 00:05:43,550
Um, another example that had a lot of hype around it is game-playing, so, um,
嗯，另一个有很多炒作的例子是游戏，所以，嗯，

110
00:05:43,550 --> 00:05:46,460
there's been work on Atari games, there's been AlphaGo,
有关Atari游戏的工作，有AlphaGo，

111
00:05:46,460 --> 00:05:50,395
uh, more recently there's been AlphaStar and OpenAI Five.
呃，最近有AlphaStar和OpenAI Five。

112
00:05:50,395 --> 00:05:53,599
Um, if you look at all three of these cases underlying
嗯，如果你看看所有这三个案例的基础

113
00:05:53,599 --> 00:05:57,200
these successes is really large amounts of data, right.
这些成功实际上是大量的数据。

114
00:05:57,200 --> 00:05:58,550
So for ImageNet, um,
对于ImageNet，嗯，

115
00:05:58,550 --> 00:06:00,020
for image recognition, um,
用于图像识别，嗯，

116
00:06:00,020 --> 00:06:03,035
there is the ImageNet dataset which has 14 million images,
ImageNet数据集有1400万张图像，

117
00:06:03,035 --> 00:06:06,320
uh, machine translation datasets often have millions of examples.
呃，机器翻译数据集通常有数百万个例子。

118
00:06:06,320 --> 00:06:09,275
Um, for game playing you can actually
嗯，对于玩游戏你实际上可以

119
00:06:09,275 --> 00:06:12,470
generate as much training data as you want essentially,
尽可能多地生成训练数据，

120
00:06:12,470 --> 00:06:14,690
um, just by running your agent,
嗯，只需运行你的代理，

121
00:06:14,690 --> 00:06:16,040
um, within the game,
嗯，在游戏中，

122
00:06:16,040 --> 00:06:18,660
um, over and over again.
嗯，一遍又一遍。

123
00:06:19,120 --> 00:06:21,360
Um, so if we,
嗯，如果我们，

124
00:06:21,360 --> 00:06:23,590
if we look to NLP, um,
如果我们看看NLP，嗯，

125
00:06:23,590 --> 00:06:27,740
the story is quite a bit different for a lot of tasks, um, right.
对于很多任务来说，这个故事有点不同，嗯，对。

126
00:06:27,740 --> 00:06:32,030
So if you look at even pretty core kind of popular tasks,
所以，如果你看一下非常核心的流行任务，

127
00:06:32,030 --> 00:06:35,060
to say, reading comprehension in English, um,
用英语阅读理解，嗯，

128
00:06:35,060 --> 00:06:39,710
datasets like SQuAD are in the order of like 100,000 examples
像SQUAD这样的数据集大约有100,000个例子

129
00:06:39,710 --> 00:06:44,810
which is considerably less than the millions or tens of millions of examples,
这比数百万或数千万的例子少得多，

130
00:06:44,810 --> 00:06:47,105
um, that these previous,
嗯，这些以前，

131
00:06:47,105 --> 00:06:50,285
um, successes have, have benefited from.
嗯，成功了，从中受益了。

132
00:06:50,285 --> 00:06:54,210
Um, and that's of course only for English, right.
嗯，这当然只适用于英语，对吧。

133
00:06:54,210 --> 00:06:55,770
Um, there are, um,
嗯，有，嗯，

134
00:06:55,770 --> 00:06:59,570
thousands of other languages and this is I think
成千上万的其他语言，我想

135
00:06:59,570 --> 00:07:03,770
a problem with NLP data as it exists today.
如今存在的NLP数据存在问题。

136
00:07:03,770 --> 00:07:06,455
Um, the vast majority of data is in English, um,
嗯，绝大多数数据是英文的，嗯，

137
00:07:06,455 --> 00:07:10,070
when in reality fewer than 10% of the world's population,
实际上，这个世界人口不到10％，

138
00:07:10,070 --> 00:07:12,190
um, speak English as their first language.
嗯，说英语作为他们的第一语言。

139
00:07:12,190 --> 00:07:17,565
Um, so these problems with small datasets are only compounded if you look at,
嗯，所以如果你看一下，这些小数据集的问题只会复杂化，

140
00:07:17,565 --> 00:07:21,465
um, the full spectrum of languages, um, that exist.
嗯，存在的各种语言，嗯。

141
00:07:21,465 --> 00:07:23,955
Um, so, as what do we do,
嗯，所以，我们做什么，

142
00:07:23,955 --> 00:07:25,800
uh, when we're limited by this data,
呃，当我们受到这些数据的限制时，

143
00:07:25,800 --> 00:07:30,560
but we want to take advantage of deep learning scale and train the biggest models we can.
但是我们希望利用深度学习规模并培养我们能够做到的最大模型。

144
00:07:30,560 --> 00:07:32,510
Um, the popular solution, um,
嗯，流行的解决方案，嗯，

145
00:07:32,510 --> 00:07:36,230
that's especially had recent success is using unlabeled data, um,
最近成功的是使用未标记的数据，嗯，

146
00:07:36,230 --> 00:07:37,805
because unlike labeled data,
因为与标记数据不同，

147
00:07:37,805 --> 00:07:40,840
unlabeled data is very easy to acquire for language.
未标记的数据很容易获得语言。

148
00:07:40,840 --> 00:07:42,120
Um, you can just go to the Internet,
嗯，你可以去互联网，

149
00:07:42,120 --> 00:07:44,685
you can go to books, you can get lots of text, um,
你可以去看书，你可以得到很多文字，嗯，

150
00:07:44,685 --> 00:07:49,370
whereas labeled data usually requires at the least crowdsourcing examples.
而标签数据通常至少需要众包示例。

151
00:07:49,370 --> 00:07:54,730
Um, in some cases you even require someone who's an expert in something like linguistics,
嗯，在某些情况下，你甚至需要一个像语言学这样的专家，

152
00:07:54,730 --> 00:07:57,820
um, to, to annotate that data.
嗯，要注释那些数据。

153
00:07:59,510 --> 00:08:03,890
Okay, so, um, this first part of the talk is going to be applying
好吧，嗯，这个演讲的第一部分将会应用

154
00:08:03,890 --> 00:08:08,195
this idea of leveraging unlabeled data to improve our NLP models,
利用未标记数据来改进我们的NLP模型的想法，

155
00:08:08,195 --> 00:08:11,070
um, to the task of machine translation.
嗯，对机器翻译的任务。

156
00:08:11,990 --> 00:08:15,170
Um, so let's talk about machine translation data.
嗯，让我们来谈谈机器翻译数据。

157
00:08:15,170 --> 00:08:20,520
Um, it is true that there do exist quite large datasets for machine translation.
嗯，确实存在用于机器翻译的相当大的数据集。

158
00:08:20,520 --> 00:08:23,165
Um, those datasets don't exist because
嗯，那些数据集不存在，因为

159
00:08:23,165 --> 00:08:26,870
NLP researchers have annotated texts for the purpose of training their models, right.
对于训练他们的模型，NLP研究人员已经注释了文本，对吧。

160
00:08:26,870 --> 00:08:29,750
They exist because, er, in various settings,
它们的存在是因为，呃，在各种环境中，

161
00:08:29,750 --> 00:08:33,195
translation is done just because it's useful, so for example,
翻译只是因为它很有用，所以例如，

162
00:08:33,195 --> 00:08:35,070
proceedings of the European Parliament,
欧洲议会的议事程序，

163
00:08:35,070 --> 00:08:37,019
um, proceedings of the United Nations,
嗯，联合国会议记录，

164
00:08:37,019 --> 00:08:41,320
um, some, uh, news sites, they translate their articles into many languages.
嗯，有些，呃，新闻网站，他们把文章翻译成多种语言。

165
00:08:41,320 --> 00:08:46,610
Um, so really, the machine translation data we use to train our models are often
嗯，实际上，我们用来训练模型的机器翻译数据经常是

166
00:08:46,610 --> 00:08:52,745
more of byproducts of existing cases where translation is wanted rather than,
更多需要翻译的现有案例的副产品，而不是

167
00:08:52,745 --> 00:08:57,500
um, kind of a full sampling of the sort of text we see in the world.
嗯，这是我们在世界上看到的那种文本的完整样本。

168
00:08:57,500 --> 00:08:58,910
Um, so that means number one,
嗯，这意味着第一，

169
00:08:58,910 --> 00:09:00,680
it's quite limited in domain, right.
它在域中非常有限，对吧。

170
00:09:00,680 --> 00:09:03,580
So it's not easy to find translated tweets,
所以找到翻译的推文并不容易，

171
00:09:03,580 --> 00:09:05,410
um, unless you happen to work for Twitter.
嗯，除非你碰巧在Twitter上工作。

172
00:09:05,410 --> 00:09:08,145
Um, in addition to that, um,
嗯，除此之外，嗯，

173
00:09:08,145 --> 00:09:12,230
there's limitations in terms of the languages that are covered, right.
对于涵盖的语言而言，这是有限制的。

174
00:09:12,230 --> 00:09:14,750
So some languages, say European languages,
有些语言，比如欧洲语言，

175
00:09:14,750 --> 00:09:16,500
there's a lot of translation data, um,
有很多翻译数据，嗯，

176
00:09:16,500 --> 00:09:19,180
for other languages there's much less.
对于其他语言来说，要少得多。

177
00:09:19,180 --> 00:09:22,040
Um, so in these settings where we want to work on
嗯，所以在我们想要处理的这些环境中

178
00:09:22,040 --> 00:09:25,220
a different domain or where we want to work with a low resource language,
一个不同的域或我们想要使用低资源语言的地方，

179
00:09:25,220 --> 00:09:27,995
um, we're limited by labeled data, um,
嗯，我们受到标签数据的限制，嗯，

180
00:09:27,995 --> 00:09:30,985
but what we can do is pretty easily find unlabeled data.
但我们能做的就是很容易找到未标记的数据。

181
00:09:30,985 --> 00:09:33,620
Um, so it's actually a pretty solved problem, um,
嗯，所以这实际上是一个非常解决的问题，嗯，

182
00:09:33,620 --> 00:09:37,010
maybe not 100%, but we can with good accuracy look at
也许不是100％，但我们可以很好的准确看待

183
00:09:37,010 --> 00:09:41,195
some text and decide what language it's in and train a classifier to do that.
一些文本并决定它所使用的语言并训练分类器来做到这一点。

184
00:09:41,195 --> 00:09:43,610
Um, so this means it's really easy to find
嗯，这意味着它很容易找到

185
00:09:43,610 --> 00:09:46,100
data in any language you care about because you can just go on
您关心的任何语言的数据，因为您可以继续

186
00:09:46,100 --> 00:09:48,440
the web and essentially search for data in
网络，基本上搜索数据

187
00:09:48,440 --> 00:09:52,650
that language and acquire a large corpus of monolingual data.
那种语言并获得了大量的单语数据。

188
00:09:55,240 --> 00:10:00,775
Okay, um, I'm now going into the first approach,
好的，嗯，我现在要进入第一种方法，

189
00:10:00,775 --> 00:10:03,095
um, I'm going to talk about on using
嗯，我要谈谈使用

190
00:10:03,095 --> 00:10:06,365
unlabeled data to improve machine translation models.
未标记的数据，以改善机器翻译模型。

191
00:10:06,365 --> 00:10:09,410
Um, this technique is called pre-training and it's
嗯，这种技术叫做预训练，就是这样

192
00:10:09,410 --> 00:10:12,790
really reminiscent of ideas like, um, ELMo.
真的让人想起像嗯，ELMo这样的想法。

193
00:10:12,790 --> 00:10:16,580
Um, the idea is to pre-train by doing language modeling.
嗯，这个想法是通过语言建模来预训练。

194
00:10:16,580 --> 00:10:18,350
So if we have, um,
如果我们有，嗯，

195
00:10:18,350 --> 00:10:21,350
two languages we're interested in translating,
我们有兴趣翻译的两种语言，

196
00:10:21,350 --> 00:10:22,535
um, from one end to the other,
嗯，从一端到另一端，

197
00:10:22,535 --> 00:10:27,480
we'll collect large datasets for both of those languages and then we can train,
我们将收集这两种语言的大型数据集，然后我们可以训练，

198
00:10:27,480 --> 00:10:29,040
uh, two language models,
呃，两种语言模型，

199
00:10:29,040 --> 00:10:33,365
one each on that data and then, um,
一个关于那个数据然后，嗯，

200
00:10:33,365 --> 00:10:34,490
we can use those, uh,
我们可以用那些，呃，

201
00:10:34,490 --> 00:10:38,450
pre-trained language models as initialization for a machine translation system.
预先训练的语言模型作为机器翻译系统的初始化。

202
00:10:38,450 --> 00:10:41,720
Um, so the encoder will get initialized with
嗯，编码器将初始化

203
00:10:41,720 --> 00:10:45,485
the weights of the language model trained on the source side language, um,
用源语言语言训练的语言模型的权重，嗯，

204
00:10:45,485 --> 00:10:49,830
the decoder will get initialized with weights trained on the target size language, uh,
解码器将使用目标大小语言训练的权重进行初始化，呃，

205
00:10:49,830 --> 00:10:51,225
and this will, um,
这会，嗯，

206
00:10:51,225 --> 00:10:55,490
improve the performance of your model because during this pre-training, um,
提高模型的性能，因为在这次预训练期间，嗯，

207
00:10:55,490 --> 00:10:59,750
we hope that our language models will be learning useful information such as, you know,
我们希望我们的语言模型能够学习有用的信息，例如，

208
00:10:59,750 --> 00:11:02,460
the meaning of words or, um, uh,
单词的意思或者，嗯，呃，

209
00:11:02,460 --> 00:11:05,250
the kind of structure of the language, um,
这种语言的结构，嗯，

210
00:11:05,250 --> 00:11:09,020
they are processing, um, and this can, uh,
他们正在处理，嗯，这可以，呃，

211
00:11:09,020 --> 00:11:12,410
down the line help the machine translation model,
下线帮助机器翻译模型，

212
00:11:12,410 --> 00:11:15,020
um, when we fine tune it.
嗯，当我们微调它。

213
00:11:15,020 --> 00:11:17,464
Um, let me pause here and ask if there are any questions,
嗯，让我暂停一下，询问是否有任何问题，

214
00:11:17,464 --> 00:11:18,620
and just in general, feel,
一般来说，感觉，

215
00:11:18,620 --> 00:11:24,870
feel free to ask questions throughout this talk. Okay.
在整个演讲中随时提出问题。好的。

216
00:11:25,920 --> 00:11:33,385
So, so here is a plot showing some results of this pre-training technique.
所以，这里有一个图表显示了这种预训练技术的一些结果。

217
00:11:33,385 --> 00:11:36,040
Um, so this is English to German translation.
嗯，这是英语到德语的翻译。

218
00:11:36,040 --> 00:11:39,805
Uh, the x-axis is how much training data,
呃，x轴是多少训练数据，

219
00:11:39,805 --> 00:11:41,920
as in unsupervised training data, um,
和无人监督的训练数据一样，嗯，

220
00:11:41,920 --> 00:11:43,075
you provide these models,
你提供这些模型，

221
00:11:43,075 --> 00:11:45,355
but of course they also have large amounts
但当然他们也有很多

222
00:11:45,355 --> 00:11:48,940
of monolingual data for this pre-training step.
这个预训练步骤的单语数据。

223
00:11:48,940 --> 00:11:51,970
And you can see that this works pretty well, right?
你可以看到这很好用，对吗？

224
00:11:51,970 --> 00:11:54,445
So you've got about two blue points, um,
所以你有两个蓝点，嗯，

225
00:11:54,445 --> 00:11:57,670
increase in performance, so that's this red line above the blue line,
性能提高，所以这条红线高于蓝线，

226
00:11:57,670 --> 00:12:00,175
um, when doing this pre-training technique.
嗯，在做这种预训练技术时。

227
00:12:00,175 --> 00:12:01,689
And not too surprisingly,
并不奇怪，

228
00:12:01,689 --> 00:12:06,740
this gain is especially large when the amount of labeled data is small.
当标记数据量很小时，这种增益特别大。

229
00:12:10,350 --> 00:12:14,080
Um, there is a problem with,
嗯，有问题，

230
00:12:14,080 --> 00:12:17,260
uh, pre-training which I want to address, which is that, uh,
呃，我想要解决的预训练，就是那个，呃，

231
00:12:17,260 --> 00:12:18,850
in pre-training, you have
在预训练中，你有

232
00:12:18,850 --> 00:12:20,890
these two separate language models and there's never
这两个独立的语言模型，从来没有

233
00:12:20,890 --> 00:12:23,035
really any interaction between the two,
真的是两者之间的任何互动，

234
00:12:23,035 --> 00:12:25,780
um, when you're running them on the unlabeled corpus.
嗯，当你在未标记的语料库上运行它们时。

235
00:12:25,780 --> 00:12:28,435
Um, so here's a simple technique, um,
嗯，所以这是一个简单的技术，嗯，

236
00:12:28,435 --> 00:12:32,485
that tries to solve this problem and it's called self-training.
试图解决这个问题，它被称为自我训练。

237
00:12:32,485 --> 00:12:37,090
Um, the idea is given a sentence from our monolingual corpus,
嗯，这个想法是从我们的单语语料库中给出一个句子，

238
00:12:37,090 --> 00:12:40,210
so in this case, "I traveled to Belgium," that's an English sentence.
所以在这种情况下，“我前往比利时”，这是一个英文句子。

239
00:12:40,210 --> 00:12:45,400
Um, we won't have a human provided translation for this sentence, uh,
嗯，我们不会为这句话提供人为翻译，呃，

240
00:12:45,400 --> 00:12:48,925
but what we can do is we can run our machine translation model,
但我们能做的就是运行我们的机器翻译模型，

241
00:12:48,925 --> 00:12:52,750
and we'll get a translation in the target language.
我们将获得目标语言的翻译。

242
00:12:52,750 --> 00:12:56,320
Um, since this is from a machine learning model it won't be perfect, uh,
嗯，因为这是一个机器学习模型，它不会是完美的，呃，

243
00:12:56,320 --> 00:13:00,160
but we can hope that maybe our model can still learn from this kind
但我们希望我们的模型仍然可以从这种模式中学习

244
00:13:00,160 --> 00:13:03,580
of noisy labeled example, right?
嘈杂的标签示例，对吧？

245
00:13:03,580 --> 00:13:05,275
So we, we treat, um,
所以，我们，我们对待，嗯，

246
00:13:05,275 --> 00:13:08,230
our original monolingual sentence and it's machine-provided
我们原来的单语句子和机器提供的

247
00:13:08,230 --> 00:13:12,490
translation as though it were a human-provided translation and,
翻译好像是人为翻译的，

248
00:13:12,490 --> 00:13:17,030
uh, train our machine learning model as normal on this example.
呃，在这个例子中正常训练我们的机器学习模型。

249
00:13:19,800 --> 00:13:24,190
Um, I think this seems pretty strange actually as- as
嗯，我觉得这实际上看起来很奇怪

250
00:13:24,190 --> 00:13:27,970
a method when you first see it because it seems really circular, right?
当你第一次看到它时，因为它看起来真的是圆形的，对吗？

251
00:13:27,970 --> 00:13:31,315
So if you look at this, um, the, uh,
所以，如果你看看这个，嗯，呃，

252
00:13:31,315 --> 00:13:33,850
translation that the model is being trained to
正在训练模型的翻译

253
00:13:33,850 --> 00:13:38,095
produce is actually exactly what it already produces to begin with,
产品实际上就是它开始生产的产品，

254
00:13:38,095 --> 00:13:43,420
right, because, um, this translation came from our model in the first place.
是的，因为，嗯，这个翻译首先来自我们的模型。

255
00:13:43,420 --> 00:13:45,700
Um, so actually in practice,
嗯，实际上在实践中，

256
00:13:45,700 --> 00:13:49,480
this is not a technique that's very widely used due to this problem,
由于这个问题，这不是一种被广泛使用的技术，

257
00:13:49,480 --> 00:13:53,365
um, but it motivates another technique called back-translation.
嗯，但它激发了另一种称为反向翻译的技术。

258
00:13:53,365 --> 00:13:56,740
And this technique is really a very popular, um,
这种技术真的非常流行，嗯，

259
00:13:56,740 --> 00:13:59,950
solution to that problem, and it's the method, um,
这个问题的解决方案，这是方法，嗯，

260
00:13:59,950 --> 00:14:04,240
that has had a lot of success in using unlabeled data for translation.
在使用未标记的数据进行翻译方面取得了很大成功。

261
00:14:04,240 --> 00:14:06,940
So here's the approach rather than only
所以这是方法，而不仅仅是

262
00:14:06,940 --> 00:14:10,855
having our translation system that goes from source language to target language,
我们的翻译系统从源语言到目标语言，

263
00:14:10,855 --> 00:14:13,210
um, we're also going to train a model that
嗯，我们也要训练一个模型

264
00:14:13,210 --> 00:14:16,375
goes from our target language to our source language.
从我们的目标语言到我们的源语言。

265
00:14:16,375 --> 00:14:18,670
And so in this case, if,
在这种情况下，如果，

266
00:14:18,670 --> 00:14:21,340
if at the end of the day we want a French to English model, um,
如果在一天结束时我们想要一个法语到英语的模型，嗯，

267
00:14:21,340 --> 00:14:24,910
we're gonna start by actually training an English to French model.
我们将从实际培训英语到法语模型开始。

268
00:14:24,910 --> 00:14:27,880
And then we can do something that's a lot like self-labeling.
然后我们可以做一些像自我标记的事情。

269
00:14:27,880 --> 00:14:30,205
So we take a English sentence.
所以我们用英语句子。

270
00:14:30,205 --> 00:14:33,370
We run our English to French model and translate it.
我们运行英语到法语模型并进行翻译。

271
00:14:33,370 --> 00:14:35,950
The difference to what we did before is that
与我们之前所做的不同之处在于

272
00:14:35,950 --> 00:14:38,500
we're actually going to switch the source and target side.
我们实际上要切换源和目标端。

273
00:14:38,500 --> 00:14:42,640
So now in this case the French sentence is the source sequence.
所以现在在这种情况下，法语句子是源序列。

274
00:14:42,640 --> 00:14:45,985
Uh, the target sequence is, um,
呃，目标序列是，嗯，

275
00:14:45,985 --> 00:14:50,740
our original English sentence that came from  monolingual corpora.
我们原来的英语句子来自单语语料库。

276
00:14:50,740 --> 00:14:52,165
And now we're training the language, uh,
现在我们正在训练语言，呃，

277
00:14:52,165 --> 00:14:54,040
the machine translation system that goes
去机器翻译系统

278
00:14:54,040 --> 00:14:57,265
the other direction so that goes French to English.
另一个方向是法语到英语。

279
00:14:57,265 --> 00:15:00,445
Um, so, so why do we think this will work better?
嗯，那么，为什么我们认为这会更好？

280
00:15:00,445 --> 00:15:02,320
Um, number one, um,
嗯，头号，嗯，

281
00:15:02,320 --> 00:15:05,230
there's no longer this kind of circularity to the training
这种训练不再具有这种循环性

282
00:15:05,230 --> 00:15:10,210
because what the model is being trained on is the output of a completely different model.
因为正在训练的模型是完全不同模型的输出。

283
00:15:10,210 --> 00:15:14,845
Um, another thing that I think is pretty crucial here is that,
嗯，我认为另一件非常关键的事情是，

284
00:15:14,845 --> 00:15:18,970
um, the translations, the model is trained to produce.
嗯，翻译，模型训练生产。

285
00:15:18,970 --> 00:15:21,520
So the things that the decoder is actually learning to
所以解码器实际上正在学习的东西

286
00:15:21,520 --> 00:15:24,430
generate are never bad translations, right?
生成从来都不是坏翻译，对吗？

287
00:15:24,430 --> 00:15:26,574
So if you look at this example,
所以，如果你看看这个例子，

288
00:15:26,574 --> 00:15:29,545
the target sequence for our French to English model,
我们的法语到英语模型的目标序列，

289
00:15:29,545 --> 00:15:31,165
I traveled to Belgium, um,
我去了比利时，嗯，

290
00:15:31,165 --> 00:15:34,645
that originally came from a monolingual corpus.
最初来自单语语料库。

291
00:15:34,645 --> 00:15:37,420
Um, so I think intuitively this makes sense is
嗯，所以我觉得这很有意义

292
00:15:37,420 --> 00:15:40,435
that if we want to train a good translation model,
如果我们想培养一个好的翻译模型，

293
00:15:40,435 --> 00:15:44,620
um, it's probably okay to expose it to noisy inputs.
嗯，把它暴露在嘈杂的输入中可能是可以的。

294
00:15:44,620 --> 00:15:47,515
So we expose it to the output of a system that's English to French,
所以我们把它暴露给一个英语到法语系统的输出，

295
00:15:47,515 --> 00:15:48,730
it might not be perfect.
它可能不完美。

296
00:15:48,730 --> 00:15:52,330
Um, but what we don't want to do is um, expose it to
嗯，但我们不想做的是嗯，把它暴露给

297
00:15:52,330 --> 00:15:54,850
poor target sequences because then it
糟糕的目标序列，因为它

298
00:15:54,850 --> 00:15:58,370
won't learn how to generate in that language effectively.
不会学习如何有效地用这种语言生成。

299
00:15:58,560 --> 00:16:04,300
Any questions on back-translation before I get to results? Um, sure.
在我得到结果之前有关于反向翻译的任何问题吗？嗯，当然。

300
00:16:04,300 --> 00:16:08,980
[BACKGROUND]
[背景]

301
00:16:08,980 --> 00:16:11,500
So this is assuming we have a large corpus of
所以这假设我们有一个大的语料库

302
00:16:11,500 --> 00:16:16,700
unlabeled data and we want to be using it to help our translation model.
未标记的数据，我们希望用它来帮助我们的翻译模型。

303
00:16:17,330 --> 00:16:19,875
Does that, does that make sense?
那是否有意义？

304
00:16:19,875 --> 00:16:23,340
Um, maybe you could clarify the question.
嗯，也许你可以澄清这个问题。

305
00:16:23,340 --> 00:16:29,160
[BACKGROUND]
[背景]

306
00:16:29,160 --> 00:16:32,830
Yeah, that's right. So we have a big corpus of English which includes the sentence,
是啊，没错。所以我们有一个很大的英语语料库，包括句子，

307
00:16:32,830 --> 00:16:36,190
"I traveled to Belgium," and we don't know the translations but we'd still like to
“我前往比利时，”我们不知道翻译，但我们仍然喜欢

308
00:16:36,190 --> 00:16:39,630
use this data. Yeah, another question.
使用这些数据。是的，另一个问题。

309
00:16:39,630 --> 00:16:45,280
[BACKGROUND]
[背景]

310
00:16:45,280 --> 00:16:47,110
Yeah, so that's a good question is how do you
是的，这是一个很好的问题，你是怎么做的

311
00:16:47,110 --> 00:16:52,300
avoid both the models let's say sort of blowing up and producing garbage?
避免两种模型让我们说有点炸毁和生产垃圾？

312
00:16:52,300 --> 00:16:54,400
And then they're just feeding garbage to each other.
然后他们只是互相喂垃圾。

313
00:16:54,400 --> 00:16:57,820
The answer is that there is some amount of labeled data here as well.
答案是这里也有一些标记数据。

314
00:16:57,820 --> 00:17:00,820
So on unlabeled data you do this, but on labeled data,
因此，对于未标记的数据，您可以执行此操作，但对于标记数据，

315
00:17:00,820 --> 00:17:02,110
you do standard training,
你做标准培训，

316
00:17:02,110 --> 00:17:04,795
and that way you avoid, you,
这样你就可以避免，

317
00:17:04,795 --> 00:17:07,900
you make sure you kind of keep the models on track because they still have to fit to
你确保你保持模型正常，因为他们仍然必须适应

318
00:17:07,900 --> 00:17:12,175
the labeled data. Yeah, another question.
标签数据。是的，另一个问题。

319
00:17:12,175 --> 00:17:15,475
How do you schedule the training of the two models?
你如何安排两种模型的培训？

320
00:17:15,475 --> 00:17:17,500
Yeah, that is a good question.
是的，这是一个很好的问题。

321
00:17:17,500 --> 00:17:21,580
And I think that's basically almost like a hyper-parameter you can tweak.
而且我认为这基本上就像一个你可以调整的超参数。

322
00:17:21,580 --> 00:17:25,720
So I think a pretty common thing to do is first,
所以我认为一件非常普遍的事情是，

323
00:17:25,720 --> 00:17:28,270
train two models only on labeled data.
仅在标记数据上训练两个模型。

324
00:17:28,270 --> 00:17:32,965
Then label, um, so then do back-translation
然后标签，嗯，然后进行反向翻译

325
00:17:32,965 --> 00:17:37,480
over a large corpus and kind of repeat that process over and over again.
在一个大的语料库上，一遍又一遍地重复这个过程。

326
00:17:37,480 --> 00:17:40,165
So each iteration, you train on the label data,
所以每次迭代，你都要对标签数据进行训练，

327
00:17:40,165 --> 00:17:43,510
label some unlabeled data and now you have more data to work with.
标记一些未标记的数据，现在您可以使用更多数据。

328
00:17:43,510 --> 00:17:46,270
But I think there'd be many kinds of scheduling that would be effective
但我认为有很多类型的调度是有效的

329
00:17:46,270 --> 00:17:50,380
here. Okay. Another question.
这里。好的。另一个问题。

330
00:17:50,380 --> 00:18:06,100
I'm curious as to the evaluation, considering if you have a very good French to English model, you could try to look up, or contest if you have a good French to English model, you could try to look up the original source and see if it matches.
我很好奇评价，考虑到你是否有一个非常好的法语到英语模型，你可以尝试查找，或者如果你有一个很好的法语到英语模型的比赛，你可以尝试查找原始来源和看它是否匹配。

331
00:18:06,100 --> 00:18:07,435
Yeah, I'm not, I'm not quite sure.
是的，我不是，我不太确定。

332
00:18:07,435 --> 00:18:10,135
Are you suggesting going like English to French to English and seeing if?
你是建议像英语到法语到英语，看看是否？

333
00:18:10,135 --> 00:18:11,635
I see, yeah, yeah,
我明白了，是的，是的，

334
00:18:11,635 --> 00:18:12,775
that's a really interesting idea.
这是一个非常有趣的想法。

335
00:18:12,775 --> 00:18:15,775
And we're actually going to talk a little bit about this sort of,
我们实际上会谈谈这种，

336
00:18:15,775 --> 00:18:17,290
it's called cycle consistency,
它被称为循环一致性，

337
00:18:17,290 --> 00:18:20,000
this idea later in this talk.
这个想法后来在这个演讲中。

338
00:18:20,970 --> 00:18:23,770
Okay, I'm going to move on to the results.
好的，我将继续讨论结果。

339
00:18:23,770 --> 00:18:28,120
So, so here's the method for using unlabeled data to improve translation.
所以，这里是使用未标记数据来改进翻译的方法。

340
00:18:28,120 --> 00:18:29,890
How well does it do?
它有多好？

341
00:18:29,890 --> 00:18:33,220
Um, the answer is that the improvements are at least to me, they
嗯，答案是这些改进至少对我而言，他们

342
00:18:33,220 --> 00:18:36,490
were surprisingly extremely good, right?
出奇的非常好，对吧？

343
00:18:36,490 --> 00:18:39,445
So, um, this is for English to German translation.
所以，嗯，这是英语到德语的翻译。

344
00:18:39,445 --> 00:18:44,515
This is from some work by Facebook, so they  used 5 million labeled sentence pairs.
这是来自Facebook的一些工作，因此他们使用了500万个带标签的句子对。

345
00:18:44,515 --> 00:18:52,040
But they also used 230 monolingual sentences, so sentences without translations.
但他们也使用了230个单语句子，所以句子没有翻译。

346
00:18:52,290 --> 00:18:56,425
And you can see that compared to previous state of the art,
你可以看到，与以前的技术水平相比，

347
00:18:56,425 --> 00:18:59,755
they get six BLEU points improvement which, um,
他们获得了六个BLEU积分改善，嗯，

348
00:18:59,755 --> 00:19:03,010
if you compare it to most previous research and machine tran- machine translation
如果你把它与大多数以前的研究和机器翻译机进行比较

349
00:19:03,010 --> 00:19:04,180
is a really big gain, right?
是一个非常大的收获，对吗？

350
00:19:04,180 --> 00:19:08,020
So even something like the invention of the transformer which most people would
所以即使是大多数人都会想到的变压器的发明

351
00:19:08,020 --> 00:19:13,165
consider to be a really significant research development in NLP,
考虑成为NLP真正重要的研究发展，

352
00:19:13,165 --> 00:19:16,825
that improved over prior work by about 2,5 BLEU points.
比先前的工作提高了约2,5 BLEU积分。

353
00:19:16,825 --> 00:19:22,330
And here without doing any sort of fancy model design just by using way more data,
这里没有采用任何类型的花式模型设计，只需使用更多的数据，

354
00:19:22,330 --> 00:19:25,400
um, we get actually much larger improvements.
嗯，我们实际上得到了更大的改进。

355
00:19:29,130 --> 00:19:34,390
Okay. So an interesting question to think about,
好的。这是一个值得思考的有趣问题，

356
00:19:34,390 --> 00:19:38,125
um, is suppose we only have our monolingual corpora.
嗯，假设我们只有单语语料库。

357
00:19:38,125 --> 00:19:41,155
So we don't have any sentences that had been human translated.
所以我们没有任何人为翻译过的句子。

358
00:19:41,155 --> 00:19:43,390
We just have sentences in two languages.
我们只有两种语言的句子。

359
00:19:43,390 --> 00:19:47,080
Um, so the scenario you can sort of imagine is suppose,
嗯，你可以想象的场景是假设的，

360
00:19:47,080 --> 00:19:48,985
um, an alien comes down and,
嗯，一个外星人来了，

361
00:19:48,985 --> 00:19:50,740
um, starts talking to you and it's a
嗯，开始跟你说话，这是一个

362
00:19:50,740 --> 00:19:53,965
weird alien language, um, and it talks a lot,
奇怪的外星语言，嗯，它谈了很多，

363
00:19:53,965 --> 00:19:58,120
would you eventually be able to translate what it's saying to English,
你最终能够将它所说的翻译成英文，

364
00:19:58,120 --> 00:20:01,670
um, just by having a really large amount of data?
嗯，只是拥有非常大量的数据？

365
00:20:03,300 --> 00:20:06,205
Um, so I'm going to start with, um,
嗯，所以我要开始，嗯，

366
00:20:06,205 --> 00:20:11,935
a simpler task than full-on translating when you only have unlabeled sentences.
当你只有未标记的句子时，比完全翻译更简单的任务。

367
00:20:11,935 --> 00:20:15,220
Um, instead of doing sentence to sentence translation,
嗯，而不是做句子翻译，

368
00:20:15,220 --> 00:20:18,640
let's start by only worrying about word to word translation.
让我们首先担心单词翻译。

369
00:20:18,640 --> 00:20:21,490
So the goal here is given a word in one language,
所以这里的目标用一种语言表达，

370
00:20:21,490 --> 00:20:25,330
find its translation but without using any labeled data.
找到它的翻译但不使用任何标记数据。

371
00:20:25,330 --> 00:20:27,100
Um, and the method,
嗯，方法，

372
00:20:27,100 --> 00:20:29,440
the method we're going to use to try to solve
我们将尝试解决的方法

373
00:20:29,440 --> 00:20:33,460
this task is called, uh, cross-lingual embeddings.
这个任务被称为，呃，跨语言嵌入。

374
00:20:33,460 --> 00:20:35,830
Um, so the goal is to learn, uh,
嗯，所以我的目标是学习，呃，

375
00:20:35,830 --> 00:20:39,265
word vectors for words in both languages,
两种语言中单词的单词向量，

376
00:20:39,265 --> 00:20:41,935
and we'd like those word vectors to have
我们希望这些单词向量具有

377
00:20:41,935 --> 00:20:45,550
all the nice properties you've already learned about word vectors having, um,
你已经学过的有关单词向量的所有好的属性，嗯，

378
00:20:45,550 --> 00:20:49,149
but we also want word vectors for a particular language,
但我们也想要特定语言的单词向量，

379
00:20:49,149 --> 00:20:52,860
um, to be close to the word vector of its translation.
嗯，接近它翻译的单词vector。

380
00:20:52,860 --> 00:20:57,090
Um, so I'm not sure if it's visible in this figure but this fis- figure shows
嗯，所以我不确定它是否在这个图中可见，但这个fis-数字显示

381
00:20:57,090 --> 00:21:02,475
a large number of English and I think German words and you can see that,
大量的英语和我认为德语单词，你可以看到，

382
00:21:02,475 --> 00:21:07,795
um, uh, the each English word has its corresponding German word,
嗯，呃，每个英文单词都有相应的德语单词，

383
00:21:07,795 --> 00:21:10,330
um, nearby to it in its embedding space.
嗯，在它的嵌入空间附近。

384
00:21:10,330 --> 00:21:15,010
So if we learn embeddings like this then it's pretty easy to do word to word translation.
因此，如果我们学习这样的嵌入，那么单词翻译就很容易了。

385
00:21:15,010 --> 00:21:16,705
Um, we just pick an English word,
嗯，我们只选一个英文单词，

386
00:21:16,705 --> 00:21:18,550
we find the nearest, uh,
我们找到最近的，呃，

387
00:21:18,550 --> 00:21:22,075
German word in this joint embedding space
这个联合嵌入空间中的德语单词

388
00:21:22,075 --> 00:21:26,030
and that will give us a translation for the English word.
这将为我们提供英文单词的翻译。

389
00:21:28,470 --> 00:21:32,185
Um, our key method for or the key
嗯，我们的关键方法或关键

390
00:21:32,185 --> 00:21:35,500
assumption that we're going to be using to solve this is that,
假设我们将要用来解决这个问题，

391
00:21:35,500 --> 00:21:40,870
um, th- even though if you run word2vec twice you'll get really different embeddings.
嗯，即使你运行word2vec两次，你会得到真正不同的嵌入。

392
00:21:40,870 --> 00:21:46,930
Um, the structure of that embedding space has a lot of regularity to it,
嗯，嵌入空间的结构有很多规律性，

393
00:21:46,930 --> 00:21:49,675
and we can take advantage of that regularity, um,
我们可以利用这种规律性，嗯，

394
00:21:49,675 --> 00:21:51,700
to help find when,
帮助找到什么时候，

395
00:21:51,700 --> 00:21:54,370
um, an alignment between those embedding spaces.
嗯，那些嵌入空间之间的对齐。

396
00:21:54,370 --> 00:21:56,830
So to be kind of more concrete here.
所以在这里要更具体一点。

397
00:21:56,830 --> 00:21:59,560
Here is a picture of two sets of word embeddings.
这是两组字嵌入的图片。

398
00:21:59,560 --> 00:22:00,820
So in red, we have, um,
所以红色，我们有，嗯，

399
00:22:00,820 --> 00:22:02,655
English words, in, uh,
英文单词，in，呃，

400
00:22:02,655 --> 00:22:04,565
blue we have Italian words,
蓝色我们有意大利语单词，

401
00:22:04,565 --> 00:22:09,280
and although, um, the vector spaces right now look very different to each other,
虽然，嗯，现在的矢量空间看起来彼此非常不同，

402
00:22:09,280 --> 00:22:12,400
um, you can see that they have a really similar structure, right?
嗯，你可以看到它们的结构非常相似，对吧？

403
00:22:12,400 --> 00:22:16,735
So you'd imagine distances are kind of similar that the distance from,
所以你可以想象距离有点类似于距离，

404
00:22:16,735 --> 00:22:19,345
uh, cat and feline in the, um,
呃，猫和猫在，嗯，

405
00:22:19,345 --> 00:22:22,570
English embedding space should be pretty similar to the distance
英语嵌入空间应该与距离非常相似

406
00:22:22,570 --> 00:22:27,880
between gatto and felino in the, um, Italian space.
意大利空间中的gatto和felino之间。

407
00:22:27,880 --> 00:22:34,310
Um, this kind of motivates an algorithm for learning these cross-lingual embeddings.
嗯，这种激发了学习这些跨语言嵌入的算法。

408
00:22:35,400 --> 00:22:38,440
Um, so here's the idea.
嗯，所以这就是主意。

409
00:22:38,440 --> 00:22:40,960
What we're going to try to do is learn what's essentially
我们要做的是从本质上学习什么

410
00:22:40,960 --> 00:22:44,080
a rotation such that we can transform,
我们可以改变的旋转，

411
00:22:44,080 --> 00:22:46,660
um, our set of English embeddings so
嗯，我们的英语嵌入集合如此

412
00:22:46,660 --> 00:22:50,515
that they match up with our Italian embe- embeddings.
它们与我们的意大利嵌入式相匹配。

413
00:22:50,515 --> 00:22:52,780
So mathematically, what this means is we're gonna learn
所以在数学上，这意味着我们要学习

414
00:22:52,780 --> 00:22:55,660
a matrix W such that if we take let's say,
矩阵W，如果我们采取让我们说，

415
00:22:55,660 --> 00:23:00,355
uh, the word vector for cat in English and we multiply it by W, um,
呃，用英语写的“猫”一词，我们将它乘以W，嗯，

416
00:23:00,355 --> 00:23:06,205
we end up with the vector for gatto in Spanish or Italian,
我们最终得到西班牙语或意大利语的gatto矢量，

417
00:23:06,205 --> 00:23:09,550
um, and a detail here is that, um,
嗯，这里的细节是，嗯，

418
00:23:09,550 --> 00:23:12,580
we're going to constrain W to be orthogonal, um,
我们要约束W是正交的，嗯，

419
00:23:12,580 --> 00:23:15,070
and what that means geometrically is just that W is
几何上的意思就是W就是

420
00:23:15,070 --> 00:23:17,980
only going to be doing a rotation to the,
只会做一个轮换，

421
00:23:17,980 --> 00:23:19,945
uh, vectors, um, in X.
呃，矢量，嗯，在X.

422
00:23:19,945 --> 00:23:24,320
It's not going to be doing some other weirder transformation.
它不会做其他一些奇怪的改造。

423
00:23:24,870 --> 00:23:29,305
So this is our goal is to learn this W. Um,
所以这是我们的目标是学习这个W.嗯，

424
00:23:29,305 --> 00:23:31,000
next I'm gonna talk about,
接下来我要谈谈，

425
00:23:31,000 --> 00:23:36,985
talking about how actually do we learn this W. Um,
谈论我们实际上是如何学习这个W.嗯，

426
00:23:36,985 --> 00:23:41,665
and there's actually a bunch of techniques for learning this W matrix,
实际上有一堆学习这个W矩阵的技术，

427
00:23:41,665 --> 00:23:44,740
um, but, um, here is one of
嗯，但是，嗯，这是其中之一

428
00:23:44,740 --> 00:23:48,310
them that I think is quite clever is called adversarial training.
我认为非常聪明的人称为对抗训练。

429
00:23:48,310 --> 00:23:50,635
Um, so it works as follows,
嗯，所以它的工作原理如下，

430
00:23:50,635 --> 00:23:53,770
is in addition to trying to learn this W matrix,
除了试图学习这个W矩阵，

431
00:23:53,770 --> 00:23:57,670
we're also going to be trying to learn a model that, uh,
我们也将试图学习一个模型，呃，

432
00:23:57,670 --> 00:23:58,915
is called a discriminator,
被称为鉴别器，

433
00:23:58,915 --> 00:24:02,800
and what it'll do is take a vector and it will try to predict,
它会做什么是采取矢量，它会尝试预测，

434
00:24:02,800 --> 00:24:05,080
is that vector originally, um,
是那个矢量最初，嗯，

435
00:24:05,080 --> 00:24:08,830
an English word embedding or is it originally an Italian word embedding?
英语单词嵌入还是原来是意大利语单词嵌入？

436
00:24:08,830 --> 00:24:11,425
Um, in other words, if you think about, um,
嗯，换句话说，如果你想的话，嗯，

437
00:24:11,425 --> 00:24:14,920
the diagram, what we're asking our discriminator to do is, uh,
图，我们要求我们的鉴别者做的是，呃，

438
00:24:14,920 --> 00:24:17,680
it's given one of these points and it's trying to predict is it
它给出了其中一个点，并试图预测它是什么

439
00:24:17,680 --> 00:24:21,055
basically a red point so an English word originally, or is it a blue point?
基本上是红点，所以最初是英文单词，还是蓝点？

440
00:24:21,055 --> 00:24:24,010
Um, so if we have no W matrix and this is
嗯，所以如果我们没有W矩阵，那就是

441
00:24:24,010 --> 00:24:27,190
a really easy task for the discriminator because,
鉴别器真的很容易，因为，

442
00:24:27,190 --> 00:24:32,425
um, the, uh, word embeddings for English and Italian are clearly separated.
嗯，呃，英语和意大利语的嵌入词是明显分开的。

443
00:24:32,425 --> 00:24:36,130
Um, however, if we learn a W matrix
嗯，如果我们学习W矩阵

444
00:24:36,130 --> 00:24:39,955
that succeeds in aligning all these embeddings on top of each other,
成功地将所有这些嵌入对齐，

445
00:24:39,955 --> 00:24:43,270
then our discriminator will never do a good job, right.
那么我们的鉴别者永远不会做得好，对吧。

446
00:24:43,270 --> 00:24:46,210
We can imagine it'll never really do better than 50%,
我们可以想象它永远不会超过50％，

447
00:24:46,210 --> 00:24:48,835
um, because given a vector for say cat,
嗯，因为给了一个说猫的矢量，

448
00:24:48,835 --> 00:24:51,190
it won't know is that the vector for cat that's been
它不会知道是猫的矢量

449
00:24:51,190 --> 00:24:54,130
transformed by W or is it actually the vector for gatto?
由W转换或它实际上是gatto的向量？

450
00:24:54,130 --> 00:24:58,885
Um, because in this case those two vectors are aligned so they are on top of each other.
嗯，因为在这种情况下，这两个向量是对齐的，所以它们在彼此的顶部。

451
00:24:58,885 --> 00:25:03,715
Um, so, um, during training, you first, um,
嗯，所以，嗯，在训练期间，你首先，嗯，

452
00:25:03,715 --> 00:25:06,790
you alternate between training the discriminator a little bit which
你之间交替训练鉴别器一点点

453
00:25:06,790 --> 00:25:09,640
means making sure it's as good as possible at
意味着确保它尽可能好

454
00:25:09,640 --> 00:25:13,120
distinguishing the English from Italian words and then you
区分英语和意大利语然后你

455
00:25:13,120 --> 00:25:16,930
train the W and the goal for training W is to,
训练W和训练W的目标是，

456
00:25:16,930 --> 00:25:20,050
uh, essentially confuse the discriminator as much as possible.
呃，基本上尽可能地混淆鉴别者。

457
00:25:20,050 --> 00:25:23,215
Um, so you want to have a situation where,
嗯，所以你想要有一种情况，

458
00:25:23,215 --> 00:25:26,170
um, you can't, um, with this machine learning model,
嗯，你不能，嗯，这个机器学习模型，

459
00:25:26,170 --> 00:25:29,290
figure out if a word embedding actually, um,
弄清楚实际嵌入一个单词，嗯，

460
00:25:29,290 --> 00:25:33,625
was, um, originally from English or if it's an Italian word vector.
是，嗯，最初来自英语，或者它是一个意大利语单词矢量。

461
00:25:33,625 --> 00:25:36,085
Um, and so at the end of the day you have,
嗯，等你结束的那天，

462
00:25:36,085 --> 00:25:39,410
you have vectors that are kind of aligned with each other.
你有一些彼此对齐的向量。

463
00:25:39,420 --> 00:25:43,460
Um, any questions about this approach?
嗯，有关这种方法的任何问题吗？

464
00:25:47,220 --> 00:25:50,650
Okay. Um, he- there's a link to a paper with more details.
好的。嗯，他 - 这是一篇关于更多细节的论文的链接。

465
00:25:50,650 --> 00:25:53,275
There's actually kind of a range of other tricks you can do,
实际上你可以做一些其他的技巧，

466
00:25:53,275 --> 00:25:55,850
um, but this is kind of a key idea.
嗯，但这是一个关键的想法。

467
00:25:58,440 --> 00:26:04,810
Um, okay. So that was doing word to word unsupervised translation.
嗯，好的。所以这就是无人监督的翻译。

468
00:26:04,810 --> 00:26:08,930
Um, how do we do full sentence to sentence translation?
嗯，我们怎么做句子翻译？

469
00:26:09,150 --> 00:26:11,725
Um, so we're going to use, um,
嗯，我们打算用，嗯，

470
00:26:11,725 --> 00:26:13,750
a standard sort of seq2seq model,
一种标准的seq2seq模型，

471
00:26:13,750 --> 00:26:16,660
um, without even an attention mechanism.
嗯，甚至没有注意机制。

472
00:26:16,660 --> 00:26:19,900
Um, there's one change to the standard seq2seq
嗯，标准seq2seq有一个变化

473
00:26:19,900 --> 00:26:23,050
model going on here which is that, um,
模特继续在这就是那个，嗯，

474
00:26:23,050 --> 00:26:25,780
we're going to use the same encoder and decoder,
我们将使用相同的编码器和解码器，

475
00:26:25,780 --> 00:26:30,160
uh, regardless of the input and output languages.
呃，不管输入和输出语言。

476
00:26:30,160 --> 00:26:31,930
So you can see, um,
所以你可以看到，嗯，

477
00:26:31,930 --> 00:26:33,340
in this example, um,
在这个例子中，嗯，

478
00:26:33,340 --> 00:26:35,815
we could give the encoder an English sentence,
我们可以给编码器一个英文句子，

479
00:26:35,815 --> 00:26:40,360
we could also give it a French sentence and it'll have these cross-lingual embeddings.
我们也可以给它一个法语句子，它将有这些跨语言嵌入。

480
00:26:40,360 --> 00:26:43,255
So it'll have vector representations for English words
所以它会有英文单词的矢量表示

481
00:26:43,255 --> 00:26:47,140
and French words which means it can handle sort of any input.
和法语单词，这意味着它可以处理任何输入。

482
00:26:47,140 --> 00:26:49,375
Um, for the decoder,
嗯，对于解码器，

483
00:26:49,375 --> 00:26:52,930
we need to give it some information about what language is it supposed to generate in.
我们需要给它一些关于它应该生成什么语言的信息。

484
00:26:52,930 --> 00:26:54,955
Is it going to generate in French or English?
是用法语还是英语生成？

485
00:26:54,955 --> 00:26:58,660
Um, so the way that is done is by, uh,
嗯，所以做的方式是，呃，

486
00:26:58,660 --> 00:27:01,915
feeding in a special token which here is Fr
喂养一个特殊的标记，这里是Fr.

487
00:27:01,915 --> 00:27:05,590
in brack- brackets to represent French that tells the model,
在括号中代表告诉模型的法语，

488
00:27:05,590 --> 00:27:07,975
okay, you should generate in French now.
好吧，你现在应该用法语生成。

489
00:27:07,975 --> 00:27:11,380
Um, here in this figure it's only French,
嗯，这个图中只有法语，

490
00:27:11,380 --> 00:27:13,975
but you could imagine also feeding this model, uh,
但你可以想象也喂这个模型，呃，

491
00:27:13,975 --> 00:27:17,635
English in brackets, and then that'll tell it to, uh, generate English.
括号中的英文，然后告诉它，呃，生成英语。

492
00:27:17,635 --> 00:27:21,670
And one thing that you can see is that you could use this sort of model to g enerate,
你能看到的一件事就是你可以使用这种模型来加强，

493
00:27:21,670 --> 00:27:23,155
do go from English to French.
从英语到法语。

494
00:27:23,155 --> 00:27:25,450
You could also use this model as an auto-encoder, right.
您也可以将此模型用作自动编码器。

495
00:27:25,450 --> 00:27:27,295
So, uh, at the bottom, um,
所以，呃，在底部，嗯，

496
00:27:27,295 --> 00:27:31,510
it's taking in a French sentence as input and it's just generating French as
它以法语句子作为输入，它只是生成法语

497
00:27:31,510 --> 00:27:37,580
output which here means just reproducing the original input sequence.
这里的输出意味着只是再现原始输入序列。

498
00:27:38,850 --> 00:27:43,104
Um, so just a small change to standard seq2seq.
嗯，所以只需对标准seq2seq进行一些小改动。

499
00:27:43,104 --> 00:27:46,765
Here's how we're going to train the seq2seq model.
以下是我们将如何训练seq2seq模型。

500
00:27:46,765 --> 00:27:50,170
Um, there's going to be two training objectives, um,
嗯，这将是两个培训目标，嗯，

501
00:27:50,170 --> 00:27:51,940
and I'll explain sort of why they're, uh,
我会解释为什么他们是，呃，

502
00:27:51,940 --> 00:27:55,060
present in this model in just a few slides.
仅在几张幻灯片中出现在此模型中。

503
00:27:55,060 --> 00:27:57,025
For now let's just say what they are.
现在让我们说出它们是什么。

504
00:27:57,025 --> 00:27:59,110
So the first one is, um,
所以第一个是，嗯，

505
00:27:59,110 --> 00:28:01,165
called a de-noising autoencoder.
称为去噪自动编码器。

506
00:28:01,165 --> 00:28:06,430
Um, what we're going to train our model to do in this case is take a, uh, sentence.
嗯，在这种情况下，我们要训练我们的模型做的是拿一个，呃，句子。

507
00:28:06,430 --> 00:28:08,140
So, um, and here it's going to be
那么，嗯，这就是它

508
00:28:08,140 --> 00:28:10,795
an English sentence but it could also be a French sentence.
英语句子，但也可以是法语句子。

509
00:28:10,795 --> 00:28:14,170
Um, we're going to scramble up the words a little bit,
嗯，我们要把这些话拼凑起来，

510
00:28:14,170 --> 00:28:16,885
and then we're going to ask the model to, uh,
然后我们要问模型，呃，

511
00:28:16,885 --> 00:28:20,560
de-noise that sentence which in other words means
消除那句话，换言之，意思是

512
00:28:20,560 --> 00:28:25,345
regenerating what the sentence actually was before it was scrambled.
在它被扰乱之前重新生成句子实际上是什么。

513
00:28:25,345 --> 00:28:31,740
And, uh, maybe one idea of why this would be a useful training objective is that,
而且，呃，或许有一个想法，为什么这将是一个有用的培训目标是，

514
00:28:31,740 --> 00:28:35,505
uh, since we have an encoder-decoder without atten- attention,
呃，因为我们有一个没有注意的编码器 - 解码器，

515
00:28:35,505 --> 00:28:41,780
the encoder is converting the entirety of the source sentence into a single vector,
编码器将整个源句子转换为单个向量，

516
00:28:41,780 --> 00:28:47,110
what an auto-encoder does is ensure that that vector contains all the information about
自动编码器的作用是确保该向量包含有关的所有信息

517
00:28:47,110 --> 00:28:52,390
the sentence such that we are able to recover what the original sentence was,
这句话使我们能够恢复原来的句子，

518
00:28:52,390 --> 00:28:56,180
um, from the vector produced by the encoder.
嗯，来自编码器产生的矢量。

519
00:28:57,960 --> 00:29:00,805
Um, so that was objective 1.
嗯，这是客观的1。

520
00:29:00,805 --> 00:29:05,005
Training objective 2 is now we're actually going to be trying to do a translation,
培训目标2现在我们实际上将尝试进行翻译，

521
00:29:05,005 --> 00:29:07,479
um, but, um, as before,
嗯，但是，嗯，和以前一样，

522
00:29:07,479 --> 00:29:09,775
we're going to be using this back-translation idea.
我们将使用这种反向翻译的想法。

523
00:29:09,775 --> 00:29:12,970
So remember, we only have unlabeled sentences,
所以请记住，我们只有无标签的句子，

524
00:29:12,970 --> 00:29:16,015
we don't have any human-provided translations,
我们没有任何人工提供的翻译，

525
00:29:16,015 --> 00:29:19,750
um, but what we can still do is, given, a,
嗯，但我们仍然可以做的是，给定，a，

526
00:29:19,750 --> 00:29:22,000
um, let's say an English sentence or let's say a French sentence,
嗯，让我们说英语句子或者说法语句子，

527
00:29:22,000 --> 00:29:24,505
given a French sentence, we can translate it to English, um,
鉴于法语判决，我们可以将其翻译成英语，嗯，

528
00:29:24,505 --> 00:29:28,120
using our model in its current state, uh,
在当前状态下使用我们的模型，呃，

529
00:29:28,120 --> 00:29:32,605
and then we can ask that model to translate from English or translate that- yeah,
然后我们可以要求该模型从英语翻译或翻译 - 是的，

530
00:29:32,605 --> 00:29:34,690
translate that English back into French.
将英语翻译成法语。

531
00:29:34,690 --> 00:29:37,105
Um, so what you can imagine is in this setting, um,
嗯，所以你能想象的就是这个环境，嗯，

532
00:29:37,105 --> 00:29:39,640
the input sequence is going to be somewhat messed
输入序列会有些混乱

533
00:29:39,640 --> 00:29:42,820
up because it's the output of our imperfect machine learning model.
因为它是我们不完美的机器学习模型的输出。

534
00:29:42,820 --> 00:29:47,050
So here the input sequence is just "I am student," um, a word has been dropped,
所以这里的输入序列只是“我是学生”，嗯，一个单词已被删除，

535
00:29:47,050 --> 00:29:51,820
but, um, we're now gonna train it to, even with this kind of bad input,
但是，嗯，我们现在要训练它，即使有这样糟糕的输入，

536
00:29:51,820 --> 00:29:55,330
to reproduce the original, um,
重现原作，嗯，

537
00:29:55,330 --> 00:29:58,270
French sentence, um, from our,
法语，嗯，来自我们，

538
00:29:58,270 --> 00:30:01,270
uh, corpus of- of monolingual, um, French text.
呃，单语，嗯，法语文本的语料库。

539
00:30:01,270 --> 00:30:07,400
[NOISE] Um, let me- let me pause here actually and ask for questions.
[NO]嗯，让我 - 让我暂停在这里，并提出问题。

540
00:30:08,910 --> 00:30:13,840
Sure.
当然。

541
00:30:13,840 --> 00:30:16,000
[NOISE] [inaudible] What if, um, the reason you have
[NOISE] [听不清]如果，嗯，你有什么原因

542
00:30:16,000 --> 00:30:20,305
this orthogonality constraint for your words to be word embedding,
这个正交性约束你的单词嵌入，

543
00:30:20,305 --> 00:30:22,900
is it to avoid overfitting?
是为了避免过度拟合？

544
00:30:22,900 --> 00:30:29,800
Have you tried to take that off, and you know, see what [inaudible]
你有没有尝试过这个，你知道，看看[音频不清晰]

545
00:30:29,800 --> 00:30:31,060
Yeah. That's a good question.
是啊。这是个好问题。

546
00:30:31,060 --> 00:30:35,305
Um, so this is going back to earlier when there was a word-word translation.
嗯，所以这可以追溯到早些时候有一个单词翻译。

547
00:30:35,305 --> 00:30:39,325
Why would we constrain that W matrix to be orthogonal?
为什么我们要将W矩阵约束为正交？

548
00:30:39,325 --> 00:30:43,030
Um, essentially, that's right. It's to avoid overfitting and in particular,
嗯，基本上，这是对的。这是为了避免过度拟合，特别是

549
00:30:43,030 --> 00:30:46,060
it's making this assumption that our embedding spaces are so
它假设我们的嵌入空间是这样的

550
00:30:46,060 --> 00:30:50,005
similar that there's actually just a rotation that distinguishes,
类似的，实际上只是一个区分的轮换，

551
00:30:50,005 --> 00:30:53,500
um, our word vectors in English versus our word vectors in Italian.
嗯，我们的单词矢量用英语对比我们的单词向量用意大利语。

552
00:30:53,500 --> 00:30:57,460
Um, I think there has been, um,
嗯，我觉得有，嗯，

553
00:30:57,460 --> 00:31:01,360
there have been results that don't include that orthogonality constraint,
有些结果不包括正交性约束，

554
00:31:01,360 --> 00:31:04,480
and I think it slightly hurts performance to not have that in there.
而且我认为在那里没有那个有点伤害性能。

555
00:31:04,480 --> 00:31:09,130
[NOISE] Okay.
[NOISE]好的。

556
00:31:09,130 --> 00:31:11,155
Um, so- so continuing with,
嗯，马上继续，

557
00:31:11,155 --> 00:31:13,765
um, unsupervised machine translation,
嗯，无人监督的机器翻译，

558
00:31:13,765 --> 00:31:17,290
um, I- I gave a training method.
嗯，我 - 我给了一个训练方法。

559
00:31:17,290 --> 00:31:20,395
I didn't quite explain why it would work, so- so,
我没有完全解释为什么它会起作用，所以，

560
00:31:20,395 --> 00:31:24,790
um, here is some more intuition for- for this idea.
嗯，对于这个想法，这里有一些更直觉。

561
00:31:24,790 --> 00:31:27,730
Um, so remember, um,
嗯，记得，嗯，

562
00:31:27,730 --> 00:31:29,665
we're going to initialize
我们要初始化

563
00:31:29,665 --> 00:31:33,264
our machine translation model with these cross-lingual embeddings,
我们的机器翻译模型与这些跨语言嵌入，

564
00:31:33,264 --> 00:31:37,015
which mean the English and French word should look close to identically.
这意味着英语和法语单词看起来应该完全相同。

565
00:31:37,015 --> 00:31:42,565
Um, we're also using the shared, um, encoder.
嗯，我们也在使用共享的嗯编码器。

566
00:31:42,565 --> 00:31:44,860
Um, so that means if you think about it,
嗯，这意味着如果你考虑一下，

567
00:31:44,860 --> 00:31:46,645
um, at the top, we have just,
嗯，在顶部，我们有，

568
00:31:46,645 --> 00:31:51,760
a auto-encoding objective and we can certainly believe that our model can learn this.
一个自动编码的目标，我们当然可以相信我们的模型可以学到这一点。

569
00:31:51,760 --> 00:31:54,250
Um, it's a pretty simple task.
嗯，这是一项非常简单的任务。

570
00:31:54,250 --> 00:31:59,395
Um, now imagine we're giving our model a French sentence as input instead.
嗯，现在想象我们正在给我们的模型一个法语句子作为输入。

571
00:31:59,395 --> 00:32:01,555
Um, since the, uh,
嗯，自从，呃，

572
00:32:01,555 --> 00:32:03,850
embeddings are going to look pretty similar,
嵌入看起来非常相似，

573
00:32:03,850 --> 00:32:06,190
and since the encoder is the same, um,
因为编码器是相同的，嗯，

574
00:32:06,190 --> 00:32:09,760
it's pretty likely that the model's representation of
它很可能是该模型的代表

575
00:32:09,760 --> 00:32:11,950
this French sentence should actually be very
这个法语句子应该是非常的

576
00:32:11,950 --> 00:32:15,520
similar to the representation of the English sentence.
类似于英语句子的表示。

577
00:32:15,520 --> 00:32:19,870
Um, so when this representation is passed into the decoder, um,
嗯，所以当这种表示传递给解码器时，嗯，

578
00:32:19,870 --> 00:32:24,980
we can hope that we'll get the same output as before.
我们希望我们能够获得与以前相同的输出。

579
00:32:25,200 --> 00:32:28,495
Um, um, so here's like sort of as a starting point.
嗯，嗯，所以这里就像一个起点。

580
00:32:28,495 --> 00:32:30,865
We- we can hope that our model, um,
我们 - 我们可以希望我们的模型，嗯，

581
00:32:30,865 --> 00:32:33,430
already is able to have some translation capability.
已经能够有一些翻译能力。

582
00:32:33,430 --> 00:32:37,840
[NOISE] Um, another way of thinking about this is
[NOISE]嗯，另一种思考方式是

583
00:32:37,840 --> 00:32:42,355
that what we really want our model to do is to be able to encode a sentence,
我们真正希望我们的模型做的是能够编码一个句子，

584
00:32:42,355 --> 00:32:44,335
such that the representation,
这样的表示，

585
00:32:44,335 --> 00:32:47,410
um, is sort of a universal kind of Interlingua.
嗯，是一种普遍的国际语。

586
00:32:47,410 --> 00:32:49,885
So a universal, um, uh,
这是一个普遍的，嗯，呃，

587
00:32:49,885 --> 00:32:53,680
universal representation of that sentence that doesn't,
这句话的普遍表示没有，

588
00:32:53,680 --> 00:32:56,245
uh, that's not specific to the language.
呃，这不是语言特有的。

589
00:32:56,245 --> 00:32:59,785
And so- so here's kind of a picture that's trying to get at this.
所以，这是一张试图解决这个问题的图片。

590
00:32:59,785 --> 00:33:03,160
So our autoencoder, um, and our, um,
那么我们的autoencoder，嗯，我们的，嗯，

591
00:33:03,160 --> 00:33:05,290
here in our back-translation example,
在我们的反向翻译示例中，

592
00:33:05,290 --> 00:33:07,210
um, here, the target sequence is the same.
嗯，这里，目标序列是相同的。

593
00:33:07,210 --> 00:33:10,090
[NOISE] Um, so what that essentially means is
[NOISE]嗯，所以这基本上意味着什么

594
00:33:10,090 --> 00:33:14,200
that the vectors for the English sentence and the French sentence,
英语句子和法语句子的向量，

595
00:33:14,200 --> 00:33:17,410
um, are going to be trained to be the same, um, right?
嗯，将会受到同样的训练，对吧？

596
00:33:17,410 --> 00:33:19,645
Because if they are different, our, uh,
因为如果他们不同，我们，呃，

597
00:33:19,645 --> 00:33:21,520
decoder would be generating different,
解码器会生成不同的，

598
00:33:21,520 --> 00:33:25,040
uh, outputs on these two examples.
呃，这两个例子的输出。

599
00:33:25,040 --> 00:33:29,640
Um, so here- this is just another sort of intuition is that what our model is
嗯，所以这里 - 这只是另一种直觉，就是我们的模型

600
00:33:29,640 --> 00:33:31,290
trying to learn here is kind of a way of
试图在这里学习是一种方式

601
00:33:31,290 --> 00:33:33,870
encoding the information of a sentence in a vector,
编码矢量中句子的信息，

602
00:33:33,870 --> 00:33:37,100
um, but in a way that is language-agnostic.
嗯，但是这种方式与语言无关。

603
00:33:37,100 --> 00:33:39,460
Um, any more questions about,
嗯，还有任何问题，

604
00:33:39,460 --> 00:33:42,740
uh, unsupervised machine translation?
呃，无人监督的机器翻译？

605
00:33:44,220 --> 00:33:50,350
Okay. Um, so going on to results of this approach, um,
好的。嗯，继续这种方法的结果，嗯，

606
00:33:50,350 --> 00:33:53,020
here, the horizontal lines are,
这里，水平线是，

607
00:33:53,020 --> 00:33:56,860
um, the results of an unsupervised machine translation model.
嗯，无监督机器翻译模型的结果。

608
00:33:56,860 --> 00:34:00,775
Um, the lines that go up are for a supervised machine translation model,
嗯，上升的线是监督机器翻译模型，

609
00:34:00,775 --> 00:34:03,900
um, as we give it more and more data.
嗯，因为我们给它提供了越来越多的数据。

610
00:34:03,900 --> 00:34:06,300
Right? So unsurprisingly, um,
对？不出所料，嗯，

611
00:34:06,300 --> 00:34:09,405
given a large amount of supervised data, um,
给出了大量的监督数据，嗯，

612
00:34:09,405 --> 00:34:11,790
the supervised machine translation models
有监督的机器翻译模型

613
00:34:11,790 --> 00:34:15,725
work much better than the unsupervised machine translation model.
比无人监督的机器翻译模型工作得更好。

614
00:34:15,725 --> 00:34:19,285
Um, but, um, the unsupervised machine translation model,
嗯，但是，嗯，无人监督的机器翻译模型，

615
00:34:19,285 --> 00:34:21,310
actually still does quite well.
实际上仍然做得很好。

616
00:34:21,310 --> 00:34:26,995
Um, so if you see it around 10,000 to 100,000 training examples,
嗯，如果你看到大约10,000到100,000个训练样例，

617
00:34:26,995 --> 00:34:30,565
um, it actually does just as well or better than supervised translation,
嗯，它实际上与监督翻译一样好或更好，

618
00:34:30,565 --> 00:34:33,580
and I think that's a really promising result,
而且我认为这是一个非常有希望的结果，

619
00:34:33,580 --> 00:34:36,640
uh, because if you think of, um,
呃，因为如果你想到的，嗯，

620
00:34:36,640 --> 00:34:39,550
low-resource settings where there isn't much labeled examples, um,
资源不足的设置，没有太多标记的例子，嗯，

621
00:34:39,550 --> 00:34:42,280
it suddenly becomes really nice that you can perform this well,
你突然变得非常好，你可以做得很好，

622
00:34:42,280 --> 00:34:46,550
um, without even needing to use a training set.
嗯，甚至不需要使用训练集。

623
00:34:48,690 --> 00:34:51,745
Um, another thing kind of fun you can do with,
嗯，你可以做的另一件好事，

624
00:34:51,745 --> 00:34:55,195
an unsupervised machine translation model is attribute transfer.
无监督的机器翻译模型是属性转移。

625
00:34:55,195 --> 00:34:58,855
Um, so basically, you can, um, take, uh,
嗯，基本上，你可以，嗯，接受，呃，

626
00:34:58,855 --> 00:35:00,520
collections of texts that,
收集的文本，

627
00:35:00,520 --> 00:35:03,190
uh, split by any attribute you want.
呃，按你想要的任何属性划分。

628
00:35:03,190 --> 00:35:04,900
So for example, you could go on Twitter,
例如，你可以去推特，

629
00:35:04,900 --> 00:35:08,650
look at hashtags to decide which tweets are annoyed and which tweets are relaxed,
查看主题标签以确定哪些推文令人生气，哪些推文放宽，

630
00:35:08,650 --> 00:35:11,080
and then you can treat those two corpora as
然后你可以将这两个语料库视为

631
00:35:11,080 --> 00:35:13,614
text as though they were two different languages,
文本好像是两种不同的语言，

632
00:35:13,614 --> 00:35:16,510
and you can train an unsupervised machine translation model,
你可以训练一个无人监督的机器翻译模型，

633
00:35:16,510 --> 00:35:19,165
uh, to convert from one to the other.
呃，从一个转换到另一个。

634
00:35:19,165 --> 00:35:22,495
Uh, and you can see these examples, um,
呃，你可以看到这些例子，嗯，

635
00:35:22,495 --> 00:35:26,650
the model actually does a pretty good job of sort of minimally changing the sentence,
该模型实际上可以很好地改变句子，

636
00:35:26,650 --> 00:35:29,680
kind of preserving a lot of that sentence's original semantics,
保留了很多句子的原始语义，

637
00:35:29,680 --> 00:35:33,620
um, such that the target attribute is changed.
嗯，这样就改变了目标属性。

638
00:35:36,540 --> 00:35:41,290
Um, I also wanna throw a little bit of cold water on this idea.
嗯，我也想对这个想法投入一点冷水。

639
00:35:41,290 --> 00:35:44,410
So I do think it's really exciting and- and almost kind of
所以我认为这真的很令人兴奋 - 而且差不多

640
00:35:44,410 --> 00:35:47,650
mind-blowing that you can do this translation without labeled data.
令人兴奋的是你可以在没有标记数据的情况下进行翻译。

641
00:35:47,650 --> 00:35:49,600
Um, certainly, right.
嗯，当然，对。

642
00:35:49,600 --> 00:35:54,520
It's really hard to imagine someone giving me a bunch of books in Italian and say, "Okay.
很难想象有人用意大利语给我一堆书并说：“好的。

643
00:35:54,520 --> 00:35:56,410
We're in Italian," um, without, you know,
我们是意大利人，“嗯，没有，你知道，

644
00:35:56,410 --> 00:35:59,755
teaching you how to specifically do the translation.
教你如何专门做翻译。

645
00:35:59,755 --> 00:36:03,955
Um, but, um, even though these methods show promise,
嗯，但是，嗯，即使这些方法显示出希望，

646
00:36:03,955 --> 00:36:08,095
um, mostly they have shown promise on languages that are quite closely related.
嗯，大多数人都对与语言密切相关的语言表现出了希望。

647
00:36:08,095 --> 00:36:09,775
So those previous results,
那些以前的结果，

648
00:36:09,775 --> 00:36:11,065
those were all, um,
那些都是，嗯，

649
00:36:11,065 --> 00:36:13,750
some combination of English to French or English to German,
英语到法语或英语到德语的某种组合，

650
00:36:13,750 --> 00:36:16,270
um, or so on, and those languages are quite similar.
嗯，等等，那些语言非常相似。

651
00:36:16,270 --> 00:36:18,280
[NOISE] Um, so if you look at, uh,
[NOISE]嗯，如果你看，呃，

652
00:36:18,280 --> 00:36:20,320
a different language pair, let's say English to Turkish,
一个不同的语言对，让我们说英语到土耳其语，

653
00:36:20,320 --> 00:36:24,685
where, um, the linguistics in those two languages are quite different, uh,
在哪里，嗯，这两种语言的语言学是完全不同的，呃，

654
00:36:24,685 --> 00:36:27,610
these methods do still work to some extent, um,
这些方法在某种程度上仍然有效，嗯，

655
00:36:27,610 --> 00:36:30,910
so they get around five BLEU points let's say, uh,
所以他们得到了五个BLEU点，让我们说，呃，

656
00:36:30,910 --> 00:36:33,190
but they don't work nearly as well,
但他们几乎没有工作，

657
00:36:33,190 --> 00:36:35,890
um, as they do in the f- uh, i- in the other settings, right?
嗯，正如他们在f-uh中所做的那样，在其他环境中，对吧？

658
00:36:35,890 --> 00:36:40,240
So there's still a huge gap to purely supervised learning. Um, right?
因此纯粹监督学习仍然存在巨大差距。嗯，对吗？

659
00:36:40,240 --> 00:36:41,350
So we're probably not, you know,
所以我们可能不是，你知道，

660
00:36:41,350 --> 00:36:45,040
quite at this stage where an alien could come down and it's sort of, no problem,
在这个阶段，一个外星人可以下来，这是一种，没有问题，

661
00:36:45,040 --> 00:36:48,220
let's use our unsupervised machine translation system, um,
让我们使用我们的无监督机器翻译系统，嗯，

662
00:36:48,220 --> 00:36:52,399
but I still think that's pretty exciting progress. Um, yeah. Question?
但我仍然认为这是非常令人兴奋的进步。嗯，是的题？

663
00:36:52,399 --> 00:36:55,270
Um, so what you're saying is that the genealogy of
嗯，所以你所说的是家谱

664
00:36:55,270 --> 00:36:58,630
a language might need it to superimpose worse, right?
一种语言可能需要它叠加更糟，对吧？

665
00:36:58,630 --> 00:37:01,510
Because my original thought was that if you took, for example,
因为我原来的想法是，如果你采取，例如，

666
00:37:01,510 --> 00:37:04,810
like Latin, which doesn't have a word for, you know,
像拉丁语，它没有一句话，你知道，

667
00:37:04,810 --> 00:37:11,440
the modern classification of car, I thought that would do more poorly. But if- but, uh, basically,
现代汽车的分类，我认为会做得更差。但是，如果 - 但是，呃，基本上，

668
00:37:11,440 --> 00:37:15,280
what I'm asking is, do you think the English maps better to Latin
我问的是，你认为英语更好地映射到拉丁语

669
00:37:15,280 --> 00:37:20,380
because they're both related, and worse to Turkish or is it the other way around?
因为它们都是相关的，对土耳其人来说更糟糕，还是相反？

670
00:37:20,380 --> 00:37:25,645
Um, I would expect English to map quite a lot better to Latin.
嗯，我希望英语能更好地映射到拉丁语。

671
00:37:25,645 --> 00:37:28,930
And I think part of the issue here is that, um,
我认为这里的部分问题是，嗯，

672
00:37:28,930 --> 00:37:33,460
the difficulty in translation I think is not really at the word level.
翻译的困难我认为并不是真正的单词级别。

673
00:37:33,460 --> 00:37:35,410
So I mean that certainly is an issue that words exist
所以我的意思是肯定存在一个词存在的问题

674
00:37:35,410 --> 00:37:37,495
in one language that don't exist in another,
用一种在另一种语言中不存在的语言，

675
00:37:37,495 --> 00:37:38,740
um, but I think actually,
嗯，但我想其实，

676
00:37:38,740 --> 00:37:43,195
more substantial differences between language is at the level of like syntax,
语言之间更大的差异在于语法水平，

677
00:37:43,195 --> 00:37:45,820
um, um, or you know, semantics, right?
嗯，嗯，或者你知道，语义，对吧？

678
00:37:45,820 --> 00:37:47,410
How ideas are expressed.
如何表达想法。

679
00:37:47,410 --> 00:37:53,515
Um, so- so I think I- I would expect Ital- Latin to have, you know,
嗯，所以我想我 - 我希望Ital-拉丁语，你知道，

680
00:37:53,515 --> 00:37:56,020
relatively similar syntax to English,
语法与英语相似，

681
00:37:56,020 --> 00:37:57,580
um, compared to say Turkish,
嗯，与土耳其语相比，

682
00:37:57,580 --> 00:37:59,860
I imagine that is probably the bigger obstacle
我想这可能是更大的障碍

683
00:37:59,860 --> 00:38:03,260
for unsupervised machine translation models.
适用于无人监督的机器翻译模型。

684
00:38:07,190 --> 00:38:10,260
Um, I'm going to really quickly go into
嗯，我会很快进入

685
00:38:10,260 --> 00:38:14,910
this last recent research paper which is basically taking BERT  which,
这篇最近的研究论文基本上是以BERT为主，

686
00:38:14,910 --> 00:38:17,265
which you've learned about, um, correct?
你了解到的，嗯，对吗？

687
00:38:17,265 --> 00:38:20,190
Yes. Okay. And making it cross-lingual.
是。好的。并使其跨语言。

688
00:38:20,190 --> 00:38:23,730
Um, so, um, here's what regular BERT is, right?
嗯，所以，嗯，这是常规BERT，对吧？

689
00:38:23,730 --> 00:38:26,295
We have a sequence of sentences in English.
我们有一系列英语句子。

690
00:38:26,295 --> 00:38:28,215
We're going to mask out some of the words.
我们要掩盖一些话。

691
00:38:28,215 --> 00:38:31,500
And we're going to ask BERT which is our transformer model, um,
我们要问BERT哪个是我们的变压器型号，嗯，

692
00:38:31,500 --> 00:38:36,675
to essentially fill in the blanks and predict what were the words that were dropped out.
从根本上填补空白并预测被删除的单词是什么。

693
00:38:36,675 --> 00:38:42,990
Um, what actually has already been done by Google is training a multilingual BERT .
嗯，Google实际上已经完成的是培训多语言BERT。

694
00:38:42,990 --> 00:38:46,835
So what they did essentially is concatenate, um,
所以他们所做的基本上是连接，嗯，

695
00:38:46,835 --> 00:38:51,560
a whole bunch of corpora in different languages and then train one model um,
一大堆不同语言的语料库，然后训练一个模型，

696
00:38:51,560 --> 00:38:54,785
doing using this masked LM objective um,
使用这个蒙面的LM物镜嗯，

697
00:38:54,785 --> 00:38:56,315
on all of that text at once.
立刻就所有这些文字。

698
00:38:56,315 --> 00:38:58,300
And that's a publicly released model.
这是一个公开发布的模型。

699
00:38:58,300 --> 00:39:03,495
Um, the, the new kind of extension to this that has recently been uh,
嗯，最近这种新的延伸，呃，

700
00:39:03,495 --> 00:39:06,300
proposed by Facebook is to actually combine
Facebook提出的实际结合

701
00:39:06,300 --> 00:39:10,965
this masked LM training objective um, with uh, translation.
这个蒙面的LM训练目标嗯，呃，翻译。

702
00:39:10,965 --> 00:39:17,130
So what they do is sometimes give this model a in this case,
所以他们所做的有时在这种情况下给这个模型一个，

703
00:39:17,130 --> 00:39:21,060
a sequence in English and a sequence in uh, French.
一个英文的序列和一个呃，法语的序列。

704
00:39:21,060 --> 00:39:24,300
Um, drop out some of the words and just as before,
嗯，辍学一些话，就像以前一样，

705
00:39:24,300 --> 00:39:26,130
ask the model to fill it in.
让模型填写。

706
00:39:26,130 --> 00:39:28,635
And the motivation here is that, um,
这里的动机是，嗯，

707
00:39:28,635 --> 00:39:31,080
this will much better cause the model
这将更好地导致模型

708
00:39:31,080 --> 00:39:33,525
to understand the relation between these two languages.
了解这两种语言之间的关系。

709
00:39:33,525 --> 00:39:37,950
Because if you're trying to find a fill in a English word that's been dropped,
因为如果你试图找到一个被删除的英文单词，

710
00:39:37,950 --> 00:39:40,500
uh, the best way to do it if you have a translation is look
呃，如果你有翻译，最好的方法就是看

711
00:39:40,500 --> 00:39:43,005
at the French side and try to find that word.
在法国方面，并尝试找到这个词。

712
00:39:43,005 --> 00:39:45,075
Hopefully, that one hasn't been dropped as well.
希望那个也没有掉线。

713
00:39:45,075 --> 00:39:48,525
And then you can um, much more easily fill in the blank.
然后你可以，更容易填补空白。

714
00:39:48,525 --> 00:39:52,575
And uh, this actually leads to very uh,
呃，这实际上导致了呃，

715
00:39:52,575 --> 00:39:55,860
substantial improvements in unsupervised machine translation.
无监督机器翻译的重大改进。

716
00:39:55,860 --> 00:39:59,670
So just like BERT is used for other tasks in NLP,
就像BERT用于NLP中的其他任务一样，

717
00:39:59,670 --> 00:40:02,010
they basically take this cross-lingual BERT.
他们基本上采用这种跨语言的BERT。

718
00:40:02,010 --> 00:40:03,930
They use it as initialization for
他们用它作为初始化

719
00:40:03,930 --> 00:40:07,410
a unsupervised machine translation system and they get, you know,
一个无人监督的机器翻译系统，你知道，

720
00:40:07,410 --> 00:40:10,425
really large gains on the order of 10 BLEU points um,
在10个BLEU点的顺序上获得了很大的收益，

721
00:40:10,425 --> 00:40:12,690
such that the gap between
这样的差距

722
00:40:12,690 --> 00:40:16,485
unsupervised machine translation and the current supervised state of the art,
无人监督的机器翻译和当前监督的现有技术，

723
00:40:16,485 --> 00:40:18,420
um, is much smaller.
嗯，要小得多。

724
00:40:18,420 --> 00:40:23,190
Uh, so this is a pretty recent idea but I think it also shows promise
呃，所以这是一个非常近期的想法，但我认为它也显示出了希望

725
00:40:23,190 --> 00:40:28,320
in really improving the quality of translation through using unlabeled data.
通过使用未标记的数据真正提高翻译质量。

726
00:40:28,320 --> 00:40:30,945
Um, although I guess yeah, I guess in this case with BERT
嗯，虽然我猜是的，我想在这种情况下用BERT

727
00:40:30,945 --> 00:40:33,930
they are using labeled translation data as well.
他们也使用带标签的翻译数据。

728
00:40:33,930 --> 00:40:37,270
Any, any questions about this?
对此有任何问题吗？

729
00:40:37,820 --> 00:40:46,350
Okay. Um, so that is all I'm going to say about using unlabeled data for translation.
好的。嗯，这就是我要说的关于使用未标记数据进行翻译的全部内容。

730
00:40:46,350 --> 00:40:48,750
The next part of this talk is about um,
这次演讲的下一部分是关于嗯，

731
00:40:48,750 --> 00:40:54,720
what happens if we really scale up these unsupervised language models.
如果我们真的扩大这些无监督的语言模型会发生什么。

732
00:40:54,720 --> 00:40:59,865
Um, so in particular I'm gonna talk about GPT-2 which is a new model by OpenAI.
嗯，特别是我要谈的是OpenAI的新型号GPT-2。

733
00:40:59,865 --> 00:41:02,265
That's essentially a really giant language model
这本质上是一个非常庞大的语言模型

734
00:41:02,265 --> 00:41:05,685
and I think it has some interesting implications.
我认为它有一些有趣的含义。

735
00:41:05,685 --> 00:41:15,060
So first of all, here's just the sizes of a bunch of different NLP models and,
首先，这里只是一堆不同NLP模型的尺寸，

736
00:41:15,060 --> 00:41:18,165
um, you know, maybe a couple years ago the,
嗯，你知道，也许是几年前的，

737
00:41:18,165 --> 00:41:19,230
the standard sort of
标准的

738
00:41:19,230 --> 00:41:24,135
LSTM medium-size model was on the order of about 10 million parameters.
LSTM中型模型大约有1000万个参数。

739
00:41:24,135 --> 00:41:30,660
Where 10- where a parameter is just a single weight let's say in the neural net um,
其中10-参数只是一个重量，让我们说在神经网络中，

740
00:41:30,660 --> 00:41:33,090
ELMo and uh, GPT.
ELMo和呃，GPT。

741
00:41:33,090 --> 00:41:35,520
So the original OpenAI paper before they did
所以在他们做之前的原始OpenAI论文

742
00:41:35,520 --> 00:41:38,820
this GPT-2 and we're about 10 times bigger than that.
这个GPT-2，我们大约是它的10倍。

743
00:41:38,820 --> 00:41:43,120
Um, GPT-2 is about another order of magnitude bigger.
嗯，GPT-2大约相当于另一个数量级。

744
00:41:44,120 --> 00:41:48,825
Um, one kind of interesting comparison point here is that uh,
嗯，这里有一个有趣的比较点就是呃，

745
00:41:48,825 --> 00:41:51,735
GPT-2 which is 1,5 billion parameters,
GPT-2，参数为15亿，

746
00:41:51,735 --> 00:41:55,635
actually has more parameters than a honey bee brain has synapses.
实际上有比蜜蜂大脑有突触更多的参数。

747
00:41:55,635 --> 00:41:58,440
Um, so that sounds kind of impressive, right?
嗯，这听起来有点令人印象深刻吧？

748
00:41:58,440 --> 00:42:01,350
You know honeybees are not the smartest of
你知道蜜蜂并不是最聪明的

749
00:42:01,350 --> 00:42:05,325
animals but they can still fly around and find nectar or whatever.
动物，但他们仍然可以飞来飞去寻找花蜜或其他什么。

750
00:42:05,325 --> 00:42:08,760
Um, but yeah. Of course, this isn't really an apples to apples comparison, right?
嗯，但是。当然，这不是一个苹果对苹果的比较，对吗？

751
00:42:08,760 --> 00:42:11,970
So a synapse and a weight in a neural net are really quite different.
因此，神经网络中的突触和重量确实非常不同。

752
00:42:11,970 --> 00:42:14,490
But I just think it's one kind of interesting milestone
但我认为这是一个有趣的里程碑

753
00:42:14,490 --> 00:42:16,815
let's say in terms of model size um,
让我们说模型尺寸嗯，

754
00:42:16,815 --> 00:42:18,150
that has been surpassed.
已被超越。

755
00:42:18,150 --> 00:42:26,835
[NOISE] Um, one thing to point out here is that um,
[NOISE]嗯，有一点要指出的是嗯，

756
00:42:26,835 --> 00:42:32,130
this increasing scaling of deep learning is really a general trend uh,
深度学习这种不断增加的规模确实是一种普遍的趋势呃，

757
00:42:32,130 --> 00:42:34,845
in all of machine learning so beyond NLP.
所有的机器学习都超越了NLP。

758
00:42:34,845 --> 00:42:41,760
So this plot is showing time on the x-axis and the y-axis is log scaled um,
所以这个图是在x轴上显示时间，y轴是对数缩放的，

759
00:42:41,760 --> 00:42:45,255
the amount of petaFLOPS used to train this model.
用于训练此模型的petaFLOPS数量。

760
00:42:45,255 --> 00:42:50,010
Um, so what this means is that the trend at least currently is that there is
嗯，这意味着至少目前的趋势是存在的

761
00:42:50,010 --> 00:42:53,130
exponential growth in how much compute power
计算能力的指数增长

762
00:42:53,130 --> 00:42:55,735
we're throwing at our machine learning models.
我们正在投入我们的机器学习模型。

763
00:42:55,735 --> 00:42:57,920
I guess it is kind of unclear, you know,
我想这有点不清楚，你知道，

764
00:42:57,920 --> 00:43:00,695
will exponential growth continue but certainly um,
将指数增长继续，但肯定，嗯，

765
00:43:00,695 --> 00:43:03,560
there's rapid growth in the size of our models.
我们的模型尺寸快速增长。

766
00:43:03,560 --> 00:43:06,200
And it's leading to some really amazing results, right?
它会导致一些非常惊人的结果，对吧？

767
00:43:06,200 --> 00:43:09,445
So here are results not from language but for vision.
所以这里的结果不是语言，而是视觉。

768
00:43:09,445 --> 00:43:13,155
Um, this is a generative adversarial network
嗯，这是一个生成性的对抗网络

769
00:43:13,155 --> 00:43:16,920
that's been trained on a lot of data and it's been trained on really large scales.
那是经过大量数据训练的，并且已经在很大的范围内进行了训练。

770
00:43:16,920 --> 00:43:22,710
So it's a big model kind of in-between the size of ELMo and BERT let's say.
因此，这是ELMo和BERT之间的大型模型。

771
00:43:22,710 --> 00:43:27,510
And uh, these photos here are actually productions of the model.
呃，这些照片实际上是模型的制作。

772
00:43:27,510 --> 00:43:28,740
So those aren't real photos.
所以那些不是真实的照片。

773
00:43:28,740 --> 00:43:31,515
Those are things the model has just kind of hallucinated out of thin air.
这些模型只是凭空产生的幻觉。

774
00:43:31,515 --> 00:43:34,770
And at least to me they look essentially photo-realistic.
至少在我看来，它们看起来基本上是照片般逼真的。

775
00:43:34,770 --> 00:43:38,010
There's also a website that um, is fun to look at it.
还有一个网站，嗯，看起来很有趣。

776
00:43:38,010 --> 00:43:39,915
If you're not- if you're interested which is,
如果你不是 - 如果你感兴趣的是，

777
00:43:39,915 --> 00:43:42,195
thispersondoesnotexist.com.
thispersondoesnotexist.com。

778
00:43:42,195 --> 00:43:43,950
So if you go there, you'll see
所以，如果你去那里，你会看到

779
00:43:43,950 --> 00:43:47,430
a very convincing photo of a person but it's not a real photo.
一张非常令人信服的照片，但这不是一张真正的照片。

780
00:43:47,430 --> 00:43:51,400
It's again like a hallucinated image produced by a GAN.
它再次像GAN产生的幻觉图像。

781
00:43:51,440 --> 00:43:55,725
We're also seeing really huge models being used for image recognition.
我们也看到真正用于图像识别的巨大模型。

782
00:43:55,725 --> 00:43:58,110
So this is recent work by Google where they trained
所以这是他们最近在Google培训的工作

783
00:43:58,110 --> 00:44:02,010
an image net model with half a billion parameters.
具有5亿个参数的图像网模型。

784
00:44:02,010 --> 00:44:06,450
So that's bigger than BERT but not as big as GPT-2.
所以这比BERT大，但没有GPT-2那么大。

785
00:44:06,450 --> 00:44:09,420
Um, this plot here is showing a
嗯，这个情节在这里显示了一个

786
00:44:09,420 --> 00:44:14,760
log scaled number of parameters on the x-axis and then accuracy at ImageNet
记录x轴上参数的缩放数量，然后是ImageNet的精度

787
00:44:14,760 --> 00:44:20,520
on the y-axis- axis and sort of unsurprisingly bigger models perform better.
在y轴 - 轴和一些不出所料的大型模型表现更好。

788
00:44:20,520 --> 00:44:24,000
And there seems to actually be a pretty consistent trend here which is uh,
这里似乎有一个非常一致的趋势，呃，

789
00:44:24,000 --> 00:44:28,240
accuracy is increasing with the log of the, the model size.
精度随着模型尺寸的对数而增加。

790
00:44:31,010 --> 00:44:35,100
Um, I wanna go into a little bit more detail, how is it
嗯，我想更详细一点，它是怎么回事

791
00:44:35,100 --> 00:44:39,060
possible that we can scale up models and train models at such a large extent.
我们可以在很大程度上扩展模型和训练模型。

792
00:44:39,060 --> 00:44:41,190
One answer is just better hardware.
一个答案就是更好的硬件。

793
00:44:41,190 --> 00:44:42,675
And in particular, um,
特别是，嗯，

794
00:44:42,675 --> 00:44:44,160
there's a growing uh,
呃正在增长，

795
00:44:44,160 --> 00:44:48,165
number of companies that are developing hardware specifically for deep learning.
正在开发专门用于深度学习的硬件的公司数量。

796
00:44:48,165 --> 00:44:50,520
So these are even more kind of constrained and the
所以这些更受限制了

797
00:44:50,520 --> 00:44:53,190
kind of operations they can do than a GPU,
他们可以做的操作比GPU，

798
00:44:53,190 --> 00:44:55,950
um but they do those operations even faster.
嗯，但他们更快地完成这些操作。

799
00:44:55,950 --> 00:44:59,610
So Google's Tensor Processing Units is one example.
所以谷歌的Tensor Processing Units就是一个例子。

800
00:44:59,610 --> 00:45:03,180
There are actually a bunch of other companies working on this idea.
实际上有很多其他公司在研究这个想法。

801
00:45:03,180 --> 00:45:06,930
Um, the other way to scale up models is by taking advantage of
嗯，扩大模型的另一种方法是利用

802
00:45:06,930 --> 00:45:11,835
parallelism and there's two kinds of parallelism that I want to talk about very briefly.
并行性，我想简单地谈谈两种并行性。

803
00:45:11,835 --> 00:45:13,980
So one is data parallelism.
所以一个是数据并行性。

804
00:45:13,980 --> 00:45:16,785
In this case, each of your,
在这种情况下，你的每一个，

805
00:45:16,785 --> 00:45:19,380
let's say GPUs, will have a copy of the model.
让我们说GPU，将有一个模型的副本。

806
00:45:19,380 --> 00:45:21,480
And what you essentially do is split
你基本上做的是拆分

807
00:45:21,480 --> 00:45:25,350
the mini-batch that you're training on across these different models.
您正在这些不同型号上进行培训的小批量产品。

808
00:45:25,350 --> 00:45:27,165
So if you have, let's say,
所以如果你有，让我们说，

809
00:45:27,165 --> 00:45:30,945
16 GPUs and each of them see a batch size of 32.
16个GPU，每个GPU的批量大小为32。

810
00:45:30,945 --> 00:45:35,670
You can aggregate the gradients of these 16 uh, uh,
你可以聚合这些16 uh的渐变，呃，

811
00:45:35,670 --> 00:45:42,540
if you do a back-prop on these 16 GPUs and you end up with effectively a batch size of 512.
如果你对这16个GPU进行反向支持，最终你的批量大小为512。

812
00:45:42,540 --> 00:45:44,700
So this allows you to train models much faster.
因此，这可以让您更快地训练模型。

813
00:45:44,700 --> 00:45:50,340
Um, the other kind of parallelism that's growing in importance is model par- parallelism.
嗯，另一种越来越重要的并行性是模型并行性。

814
00:45:50,340 --> 00:45:54,510
Um, so eventually models get so big that they
嗯，所以最终模特变得如此之大

815
00:45:54,510 --> 00:45:59,070
can't even fit on a single GPU and they can't even do a batch size of one.
甚至不能适用于单个GPU，它们甚至不能做一个批量大小。

816
00:45:59,070 --> 00:46:00,660
Um, in this case,
嗯，在这种情况下，

817
00:46:00,660 --> 00:46:02,985
you actually need to split up the model across
你实际上需要将模型分开

818
00:46:02,985 --> 00:46:06,075
multiple computers- multiple compute units.
多台计算机 - 多个计算单元。

819
00:46:06,075 --> 00:46:10,485
Um, and that's what's done for models kind of the size of,
嗯，这就是那种大小的模型所做的，

820
00:46:10,485 --> 00:46:12,720
of let's say GPT-2.
让我们说GPT-2。

821
00:46:12,720 --> 00:46:15,540
There are new frameworks such as Mesh-TensorFlow, um,
有一些新的框架，比如Mesh-TensorFlow，嗯，

822
00:46:15,540 --> 00:46:21,430
which are basically designed to make this sort of model parallelism easier.
它们基本上是为了使这种模型并行化更容易。

823
00:46:23,990 --> 00:46:27,390
Um, okay. So onto GPT-2, um,
嗯，好的。所以去GPT-2，嗯，

824
00:46:27,390 --> 00:46:31,560
I know you already saw this a little bit in the contextualized uh,
我知道你已经在语境中看到了这一点呃，

825
00:46:31,560 --> 00:46:36,540
um, embeddings um, lecture but I'm going to go into some more depth here.
嗯，嵌入式，讲课，但我会在这里深入探讨。

826
00:46:36,540 --> 00:46:41,265
[NOISE] So so essentially it's a really large transformer language model.
[NOISE]所以基本上它是一个非常大的变换器语言模型。

827
00:46:41,265 --> 00:46:45,165
Um, so there's nothing really kind of novel here in terms
嗯，所以这里没有什么真正的小说

828
00:46:45,165 --> 00:46:49,305
of new training algorithms or in terms of um,
新的训练算法或um，

829
00:46:49,305 --> 00:46:51,645
the loss function or anything like that.
损失函数或类似的东西。

830
00:46:51,645 --> 00:46:53,340
Um, the thing that makes it different from
嗯，让它与众不同的东西

831
00:46:53,340 --> 00:46:56,070
prior work is that it's just really really big.
以前的工作是它真的很大。

832
00:46:56,070 --> 00:46:59,970
Uh, it's trained on a correspondingly huge amount of text.
呃，它训练了相应的大量文本。

833
00:46:59,970 --> 00:47:04,800
So it's trained on 40 gigabytes and that's roughly 10 times larger than previous uh,
所以它训练了40千兆字节，大约是以前的10倍，

834
00:47:04,800 --> 00:47:07,215
language models have been trained on.
语言模型已经过培训。

835
00:47:07,215 --> 00:47:11,070
Um, when you have that size of dataset,
嗯，当你有那么大的数据集时，

836
00:47:11,070 --> 00:47:14,325
um, the only way to get that much text is essentially to go to the web.
嗯，获得那么多文本的唯一方法就是去网上。

837
00:47:14,325 --> 00:47:18,840
Um, so one thing OpenAI put a quite a bit of effort into when they're developing
嗯，所以OpenAI在开发过程中付出了相当大的努力

838
00:47:18,840 --> 00:47:23,570
this network was to ensure that that text was pretty high-quality.
这个网络是为了确保该文本非常高质量。

839
00:47:23,570 --> 00:47:26,180
Um, and they did that in a kind of interesting way.
嗯，他们以一种有趣的方式做到了。

840
00:47:26,180 --> 00:47:28,970
They, they looked at Reddit which is this website where people uh,
他们，他们看着Reddit，这是人们呃这个网站，

841
00:47:28,970 --> 00:47:30,140
can vote on links.
可以投票链接。

842
00:47:30,140 --> 00:47:31,640
And then they said uh, if
然后他们说呃，如果

843
00:47:31,640 --> 00:47:35,090
a link has a lot of votes then it's probably sort of a decent link.
一个链接有很多投票，那么它可能是一个不错的链接。

844
00:47:35,090 --> 00:47:36,830
There's probably um, you know,
可能嗯，你知道，

845
00:47:36,830 --> 00:47:39,750
reasonable text there for a model to learn.
合理的文字有一个模型可供学习。

846
00:47:40,610 --> 00:47:43,080
Um, okay, so if we have
嗯，好的，如果有的话

847
00:47:43,080 --> 00:47:45,600
this super huge language model like
这个超级庞大的语言模型就像

848
00:47:45,600 --> 00:47:49,515
GPT-2 on this question of what can you actually do with it,
GPT-2关于你可以用它做什么的这个问题，

849
00:47:49,515 --> 00:47:53,415
um, well obviously if you have a language model you can do language modelling with it.
嗯，很明显，如果你有一个语言模型，你可以用它进行语言建模。

850
00:47:53,415 --> 00:47:56,790
Uh, but one thing kind of interestingly interesting is that you
呃，但有一点有趣的是你

851
00:47:56,790 --> 00:48:00,525
can run this language model on er,
可以在er上运行这个语言模型，

852
00:48:00,525 --> 00:48:03,435
existing benchmarks, um, for,
现有基准，嗯，for，

853
00:48:03,435 --> 00:48:05,250
for language modelling, um,
对于语言建模，嗯，

854
00:48:05,250 --> 00:48:08,520
and it gets state of the art perplexity on these benchmarks even
它甚至可以在这些基准测试中获得最先进的困惑

855
00:48:08,520 --> 00:48:11,700
though it never sees the training data for these benchmarks, right?
虽然从未见过这些基准的训练数据，对吧？

856
00:48:11,700 --> 00:48:16,770
So normally, if you want to say evaluate your language model on the Penn Treebank.
通常情况下，如果你想在Penn Treebank上评估你的语言模型。

857
00:48:16,770 --> 00:48:21,510
You first train on the Penn Treebank and then you evaluate on this held-out set.
你首先在Penn Treebank上训练，然后你对这个保持套装进行评估。

858
00:48:21,510 --> 00:48:23,790
Uh, in this case, uh,
呃，在这种情况下，呃，

859
00:48:23,790 --> 00:48:28,515
a GPT-2 just by virtue of having seen so much text and being such a large model,
GPT-2只是因为看过这么多文字而成为如此庞大的模特，

860
00:48:28,515 --> 00:48:31,095
outperforms all these other uh,
优于其他所有呃，

861
00:48:31,095 --> 00:48:34,540
prior works even though it's not seeing that data.
以前的工作，即使它没有看到这些数据。

862
00:48:34,580 --> 00:48:39,430
Um, on a bunch of different uh, language modelling benchmarks.
嗯，在一堆不同的呃，语言建模基准。

863
00:48:40,800 --> 00:48:46,315
Um, but there's a bunch of other interesting experiments that OpenAI
嗯，但OpenAI还有其他一些有趣的实验

864
00:48:46,315 --> 00:48:51,700
ran with this language modeling and these were based on zero-shot learning.
使用这种语言建模，这些是基于零射击学习。

865
00:48:51,700 --> 00:48:57,250
So zero-shot learning just means trying to do a task without ever training on it.
所以零射击学习只是意味着在没有接受过训练的情况下尝试完成任务。

866
00:48:57,250 --> 00:49:00,445
And, uh, the way you can do this with a language model
而且，呃，你可以用语言模型做到这一点

867
00:49:00,445 --> 00:49:03,460
is by designing a prompt you feed into
是通过设计你提供的提示

868
00:49:03,460 --> 00:49:06,880
the language model and then have it just generate from there and
语言模型，然后让它从那里生成

869
00:49:06,880 --> 00:49:11,065
hopefully it generates something relevant to the task you're trying to solve.
希望它能产生与你想要解决的任务相关的东西。

870
00:49:11,065 --> 00:49:13,225
So for example, for reading comprehension,
例如，对于阅读理解，

871
00:49:13,225 --> 00:49:16,090
what you can do is take the context paragraph,
你可以做的是采取上下文段落，

872
00:49:16,090 --> 00:49:20,080
uh, concatenate the question to it and then add uh,
呃，将问题连接到它然后添加呃，

873
00:49:20,080 --> 00:49:21,430
a colon which is a way,
一个结肠，这是一种方式，

874
00:49:21,430 --> 00:49:22,705
I guess, of telling the model,
我想，告诉模特，

875
00:49:22,705 --> 00:49:25,210
''Okay you should be producing an answer to this question,''
“好的，你应该回答这个问题，”

876
00:49:25,210 --> 00:49:27,790
and then just have it generate text, um,
然后让它生成文本，嗯，

877
00:49:27,790 --> 00:49:30,940
and perhaps it'll generate something that is actually answering,
也许它会产生实际回答的东西，

878
00:49:30,940 --> 00:49:32,365
um, the question and is,
嗯，问题是，

879
00:49:32,365 --> 00:49:34,060
is paying attention to the context.
正在关注背景。

880
00:49:34,060 --> 00:49:37,390
[NOISE] Um, and similarly, for summarization,
[NOISE]嗯，同样，对于摘要，

881
00:49:37,390 --> 00:49:41,740
you can get the article then TL;DR and perhaps the model will produce the summary.
你可以得到文章然后TL; DR和模型可能会产生摘要。

882
00:49:41,740 --> 00:49:43,795
Um, you can even do translation,
嗯，你甚至可以做翻译，

883
00:49:43,795 --> 00:49:45,655
where you give the model,
在哪里给出模型，

884
00:49:45,655 --> 00:49:49,720
um, some ex- a list of known English to French translations so you, sort of,
嗯，一些已知的英语到法语翻译列表，所以你，有点像，

885
00:49:49,720 --> 00:49:53,770
prime it to tell it that it should be doing translation and then you give
我要告诉它它应该做翻译，然后你给

886
00:49:53,770 --> 00:49:58,120
it the source sequence equals blank and have it just run and,
它的源序列等于空白并让它刚刚运行，

887
00:49:58,120 --> 00:49:59,920
um, perhaps it'll generate,
嗯，也许它会产生，

888
00:49:59,920 --> 00:50:02,690
um, the sequence in the target language.
嗯，目标语言中的序列。

889
00:50:03,300 --> 00:50:06,895
Um, okay. So so here's what the results look like.
嗯，好的。所以这就是结果的样子。

890
00:50:06,895 --> 00:50:09,100
Um, for all of these,
嗯，对于所有这些，

891
00:50:09,100 --> 00:50:11,545
uh, the X-axis is,
呃，X轴是，

892
00:50:11,545 --> 00:50:16,210
is log scaled model size and the Y-axis is accuracy, um,
是按比例缩放的模型大小，Y轴是精度，嗯，

893
00:50:16,210 --> 00:50:18,715
and the dotted lines basically correspond to,
和虚线基本对应，

894
00:50:18,715 --> 00:50:22,090
um, existing works on these tasks.
嗯，现有的工作就是完成这些任务。

895
00:50:22,090 --> 00:50:26,290
Um, so for most of these tasks, um,
嗯，对于大多数这些任务，嗯，

896
00:50:26,290 --> 00:50:31,765
GPT-2 is quite a bit below existing systems,
GPT-2比现有系统低很多，

897
00:50:31,765 --> 00:50:33,625
um, but there's of course this big difference, right?
嗯，但当然这有很大的不同，对吧？

898
00:50:33,625 --> 00:50:37,195
Existing systems are trained specifically to do,
现有系统经过专门培训，

899
00:50:37,195 --> 00:50:39,775
um, whatever task they're being evaluated on,
嗯，无论他们正在评估什么任务，

900
00:50:39,775 --> 00:50:42,520
where GPT-2 is um,
GPT-2在哪里，

901
00:50:42,520 --> 00:50:46,540
only trained to do language modeling and as it learns language modeling,
只训练进行语言建模，并学习语言建模，

902
00:50:46,540 --> 00:50:48,865
it's sort of picking up on these other tasks.
它有点接受这些其他任务。

903
00:50:48,865 --> 00:50:50,785
Um, so right. So for example, um,
嗯，好吧。所以，例如，嗯，

904
00:50:50,785 --> 00:50:54,385
it does, uh, English to French machine translation, um,
它，呃，英语到法语机器翻译，嗯，

905
00:50:54,385 --> 00:50:56,875
not as well as, uh,
不是，呃，

906
00:50:56,875 --> 00:51:00,400
standard unsupervised machine translation which is those, uh,
标准无监督机器翻译，那些，呃，

907
00:51:00,400 --> 00:51:02,920
dotted lines, um, but it still,
虚线，嗯，但它仍然，

908
00:51:02,920 --> 00:51:04,300
it still does quite well.
它仍然很好。

909
00:51:04,300 --> 00:51:06,370
And, um, one thing, kind of,
而且，嗯，有一点，有点，

910
00:51:06,370 --> 00:51:07,810
interesting is the trend line, right,
有趣的是趋势线，对，

911
00:51:07,810 --> 00:51:09,520
for almost all of these tasks.
几乎所有这些任务。

912
00:51:09,520 --> 00:51:11,530
Um, performance is getting uh,
嗯，表现越来越呃，

913
00:51:11,530 --> 00:51:13,600
much better as the model increases in size.
随着模型尺寸的增加，情况要好得多。

914
00:51:13,600 --> 00:51:18,535
[NOISE] Um, I think a particularly interesting,
[NOISE]嗯，我觉得特别有趣，

915
00:51:18,535 --> 00:51:21,580
uh, one of these tasks is machine translation, right?
呃，这些任务之一是机器翻译，对吧？

916
00:51:21,580 --> 00:51:23,290
So the question is, how can it be doing
所以问题是，它是如何做的

917
00:51:23,290 --> 00:51:26,440
machine translation when all we're giving it as a bunch of
机器翻译，当我们把它作为一堆

918
00:51:26,440 --> 00:51:28,540
web pages and those web pages are almost all in
网页和那些网页几乎都在

919
00:51:28,540 --> 00:51:31,810
English and yet somehow it sort of magically picks up uh,
英语，但不知怎的，它有点神奇地捡起来，

920
00:51:31,810 --> 00:51:33,340
a little bit of machine translation, right.
一点机器翻译，对吧。

921
00:51:33,340 --> 00:51:35,395
So it's not a great model but it can still,
所以它不是一个伟大的模型，但它仍然可以，

922
00:51:35,395 --> 00:51:38,260
um, you know, do a decent job in some cases.
嗯，你知道，在某些情况下做得不错。

923
00:51:38,260 --> 00:51:40,510
Um, and the answer is that,
嗯，答案是，

924
00:51:40,510 --> 00:51:43,810
if you look at this giant corpus of English,
如果你看看这个巨大的英语语料库，

925
00:51:43,810 --> 00:51:47,050
occasionally, uh, within, within that corpus,
偶尔，呃，在那个语料库内，

926
00:51:47,050 --> 00:51:48,880
you see examples of translations, right?
你看到翻译的例子，对吧？

927
00:51:48,880 --> 00:51:50,290
So you see, um,
所以你看，嗯，

928
00:51:50,290 --> 00:51:52,810
a French idiom and its translation or
法语成语及其翻译或

929
00:51:52,810 --> 00:51:56,035
a quote from someone who's French and then the translation in English.
引用来自法语的人，然后是英文翻译。

930
00:51:56,035 --> 00:51:57,400
And, um, kind of,
而且，嗯，有点，

931
00:51:57,400 --> 00:52:00,700
amazingly I think this big model, um,
令人惊讶的是我觉得这个大模型，嗯，

932
00:52:00,700 --> 00:52:05,380
sees enough of these examples that it actually starts to learn how to generate French,
看到足够的这些例子，它实际上开始学习如何生成法语，

933
00:52:05,380 --> 00:52:07,030
um, even though that wasn't really,
嗯，即使那不是真的，

934
00:52:07,030 --> 00:52:09,980
sort of, an intended part of its training.
一种，是其培训的预期部分。

935
00:52:11,970 --> 00:52:14,560
Um, another interesting, um,
嗯，另一个有趣的，嗯，

936
00:52:14,560 --> 00:52:18,700
thing to dig a bit more into is its ability to do question answering.
更深入挖掘的是它能够回答问题的能力。

937
00:52:18,700 --> 00:52:24,040
So uh, a simple baseline for question answering gets about 1% accuracy,
那么，一个简单的问答基线可以获得大约1％的准确率，

938
00:52:24,040 --> 00:52:27,295
GPT-2 barely does better at 4% accuracy.
GPT-2在4％的准确率下几乎没有做得更好。

939
00:52:27,295 --> 00:52:28,840
So this isn't, like, you know,
所以这不是，就像你知道的那样

940
00:52:28,840 --> 00:52:32,440
super amazingly solved question answering, um, but, um,
超级惊人地解决了问题，嗯，但是，嗯，

941
00:52:32,440 --> 00:52:34,420
it's still pretty interesting in that,
它仍然非常有趣，

942
00:52:34,420 --> 00:52:37,435
if you look at answers the model's most confident about,
如果你看看模特最有信心的答案，

943
00:52:37,435 --> 00:52:39,010
you can see that it sort of
你可以看到它

944
00:52:39,010 --> 00:52:41,320
has learned some facts about the world, right.
正确地了解了有关这个世界的一些事实。

945
00:52:41,320 --> 00:52:45,550
So it's learned that Charles Darwin wrote Origin of Species.
据了解，查尔斯达尔文撰写了物种起源。

946
00:52:45,550 --> 00:52:50,740
Um, normally in the history of NLP, if you want to get, kind of,
嗯，通常在NLP的历史中，如果你想得到的，那种，

947
00:52:50,740 --> 00:52:52,765
world knowledge into an NLP system,
世界知识进入NLP系统，

948
00:52:52,765 --> 00:52:55,435
you'd need something like a big database of facts.
你需要像大事实数据库这样的东西。

949
00:52:55,435 --> 00:52:57,340
And even though this is still,
即使这仍然是，

950
00:52:57,340 --> 00:52:59,500
kind of, very early stages and that, um,
那种非常早期的阶段，嗯，

951
00:52:59,500 --> 00:53:04,000
there's still a huge gap between 4% accuracy and the, uh, you know,
4％的准确度和呃之间仍然存在巨大的差距，呃，你知道，

952
00:53:04,000 --> 00:53:05,875
70% or so that, uh,
70％左右，呃，

953
00:53:05,875 --> 00:53:09,550
state of the art open domain question answering systems can do,
最先进的开放域名问答系统可以做到，

954
00:53:09,550 --> 00:53:12,010
um, it, it, um,
嗯，它，它，嗯，

955
00:53:12,010 --> 00:53:14,200
it still can, uh,
它仍然可以，呃，

956
00:53:14,200 --> 00:53:17,740
pick up some world knowledge just by reading a lot of text, um, without,
通过阅读大量文本来获取一些世界知识，嗯，没有，

957
00:53:17,740 --> 00:53:21,895
kind of, explicitly having that knowledge put into the model.
有点明确地将这些知识放入模型中。

958
00:53:21,895 --> 00:53:26,810
Um, any questions by the way on GPT-2 so far?
嗯，到目前为止，GPT-2上有任何问题吗？

959
00:53:28,050 --> 00:53:33,865
Okay. So one question that's interesting to think about is,
好的。所以有一个值得思考的问题是，

960
00:53:33,865 --> 00:53:36,505
what happens if our models get even bigger?
如果我们的模型变得更大，会发生什么？

961
00:53:36,505 --> 00:53:38,305
Um, so here I've done the, um,
嗯，所以我在这里完成了，嗯，

962
00:53:38,305 --> 00:53:42,565
very scientific thing of drawing some lines in PowerPoint and seeing where they meet up.
非常科学的东西，在PowerPoint中绘制一些线条，看看他们在哪里见面。

963
00:53:42,565 --> 00:53:44,655
Um, and you can see that, um,
嗯，你可以看到，嗯，

964
00:53:44,655 --> 00:53:48,429
if the trend holds at about 1 trillion parameters,
如果趋势持续约1万亿参数，

965
00:53:48,429 --> 00:53:52,390
um, we get to human level reading comprehension performance.
嗯，我们达到了人类水平的阅读理解能力。

966
00:53:52,390 --> 00:53:55,480
Um, so if that's true it would be really astonishing.
嗯，如果这是真的，那真是令人惊讶。

967
00:53:55,480 --> 00:54:00,505
I actually do expect that a 1 trillion parameter model would be attainable in,
我实际上确实可以获得1万亿参数模型，

968
00:54:00,505 --> 00:54:02,155
I don't know, ten years or so,
我不知道，十年左右，

969
00:54:02,155 --> 00:54:04,240
um, but of course,
嗯，当然，

970
00:54:04,240 --> 00:54:05,665
right, the trend isn't clear.
对，趋势不明确。

971
00:54:05,665 --> 00:54:07,630
So if you look at summarization for example,
所以，如果你看一下摘要，例如，

972
00:54:07,630 --> 00:54:09,040
it seems like performance is already,
表现似乎已经，

973
00:54:09,040 --> 00:54:11,005
uh, uh, topped out.
呃，呃，超过了。

974
00:54:11,005 --> 00:54:15,760
Um, so I think this will be a really interesting thing kinda going forward,
嗯，所以我觉得这将是一件非常有趣的事情，

975
00:54:15,760 --> 00:54:17,980
looking at the future of NLP, um,
看着NLP的未来，嗯，

976
00:54:17,980 --> 00:54:20,710
is how the scaling will change,
缩放是如何改变的，

977
00:54:20,710 --> 00:54:23,570
um, the way NLP is approached.
嗯，接近NLP的方式。

978
00:54:24,120 --> 00:54:29,755
Um, the other interesting thing about GPT-2 was its reaction from uh,
嗯，关于GPT-2的另一个有趣的事情是它的反应来自呃，

979
00:54:29,755 --> 00:54:32,125
the media and also from other researchers.
媒体和其他研究人员。

980
00:54:32,125 --> 00:54:35,455
Um, and the real cause of
嗯，真正的原因

981
00:54:35,455 --> 00:54:39,295
a lot of the controversy about it was this statement from OpenAI.
很多关于它的争议都来自OpenAI的这一声明。

982
00:54:39,295 --> 00:54:43,000
They said that, ''We're not going to release our full language model,
他们说，''我们不会发布我们的全语言模型，

983
00:54:43,000 --> 00:54:44,590
um, because it's too dangerous,
嗯，因为它太危险了，

984
00:54:44,590 --> 00:54:46,015
you know, our language model is too good.''
你知道，我们的语言模型太好了。''

985
00:54:46,015 --> 00:54:51,115
Um, so the media really enjoyed this and,
嗯，所以媒体真的很喜欢这个，

986
00:54:51,115 --> 00:54:52,330
you know, said that,
你知道，说，

987
00:54:52,330 --> 00:54:55,135
uh, machine learning is going to break the Internet.
呃，机器学习将打破互联网。

988
00:54:55,135 --> 00:55:00,580
Um, there's also some pretty interesting reactions from our researchers, right.
嗯，我们的研究人员也有一些非常有趣的反应，对吧。

989
00:55:00,580 --> 00:55:02,020
So um, there's some,
嗯，有一些，

990
00:55:02,020 --> 00:55:04,195
kind of, tongue-in-cheek responses here, right.
对，这里有一种诙谐的回答，对吧。

991
00:55:04,195 --> 00:55:05,755
You know, I trained the model on MNIST.
你知道，我在MNIST上训练了这个模型。

992
00:55:05,755 --> 00:55:07,915
Is it too dangerous for me to release it?
释放它对我来说太危险了吗？

993
00:55:07,915 --> 00:55:11,530
Um, and similarly, we've done really great work
嗯，同样地，我们做了很棒的工作

994
00:55:11,530 --> 00:55:15,715
but we can't release it it's too dangerous so you're just gonna have to trust us on this.
但我们不能释放它太危险了，所以你只需要相信我们。

995
00:55:15,715 --> 00:55:18,970
Looking at more, kind of, reasoned, um,
看着更多，有点，有理由，嗯，

996
00:55:18,970 --> 00:55:20,665
debate about this issue,
关于这个问题的辩论，

997
00:55:20,665 --> 00:55:22,885
you still see articles,
你还看文章，

998
00:55:22,885 --> 00:55:24,610
um, arguing both sides.
嗯，争论双方。

999
00:55:24,610 --> 00:55:26,469
So these are two ar- articles,
所以这些是两篇文章，

1000
00:55:26,469 --> 00:55:29,545
um, from The Gradient which is a, sort of,
嗯，来自The Gradient是一种，有点像，

1001
00:55:29,545 --> 00:55:31,690
machine learning newsletter, um,
机器学习通讯，嗯，

1002
00:55:31,690 --> 00:55:35,875
and they're arguing precisely opposite sides of this issue,
他们正在争论这个问题正好相反的一面，

1003
00:55:35,875 --> 00:55:38,510
um, should it be released or not.
嗯，是否应该被释放。

1004
00:55:40,770 --> 00:55:47,120
So I guess I can briefly go over a few arguments for or against.
所以我想我可以简单地讨论一些赞成或反对的论点。

1005
00:55:47,130 --> 00:55:50,185
There is, kind of, a lot of debate about this and I don't want to
关于这个，有很多争论，我不想

1006
00:55:50,185 --> 00:55:53,480
go too deep into a controversial issue,
深入探讨一个有争议的问题，

1007
00:55:54,150 --> 00:55:56,710
um, but here's a long list of,
嗯，但这里有一长串清单，

1008
00:55:56,710 --> 00:55:58,570
kind of, things people have said about this, right.
有点，人们对此有所说的，对吧。

1009
00:55:58,570 --> 00:56:01,450
So um, here's why you should release.
嗯，这就是你应该发布的原因。

1010
00:56:01,450 --> 00:56:03,280
One complaint is that,
一个抱怨是，

1011
00:56:03,280 --> 00:56:05,065
is this model really that special?
这个型号真的那么特别吗？

1012
00:56:05,065 --> 00:56:06,595
There's nothing new going on here.
这里没什么新鲜事。

1013
00:56:06,595 --> 00:56:09,640
It's just 10 times bigger than previous models, um,
它比以前的型号大10倍，嗯，

1014
00:56:09,640 --> 00:56:11,860
and there's also some arguments that,
并且还有一些论点，

1015
00:56:11,860 --> 00:56:14,500
um, even if this one isn't released, you know,
嗯，即使这个没有发布，你知道，

1016
00:56:14,500 --> 00:56:17,185
in five years everybody can train a model this good, um,
在五年内，每个人都可以训练一个好的模型，嗯，

1017
00:56:17,185 --> 00:56:22,270
and actually if you look at image recognition or look at images and speech data, um,
实际上，如果你看图像识别或看图像和语音数据，嗯，

1018
00:56:22,270 --> 00:56:25,780
it already is possible to synthesize highly convincing,
它已经有可能合成非常有说服力，

1019
00:56:25,780 --> 00:56:28,405
um, fake images and fake speech.
嗯，假图像和假言论。

1020
00:56:28,405 --> 00:56:34,750
So kinda, what makes this thing different from those other, um, systems.
有点，是什么让这个东西与其他系统不同。

1021
00:56:34,750 --> 00:56:36,310
And speaking of other systems, right,
说到其他系统，对，

1022
00:56:36,310 --> 00:56:38,335
Photoshop has existed for a long time,
Photoshop已存在很长时间了，

1023
00:56:38,335 --> 00:56:41,950
so we can already convincingly fake images, um,
所以我们已经可以令人信服地伪造图像了，嗯，

1024
00:56:41,950 --> 00:56:44,140
people have just learned to adjust and learned
人们刚学会调整和学习

1025
00:56:44,140 --> 00:56:46,645
that you shouldn't always trust what's in an image,
你不应该总是相信图像中的内容，

1026
00:56:46,645 --> 00:56:47,995
um, because it may have been,
嗯，因为它可能是，

1027
00:56:47,995 --> 00:56:50,065
um, altered in some way.
嗯，以某种方式改变了。

1028
00:56:50,065 --> 00:56:52,450
Um, on the other hand, you could say,
嗯，另一方面，你可以说，

1029
00:56:52,450 --> 00:56:55,780
''Okay, uh, Photoshop exists but, um, you can't, sort of,
“好吧，呃，Photoshop存在，但是，你不能，有点像，

1030
00:56:55,780 --> 00:57:00,130
scale up Photoshop and start mass producing fake content the way you can with this sort
扩大Photoshop并以这种方式开始批量生产虚假内容

1031
00:57:00,130 --> 00:57:04,660
of model,'' and they pointed at the danger of uh, fake news, um,
模特，''他们指着呃，假新闻的危险，嗯，

1032
00:57:04,660 --> 00:57:08,950
fake reviews, um, in general just astroturfing, which means basically,
假的评论，嗯，一般只是天体冲浪，这基本上意味着，

1033
00:57:08,950 --> 00:57:15,370
uh, creating fake user content that's supporting a view you want other people to hold.
呃，创建假的用户内容，支持你希望其他人持有的视图。

1034
00:57:15,370 --> 00:57:18,870
Um, this is actually something that's already done,
嗯，这实际上是已经完成的事情，

1035
00:57:18,870 --> 00:57:21,660
um, pretty widely by country- companies and governments.
嗯，相当广泛的国家 - 公司和政府。

1036
00:57:21,660 --> 00:57:23,475
There's a lot of evidence for this, um,
这有很多证据，嗯，

1037
00:57:23,475 --> 00:57:25,500
but they are of course hiring people to
但他们当然正在招人

1038
00:57:25,500 --> 00:57:27,795
write all these comments on news articles let's say
把所有这些评论都写在新闻文章上

1039
00:57:27,795 --> 00:57:30,390
and we don't want to make their job any easier
我们不想让他们的工作变得更容易

1040
00:57:30,390 --> 00:57:33,620
by producing a machine that could potentially do this.
通过生产可能做到这一点的机器。

1041
00:57:33,620 --> 00:57:37,330
So um, I'm not really gonna take a side here,
所以，嗯，我真的不想站在这里，

1042
00:57:37,330 --> 00:57:39,565
um, there's still a lot of debate about this.
嗯，关于这一点还有很多争论。

1043
00:57:39,565 --> 00:57:41,110
I think, you know,
我想，你知道，

1044
00:57:41,110 --> 00:57:43,300
the main, the main takeaway here is that,
主要的，这里的主要内容是，

1045
00:57:43,300 --> 00:57:46,959
as a community on people in machine learning and NLP,
作为机器学习和NLP人员的社区，

1046
00:57:46,959 --> 00:57:48,910
don't really have a handle on this, right?
对此没有真正的把握，对吧？

1047
00:57:48,910 --> 00:57:51,355
We are sort of caught by surprise by, um,
我们有点惊讶，嗯，

1048
00:57:51,355 --> 00:57:56,095
OpenAI's, um, decision here and, um, uh,
OpenAI，嗯，这里的决定，嗯，呃，

1049
00:57:56,095 --> 00:57:57,760
that means that, you know,
这意味着，你知道，

1050
00:57:57,760 --> 00:58:01,120
there really is some figuring out that needs to be done on what
确实有一些想法需要在什么上做

1051
00:58:01,120 --> 00:58:05,515
exactly is responsible to release publicly.
确切地负责公开发布。

1052
00:58:05,515 --> 00:58:09,430
What kind of research problems should we be working on and so on.
我们应该研究什么样的研究问题等等。

1053
00:58:09,430 --> 00:58:11,530
[NOISE] So yeah.
[NOISE]所以是的。

1054
00:58:11,530 --> 00:58:13,795
Any questions about uh, this,
有任何关于呃的问题，这个，

1055
00:58:13,795 --> 00:58:16,450
this reaction or this debate in general?
这个反应还是一般的辩论？

1056
00:58:16,450 --> 00:58:20,930
[NOISE] Okay.
[NOISE]好的。

1057
00:58:22,140 --> 00:58:27,610
Um, I think something arising from this debate is, um,
嗯，我认为这场辩论产生的一些事情是，嗯，

1058
00:58:27,610 --> 00:58:29,310
the question of, um,
这个问题，嗯，

1059
00:58:29,310 --> 00:58:32,580
should really the ML people be the people making these, sort of,
应该真的是ML人是制造这些的人，有点像，

1060
00:58:32,580 --> 00:58:38,085
decisions or is there a need for more interdisciplinary science where we look at, um,
决定还是需要更多的跨学科科学，我们看一下，嗯，

1061
00:58:38,085 --> 00:58:40,425
experts in say, computer security,
专家说，计算机安全，

1062
00:58:40,425 --> 00:58:42,705
um, people from social sciences,
嗯，社会科学的人，

1063
00:58:42,705 --> 00:58:46,185
um, you know, people who are experts in ethics,
嗯，你知道，是道德专家，

1064
00:58:46,185 --> 00:58:48,365
um, to look at these decisions.
嗯，看看这些决定。

1065
00:58:48,365 --> 00:58:54,595
Um, right. So GPT-2 was definitely one example of where suddenly it seems like,
嗯，对。所以GPT-2绝对是突然之间的一个例子，

1066
00:58:54,595 --> 00:58:58,420
um, our NLP technology has a lot of pitfalls, right.
嗯，我们的NLP技术有很多陷阱，对吧。

1067
00:58:58,420 --> 00:59:02,005
Where they could be used in a malicious way or they could cause damage.
它们可能以恶意方式使用，或者可能造成损害。

1068
00:59:02,005 --> 00:59:05,725
And I think this trend is only going to increase, um,
我认为这种趋势只会增加，嗯，

1069
00:59:05,725 --> 00:59:07,165
if you look at, kind of,
如果你看，有点，

1070
00:59:07,165 --> 00:59:10,540
areas of NLP that people are working on, uh,
人们正在研究的NLP领域，呃，

1071
00:59:10,540 --> 00:59:16,510
increasingly people are working on really high stakes applications of NLP,
越来越多的人正在研究NLP真正高风险的应用，

1072
00:59:16,510 --> 00:59:19,570
um, and those often have really big, um,
嗯，那些经常真的很大，嗯，

1073
00:59:19,570 --> 00:59:25,280
ramifications, especially if you think from the angle of bias and fairness.
影响，特别是如果你从偏见和公平的角度思考。

1074
00:59:25,980 --> 00:59:32,420
Um, so, so let's go over a couple examples of this, um-
嗯，所以，让我们来看几个这样的例子，嗯 -

1075
00:59:32,690 --> 00:59:35,955
Um, one- so some, some areas where,
嗯，有的，有些地方，

1076
00:59:35,955 --> 00:59:37,875
where this is happening is people are looking at,
发生这种情况的人正在看，

1077
00:59:37,875 --> 00:59:40,050
uh, NLP to look at judicial decisions.
呃，NLP来看待司法判决。

1078
00:59:40,050 --> 00:59:41,895
So for example, should this person,
所以，例如，这个人应该

1079
00:59:41,895 --> 00:59:43,305
uh, get bail or not?
呃，保释还是不保释？

1080
00:59:43,305 --> 00:59:45,210
Um, for hiring decisions, right?
嗯，招聘决定，对吧？

1081
00:59:45,210 --> 00:59:46,680
So you look at someone's resume,
所以你看一下某人的简历，

1082
00:59:46,680 --> 00:59:48,000
you run NLP on it,
你运行NLP，

1083
00:59:48,000 --> 00:59:50,775
and then you'd make a decision automatically,
然后你会自动做出决定，

1084
00:59:50,775 --> 00:59:53,130
um, sh- should we throw out this resume or not?
嗯，我们应该抛弃这份简历吗？

1085
00:59:53,130 --> 00:59:56,850
So do some, sort of, screening, um, grading tests.
那么做一些，筛选，嗯，评分测试。

1086
00:59:56,850 --> 00:59:58,650
Um, if you take the GRE, um,
嗯，如果你参加GRE，嗯，

1087
00:59:58,650 --> 01:00:00,825
your, your tests will be graded by a machine.
你的，你的测试将由机器评分。

1088
01:00:00,825 --> 01:00:03,090
Um, a person will also look at it, um,
嗯，一个人也会看着它，嗯，

1089
01:00:03,090 --> 01:00:05,295
but nevertheless, um, that's, you know,
但不管怎样，嗯，你知道，

1090
01:00:05,295 --> 01:00:09,090
a sometimes very impactful part of your life, um, when it's,
你生活中有时非常有影响力的一部分，嗯，当它的时候，

1091
01:00:09,090 --> 01:00:11,085
when it's the tests that, um, inf- you know,
当它是测试时，嗯，你知道吗，

1092
01:00:11,085 --> 01:00:14,490
affects your, um, acceptance into a school, let's say.
让我们说，影响你，嗯，接受进入学校。

1093
01:00:14,490 --> 01:00:17,265
Um, so I think there is- are some,
嗯，所以我觉得有一些，

1094
01:00:17,265 --> 01:00:20,790
some good sides of using Machine Learning in these kinds of contexts.
在这些情境中使用机器学习的一些好方面。

1095
01:00:20,790 --> 01:00:24,120
So one is that we can pretty quickly evaluate,
所以一个是我们可以很快评估，

1096
01:00:24,120 --> 01:00:26,985
a machine learning system and search out.
机器学习系统并搜索出来。

1097
01:00:26,985 --> 01:00:28,680
Does it have some, kind of, bias,
它是否有某种偏见，

1098
01:00:28,680 --> 01:00:31,350
just by running it on a bunch of data and seeing what it does,
只需在一堆数据上运行它并查看它的作用，

1099
01:00:31,350 --> 01:00:34,350
and also perhaps even more importantly,
也许更重要的是，

1100
01:00:34,350 --> 01:00:35,640
um, we can fix this, kind of,
嗯，我们可以解决这个问题，

1101
01:00:35,640 --> 01:00:37,080
problem if it arises, right?
如果出现问题，对吧？

1102
01:00:37,080 --> 01:00:42,240
So, um, it's probably easier to fix a machine learning system that screens resumes,
所以，嗯，修复一个筛选简历的机器学习系统可能更容易，

1103
01:00:42,240 --> 01:00:44,730
than it is to s- to fix having, you know,
你要知道的是，要解决问题，

1104
01:00:44,730 --> 01:00:48,300
5,000 executives that are slightly sexist or something, right?
5000名略有性别歧视的高管，对吧？

1105
01:00:48,300 --> 01:00:49,725
So, so in this way,
那么，就这样，

1106
01:00:49,725 --> 01:00:51,180
um, there is a, sort of,
嗯，还有一种，

1107
01:00:51,180 --> 01:00:57,840
positive angle on using machine learning in these high-stakes, um, uh, decisions.
在这些高风险，嗯，呃，决策中使用机器学习的积极角度。

1108
01:00:57,840 --> 01:01:00,015
Um, on the other hand, um,
嗯，另一方面，嗯，

1109
01:01:00,015 --> 01:01:02,220
it's been pretty well, uh, s- known,
它很好，呃，知道，

1110
01:01:02,220 --> 01:01:04,770
and I know you had a lecture on bias and fairness,
我知道你有关于偏见和公平的讲座，

1111
01:01:04,770 --> 01:01:07,770
that machine learning often reflects bias in a data-set,
机器学习经常反映数据集的偏差，

1112
01:01:07,770 --> 01:01:11,025
um, it can even amplify bias in the data-set.
嗯，它甚至可以放大数据集中的偏差。

1113
01:01:11,025 --> 01:01:12,660
Um, and there's concern of, kind of,
嗯，还有那种担心，

1114
01:01:12,660 --> 01:01:15,315
a feedback loop where a biased algorithm
反馈循环，其中有偏差的算法

1115
01:01:15,315 --> 01:01:18,360
actually will lead to the creation of more biased data,
实际上会导致创建更多有偏见的数据，

1116
01:01:18,360 --> 01:01:23,050
um, in which case these problems will only compound and get worse.
嗯，在这种情况下，这些问题只会复合并变得更糟。

1117
01:01:23,150 --> 01:01:28,950
Um, so for all of the, uh, high-impact decisions,
嗯，对于所有呃，影响很大的决定，

1118
01:01:28,950 --> 01:01:30,990
um, I, I had listed on that slide,
嗯，我，我已在那张幻灯片上列出，

1119
01:01:30,990 --> 01:01:34,320
there are examples where things have gone awry, right?
有些事情出了问题，对吧？

1120
01:01:34,320 --> 01:01:36,690
So Amazon had some AI that was,
所以亚马逊有一些AI，

1121
01:01:36,690 --> 01:01:39,975
um, working as a recruiting tool and it turned out to be sexist.
嗯，作为招聘工具工作，结果证明是性别歧视。

1122
01:01:39,975 --> 01:01:42,255
Um, um, there have been some, kind of,
嗯，嗯，有一些，有点，

1123
01:01:42,255 --> 01:01:44,550
early pilots of using AI, um,
使用人工智能的早期飞行员，嗯，

1124
01:01:44,550 --> 01:01:46,680
in the justice system and those also have had,
在司法系统和那些也有，

1125
01:01:46,680 --> 01:01:49,710
um, in some cases, really bad results.
嗯，在某些情况下，结果非常糟糕。

1126
01:01:49,710 --> 01:01:52,920
Um, if you look at automatic,
嗯，如果你看看自动，

1127
01:01:52,920 --> 01:01:54,855
automatic essay grading, um,
自动论文评分，嗯，

1128
01:01:54,855 --> 01:01:56,430
it's not really a great,
它真的不是很棒，

1129
01:01:56,430 --> 01:01:57,720
you know, NLP system, right?
你知道吗，NLP系统吧？

1130
01:01:57,720 --> 01:01:59,730
So here's an example, um,
所以这是一个例子，嗯，

1131
01:01:59,730 --> 01:02:02,355
excerpt of an essay that, um,
一篇文章的摘录，嗯，

1132
01:02:02,355 --> 01:02:06,240
a automatic grading system used by the GRE test gives, uh,
GRE考试使用的自动评分系统，呃，

1133
01:02:06,240 --> 01:02:08,040
a very high score, um,
非常高的分数，嗯，

1134
01:02:08,040 --> 01:02:10,230
but really it's just, kind of, a solid of,
但实际上它只是，有点，坚实，

1135
01:02:10,230 --> 01:02:12,420
uh, big fancy words and that's
呃，很棒的花哨的话，那就是

1136
01:02:12,420 --> 01:02:16,060
enough to convince the model that this is a, a great essay.
足以让模特相信这是一篇伟大的文章。

1137
01:02:17,240 --> 01:02:19,410
Um, the last, um,
嗯，最后一个，嗯，

1138
01:02:19,410 --> 01:02:21,555
area I wanna talk about where, where, um,
我想谈谈哪里，哪里，嗯，

1139
01:02:21,555 --> 01:02:23,550
you can see there's really some risks and
你可以看到真的存在一些风险

1140
01:02:23,550 --> 01:02:26,655
some pitfalls with using NLP technology, is chatbots.
使用NLP技术的一些缺陷是聊天机器人。

1141
01:02:26,655 --> 01:02:31,560
Um, so I think chatbots do have a side where they can be very beneficial.
嗯，所以我认为聊天机器人确实有一个可以非常有益的方面。

1142
01:02:31,560 --> 01:02:33,930
Um, Woebot is one example,
嗯，Woebot就是一个例子，

1143
01:02:33,930 --> 01:02:37,545
is this company that has this chatbot you can talk to if you're not,
这个公司有这个聊天机器人你可以谈谈，如果你不是，

1144
01:02:37,545 --> 01:02:39,480
um, feeling too great and it'll try to,
嗯，感觉太棒了，它会尝试，

1145
01:02:39,480 --> 01:02:41,565
um, I don't know, cheer you up.
嗯，我不知道，为你振作起来。

1146
01:02:41,565 --> 01:02:43,830
Um, so, so that, you know,
嗯，所以，你知道，

1147
01:02:43,830 --> 01:02:46,770
could be a- a really nice piece of technology that helps people,
可能是一个非常好的技术，可以帮助人们，

1148
01:02:46,770 --> 01:02:49,380
um, but on the other hand, there's some big risks.
嗯，但另一方面，存在一些很大的风险。

1149
01:02:49,380 --> 01:02:53,520
So, so one example is Microsoft research had a chatbot trained on tweets,
所以，一个例子是微软的研究有一个聊天机器人训练的推文，

1150
01:02:53,520 --> 01:02:56,850
and it started quickly saying racist things and had to be pulled.
它很快开始说种族主义的东西，不得不被拉。

1151
01:02:56,850 --> 01:02:59,625
Um, so I think all of this highlights that, um,
嗯，所以我想所有这些都突出了，嗯，

1152
01:02:59,625 --> 01:03:02,505
as NLP is becoming more effective,
随着NLP越来越有效，

1153
01:03:02,505 --> 01:03:05,835
people are seeing opportunities to use it in, um,
人们正在看到使用它的机会，嗯，

1154
01:03:05,835 --> 01:03:09,300
increasingly high-stakes decisions and although,
越来越高风险的决定，尽管如此

1155
01:03:09,300 --> 01:03:11,775
you know, there are some nice- there's some appeal to that,
你知道，有一些不错 - 有一些吸引力，

1156
01:03:11,775 --> 01:03:14,310
um, there's also a lot of risk.
嗯，还有很多风险。

1157
01:03:14,310 --> 01:03:17,310
Um, any more questions on, uh,
嗯，还有其他问题，呃，

1158
01:03:17,310 --> 01:03:20,620
this sort of social impact of NLP?
NLP的这种社会影响？

1159
01:03:21,650 --> 01:03:29,250
Okay. Um, last part of this lecture is looking more at future research, right?
好的。嗯，这个讲座的最后一部分是关注未来的研究，对吧？

1160
01:03:29,250 --> 01:03:30,465
And in particular, um,
特别是，嗯，

1161
01:03:30,465 --> 01:03:33,510
I think a lot of the current research trends are,
我认为目前很多研究趋势是，

1162
01:03:33,510 --> 01:03:35,760
kind of reactions to BERT, um, right?
那种对BERT的反应，对吧？

1163
01:03:35,760 --> 01:03:40,080
So, so the question is what did BERT solve and- and what do we work on next?
那么，问题是BERT解决了什么 - 以及我们接下来会做些什么？

1164
01:03:40,080 --> 01:03:44,295
Um, so here are results on the GLUE benchmark.
嗯，所以这里是GLUE基准测试的结果。

1165
01:03:44,295 --> 01:03:47,070
Um, that is, uh, a compendium of,
嗯，也就是说，呃，简历，

1166
01:03:47,070 --> 01:03:50,280
uh, 10 natural language understanding tasks.
呃，10种自然语言理解任务。

1167
01:03:50,280 --> 01:03:54,420
Um, and you get an average score across those 10 tasks.
嗯，你可以在这10个任务中得到平均分。

1168
01:03:54,420 --> 01:03:57,810
Um, the left, uh, two- the two are,
嗯，左边，呃，两个是两个，

1169
01:03:57,810 --> 01:04:00,720
sorry the right- two right most models are,
对不起右边 - 最右边的两个型号是，

1170
01:04:00,720 --> 01:04:03,330
um, uh, s- non, uh,
嗯，呃，非 - ，呃，

1171
01:04:03,330 --> 01:04:06,480
are just supervised trained machine learning systems, right?
只是受过监督训练的机器学习系统，对吗？

1172
01:04:06,480 --> 01:04:08,355
So we have Bag-of-Vectors, um,
所以我们有Bag-of-Vectors，嗯，

1173
01:04:08,355 --> 01:04:10,920
we instead use our fancy neural net architecture
我们改为使用我们的花式神经网络架构

1174
01:04:10,920 --> 01:04:13,650
of BiLSTM + Attention and we get about five points.
BiLSTM +注意力，我们得到大约五分。

1175
01:04:13,650 --> 01:04:15,600
Um, but the gains from BERT,
嗯，但BERT的收益，

1176
01:04:15,600 --> 01:04:17,520
uh, really dwarf that difference, right?
呃，真的让这个差异相形见绌吧？

1177
01:04:17,520 --> 01:04:19,890
So, so BERT improves results by about, uh,
那么，BERT通过大约改善结果，呃，

1178
01:04:19,890 --> 01:04:24,120
17 points and we end up being actually quite close,
17点，我们最终真的非常接近，

1179
01:04:24,120 --> 01:04:26,925
um, to human performance on these tasks.
嗯，人类在这些任务上的表现。

1180
01:04:26,925 --> 01:04:29,820
Um, so one, sort of,
嗯，所以，有点，

1181
01:04:29,820 --> 01:04:32,220
implication of this that people are wondering about is,
这是人们想知道的含义是，

1182
01:04:32,220 --> 01:04:35,115
is this, kind of, the death of architecture engineering?
这就是建筑工程的死亡吗？

1183
01:04:35,115 --> 01:04:39,225
Um, so I'm sure all of you who have worked on the default final project, um,
嗯，所以我相信所有参与过默认最终项目的人，嗯，

1184
01:04:39,225 --> 01:04:42,570
have seen a whole bunch of fancy pictures showing different,
已经看到一大堆花哨的图片显示不同，

1185
01:04:42,570 --> 01:04:44,490
uh, architectures for solving SQuAD.
呃，解决SQuAD的架构。

1186
01:04:44,490 --> 01:04:46,710
Um, there are a lot of papers.
嗯，有很多论文。

1187
01:04:46,710 --> 01:04:48,390
They all propose some, kind of,
他们都提出了一些，有点，

1188
01:04:48,390 --> 01:04:50,895
uh, attention mechanism or something like that.
呃，注意机制或类似的东西。

1189
01:04:50,895 --> 01:04:53,880
Um, and, um, right.
嗯，嗯，对。

1190
01:04:53,880 --> 01:04:55,170
With BERT, it's, sort of,
有了BERT，有点像，

1191
01:04:55,170 --> 01:04:56,970
um, you don't need to do any of that, right?
嗯，你不需要做那些，对吧？

1192
01:04:56,970 --> 01:04:59,190
You just train a transformer and you give it enough data,
你只需要训练一个变压器并给它足够的数据，

1193
01:04:59,190 --> 01:05:01,020
and actually you're doing great on SQuAD,
实际上你在SQUAD上做得很好，

1194
01:05:01,020 --> 01:05:03,885
you know, maybe, um, these, uh,
你知道，也许，嗯，这些，呃，

1195
01:05:03,885 --> 01:05:07,800
architectural enhancements are not necessarily, um,
架构增强不一定，嗯，

1196
01:05:07,800 --> 01:05:10,590
the key thing that'll drive progress in,
是推动进步的关键因素，

1197
01:05:10,590 --> 01:05:13,720
uh, improving results on these tasks.
呃，改善这些任务的结果。

1198
01:05:14,150 --> 01:05:16,740
Um, right. So, uh,
嗯，对。所以，呃，

1199
01:05:16,740 --> 01:05:18,630
if you look at this with the perspective of a researcher,
如果从研究人员的角度来看这个，

1200
01:05:18,630 --> 01:05:20,610
you can think a researcher will say, "Okay,
你可以认为研究人员会说：“好的，

1201
01:05:20,610 --> 01:05:23,520
I can spend six months designing a fancy new architecture for
我可以花六个月的时间设计一个奇特的新架构

1202
01:05:23,520 --> 01:05:27,930
SQuAD and if I do a good job maybe I'll improve results by 1, uh, F1 point."
SQuAD，如果我做得好，也许我会把结果改进1，呃，F1点。“

1203
01:05:27,930 --> 01:05:30,030
Um, but in the case of BERT, um,
嗯，但在BERT的情况下，嗯，

1204
01:05:30,030 --> 01:05:32,160
increasing the size of their model of 3x,
增加他们的3x模型的大小，

1205
01:05:32,160 --> 01:05:33,240
which is the difference between,
这是区别，

1206
01:05:33,240 --> 01:05:36,090
they've like a base size model and a large model,
他们喜欢基本尺寸模型和大型模型，

1207
01:05:36,090 --> 01:05:39,585
um, that improve results by 5 F1 points.
嗯，这可以提高5个F1积分。

1208
01:05:39,585 --> 01:05:42,150
Um, so it does seem to suggest we need to, sort of,
嗯，所以它似乎表明我们需要，有点，

1209
01:05:42,150 --> 01:05:46,635
re-prioritize, um, which avenues of research we'd pursue,
重新确定优先顺序，嗯，我们追求的研究途径，

1210
01:05:46,635 --> 01:05:49,500
because this architecture engineering isn't providing, kind of,
因为这个架构工程没有提供，有点像，

1211
01:05:49,500 --> 01:05:52,605
gains for its time investment the way,
通过时间投资获得收益，

1212
01:05:52,605 --> 01:05:54,765
uh, leveraging unlabeled data is.
呃，利用未标记的数据。

1213
01:05:54,765 --> 01:05:57,735
Um, so now, if you look at the SQuAD leaderboard, um,
嗯，现在，如果你看看SQuAD排行榜，嗯，

1214
01:05:57,735 --> 01:06:02,350
I think at least the top 20 entrants are all BERT plus something.
我认为至少前20名参赛者都是BERT加上一些东西。

1215
01:06:04,190 --> 01:06:07,725
Um, one other issue, uh,
嗯，另一个问题，呃，

1216
01:06:07,725 --> 01:06:09,540
I think BERT has raised is that,
我认为BERT提出的是，

1217
01:06:09,540 --> 01:06:11,400
um, we need harder tasks, right?
嗯，我们需要更艰巨的任务，对吧？

1218
01:06:11,400 --> 01:06:13,560
BERT has almost solved SQuAD,
BERT几乎解决了SQuAD，

1219
01:06:13,560 --> 01:06:15,060
if you define it by, uh,
如果你定义它，呃，

1220
01:06:15,060 --> 01:06:16,860
getting close to human performance.
接近人类的表现。

1221
01:06:16,860 --> 01:06:19,230
Um, so there's been, um,
嗯，所以有，嗯，

1222
01:06:19,230 --> 01:06:22,635
a growth in new datasets that are, uh,
新数据集的增长，呃，

1223
01:06:22,635 --> 01:06:25,020
more challenging and there are a couple of ways in which,
更具挑战性，有几种方法，

1224
01:06:25,020 --> 01:06:26,370
um, they can be more challenging.
嗯，他们可能更具挑战性。

1225
01:06:26,370 --> 01:06:28,140
So one is, um,
那么一个是，嗯，

1226
01:06:28,140 --> 01:06:30,240
doing reading comprehension on longer documents,
对较长的文件进行阅读理解，

1227
01:06:30,240 --> 01:06:32,625
or doing it across more than one document.
或者在多个文档中进行。

1228
01:06:32,625 --> 01:06:35,280
Um, one area is looking at c- uh,
嗯，有一个区域在看c-uh，

1229
01:06:35,280 --> 01:06:38,850
coming up with harder questions that require a multi-hop reasoning.
提出需要多跳推理的难题。

1230
01:06:38,850 --> 01:06:41,550
Um, so that essentially meas- means you have to string
嗯，所以基本上测量你必须串

1231
01:06:41,550 --> 01:06:45,180
together multiple supporting facts from different places,
来自不同地方的多个支持事实，

1232
01:06:45,180 --> 01:06:47,670
um, to produce the correct answer.
嗯，要产生正确的答案。

1233
01:06:47,670 --> 01:06:49,350
Um, and another area,
嗯，还有另一个地方，

1234
01:06:49,350 --> 01:06:51,870
situating question-answering within a dialogue.
在对话中定位问答。

1235
01:06:51,870 --> 01:06:54,330
Um, there's also been a, kind of,
嗯，还有一种，有点像，

1236
01:06:54,330 --> 01:06:58,260
small detail with the construction of reading comprehension datasets,
阅读理解数据集构建的细节，

1237
01:06:58,260 --> 01:07:00,600
that has actually really affected,
实际上真的受到了影响，

1238
01:07:00,600 --> 01:07:02,835
um, the, the difficulty of the task.
嗯，这个任务的难度。

1239
01:07:02,835 --> 01:07:04,110
And that is whether, um,
那就是，嗯，

1240
01:07:04,110 --> 01:07:06,495
when you create these datasets, um,
当你创建这些数据集时，嗯，

1241
01:07:06,495 --> 01:07:09,420
is the person who writes questions about a passage,
是关于段落的问题的人，

1242
01:07:09,420 --> 01:07:11,535
can they see that passage or not?
他们能看到那段经文吗？

1243
01:07:11,535 --> 01:07:14,070
Um, so of course, it's much easier to come up
嗯，当然，它更容易出现

1244
01:07:14,070 --> 01:07:16,110
with a question that when you see the passage,
有一个问题，当你看到这段经文，

1245
01:07:16,110 --> 01:07:18,870
and if you come up with a question without seeing the passage,
如果你提出一个没有看到段落的问题，

1246
01:07:18,870 --> 01:07:21,810
you may not even have a answerable question.
你甚至可能没有一个可回答的问题。

1247
01:07:21,810 --> 01:07:23,730
Um, but the problem with looking at
嗯，但看着的问题

1248
01:07:23,730 --> 01:07:26,460
the passage is that first of all it's not realistic, right?
这段经文首先是不现实的，对吧？

1249
01:07:26,460 --> 01:07:28,845
So, uh, if I'm asking a question, you know,
所以，呃，如果我问一个问题，你知道，

1250
01:07:28,845 --> 01:07:30,585
I'm not going to have usually
我通常不会

1251
01:07:30,585 --> 01:07:33,870
the paragraph that answers that question sitting in front of me.
回答那个问题的段落坐在我面前。

1252
01:07:33,870 --> 01:07:35,670
Um, on top of that,
嗯，最重要的是，

1253
01:07:35,670 --> 01:07:37,560
it really encourages easy questions, right?
它真的鼓励简单的问题，对吗？

1254
01:07:37,560 --> 01:07:39,840
So, um, if you're a Mechanical Turker,
所以，嗯，如果你是一名机械车手，

1255
01:07:39,840 --> 01:07:42,869
and you're paid to write as many questions as possible,
并且你付出了尽可能多的问题，

1256
01:07:42,869 --> 01:07:44,790
and then you see an article that says,
然后你看到一篇文章说，

1257
01:07:44,790 --> 01:07:46,350
um, I don't know, you know,
嗯，我不知道，你知道，

1258
01:07:46,350 --> 01:07:50,040
uh, Abraham Lincoln was the 16th president of the United States,
呃，亚伯拉罕林肯是美国第16任总统，

1259
01:07:50,040 --> 01:07:51,600
um, what are you gonna write?
嗯，你要写什么？

1260
01:07:51,600 --> 01:07:53,100
As your question, you're gonna write,
作为你的问题，你要写，

1261
01:07:53,100 --> 01:07:55,365
who was the 16th president of the United States.
谁是美国第16任总统。

1262
01:07:55,365 --> 01:07:58,035
You're not gonna write something more interesting that's harder to answer.
你不会写一些更有趣的东西，更难回答。

1263
01:07:58,035 --> 01:08:01,890
Um, so- so this is one way in which crowdsourced datasets have changed, um,
嗯，所以这是众包数据集发生变化的一种方式，嗯，

1264
01:08:01,890 --> 01:08:04,170
people are now making sure questions are,
人们现在确定问题是，

1265
01:08:04,170 --> 01:08:07,410
sort of, independent of, of the contexts.
一种独立于上下文的东西。

1266
01:08:07,410 --> 01:08:09,375
Um, so I'm gonna briefly, uh,
嗯，所以我要简单地说，呃，

1267
01:08:09,375 --> 01:08:11,610
go over a couple of new datasets in this line.
在这一行中查看几个新的数据集。

1268
01:08:11,610 --> 01:08:15,150
So one is called QuAC, which stands for Question Answering in Context.
所以一个叫做QuAC，它代表Context中的Question Answering。

1269
01:08:15,150 --> 01:08:16,815
Um, in this dataset,
嗯，在这个数据集中，

1270
01:08:16,815 --> 01:08:18,690
there is a teacher and a student,
有老师和学生，

1271
01:08:18,690 --> 01:08:21,390
um, the teacher sees a Wikipedia article.
嗯，老师看到一篇维基百科的文章。

1272
01:08:21,390 --> 01:08:24,195
The student wants to learn about this Wikipedia article,
学生想要了解这篇维基百科的文章，

1273
01:08:24,195 --> 01:08:28,005
and the goal is to train a machine learning model that acts as the teacher.
目标是培养充当教师的机器学习模型。

1274
01:08:28,005 --> 01:08:30,000
Um, so you can imagine maybe in the future, this,
嗯，所以你可以想象将来，这个，

1275
01:08:30,000 --> 01:08:32,190
sort of, technology would be useful for,
某种技术对我们有用，

1276
01:08:32,190 --> 01:08:34,320
uh, um, education for, kind of,
呃，嗯，教育，有点，

1277
01:08:34,320 --> 01:08:37,035
having, uh, adding some automation.
有，呃，添加一些自动化。

1278
01:08:37,035 --> 01:08:42,495
Um, uh, one thing that makes this task difficult is that,
嗯，呃，使这个任务变得困难的一件事是，

1279
01:08:42,495 --> 01:08:46,545
uh, questions depend on the entire history of the conversation.
呃，问题取决于谈话的整个历史。

1280
01:08:46,545 --> 01:08:48,225
Um, so for example, uh,
嗯，例如，呃，

1281
01:08:48,225 --> 01:08:50,790
if you look, um, on the left here, uh,
如果你看，嗯，在左边这里，呃，

1282
01:08:50,790 --> 01:08:54,810
the example, um, dialogue,
例子，嗯，对话，

1283
01:08:54,810 --> 01:08:57,315
um, the third question is was he the star?
嗯，第三个问题是他是明星吗？

1284
01:08:57,315 --> 01:09:02,070
Um, clearly you can't answer that question unless you look back earlier in the dialogue,
嗯，显然你不能回答这个问题，除非你在对话中回顾一下，

1285
01:09:02,070 --> 01:09:04,095
and realize that the subject of this,
并意识到这个主题，

1286
01:09:04,095 --> 01:09:06,180
uh, conversation is Daffy Duck.
呃，谈话是Daffy Duck。

1287
01:09:06,180 --> 01:09:09,060
Um, a- and, sort of,
嗯，等等，

1288
01:09:09,060 --> 01:09:11,040
because this dataset is more challenging,
因为这个数据集更具挑战性，

1289
01:09:11,040 --> 01:09:14,340
and you can see there's a, there's a much bigger gap to human performance, right?
而且你可以看到有一个，人类表现有更大的差距，对吗？

1290
01:09:14,340 --> 01:09:17,610
So if you train some BERT with some extensions, you'll st- uh,
所以，如果你训练一些带有一些扩展的BERT，你就会这样做，

1291
01:09:17,610 --> 01:09:22,185
the results are still like 15 F1 points worse than human performance.
结果仍然比人类表现差15个F1点。

1292
01:09:22,185 --> 01:09:28,935
Um, um, here's one other dataset, um, called HotPotQA.
嗯，嗯，这是另外一个名为HotPotQA的数据集。

1293
01:09:28,935 --> 01:09:30,510
Um, it is, uh,
嗯，是的，呃，

1294
01:09:30,510 --> 01:09:32,760
designed instead for multi-hop reasoning.
专为多跳推理而设计。

1295
01:09:32,760 --> 01:09:35,610
Um, so essentially, in order to answer a question,
嗯，基本上，为了回答一个问题，

1296
01:09:35,610 --> 01:09:37,875
you have to look at multiple documents,
你必须看多个文件，

1297
01:09:37,875 --> 01:09:40,350
you have to look at different facts from those documents,
你必须从这些文件中查看不同的事实，

1298
01:09:40,350 --> 01:09:41,925
and perform some inference,
并进行一些推断，

1299
01:09:41,925 --> 01:09:44,655
um, to get what the correct answer is.
嗯，得到正确的答案。

1300
01:09:44,655 --> 01:09:48,645
Um, so I think, you know, this is a- a much harder task.
嗯，所以我想，你知道，这是一项艰巨的任务。

1301
01:09:48,645 --> 01:09:54,040
And again, um, there's a much bigger gap between human performance.
而且，嗯，人类表现之间的差距要大得多。

1302
01:09:54,590 --> 01:09:57,390
Um, any questions on, uh,
嗯，有什么问题，呃，

1303
01:09:57,390 --> 01:10:01,720
new datasets, um, harder chi- tasks for NLP?
NLP的新数据集，嗯，更难的任务？

1304
01:10:01,900 --> 01:10:07,035
Okay. Um, I'm gonna,
好的。嗯，我要的，

1305
01:10:07,035 --> 01:10:09,360
kind of, rapid fire and go through, um,
那种，快速的火，并通过，嗯，

1306
01:10:09,360 --> 01:10:12,210
a couple of more areas in the last minutes of this talk.
在本次演讲的最后几分钟，还有几个领域。

1307
01:10:12,210 --> 01:10:16,335
Um, so multitask learning I think is really growing in importance.
嗯，我认为多任务学习的重要性正在增加。

1308
01:10:16,335 --> 01:10:18,390
Um, of course, um,
嗯，当然，嗯，

1309
01:10:18,390 --> 01:10:20,190
you've had a whole lecture on this, right?
你有关于此的整个讲座，对吧？

1310
01:10:20,190 --> 01:10:21,750
So I'm not gonna spend too much time on it.
所以我不会花太多时间在上面。

1311
01:10:21,750 --> 01:10:24,330
Um, but maybe one, uh,
嗯，但也许一个，呃，

1312
01:10:24,330 --> 01:10:28,920
point of interest is that if you look at performance on this GLUE benchmark,
兴趣点是，如果你看一下这个GLUE基准的表现，

1313
01:10:28,920 --> 01:10:31,320
so this benchmark for natural language understanding,
所以这个自然语言理解的基准，

1314
01:10:31,320 --> 01:10:34,920
um, all the top couple results, um,
嗯，所有顶级夫妇的结果，嗯，

1315
01:10:34,920 --> 01:10:37,980
are- that are now actually surpassing BERT in
是 - 现在实际上超过了BERT

1316
01:10:37,980 --> 01:10:42,390
performance are- is taking BERT and training it in a multi-task way.
性能是 - 采用BERT并以多任务方式对其进行培训。

1317
01:10:42,390 --> 01:10:47,370
Um, I think another interesting, uh,
嗯，我觉得另一个有趣的，呃，

1318
01:10:47,370 --> 01:10:52,020
motivation for multi-task learning is that if you are training BERT, you have a really,
多任务学习的动机是，如果你正在训练BERT，你真的，

1319
01:10:52,020 --> 01:10:54,480
really large model and one way to make
非常大的模型和一种制作方法

1320
01:10:54,480 --> 01:10:58,690
more efficient use of that model is training it to do many things at once.
更有效地使用该模型正在训练它一次做很多事情。

1321
01:11:00,950 --> 01:11:04,920
Another area that's definitely important, um,
另一个非常重要的领域，嗯，

1322
01:11:04,920 --> 01:11:09,090
and I think will be important going in the future is dealing with low-resource settings.
而且我认为将来重要的是处理资源匮乏的环境。

1323
01:11:09,090 --> 01:11:10,890
Um, and here I'm using a really broad,
嗯，这里我用的很宽，

1324
01:11:10,890 --> 01:11:13,020
uh, definition of resources, right.
呃，资源的定义，对。

1325
01:11:13,020 --> 01:11:15,435
So that could mean compute power, um, you know,
这可能意味着计算能力，嗯，你知道，

1326
01:11:15,435 --> 01:11:18,990
BERT is great but it also takes huge amounts of compute to run it.
BERT很棒，但运行它也需要大量的计算。

1327
01:11:18,990 --> 01:11:20,310
So it's not realistic to say,
所以说，这是不现实的，

1328
01:11:20,310 --> 01:11:22,545
um, if you're building, let's say a mobile, uh,
嗯，如果你正在建设，那就说一个手机，呃，

1329
01:11:22,545 --> 01:11:27,510
an app for a mobile device that you could run a model the size of BERT.
移动设备的应用程序，您可以运行BERT大小的模型。

1330
01:11:27,510 --> 01:11:31,845
Um, as I already ga- went into earlier in this talk, um, you know,
嗯，正如我早些时候在这次演讲中所说的那样，嗯，你知道，

1331
01:11:31,845 --> 01:11:36,225
low-resource languages is an area that I think is pretty, um,
低资源语言是我觉得很漂亮的一个领域，嗯，

1332
01:11:36,225 --> 01:11:39,120
under-represented in NLP research right now,
目前在NLP研究中代表性不足，

1333
01:11:39,120 --> 01:11:41,460
because most datasets are in English, um,
因为大多数数据集都是英文的，嗯，

1334
01:11:41,460 --> 01:11:42,570
but I do think, right,
但我确实认为，对，

1335
01:11:42,570 --> 01:11:44,130
there's a really, you know,
你知道，有一个真的，

1336
01:11:44,130 --> 01:11:49,245
large number of people that in order to benefit from NLP technology, um,
为了从NLP技术中受益，大量的人，嗯，

1337
01:11:49,245 --> 01:11:52,200
we'll need to have technologies that work well in a lot of
我们需要拥有在很多方面运作良好的技术

1338
01:11:52,200 --> 01:11:56,055
different languages especially those without much training data.
不同语言，特别是那些没有太多训练数据

1339
01:11:56,055 --> 01:12:00,870
And, um, speaking of low- low amounts of training data, I think in general this is,
而且，嗯，谈到低数量的培训数据，我认为一般来说，这是

1340
01:12:00,870 --> 01:12:04,065
uh, a- an interesting area of research,
呃，一个有趣的研究领域，

1341
01:12:04,065 --> 01:12:05,550
um, within machine learning.
嗯，在机器学习中。

1342
01:12:05,550 --> 01:12:07,305
Actually, people are, um,
实际上，人们，嗯，

1343
01:12:07,305 --> 01:12:09,315
working a lot on this as well.
在这方面也做了很多工作。

1344
01:12:09,315 --> 01:12:11,460
Um, so a term is often, uh,
嗯，所以一个术语经常是，呃，

1345
01:12:11,460 --> 01:12:14,025
a term often used is few shot learning.
经常使用的术语是很少的镜头学习。

1346
01:12:14,025 --> 01:12:16,410
Um, and that essentially means being able to
嗯，这基本上意味着能够

1347
01:12:16,410 --> 01:12:18,720
train a machine learning model that only sees,
训练只看到的机器学习模型，

1348
01:12:18,720 --> 01:12:20,730
let's say five or ten examples.
让我们说五个或十个例子。

1349
01:12:20,730 --> 01:12:23,370
Um, one motivation there is, um,
嗯，有一个动机，嗯，

1350
01:12:23,370 --> 01:12:29,445
I think a clear distinction between how our existing machine learning systems learn,
我认为我们现有的机器学习系统如何学习，

1351
01:12:29,445 --> 01:12:31,875
and how humans learn is that, um,
以及人类如何学习，嗯，

1352
01:12:31,875 --> 01:12:35,550
humans can generalize very quickly from five or so examples.
人类可以从五个左右的例子中迅速概括。

1353
01:12:35,550 --> 01:12:37,185
Um, if you're training a neural net,
嗯，如果你正在训练一个神经网络，

1354
01:12:37,185 --> 01:12:38,580
you normally need, you know,
你通常需要，你知道，

1355
01:12:38,580 --> 01:12:41,610
thousands of examples or perhaps even tens of thousands,
成千上万的例子，甚至可能成千上万，

1356
01:12:41,610 --> 01:12:45,060
hundreds of thousands of examples to get something that works.
成千上万的例子来获得有用的东西。

1357
01:12:45,060 --> 01:12:49,650
Um, so I also see this being a pretty important area in the future.
嗯，所以我也看到这是未来非常重要的一个领域。

1358
01:12:49,650 --> 01:12:53,730
Um, the last area where I want to go in, um,
嗯，我想进去的最后一个区域，嗯，

1359
01:12:53,730 --> 01:12:57,600
a little bit more depth is interpreting and understanding models.
更深入的是解释和理解模型。

1360
01:12:57,600 --> 01:13:00,570
Um, so, so really there's two aspects of this.
嗯，所以，这真的有两个方面。

1361
01:13:00,570 --> 01:13:04,095
One is if I have a machine learning model and it makes a prediction,
一个是如果我有机器学习模型并进行预测，

1362
01:13:04,095 --> 01:13:06,450
I would like to be able to, uh,
我希望能够，呃，

1363
01:13:06,450 --> 01:13:08,790
know why did it make that prediction?
知道为什么会做出预测吗？

1364
01:13:08,790 --> 01:13:11,385
So gets some rationale, get some explanation,
得到一些理由，得到一些解释，

1365
01:13:11,385 --> 01:13:15,180
um, that would especially be important in an area like health care, right?
嗯，这在医疗保健等领域尤为重要，对吧？

1366
01:13:15,180 --> 01:13:17,910
So if you're a doctor and you're making a decision, um,
所以，如果你是一名医生并且你正在做出决定，嗯，

1367
01:13:17,910 --> 01:13:21,090
it's probably not good enough for your machine learning model to say,
你的机器学习模型可能不够好，

1368
01:13:21,090 --> 01:13:22,470
"Patient has disease X."
“患者有疾病X.”

1369
01:13:22,470 --> 01:13:23,805
You really want it to say,
你真的想要它说，

1370
01:13:23,805 --> 01:13:26,070
"Patient has disease X for these reasons."
“由于这些原因，患者患有疾病X.”

1371
01:13:26,070 --> 01:13:28,589
Um, because then you as a doctor can double-check,
嗯，因为那时你作为医生可以仔细检查，

1372
01:13:28,589 --> 01:13:30,540
and, and try to validate the, the,
并且，并尝试验证，

1373
01:13:30,540 --> 01:13:33,165
uh, machine's, um, thinking I guess,
呃，机器，嗯，我想，

1374
01:13:33,165 --> 01:13:35,610
um, to come up with that diagnosis.
嗯，想出那个诊断。

1375
01:13:35,610 --> 01:13:38,640
Um, the other area of interpreting
嗯，另一个解释领域

1376
01:13:38,640 --> 01:13:41,370
understanding models is more of a scientific question, right?
理解模型更像是一个科学问题，对吧？

1377
01:13:41,370 --> 01:13:43,860
Is we know things like BERT work really well,
我们知道像BERT这样的东西真的很好用，

1378
01:13:43,860 --> 01:13:45,960
um, we want to know why do they work well?
嗯，我们想知道他们为什么运作良好？

1379
01:13:45,960 --> 01:13:48,195
What -what what aspects of language do they model?
什么 - 他们建模的语言有哪些方面？

1380
01:13:48,195 --> 01:13:49,995
Um, what things don't they model?
嗯，他们模特的东西不是什么？

1381
01:13:49,995 --> 01:13:52,020
Um, and that might lead to, um,
嗯，这可能导致，嗯，

1382
01:13:52,020 --> 01:13:55,695
ideas of improving, um, those- those models.
改善那些模型的想法。

1383
01:13:55,695 --> 01:13:59,580
Um, so, um, here is a, uh,
嗯，所以，嗯，这是一个，呃，

1384
01:13:59,580 --> 01:14:04,935
couple slides on the main approach for evalu- answering the sort of scientific questions.
几个幻灯片的主要方法是评估这类科学问题。

1385
01:14:04,935 --> 01:14:06,975
What does a machine-learning model learn?
机器学习模型学到了什么？

1386
01:14:06,975 --> 01:14:10,530
Um, what you do is you have a model so let's say it's BERT.
嗯，你做的是你有一个模型所以让我们说它是BERT。

1387
01:14:10,530 --> 01:14:13,440
It takes as input a sequence of words, um,
它需要输入一系列单词，嗯，

1388
01:14:13,440 --> 01:14:16,470
it produces as output a sequence of vectors, um,
它产生一系列向量作为输出，嗯，

1389
01:14:16,470 --> 01:14:18,570
we want to ask does it know for example,
我们想问它是否知道例如，

1390
01:14:18,570 --> 01:14:19,680
the part of speech of words?
单词的词性？

1391
01:14:19,680 --> 01:14:22,455
So, so it does in its vector representations,
所以，它在矢量表示中也是如此，

1392
01:14:22,455 --> 01:14:24,630
does that capture something about syntax?
这会捕获一些语法吗？

1393
01:14:24,630 --> 01:14:29,850
Um, and a simple way of asking this question is train another classifier on top of BERT,
嗯，问一个简单的方法就是在BERT上训练另一个分类器，

1394
01:14:29,850 --> 01:14:31,965
uh, that's trained to do,
呃，那是训练有素的，

1395
01:14:31,965 --> 01:14:34,395
um, let's say part-of-speech tagging.
嗯，让我们说一下词性标注。

1396
01:14:34,395 --> 01:14:36,825
Um, but we only, um,
嗯，但我们只是，嗯，

1397
01:14:36,825 --> 01:14:39,945
backprop into that diagnostic classifier itself.
backprop进入该诊断分类器本身。

1398
01:14:39,945 --> 01:14:43,680
So in other words we're treating the output of BERT, um,
换句话说，我们正在处理BERT的输出，嗯，

1399
01:14:43,680 --> 01:14:46,185
that sequence of vectors as a fixed input,
那个矢量序列作为固定输入，

1400
01:14:46,185 --> 01:14:48,600
and we're sort of probing those vectors to see,
我们正在探测那些向量，

1401
01:14:48,600 --> 01:14:50,505
um, do they contain, um,
嗯，他们是否包含，嗯，

1402
01:14:50,505 --> 01:14:52,440
information about a part of speech that
关于词性的信息

1403
01:14:52,440 --> 01:14:56,445
this second diagnostic classifier on top can decode,
顶部的第二个诊断分类器可以解码，

1404
01:14:56,445 --> 01:14:59,050
um, to get the correct labels?
嗯，要获得正确的标签？

1405
01:14:59,120 --> 01:15:03,690
Um, so, um, it was kind of quite a few concerns here.
嗯，嗯，这里有点担心。

1406
01:15:03,690 --> 01:15:06,540
Um, one concern is, uh,
嗯，一个问题是，呃，

1407
01:15:06,540 --> 01:15:09,915
if you make your diagnostic classifier too complicated,
如果你的诊断分类器过于复杂，

1408
01:15:09,915 --> 01:15:13,200
it can just solve the classif- the task all on itself,
它可以解决分类 - 任务本身，

1409
01:15:13,200 --> 01:15:15,210
and it can basically ignore, uh,
它基本上可以忽略，呃，

1410
01:15:15,210 --> 01:15:17,565
whatever representations were produced by BERT.
无论BERT产生什么样的陈述。

1411
01:15:17,565 --> 01:15:20,040
Um, so- so the kind of standard thing right now is to use
嗯，所以现在标准的东西就是使用

1412
01:15:20,040 --> 01:15:23,205
a single softmax layer on top of BERT,
BERT顶部的单个softmax层，

1413
01:15:23,205 --> 01:15:25,185
um, to do these decisions.
嗯，做这些决定。

1414
01:15:25,185 --> 01:15:29,100
Um, and there's been a whole bunch of tasks proposed for
嗯，并且提出了一大堆任务

1415
01:15:29,100 --> 01:15:32,895
evaluating essentially the linguistic knowledge of these models.
基本上评估这些模型的语言知识。

1416
01:15:32,895 --> 01:15:34,785
Um, so you could do part-of-speech tagging,
嗯，所以你可以做一些词性标注，

1417
01:15:34,785 --> 01:15:37,080
you could do more semantic tasks like,
你可以做更多的语义任务，比如

1418
01:15:37,080 --> 01:15:39,285
uh, relation extraction, um,
呃，关系提取，嗯，

1419
01:15:39,285 --> 01:15:41,265
or- or something like co-reference.
or-或类似共同参考的东西。

1420
01:15:41,265 --> 01:15:44,280
Um, and this is a pretty active area of work.
嗯，这是一个非常活跃的工作领域。

1421
01:15:44,280 --> 01:15:47,055
Um, here is, uh, just one, uh,
嗯，这是，呃，只有一个，呃，

1422
01:15:47,055 --> 01:15:51,195
plot showing some of the results, um, of this approach.
情节显示了这种方法的一些结果。

1423
01:15:51,195 --> 01:15:53,865
So here what we're doing is we're adding
所以我们正在做的是我们正在添加

1424
01:15:53,865 --> 01:15:56,955
diagnostic classifiers to different layers of BERT,
诊断分类器到不同的BERT层，

1425
01:15:56,955 --> 01:16:02,620
and we are seeing which layers of BERT are more useful for particular tasks.
我们看到哪些BERT层对特定任务更有用。

1426
01:16:02,620 --> 01:16:07,025
Um, and, um, something kind of interesting comes out of this which is that, um,
嗯，而且，嗯，有些有趣的东西来自于这个，嗯，

1427
01:16:07,025 --> 01:16:10,310
the different layers of BERT seem to be corresponding, um,
BERT的不同层似乎是对应的，嗯，

1428
01:16:10,310 --> 01:16:12,890
fairly well with notions of,
相当好的概念，

1429
01:16:12,890 --> 01:16:15,395
uh, different layers of li- of linguistics.
呃，不同层次的语言学。

1430
01:16:15,395 --> 01:16:19,110
Um, so, uh, dependency parsing which is a syntactic task,
嗯，所以，呃，依赖解析这是一个句法任务，

1431
01:16:19,110 --> 01:16:20,940
um, it's, uh, considered sort of a, you know,
嗯，是的，呃，被认为是一种，你知道，

1432
01:16:20,940 --> 01:16:23,430
medium level task in understanding a sentence.
理解句子的中级任务。

1433
01:16:23,430 --> 01:16:28,125
Um, the medium layers of BERT, so layers kind of 6 through 8 or something,
嗯，BERT的中间层，所以层数为6到8之类，

1434
01:16:28,125 --> 01:16:30,480
are the ones best at dependency parsing.
是最依赖于解析的人。

1435
01:16:30,480 --> 01:16:34,095
Um, if you have a se- very semantic task like sentiment analysis,
嗯，如果你有像情绪分析这样的语义任务，

1436
01:16:34,095 --> 01:16:35,880
um, where you're trying to learn some kind of, uh,
嗯，你在哪里学习了某种东西，呃，

1437
01:16:35,880 --> 01:16:38,325
semantic property of the whole sentence, um,
整个句子的语义属性，嗯，

1438
01:16:38,325 --> 01:16:41,490
then the very last layers of BERT are the ones that seem
那么BERT的最后几层就是那些

1439
01:16:41,490 --> 01:16:45,700
to encode the most information about- about this, uh, phenomenon.
编码关于这个，呃，现象的最多信息。

1440
01:16:46,310 --> 01:16:48,690
Um, okay.
嗯，好的。

1441
01:16:48,690 --> 01:16:50,835
So this is almost it for the talk, um,
所以这几乎是为了谈话，嗯，

1442
01:16:50,835 --> 01:16:54,600
I just have one slide here of, uh, um,
我这里只有一张幻灯片，呃，嗯，

1443
01:16:54,600 --> 01:16:57,870
NLP not in kind of the academic researching context,
NLP不是学术研究的背景，

1444
01:16:57,870 --> 01:17:00,735
which I have already been talking a lot about but NLP in industry,
我已经谈了很多关于NLP的工业，

1445
01:17:00,735 --> 01:17:03,075
and really there's rapid progress there.
那里确实有快速的进步。

1446
01:17:03,075 --> 01:17:06,315
And I wanted to point to you two areas where I think there's
我想指出你认为存在的两个方面

1447
01:17:06,315 --> 01:17:10,650
especially a large interest in using NLP technology.
尤其是对使用NLP技术的浓厚兴趣。

1448
01:17:10,650 --> 01:17:12,240
Um, one is dialogue,
嗯，一个是对话，

1449
01:17:12,240 --> 01:17:14,010
um, so for things like chatbots, right?
嗯，对于像聊天机器人这样的东西，对吧？

1450
01:17:14,010 --> 01:17:17,580
There's the Alexa Prize where they're actually investing a lot of money in,
有Alexa奖，他们实际投入了大量资金，

1451
01:17:17,580 --> 01:17:21,105
um, having groups figure out how to improve chitchat dialogue.
嗯，让小组弄清楚如何改善闲聊对话。

1452
01:17:21,105 --> 01:17:25,230
Um, there's also I think a lot of potential for customer service, right?
嗯，还有我认为很有潜力的客户服务，对吧？

1453
01:17:25,230 --> 01:17:28,170
So improving basically automated systems that'll, um,
所以改进基本上自动化的系统，嗯，嗯，

1454
01:17:28,170 --> 01:17:29,580
you know, book you a flight,
你知道的，给你预订航班，

1455
01:17:29,580 --> 01:17:32,385
or help you cancel a subscription, or anything like that.
或者帮助您取消订阅或类似的任何内容。

1456
01:17:32,385 --> 01:17:35,460
Um, and similarly, there's a lot of potential in health care.
嗯，同样，医疗保健也有很大的潜力。

1457
01:17:35,460 --> 01:17:39,180
Um, one is understanding the records of someone who,
嗯，人们正在了解某人的记录，

1458
01:17:39,180 --> 01:17:42,060
um, is sick and to help them- to help with diagnoses.
嗯，病了，帮助他们 - 帮助诊断。

1459
01:17:42,060 --> 01:17:43,935
Um, I think another, um,
嗯，我想另一个，嗯，

1460
01:17:43,935 --> 01:17:46,215
equally important area is actually, uh,
同样重要的领域实际上是，呃，

1461
01:17:46,215 --> 01:17:49,020
parsing, uh, biomedical papers.
解析，呃，生物医学论文。

1462
01:17:49,020 --> 01:17:54,285
Um, so, um, the number of biomedical papers that are being written is really insane,
嗯，嗯，正在编写的生物医学论文的数量真是疯了，

1463
01:17:54,285 --> 01:17:56,100
um, it's, it's way larger than the number
嗯，它比数字大

1464
01:17:56,100 --> 01:17:57,960
of computer science papers that are being written.
正在编写的计算机科学论文。

1465
01:17:57,960 --> 01:18:01,530
[NOISE] Um, often if you're a doctor,
[NOISE]嗯，通常如果你是医生，

1466
01:18:01,530 --> 01:18:03,150
or if you're a researcher, um,
或者如果你是研究员，嗯，

1467
01:18:03,150 --> 01:18:06,360
in medicine, you might want to look up something very specific, right?
在医学上，你可能想要查找一些非常具体的东西，对吧？

1468
01:18:06,360 --> 01:18:07,620
You might want to know what is
你可能想知道它是什么

1469
01:18:07,620 --> 01:18:11,370
the effect of this particular drug on this particular gene,
这种特殊药物对这种特殊基因的影响，

1470
01:18:11,370 --> 01:18:13,140
or a cell with this particular gene.
或具有该特定基因的细胞。

1471
01:18:13,140 --> 01:18:16,710
Um, there's no good way right now of searching through, um,
嗯，现在没有好办法搜索，嗯，

1472
01:18:16,710 --> 01:18:20,175
hundreds of thousands of papers to find if someone has a- has, uh,
成千上万的论文，如果有人有，有，呃，

1473
01:18:20,175 --> 01:18:23,085
done this experiment and have results for this,
做了这个实验并得到了这个结果，

1474
01:18:23,085 --> 01:18:25,095
um, particular combination of things.
嗯，特别是事物的组合。

1475
01:18:25,095 --> 01:18:28,590
Um, so automated reading of all this biomedical literature,
嗯，所以这些生物医学文献的自动阅读，

1476
01:18:28,590 --> 01:18:30,850
um, could have a lot of value.
嗯，可能有很多价值。

1477
01:18:31,100 --> 01:18:33,960
Okay, um, to conclude, um,
好的，嗯，总结一下，嗯，

1478
01:18:33,960 --> 01:18:38,280
there's been rapid progress in the last five years due to deep learning, um, in NLP.
由于深入学习，嗯，在NLP，过去五年来取得了快速进展。

1479
01:18:38,280 --> 01:18:42,780
Um, in the last year, we've seen another really kind of, uh,
嗯，在去年，我们看到了另一种真正的，呃，

1480
01:18:42,780 --> 01:18:45,300
a dramatic increase in the capability of our systems,
我们系统的能力急剧增加，

1481
01:18:45,300 --> 01:18:47,610
thanks to, uh, using unlabeled data.
谢谢，呃，使用未标记的数据。

1482
01:18:47,610 --> 01:18:49,095
So that's methods like BERT.
这就像BERT这样的方法。

1483
01:18:49,095 --> 01:18:54,210
Um, and, um, the other kind of thing that's I think important to think about is that,
嗯，嗯，我认为重要的另一件事是，

1484
01:18:54,210 --> 01:18:58,170
NLP systems are starting to be at a place where they can have big social impact.
NLP系统开始处于一个可以产生巨大社会影响的地方。

1485
01:18:58,170 --> 01:19:04,845
Um, so that makes some issues like bias and security very important. Um, thank you.
嗯，这使得偏见和安全等问题非常重要。嗯，谢谢。

1486
01:19:04,845 --> 01:19:06,690
Uh, good luck finishing all your projects.
呃，祝你好运完成所有项目。

1487
01:19:06,690 --> 01:19:14,800
[APPLAUSE].
[掌声]。

1488


