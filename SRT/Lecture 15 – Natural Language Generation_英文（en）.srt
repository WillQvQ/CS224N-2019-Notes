1
00:00:04,760 --> 00:00:09,990
So today we're gonna be learning about Natural Language Generation.

2
00:00:09,990 --> 00:00:12,460
And uh, this is probably going to be a little different to

3
00:00:12,460 --> 00:00:16,380
my previous lectures because this is going to be much more of a kind of survey,

4
00:00:16,380 --> 00:00:17,745
of lots of cutting edge, uh,

5
00:00:17,745 --> 00:00:21,060
research topics that are happening in NLG right now.

6
00:00:21,060 --> 00:00:22,800
So before we get to that, uh,

7
00:00:22,800 --> 00:00:24,315
we've got a few announcements.

8
00:00:24,315 --> 00:00:26,370
Uh, so I guess the main announcement is just,

9
00:00:26,370 --> 00:00:28,290
thank you all so much for your hard work.

10
00:00:28,290 --> 00:00:31,020
I know, um, the last week or two have been pretty tough.

11
00:00:31,020 --> 00:00:33,450
Uh, assignment five was really quite difficult,

12
00:00:33,450 --> 00:00:35,300
I think, and it was a challenge to do it in eight days.

13
00:00:35,300 --> 00:00:38,000
So we just really appreciate all the hard work you've put in.

14
00:00:38,000 --> 00:00:40,910
Um, we also understand the project proposal was,

15
00:00:40,910 --> 00:00:45,590
uh, sometimes a bit difficult to understand the expectations for some people.

16
00:00:45,590 --> 00:00:47,990
Um, so, yeah, these are both new components of

17
00:00:47,990 --> 00:00:50,465
the class this year that were not present last year.

18
00:00:50,465 --> 00:00:52,115
Um, so you know,

19
00:00:52,115 --> 00:00:55,265
we have to go through some learning curves as well as the teaching staff.

20
00:00:55,265 --> 00:00:57,710
So just we really want to say thank you so much, uh,

21
00:00:57,710 --> 00:01:00,365
for putting everything into this class.

22
00:01:00,365 --> 00:01:03,230
And please do continue to give us your feedback both right

23
00:01:03,230 --> 00:01:06,990
now and in the end of quarter feedback survey.

24
00:01:07,080 --> 00:01:10,685
Okay, so here's the overview for what we're going to be doing today.

25
00:01:10,685 --> 00:01:13,955
So today we're going to learn about what's happening in the world

26
00:01:13,955 --> 00:01:16,960
of neural approaches for Natural Language Generation.

27
00:01:16,960 --> 00:01:18,690
Uh, that is a super,

28
00:01:18,690 --> 00:01:22,065
super broad title, Natural Language Generation.

29
00:01:22,065 --> 00:01:25,685
Um, NLG encompasses a huge variety of research areas

30
00:01:25,685 --> 00:01:27,800
and pretty much each of those could have had

31
00:01:27,800 --> 00:01:29,825
their own lectures and we could have taught a whole,

32
00:01:29,825 --> 00:01:34,035
a whole quarter- quarter's worth of classes on, ah, NLG.

33
00:01:34,035 --> 00:01:37,475
Uh, but we're going to try to cover a selection of things today.

34
00:01:37,475 --> 00:01:41,520
And, um, uh, it's mostly going to be, uh,

35
00:01:41,520 --> 00:01:42,895
guided by the things which, uh,

36
00:01:42,895 --> 00:01:46,040
I've seen that I think are cool or interesting or exciting.

37
00:01:46,040 --> 00:01:48,140
So it's by no means going to be comprehensive but

38
00:01:48,140 --> 00:01:51,300
I hope you're going to enjoy some of the stuff we're going to learn about.

39
00:01:52,070 --> 00:01:56,660
Okay, so in particular we're going to start off by having a recap of what we

40
00:01:56,660 --> 00:02:01,070
already know about Natural Language Generation to make sure we're on the same page.

41
00:02:01,070 --> 00:02:04,235
And we're also going to learn a little bit extra about decoding algorithms.

42
00:02:04,235 --> 00:02:05,870
So we learned a bit before about, uh,

43
00:02:05,870 --> 00:02:08,329
greedy decoding and beam search decoding,

44
00:02:08,329 --> 00:02:10,280
but today we're going to learn some extra information about

45
00:02:10,280 --> 00:02:13,085
that and some other types of decoding algorithms.

46
00:02:13,085 --> 00:02:15,710
After that we're going to go through, um,

47
00:02:15,710 --> 00:02:17,570
a pretty quick tour of lots of

48
00:02:17,570 --> 00:02:21,860
different NLG tasks and a selection of neural approaches to them.

49
00:02:21,860 --> 00:02:25,580
And then after that we're gonna talk about probably the biggest problem in NLG research,

50
00:02:25,580 --> 00:02:29,945
which is NLG evaluation and why it is such a tricky situation.

51
00:02:29,945 --> 00:02:33,665
And then lastly, we're going to have some concluding thoughts on NLG research.

52
00:02:33,665 --> 00:02:37,440
What are the current trends and where are we going in the future?

53
00:02:38,260 --> 00:02:47,875
Okay. So, uh, section one, let's do a recap.

54
00:02:47,875 --> 00:02:51,440
Okay, so Natural Language Generation to define it just

55
00:02:51,440 --> 00:02:55,090
refers to any setting in which we are generating some kind of text.

56
00:02:55,090 --> 00:02:58,385
So for example, NLG is an important sub-component

57
00:02:58,385 --> 00:03:01,400
of lots of different tasks such as, uh, machine translation,

58
00:03:01,400 --> 00:03:04,370
we've already met, uh, abstracted summarization,

59
00:03:04,370 --> 00:03:06,305
we'll learn a bit more about that later, um,

60
00:03:06,305 --> 00:03:09,760
dialogue both chit-chat and task-based.

61
00:03:09,760 --> 00:03:15,155
Uh, also creative writing tasks such as writing stories and writing poems even.

62
00:03:15,155 --> 00:03:17,420
NLG is also a sub-component of,

63
00:03:17,420 --> 00:03:19,610
uh, free-form question answering.

64
00:03:19,610 --> 00:03:22,280
So I know a lot of you are doing the SQuAD project right now, uh,

65
00:03:22,280 --> 00:03:26,030
that is not an NLG task because you're just extracting the answer from the,

66
00:03:26,030 --> 00:03:27,245
uh, the source document.

67
00:03:27,245 --> 00:03:29,510
But there are other question answering tasks

68
00:03:29,510 --> 00:03:31,775
that do have a Natural Language Generation component.

69
00:03:31,775 --> 00:03:35,870
Uh, image captioning is another example of,

70
00:03:35,870 --> 00:03:38,610
uh, a task that has an NLG sub-component.

71
00:03:38,810 --> 00:03:43,625
So NLG is a pretty cool component of a lot of different NLP tasks.

72
00:03:43,625 --> 00:03:45,440
All right, let's go into our recap.

73
00:03:45,440 --> 00:03:47,450
So the first thing I want to recap is,

74
00:03:47,450 --> 00:03:48,725
uh, what is language modeling?

75
00:03:48,725 --> 00:03:53,300
Um, I've noticed that some people are a little bit confused about this, I think it, uh,

76
00:03:53,300 --> 00:03:56,300
might be because the name language modeling sounds like it might mean

77
00:03:56,300 --> 00:04:00,635
just simply encoding language like representing language using embeddings or something.

78
00:04:00,635 --> 00:04:02,930
So as a reminder language modeling,

79
00:04:02,930 --> 00:04:04,135
uh, has a more precise meaning.

80
00:04:04,135 --> 00:04:09,245
Language modeling is the task of predicting the next word given the word so far.

81
00:04:09,245 --> 00:04:11,300
So any system which produces

82
00:04:11,300 --> 00:04:16,420
this conditional probability distribution that does this task is called a Language Model.

83
00:04:16,420 --> 00:04:18,645
And if that language model,

84
00:04:18,645 --> 00:04:20,025
uh, system is an RNN,

85
00:04:20,025 --> 00:04:24,190
then we often abbreviate it as RNN-Language Model.

86
00:04:24,350 --> 00:04:27,390
Okay, so I hope, uh, you'll remember that.

87
00:04:27,390 --> 00:04:29,060
Uh, the next thing we're going to recap is do you

88
00:04:29,060 --> 00:04:31,060
remember what a Conditional Language Model is?

89
00:04:31,060 --> 00:04:35,040
Uh, the task of Conditional Language Modeling is when you're predicting, uh,

90
00:04:35,040 --> 00:04:37,910
what word's going to come next but you're also conditioning on

91
00:04:37,910 --> 00:04:41,585
some other input x as well as all of your words so far.

92
00:04:41,585 --> 00:04:45,640
So to recap some examples of conditional language modeling include, uh,

93
00:04:45,640 --> 00:04:49,145
machine translation where you're conditioning on the source sentence x,

94
00:04:49,145 --> 00:04:52,940
uh, summarization you're conditioning on your input text that you're trying to summarize.

95
00:04:52,940 --> 00:04:57,510
Dialogue, you're conditioning on your dialogue history and so on.

96
00:04:57,680 --> 00:05:03,455
Okay, uh, next we're going to quickly recap how do you train an RNN-Language model?

97
00:05:03,455 --> 00:05:07,760
I guess, it could also be a transformer-based language model or a CNN-based language model,

98
00:05:07,760 --> 00:05:11,090
now that you know about those, uh, and it could be conditional or it could be not.

99
00:05:11,090 --> 00:05:15,620
So the main thing I want to remind you about is that when you are training the system,

100
00:05:15,620 --> 00:05:18,410
then you feed in the target sequence that you're trying to

101
00:05:18,410 --> 00:05:21,620
generate so where it says target sentence from corpus, uh,

102
00:05:21,620 --> 00:05:24,290
that's saying that you have some sequence that you're trying to

103
00:05:24,290 --> 00:05:27,785
generate and you feed that into the decoder, the RNN-Language model.

104
00:05:27,785 --> 00:05:31,385
And then it predicts what words are going to come next.

105
00:05:31,385 --> 00:05:35,010
So the super important thing is that during training,

106
00:05:35,010 --> 00:05:39,430
we're feeding the gold, that is the reference target sentence into the decoder,

107
00:05:39,430 --> 00:05:41,980
regardless of what the decoder is predicting.

108
00:05:41,980 --> 00:05:46,750
So even if let's say this is a very bad decoder that isn't predicting the correct words,

109
00:05:46,750 --> 00:05:48,550
uh, it's not, you know, predicting them high at all,

110
00:05:48,550 --> 00:05:51,790
um, that doesn't matter we still just, um,

111
00:05:51,790 --> 00:05:55,595
input the targets- the gold target sequence into the decoder.

112
00:05:55,595 --> 00:05:58,660
And, um, I'm emphasizing this because it's going to come up later,

113
00:05:58,660 --> 00:06:00,895
uh, this training method is called Teacher Forcing.

114
00:06:00,895 --> 00:06:03,170
Which might be a phrase that you have come across elsewhere.

115
00:06:03,170 --> 00:06:05,975
So, yeah, it refers to the fact that the teacher,

116
00:06:05,975 --> 00:06:09,160
that is kind of like the gold input is- is forcing, uh,

117
00:06:09,160 --> 00:06:11,260
the language model to use that on every step

118
00:06:11,260 --> 00:06:14,495
instead of using its own predictions on each step.

119
00:06:14,495 --> 00:06:18,630
So that's how you train a RNN-Language model which might be conditional.

120
00:06:18,630 --> 00:06:20,610
Uh, okay.

121
00:06:20,610 --> 00:06:22,815
So now a recap on decoding algorithms.

122
00:06:22,815 --> 00:06:26,945
So, uh, you've got your trained language model which might be conditional.

123
00:06:26,945 --> 00:06:29,345
The question is how do you use it to generate a text?

124
00:06:29,345 --> 00:06:32,000
So the answer is you need a decoding algorithm.

125
00:06:32,000 --> 00:06:34,640
A decoding algorithm is an algorithm you use to

126
00:06:34,640 --> 00:06:37,585
generate the text from your trained language model.

127
00:06:37,585 --> 00:06:39,680
So, uh, in the NMT lecture

128
00:06:39,680 --> 00:06:42,470
a few weeks ago we learned about two different decoding algorithms.

129
00:06:42,470 --> 00:06:45,620
We learned about greedy decoding and beam search.

130
00:06:45,620 --> 00:06:47,795
So let's quickly recap those.

131
00:06:47,795 --> 00:06:51,215
Uh, greedy decoding is a pretty simple algorithm.

132
00:06:51,215 --> 00:06:53,030
On each step you just take what's

133
00:06:53,030 --> 00:06:55,895
the most probable words according to the language model.

134
00:06:55,895 --> 00:06:59,245
You could deal with the argmax and then use that as the next word,

135
00:06:59,245 --> 00:07:01,340
you feed it in as the input on the next step.

136
00:07:01,340 --> 00:07:03,860
And you just keep going until you produce some kind of END

137
00:07:03,860 --> 00:07:06,620
token or maybe when you reach some maximum length.

138
00:07:06,620 --> 00:07:09,805
And I think you're all quite familiar with this because you did it in assignment five.

139
00:07:09,805 --> 00:07:15,220
So uh, yes this diagram shows how greedy decoding would work to generate the sentence.

140
00:07:15,220 --> 00:07:17,415
So as we learned before,

141
00:07:17,415 --> 00:07:19,610
due to a kind of lack of backtracking and

142
00:07:19,610 --> 00:07:22,130
inability to go back if you made a wrong choice, uh,

143
00:07:22,130 --> 00:07:25,050
the output from greedy decoding is generally, uh,

144
00:07:25,050 --> 00:07:30,280
pretty poor like it can be ungrammatical, or it can be unnatural, kind of nonsensical.

145
00:07:30,280 --> 00:07:33,275
Okay, let's recap beam search decoding.

146
00:07:33,275 --> 00:07:38,660
So beam search is a search algorithm which aims to find a high probability sequence.

147
00:07:38,660 --> 00:07:43,610
So if we're doing translation that sequence is the sequence of translation words,

148
00:07:43,610 --> 00:07:48,200
um, by tracking multiple possible sequences at once.

149
00:07:48,200 --> 00:07:51,980
So the core idea is that on each step of the decoder,

150
00:07:51,980 --> 00:07:53,060
you're going to be keeping track of

151
00:07:53,060 --> 00:07:57,515
the K most probable partial sequences which we call hypotheses.

152
00:07:57,515 --> 00:08:01,000
And here K is some hyper- hyper parameter called the beam size.

153
00:08:01,000 --> 00:08:02,570
So the idea is by um,

154
00:08:02,570 --> 00:08:05,870
considering lots of different hypotheses we're going to try to search effectively for

155
00:08:05,870 --> 00:08:07,790
a high probability sequence but there is

156
00:08:07,790 --> 00:08:10,285
no guarantee that this is going to be the optimal,

157
00:08:10,285 --> 00:08:12,870
most high probability sequence.

158
00:08:12,870 --> 00:08:15,930
So, uh, at the end of beam search, uh,

159
00:08:15,930 --> 00:08:17,930
you reach some kind of stopping criterion which we talked

160
00:08:17,930 --> 00:08:20,300
about before but I won't cover in detail again.

161
00:08:20,300 --> 00:08:22,610
Uh, and once you've reached your stopping criterion,

162
00:08:22,610 --> 00:08:25,175
you choose the sequence with the highest probability,

163
00:08:25,175 --> 00:08:29,495
um, factoring in some adjustments for length and then that's your output.

164
00:08:29,495 --> 00:08:31,400
So just to do this one more time.

165
00:08:31,400 --> 00:08:35,435
Here's the diagram that we saw in the NMT lecture of beam search decoding um,

166
00:08:35,435 --> 00:08:40,025
once it's completed and in this scenario we have a beam size of two.

167
00:08:40,025 --> 00:08:43,180
So this is what it looks like after we've done this exploration problem,

168
00:08:43,180 --> 00:08:45,420
this shows the full tree that we explored,

169
00:08:45,420 --> 00:08:49,145
and then we've come to some kind of stopping criterion and we identify the top,

170
00:08:49,145 --> 00:08:50,480
uh, hypothesis and, uh,

171
00:08:50,480 --> 00:08:52,510
that's highlighted in green.

172
00:08:52,510 --> 00:08:55,815
So on the subject of beam search decoding,

173
00:08:55,815 --> 00:08:57,810
I was watching TV the other day,

174
00:08:57,810 --> 00:09:00,710
and I notice something in Westworld.

175
00:09:00,710 --> 00:09:06,020
I think the hosts- [LAUGHTER] the AI hosts in Westworld maybe used beam search.

176
00:09:06,020 --> 00:09:08,840
Which is something I wasn't expecting to see on TV.

177
00:09:08,840 --> 00:09:10,945
[LAUGHTER] So there's this scene,

178
00:09:10,945 --> 00:09:12,390
uh, Westworld is, by the way,

179
00:09:12,390 --> 00:09:14,240
a sci-fi series that has these, um,

180
00:09:14,240 --> 00:09:16,580
very convincing humanoid AI systems.

181
00:09:16,580 --> 00:09:19,040
Um, and there's a scene where one of

182
00:09:19,040 --> 00:09:22,055
the AI systems is confronted with the reality of the fact that,

183
00:09:22,055 --> 00:09:23,975
um, she, I suppose is,

184
00:09:23,975 --> 00:09:29,850
um, not human because she sees the generation system of words as she says them,

185
00:09:29,850 --> 00:09:31,680
and I was looking at the TV and I thought,

186
00:09:31,680 --> 00:09:32,775
is that beam search?

187
00:09:32,775 --> 00:09:35,840
Because that diagram looks a lot like this diagram here,

188
00:09:35,840 --> 00:09:38,580
um, but maybe with a bigger beam size.

189
00:09:38,580 --> 00:09:40,605
So, I thought, that was pretty cool because, you know,

190
00:09:40,605 --> 00:09:43,490
AI has hit the mainstream when you see beam search on TV.

191
00:09:43,490 --> 00:09:45,200
And then if you zoom in really hard you can see

192
00:09:45,200 --> 00:09:49,470
some other exciting words in this screenshot like knowledge base,

193
00:09:49,470 --> 00:09:51,170
forward chaining and backward chaining,

194
00:09:51,170 --> 00:09:54,200
identifies the same thing as forward prop and backward prop,

195
00:09:54,200 --> 00:09:57,185
um, and also fuzzy logic algorithms and neural net.

196
00:09:57,185 --> 00:09:59,390
Um, so yeah, beam search,

197
00:09:59,390 --> 00:10:00,875
I think, has hit the mainstream now,

198
00:10:00,875 --> 00:10:04,100
um, so it's good enough for Westworld,

199
00:10:04,100 --> 00:10:05,060
maybe it's good enough for us.

200
00:10:05,060 --> 00:10:08,500
Uh, so with beam search, right?

201
00:10:08,500 --> 00:10:12,055
We've talked about how you have this hyperparameter k or the beam size.

202
00:10:12,055 --> 00:10:14,110
And one thing we didn't talk about in the last lecture,

203
00:10:14,110 --> 00:10:16,660
so now we're leaving the recap portion, um,

204
00:10:16,660 --> 00:10:20,980
is what's the effect of changing that beam size k. So, uh,

205
00:10:20,980 --> 00:10:22,480
if you have a really small k,

206
00:10:22,480 --> 00:10:26,065
then you're gonna have similar problems to greedy decoding.

207
00:10:26,065 --> 00:10:27,370
And in fact, if k equals one,

208
00:10:27,370 --> 00:10:29,890
then you are actually just doing greedy decoding.

209
00:10:29,890 --> 00:10:32,410
So those same problems are, you know, ungrammatical,

210
00:10:32,410 --> 00:10:36,805
maybe unnatural, nonsensical, just kind of plain incorrect output.

211
00:10:36,805 --> 00:10:39,415
So once if we get larger k,

212
00:10:39,415 --> 00:10:41,305
if you have a larger beam size,

213
00:10:41,305 --> 00:10:46,750
then you're doing your search algorithm but considering more hypotheses, right?

214
00:10:46,750 --> 00:10:48,610
You're, you're having a larger search space and

215
00:10:48,610 --> 00:10:51,100
you're considering more different possibilities.

216
00:10:51,100 --> 00:10:55,495
So if you do that, then we often find that this reduces some of the problems above.

217
00:10:55,495 --> 00:10:58,540
So you're much less likely to have this ungrammatical,

218
00:10:58,540 --> 00:11:01,015
uh, you know, disjointed output.

219
00:11:01,015 --> 00:11:04,930
But there are some downsides to raising k. So of course,

220
00:11:04,930 --> 00:11:06,969
larger k is more computationally expensive

221
00:11:06,969 --> 00:11:09,250
and that can get pretty bad if you're trying to, um,

222
00:11:09,250 --> 00:11:11,530
for example, generate your, uh,

223
00:11:11,530 --> 00:11:12,850
outputs for a large, you know,

224
00:11:12,850 --> 00:11:15,475
test set of NMT examples.

225
00:11:15,475 --> 00:11:17,680
Um, but more seriously than that,

226
00:11:17,680 --> 00:11:19,870
increasing k can introduce some other problems.

227
00:11:19,870 --> 00:11:23,245
So for example, it's been shown that in NMT,

228
00:11:23,245 --> 00:11:28,030
increasing the beam size too much actually decreases the BLEU score.

229
00:11:28,030 --> 00:11:30,625
And this is kind of counter-intuitive, right?

230
00:11:30,625 --> 00:11:32,875
Because we were thinking of beam search

231
00:11:32,875 --> 00:11:35,410
as this algorithm that tries to find the optimal solution.

232
00:11:35,410 --> 00:11:37,015
So surely, if you increase k,

233
00:11:37,015 --> 00:11:39,970
then you're only going to find a better solution, right?

234
00:11:39,970 --> 00:11:44,440
Um, so I think maybe the key here is the difference between optimality

235
00:11:44,440 --> 00:11:46,300
in terms of the search problem that is finding

236
00:11:46,300 --> 00:11:48,895
a high probability sequence and BLEU score,

237
00:11:48,895 --> 00:11:50,080
which are two separate things,

238
00:11:50,080 --> 00:11:54,310
and there's no guarantee that they actually, um, correspond, right?

239
00:11:54,310 --> 00:11:57,850
And I mean, there's a difference, again, between BLEU score and actual translation,

240
00:11:57,850 --> 00:11:59,440
uh, quality as we know.

241
00:11:59,440 --> 00:12:01,720
So if you look at the two papers which I've linked to

242
00:12:01,720 --> 00:12:04,390
here which are the ones that show that,

243
00:12:04,390 --> 00:12:07,330
uh, increasing beam size too much decreases the BLEU score.

244
00:12:07,330 --> 00:12:10,690
They explain it by saying that the main reason why this

245
00:12:10,690 --> 00:12:14,365
happens is because when you increase the beam size too much,

246
00:12:14,365 --> 00:12:18,370
then you end up producing translations that are too short.

247
00:12:18,370 --> 00:12:22,720
So I mean, that kind of explains it to a degree that translations are too short,

248
00:12:22,720 --> 00:12:24,130
therefore they have low BLEU because they're

249
00:12:24,130 --> 00:12:26,230
probably missing words that they should contain.

250
00:12:26,230 --> 00:12:29,860
But the question is, why does large beam size gives you short translations?

251
00:12:29,860 --> 00:12:31,210
I think that's harder to answer.

252
00:12:31,210 --> 00:12:34,975
Wherever, in these two papers, I didn't see an explicit explanation of why.

253
00:12:34,975 --> 00:12:37,920
Um, I think it's possible larger kind of passing,

254
00:12:37,920 --> 00:12:41,565
we see sometimes with beam search which is when you really increase your, uh,

255
00:12:41,565 --> 00:12:43,440
search space and make the search much more

256
00:12:43,440 --> 00:12:46,620
powerful so that it can consider lots of different alternatives.

257
00:12:46,620 --> 00:12:49,620
It can end up finding these high probability,

258
00:12:49,620 --> 00:12:53,205
um, sequences which aren't actually the thing that you want.

259
00:12:53,205 --> 00:12:55,260
Sure, they're high probabili- probability

260
00:12:55,260 --> 00:12:57,350
but they're not actually the thing that you wanted.

261
00:12:57,350 --> 00:13:00,550
Um, so another example of that is

262
00:13:00,550 --> 00:13:03,625
that in open-ended tasks like for example chit-chat dialogue

263
00:13:03,625 --> 00:13:04,825
where you're trying to just, um,

264
00:13:04,825 --> 00:13:07,330
say something interesting back to your conversational partner,

265
00:13:07,330 --> 00:13:10,300
if we use a beam search with a large beam size,

266
00:13:10,300 --> 00:13:13,495
we find that that can give you some output that is really generic.

267
00:13:13,495 --> 00:13:16,405
Um, and I'll give you an example here to show you what I mean.

268
00:13:16,405 --> 00:13:20,545
So these are examples from a chit-chat,

269
00:13:20,545 --> 00:13:22,825
uh, dialogue project that I was doing.

270
00:13:22,825 --> 00:13:24,190
So here you've got, uh,

271
00:13:24,190 --> 00:13:28,330
your human chit-chat partner said something like I mostly eat a fresh and raw diet,

272
00:13:28,330 --> 00:13:29,785
so I save on groceries.

273
00:13:29,785 --> 00:13:34,030
And then here's what the chat bot said back depending on the beam size.

274
00:13:34,030 --> 00:13:37,790
I will let you read that.

275
00:13:43,590 --> 00:13:47,350
So I would say that this is fairly characteristic of what you see

276
00:13:47,350 --> 00:13:50,500
happening when you raise and lower the beam size [NOISE].

277
00:13:50,500 --> 00:13:51,955
When you have a low beam size,

278
00:13:51,955 --> 00:13:54,700
um, it might be more kind of on topic.

279
00:13:54,700 --> 00:13:57,580
Like here, we can see that eat healthy, eat healthy,

280
00:13:57,580 --> 00:13:59,710
I am a nurse so I do not eat raw food and so on,

281
00:13:59,710 --> 00:14:02,335
that kind of relates to what the user said,

282
00:14:02,335 --> 00:14:04,150
uh, but it's kind of bad English, right?

283
00:14:04,150 --> 00:14:06,100
There's some repetition and,

284
00:14:06,100 --> 00:14:08,020
uh, it doesn't always make that much sense, right?

285
00:14:08,020 --> 00:14:09,580
Um, [NOISE] but then,

286
00:14:09,580 --> 00:14:10,885
when you raise the beam size,

287
00:14:10,885 --> 00:14:12,250
then it kind of converges to

288
00:14:12,250 --> 00:14:17,185
a safe so-called correct response but it's kind of generic and less relevant, right?

289
00:14:17,185 --> 00:14:19,600
And it's kind of applicable in all scenarios, what do you do for a living.

290
00:14:19,600 --> 00:14:21,970
Um, so the, the,

291
00:14:21,970 --> 00:14:24,160
the particular dataset I was using here is, uh,

292
00:14:24,160 --> 00:14:25,240
one called Persona-Chat,

293
00:14:25,240 --> 00:14:26,440
that I'll tell you more about later.

294
00:14:26,440 --> 00:14:28,240
Um, but it's a,

295
00:14:28,240 --> 00:14:31,315
it's a chit-chat dialog dataset where each,

296
00:14:31,315 --> 00:14:35,575
uh, conv- conversational partner has a persona which is a set of traits.

297
00:14:35,575 --> 00:14:37,600
Um, so the reason it keeps talking about being a nurse,

298
00:14:37,600 --> 00:14:39,160
I think is because it was in the persona.

299
00:14:39,160 --> 00:14:42,340
[NOISE] But the main point here is that, um,

300
00:14:42,340 --> 00:14:45,685
we kind of have an unfortunate trade off with no,

301
00:14:45,685 --> 00:14:48,805
with no Goldilocks zone that's very obvious.

302
00:14:48,805 --> 00:14:50,410
I mean, there's, there's a, yeah,

303
00:14:50,410 --> 00:14:53,290
kind of an unfortunate trade-off between having kind of bad,

304
00:14:53,290 --> 00:14:56,680
bad output, bad English and just having something very boring.

305
00:14:56,680 --> 00:15:00,800
So this is one of the problems that we get with beam, beam search.

306
00:15:01,320 --> 00:15:03,790
Okay. So we've talked about, uh,

307
00:15:03,790 --> 00:15:06,445
greedy decoding and beam search. Yes.

308
00:15:06,445 --> 00:15:13,000
So beam size depending on the [inaudible]

309
00:15:13,000 --> 00:15:14,050
The question is, can we have

310
00:15:14,050 --> 00:15:17,755
an adaptive beam size dependent on the position that you're in?

311
00:15:17,755 --> 00:15:19,060
You mean like in the sequence?

312
00:15:19,060 --> 00:15:26,040
Yeah. That is in [inaudible].

313
00:15:26,040 --> 00:15:29,220
Yeah. I mean, I think I- I might have heard of a research paper that does that?

314
00:15:29,220 --> 00:15:34,885
That adaptively like raises the capacity of the, the hypothesis space.

315
00:15:34,885 --> 00:15:37,135
I mean, it sounds awkward to implement, uh,

316
00:15:37,135 --> 00:15:40,990
because, you know, things fitting into a fixed space in your GPU.

317
00:15:40,990 --> 00:15:42,580
Um, but I think that might be possible,

318
00:15:42,580 --> 00:15:46,225
I suppose you'd would have to learn the criterion on which you increase beam,

319
00:15:46,225 --> 00:15:49,300
beam size, yeah. Seems possible.

320
00:15:49,300 --> 00:15:51,625
Okay. So we've talked about, uh,

321
00:15:51,625 --> 00:15:53,365
beam search and greedy decoding.

322
00:15:53,365 --> 00:15:55,990
So here's a new family of decoding

323
00:15:55,990 --> 00:15:59,095
algorithms which are pretty simple, uh, sampling-based decoding.

324
00:15:59,095 --> 00:16:03,235
So something which I'm calling pure sampling because I didn't know what else to call it.

325
00:16:03,235 --> 00:16:04,855
Um, this is just the,

326
00:16:04,855 --> 00:16:07,360
the simple sampling method that says that on each, uh,

327
00:16:07,360 --> 00:16:08,890
timestep of your decoder t,

328
00:16:08,890 --> 00:16:12,040
you just want to randomly sample from the probability distribution,

329
00:16:12,040 --> 00:16:13,780
uh, to obtain your next word.

330
00:16:13,780 --> 00:16:15,490
So this is very simple.

331
00:16:15,490 --> 00:16:17,335
It's just like greedy decoding.

332
00:16:17,335 --> 00:16:19,285
But instead of taking the top words,

333
00:16:19,285 --> 00:16:22,220
instead just sample from that distribution.

334
00:16:22,350 --> 00:16:28,600
So the reason I call this pure sampling was to differentiate it from top-n sampling.

335
00:16:28,600 --> 00:16:30,640
And again, this is actually usually called top-k

336
00:16:30,640 --> 00:16:33,400
sampling but I already called k the beam size,

337
00:16:33,400 --> 00:16:36,340
and I didn't want to be confusing, so I'm gonna call it top-n sampling for now.

338
00:16:36,340 --> 00:16:38,935
Um, so the idea here is also pretty simple.

339
00:16:38,935 --> 00:16:40,585
On each step t,

340
00:16:40,585 --> 00:16:44,035
you want to randomly sample from your probability distribution but

341
00:16:44,035 --> 00:16:48,265
you're gonna restrict to just the top n most probable words.

342
00:16:48,265 --> 00:16:50,185
So this is saying that it's,

343
00:16:50,185 --> 00:16:51,430
it's like the simple, you know,

344
00:16:51,430 --> 00:16:56,515
pure sampling method but you want to truncate your probability distribution just to be,

345
00:16:56,515 --> 00:16:59,020
you know, the, the top most probable words.

346
00:16:59,020 --> 00:17:03,145
So, uh, the idea here kind of like how beam search, um,

347
00:17:03,145 --> 00:17:06,610
gave you a hyperparameter is kind of go between greedy decoding and,

348
00:17:06,610 --> 00:17:08,935
you know, uh, a very exhaustive search.

349
00:17:08,935 --> 00:17:12,025
In the same way here, you've got a hyperparameter n

350
00:17:12,025 --> 00:17:15,340
which can take you between greedy search and pure sampling.

351
00:17:15,340 --> 00:17:16,630
If you think about this for a moment,

352
00:17:16,630 --> 00:17:19,150
if n is one, then you would truncate it the top one.

353
00:17:19,150 --> 00:17:21,085
So you're just taking arg max which is greedy.

354
00:17:21,085 --> 00:17:22,660
And if n is vocab size,

355
00:17:22,660 --> 00:17:24,085
then you don't truncate it at all.

356
00:17:24,085 --> 00:17:25,510
You're sampling from everything,

357
00:17:25,510 --> 00:17:27,790
that's just the pure sampling method.

358
00:17:27,790 --> 00:17:31,000
So here, um, it should be clear, I hope,

359
00:17:31,000 --> 00:17:33,715
if you think about that if you increase n,

360
00:17:33,715 --> 00:17:36,910
then you're gonna get more diverse and risky output, right?

361
00:17:36,910 --> 00:17:39,235
Because you're, uh, giving it more,

362
00:17:39,235 --> 00:17:42,760
more to choose from and you're going lower into the probability distribution,

363
00:17:42,760 --> 00:17:44,770
going lower into less likely things.

364
00:17:44,770 --> 00:17:46,270
And then, if you decrease n,

365
00:17:46,270 --> 00:17:48,580
then you're gonna get more kind of generic safe output because you're

366
00:17:48,580 --> 00:17:51,890
restricting more to the most high probability options.

367
00:17:53,460 --> 00:17:56,440
So both of these are more efficient than

368
00:17:56,440 --> 00:17:58,630
beam search which I think is something important to note,

369
00:17:58,630 --> 00:18:02,425
uh, because there are no multiple hypotheses to track, right?

370
00:18:02,425 --> 00:18:04,735
Because in beam search, on every step t of the decoder,

371
00:18:04,735 --> 00:18:06,115
you've got k different, you know,

372
00:18:06,115 --> 00:18:08,770
beam size, many hypotheses to track.

373
00:18:08,770 --> 00:18:11,560
Uh, whereas here, at least if you're only generating one sample,

374
00:18:11,560 --> 00:18:12,760
there's only one thing to track.

375
00:18:12,760 --> 00:18:14,440
So it, it's a very simple algorithm.

376
00:18:14,440 --> 00:18:19,310
So that is one advantage of these sampling-based algorithms over beam search.

377
00:18:21,200 --> 00:18:25,560
Okay. So, the last thing I want to tell you that's kind of related to decoding is,

378
00:18:25,560 --> 00:18:27,165
uh, softmax [NOISE] temperature.

379
00:18:27,165 --> 00:18:30,930
So, if you recall on timestep t of your decoder,

380
00:18:30,930 --> 00:18:34,590
your language model computes some kind of probability distribution P_t, uh,

381
00:18:34,590 --> 00:18:39,030
by applying the softmax function to a vector of scores that you got from somewhere.

382
00:18:39,030 --> 00:18:42,735
Like from your transformer or from your RNN or something.

383
00:18:42,735 --> 00:18:44,670
So, there's the softmax function again.

384
00:18:44,670 --> 00:18:47,580
It's saying that the probability of a word W is this softmax function,

385
00:18:47,580 --> 00:18:50,115
uh, given, given the scores.

386
00:18:50,115 --> 00:18:55,080
So, the idea here of a temperature on the softmax is that you have some kind of

387
00:18:55,080 --> 00:19:01,200
temperature hyperparameter tau and you're going to apply that to this, uh, softmax.

388
00:19:01,200 --> 00:19:04,920
So, all that we're doing is we're div- dividing all of the scores,

389
00:19:04,920 --> 00:19:06,375
or logits you might call them,

390
00:19:06,375 --> 00:19:08,565
by the temperature hyperparameter.

391
00:19:08,565 --> 00:19:10,545
So again, if you just think about this a little bit,

392
00:19:10,545 --> 00:19:12,570
you'll see that raising the temperature,

393
00:19:12,570 --> 00:19:13,800
that is increasing, uh,

394
00:19:13,800 --> 00:19:19,935
the hyperparameter, this is going to make your probability distribution more uniform.

395
00:19:19,935 --> 00:19:23,415
And this kind of comes down to the question about when you,

396
00:19:23,415 --> 00:19:25,830
when you multiply all of your scores by a constant,

397
00:19:25,830 --> 00:19:28,980
um, how does that affect the softmax, right?

398
00:19:28,980 --> 00:19:33,885
So, do things get more far apart or less far apart once you take the exponential?

399
00:19:33,885 --> 00:19:36,690
So, this is something you can just work up by yourself on paper,

400
00:19:36,690 --> 00:19:38,520
but as a, uh,

401
00:19:38,520 --> 00:19:40,125
a kind of a memory shortcut,

402
00:19:40,125 --> 00:19:43,500
a good way to think about it is that if you raise the temperature,

403
00:19:43,500 --> 00:19:47,490
then the distribution kind of melts and goes soft and mushy and uniform.

404
00:19:47,490 --> 00:19:48,810
And if you, uh,

405
00:19:48,810 --> 00:19:51,150
lower the temperature, like make it cold then,

406
00:19:51,150 --> 00:19:54,690
the probability distribution becomes more spiky, right?

407
00:19:54,690 --> 00:19:59,115
So, like the things which are rated as high probability become like even more,

408
00:19:59,115 --> 00:20:02,670
uh, disproportionately high probability compared to the other things.

409
00:20:02,670 --> 00:20:05,535
Um, I think that's a easy way to remember it.

410
00:20:05,535 --> 00:20:07,935
Today I had to work it out on paper and then, uh,

411
00:20:07,935 --> 00:20:09,135
I realized that just the, the,

412
00:20:09,135 --> 00:20:12,375
the temperature visualization thing usually gets me there quicker.

413
00:20:12,375 --> 00:20:18,480
So, um, one thing I want to note is that softmax temperature is not a decoding algorithm.

414
00:20:18,480 --> 00:20:21,120
I know that I put it in the decoding algorithm section,

415
00:20:21,120 --> 00:20:23,715
uh, that was just because it's kind of a thing, a

416
00:20:23,715 --> 00:20:29,880
simple thing that you can do at test time to change how the decoding happens, right?

417
00:20:29,880 --> 00:20:31,320
You don't need to train, uh,

418
00:20:31,320 --> 00:20:33,765
with the, the softmax temperature.

419
00:20:33,765 --> 00:20:36,225
So, it's not a decoding algorithm itself.

420
00:20:36,225 --> 00:20:38,415
It's a technique that you can apply at test time

421
00:20:38,415 --> 00:20:41,040
in conjunction with a decoding algorithm.

422
00:20:41,040 --> 00:20:44,385
So, for example, if you're doing beam search or you're doing some kind of sampling,

423
00:20:44,385 --> 00:20:48,615
then you can also apply a softmax temperature, um, to change,

424
00:20:48,615 --> 00:20:54,160
you know, this kind of risky versus safe, um, trade-off.

425
00:20:55,220 --> 00:21:03,060
Any questions on this? Okay. So, here's

426
00:21:03,060 --> 00:21:06,270
a summary of what we just learned about decoding algorithms.

427
00:21:06,270 --> 00:21:09,375
Um, Greedy decoding is a simple method.

428
00:21:09,375 --> 00:21:14,265
It gives kind of low quality output in comparison to the others, at least beam search.

429
00:21:14,265 --> 00:21:17,160
Beam search, especially when you've got a high beam size, uh,

430
00:21:17,160 --> 00:21:20,955
it searches through lots of different hypotheses for high-probability outputs.

431
00:21:20,955 --> 00:21:24,120
And this generally is gonna deliver better quality than greedy search, uh,

432
00:21:24,120 --> 00:21:26,730
but if the beam size is too high, then you can have these,

433
00:21:26,730 --> 00:21:29,385
uh, kind of counter-intuitive problems we talked about before.

434
00:21:29,385 --> 00:21:32,865
Where you've retrieved some kind of high-probability but unsuitable output.

435
00:21:32,865 --> 00:21:35,415
Say, something is too generic or something is too short.

436
00:21:35,415 --> 00:21:37,290
And we're gonna talk about that more later.

437
00:21:37,290 --> 00:21:41,220
Uh, sampling methods are a way to get more diversity,

438
00:21:41,220 --> 00:21:43,095
uh, via, via randomness.

439
00:21:43,095 --> 00:21:46,380
Uh, well, getting randomness might be your goal in itself.

440
00:21:46,380 --> 00:21:49,485
Um, so, this is good if you want to have some kind of, for example,

441
00:21:49,485 --> 00:21:51,930
open-ended or creative generation setting like,

442
00:21:51,930 --> 00:21:53,910
uh, generating poetry or stories,

443
00:21:53,910 --> 00:21:56,370
then sampling is probably a better idea than

444
00:21:56,370 --> 00:21:59,700
beam search because you want to have a kind of source of randomness to,

445
00:21:59,700 --> 00:22:02,160
uh, write different things creatively.

446
00:22:02,160 --> 00:22:07,170
And top-n sampling allows you to control the diversity by,

447
00:22:07,170 --> 00:22:09,330
uh, changing n. And then lastly,

448
00:22:09,330 --> 00:22:11,610
softmax temperature is another way to control diversity.

449
00:22:11,610 --> 00:22:14,520
So there's quite a few different knobs you can turn here.

450
00:22:14,520 --> 00:22:16,260
And it's not a decoding algorithm,

451
00:22:16,260 --> 00:22:20,190
it's just a technique that you can apply alongside any decoding algorithm.

452
00:22:20,190 --> 00:22:22,830
Although it wouldn't make sense to apply it with

453
00:22:22,830 --> 00:22:26,370
greedy decoding because even if you make it more spiky or more flat,

454
00:22:26,370 --> 00:22:29,950
the argmax is still the argmax, so it doesn't make sense.

455
00:22:31,400 --> 00:22:34,350
Okay. Cool. I'm going to move on to section two.

456
00:22:34,350 --> 00:22:39,135
So, uh, section two is NLG tasks and neural approaches to them.

457
00:22:39,135 --> 00:22:42,330
Uh, as mentioned before, this is not going to be an overview of all of NLG.

458
00:22:42,330 --> 00:22:43,620
That will be quite impossible.

459
00:22:43,620 --> 00:22:45,195
This is gonna be some selected highlights.

460
00:22:45,195 --> 00:22:47,490
So, in particular, I'm gonna start off with

461
00:22:47,490 --> 00:22:51,270
a fairly deep dive into a particular NLG task that I'm a bit more familiar with,

462
00:22:51,270 --> 00:22:53,250
and that is, uh, summarization.

463
00:22:53,250 --> 00:22:57,660
So, let's start off with a task definition for summarization.

464
00:22:57,660 --> 00:23:02,325
Um, one sensible definition would be: Given some kind of input text x,

465
00:23:02,325 --> 00:23:04,890
you want to write a summary y which is shorter than

466
00:23:04,890 --> 00:23:07,825
x and contains the main information of x.

467
00:23:07,825 --> 00:23:11,360
So, summarization can be single-document or multi-document.

468
00:23:11,360 --> 00:23:16,510
Uh, single-document means that you just have a summary y of a single document x.

469
00:23:16,510 --> 00:23:20,040
In multi-document summarization, you're saying that you want to write

470
00:23:20,040 --> 00:23:24,390
a single summary y of multiple documents x_1 up to x_n.

471
00:23:24,390 --> 00:23:28,980
And here typically x_1 up to x_n will have some kind of overlapping content.

472
00:23:28,980 --> 00:23:32,040
So, for example, they might all be different news articles

473
00:23:32,040 --> 00:23:35,220
from different newspapers about the same event, right?

474
00:23:35,220 --> 00:23:39,030
Because it kind of makes sense to write a single summary that draws from all of those.

475
00:23:39,030 --> 00:23:45,010
Um, makes less sense to summarize things that are about different topics.

476
00:23:45,920 --> 00:23:48,015
There is further, uh,

477
00:23:48,015 --> 00:23:51,270
subdivision of, uh, task definitions in, in summarization.

478
00:23:51,270 --> 00:23:53,835
So, I'm gonna describe it via some datasets.

479
00:23:53,835 --> 00:23:58,455
Uh, here are some different really common datasets especially in, uh,

480
00:23:58,455 --> 00:24:01,800
neural summarization, um, and they kind of correspond to different,

481
00:24:01,800 --> 00:24:04,035
like, lengths and different styles of text.

482
00:24:04,035 --> 00:24:05,430
So, a common one is,

483
00:24:05,430 --> 00:24:07,050
uh, the Gigaword dataset.

484
00:24:07,050 --> 00:24:09,360
And the task here is that you want to map from

485
00:24:09,360 --> 00:24:13,710
the first one or two sentences of a news article to write the headline.

486
00:24:13,710 --> 00:24:16,290
[NOISE] And you could think of this as sentence compression,

487
00:24:16,290 --> 00:24:19,140
especially if it's kind of one sentence to headline because you're going from

488
00:24:19,140 --> 00:24:22,710
a longish sentence to a shortish headline style sentence.

489
00:24:22,710 --> 00:24:26,955
Uh, next one that I, um,

490
00:24:26,955 --> 00:24:29,130
wanted to tell you about is this, uh,

491
00:24:29,130 --> 00:24:31,320
it's a Chinese summarization dataset but I,

492
00:24:31,320 --> 00:24:33,690
I see people using it a lot.

493
00:24:33,690 --> 00:24:36,480
And it's, uh, from a micro-blogging,

494
00:24:36,480 --> 00:24:39,945
um, website where people write summaries of their posts.

495
00:24:39,945 --> 00:24:42,270
So, the actual summarization task is

496
00:24:42,270 --> 00:24:44,790
you've got some paragraph of text and then you want to,

497
00:24:44,790 --> 00:24:46,230
uh, summarize that into,

498
00:24:46,230 --> 00:24:48,180
I think, a single sentence summary.

499
00:24:48,180 --> 00:24:51,120
Uh, another one, uh, two actually,

500
00:24:51,120 --> 00:24:55,650
are the New York Times and CNN/Daily Mail, uh, datasets.

501
00:24:55,650 --> 00:24:57,180
So, these ones are both of the form,

502
00:24:57,180 --> 00:24:59,940
you've got a whole news article which is actually pretty long like

503
00:24:59,940 --> 00:25:03,690
hun-hundreds of words and then you want to summarize that into,

504
00:25:03,690 --> 00:25:06,840
uh, like, maybe a single-sentence or multi-sentence summary.

505
00:25:06,840 --> 00:25:10,560
Uh, The New York Times ones are written by, I think, uh,

506
00:25:10,560 --> 00:25:13,125
librarians or people who, who,

507
00:25:13,125 --> 00:25:16,440
um, write summaries for, for library purposes.

508
00:25:16,440 --> 00:25:18,885
Uh, and then, uh,

509
00:25:18,885 --> 00:25:22,365
one I just spotted today when I was writing this list is there's a new,

510
00:25:22,365 --> 00:25:25,845
fairly new like last six months dataset from wikiHow.

511
00:25:25,845 --> 00:25:27,840
So, from what I can tell this seems to be,

512
00:25:27,840 --> 00:25:31,950
you've got a full how-to-article from wikiHow and then you want to boil this down to

513
00:25:31,950 --> 00:25:34,200
the summary sentences which are kind of cleverly

514
00:25:34,200 --> 00:25:37,185
extracted from throughout the wikiHow article.

515
00:25:37,185 --> 00:25:38,790
They are kind of like headings.

516
00:25:38,790 --> 00:25:42,390
So, um, I looked at this paper and it seems that, um,

517
00:25:42,390 --> 00:25:45,840
this is kind of interesting because it's a different type of text.

518
00:25:45,840 --> 00:25:48,990
As you might have noticed most of the other ones are news-based and this is,

519
00:25:48,990 --> 00:25:51,825
uh, not, so that kind of poses different challenges.

520
00:25:51,825 --> 00:25:57,360
Uh, another kind of division of summarization is sentence simplification.

521
00:25:57,360 --> 00:26:00,690
So, this is a related but actually different task.

522
00:26:00,690 --> 00:26:04,410
In summarization, you want to write something which is shorter and contains

523
00:26:04,410 --> 00:26:08,220
main information but is still maybe written in just as complex language,

524
00:26:08,220 --> 00:26:13,785
whereas in sentence simplification you want to rewrite the source text using simpler,

525
00:26:13,785 --> 00:26:15,420
uh, simpler language, right?

526
00:26:15,420 --> 00:26:18,765
So, like simpler word choices and simpler sentence structure.

527
00:26:18,765 --> 00:26:21,240
That might mean it's shorter but not necessarily.

528
00:26:21,240 --> 00:26:22,890
So, for example, uh,

529
00:26:22,890 --> 00:26:25,950
simple Wiki- Wikipedia is a standard dataset for this.

530
00:26:25,950 --> 00:26:28,470
And the idea is you've got, um, you know,

531
00:26:28,470 --> 00:26:31,440
standard Wikipedia and you've got a simple Wikipedia version.

532
00:26:31,440 --> 00:26:32,550
And they mostly align up,

533
00:26:32,550 --> 00:26:33,960
so you want to map from

534
00:26:33,960 --> 00:26:37,365
some sentence in one to the equivalent sentence in the [NOISE] other.

535
00:26:37,365 --> 00:26:41,880
Another source of data for this is Newsela which is a website that,

536
00:26:41,880 --> 00:26:44,085
uh, rewrites news for children.

537
00:26:44,085 --> 00:26:46,320
Actually, at different learning levels I think.

538
00:26:46,320 --> 00:26:49,900
So, you have multiple options for how much it's simplified.

539
00:26:50,180 --> 00:26:54,690
Okay. So, um, so

540
00:26:54,690 --> 00:26:59,085
that's the definition or the many definitions of summarization as different tasks.

541
00:26:59,085 --> 00:27:00,810
So, now I'm gonna give an overview of, like,

542
00:27:00,810 --> 00:27:02,190
what are the main, uh,

543
00:27:02,190 --> 00:27:04,095
techniques for doing summarization.

544
00:27:04,095 --> 00:27:06,390
So, there's two main strategies for summarization.

545
00:27:06,390 --> 00:27:10,560
Uh, you can call them extractive summarization and abstractive summarization.

546
00:27:10,560 --> 00:27:12,735
And the main idea as I had hinted out earlier,

547
00:27:12,735 --> 00:27:15,720
is that in extractive summarization you're just selecting

548
00:27:15,720 --> 00:27:19,050
parts of the original texts to form a summary.

549
00:27:19,050 --> 00:27:22,770
And often this will be whole sentences but maybe it'll be more granular than that;

550
00:27:22,770 --> 00:27:24,825
maybe, uh, phrases or words.

551
00:27:24,825 --> 00:27:27,360
Whereas abstractive summarization, you're going to be

552
00:27:27,360 --> 00:27:31,275
generating some new text using NLG techniques.

553
00:27:31,275 --> 00:27:33,840
So the idea is that it's, you know, generation from scratch.

554
00:27:33,840 --> 00:27:37,590
And my visual metaphor for this is this kind of like the difference between highlighting

555
00:27:37,590 --> 00:27:42,370
the parts with a highlighter or writing the summary yourself with a pen.

556
00:27:43,100 --> 00:27:47,160
I think the high level things to know about these two techniques are that

557
00:27:47,160 --> 00:27:50,610
extractive summarization is basically easier,

558
00:27:50,610 --> 00:27:52,725
at least to make a decent system to start,

559
00:27:52,725 --> 00:27:57,120
because selecting things is probably easier than writing text from scratch.

560
00:27:57,120 --> 00:28:00,945
Um, but extractive summarization is pretty restrictive, right?

561
00:28:00,945 --> 00:28:02,760
Because you can't really paraphrase anything,

562
00:28:02,760 --> 00:28:05,430
you can't really do any powerful sentence compression

563
00:28:05,430 --> 00:28:08,475
if you can only just select sentences.

564
00:28:08,475 --> 00:28:12,195
Um, and, of course, abstractive summarization as a paradigm

565
00:28:12,195 --> 00:28:15,645
is more flexible and it's more how humans might summarize,

566
00:28:15,645 --> 00:28:18,150
uh, but as noted it's pretty difficult.

567
00:28:18,150 --> 00:28:23,835
So, I'm gonna give you a very quick view of what pre-neural summarization looks like.

568
00:28:23,835 --> 00:28:24,945
And here we've got, uh,

569
00:28:24,945 --> 00:28:26,700
this is a diagram from the, uh,

570
00:28:26,700 --> 00:28:28,800
Speech and Language Processing book.

571
00:28:28,800 --> 00:28:33,120
So, uh, pre-neural summarization systems were mostly extractive.

572
00:28:33,120 --> 00:28:35,370
And like pre-neural NMT,

573
00:28:35,370 --> 00:28:37,125
which we learnt about in the NMT lecture,

574
00:28:37,125 --> 00:28:40,395
it typically had a pipeline which is what this picture is showing.

575
00:28:40,395 --> 00:28:43,065
So, a typical pipeline might have three parts.

576
00:28:43,065 --> 00:28:46,170
First, you have content selection which is, uh,

577
00:28:46,170 --> 00:28:49,785
essentially choosing some of the sentences from the source document to include.

578
00:28:49,785 --> 00:28:52,155
And then secondly, you're going to do some kind of information

579
00:28:52,155 --> 00:28:56,050
ordering which means choosing what order should I put these sentences in.

580
00:28:56,050 --> 00:28:59,750
And this is particularly a more nontrivial question if you were

581
00:28:59,750 --> 00:29:01,580
doing multiple document summarization

582
00:29:01,580 --> 00:29:03,560
because your sentences might come from different documents.

583
00:29:03,560 --> 00:29:05,060
Uh, and then lastly,

584
00:29:05,060 --> 00:29:08,255
you're going to do a sentence realization that is actually, um,

585
00:29:08,255 --> 00:29:12,135
turning your selected sentences into your actual summary.

586
00:29:12,135 --> 00:29:13,680
So, although we're not doing, kind of,

587
00:29:13,680 --> 00:29:15,825
free-form text generation, uh,

588
00:29:15,825 --> 00:29:19,290
there might be some kind of editing for example like, uh, simplifying, editing,

589
00:29:19,290 --> 00:29:21,885
or removing parts that are redundant,

590
00:29:21,885 --> 00:29:23,865
or fixing continuity issues.

591
00:29:23,865 --> 00:29:26,220
So for example, you can't refer to

592
00:29:26,220 --> 00:29:28,920
a person as she if you never introduced them in the first place.

593
00:29:28,920 --> 00:29:32,380
So maybe you need to change that she to the name of the person.

594
00:29:33,180 --> 00:29:35,890
So in particular [NOISE] uh,

595
00:29:35,890 --> 00:29:37,945
these pre-neural summarization systems, uh,

596
00:29:37,945 --> 00:29:41,230
have some pretty sophisticated algorithms of content selection.

597
00:29:41,230 --> 00:29:43,450
Um, so, for example,

598
00:29:43,450 --> 00:29:46,240
uh, you would have some sentence scoring functions.

599
00:29:46,240 --> 00:29:48,145
This is the most simple, uh, way you might do it,

600
00:29:48,145 --> 00:29:50,770
is you might score all of the sentences individually

601
00:29:50,770 --> 00:29:53,620
and you could score them based on features such as,

602
00:29:53,620 --> 00:29:56,650
um, are there, you know, topic keywords in the sentence?

603
00:29:56,650 --> 00:29:59,380
If so, maybe it's an important sentence that we should include.

604
00:29:59,380 --> 00:30:02,725
Um, and you could compute those, uh,

605
00:30:02,725 --> 00:30:06,760
keywords using, uh, statistics such as tf-idf for example.

606
00:30:06,760 --> 00:30:10,960
[NOISE] You can also use pretty basic but powerful features such as,

607
00:30:10,960 --> 00:30:12,925
uh, where does the sentence appear in the document?

608
00:30:12,925 --> 00:30:14,260
If it's near the top of the document,

609
00:30:14,260 --> 00:30:16,510
then it's more likely to be important.

610
00:30:16,510 --> 00:30:18,100
Uh, there are also

611
00:30:18,100 --> 00:30:21,910
some more complex content selection algorithms such as for example, uh,

612
00:30:21,910 --> 00:30:25,420
there are these graph-based algorithms which kind of view the document as

613
00:30:25,420 --> 00:30:29,005
a set of sentences and those sentences are the nodes of the graph,

614
00:30:29,005 --> 00:30:30,760
and you imagine that all sentences, er,

615
00:30:30,760 --> 00:30:33,190
sentence pairs have an edge between them,

616
00:30:33,190 --> 00:30:36,760
and the weight of the edge is kind of how similar the sentences are.

617
00:30:36,760 --> 00:30:39,925
So, then, if you think about the graph in that sense,

618
00:30:39,925 --> 00:30:43,600
then now you can try to identify which sentences are

619
00:30:43,600 --> 00:30:47,500
important by finding which sentences are central in the graph.

620
00:30:47,500 --> 00:30:49,540
So you can apply some kind of general purpose

621
00:30:49,540 --> 00:30:52,930
gla- graph algorithms to figure out which [NOISE] nodes are central,

622
00:30:52,930 --> 00:30:56,180
and this is a way to find central sentences.

623
00:30:56,340 --> 00:31:03,355
Okay. So um, [NOISE] back to summarization as a task.

624
00:31:03,355 --> 00:31:06,940
Um, we've, I can't remember if we've talked about ROUGE already.

625
00:31:06,940 --> 00:31:08,140
We've certainly talked about BLEU.

626
00:31:08,140 --> 00:31:09,820
But I'm gonna tell you about ROUGE now which is

627
00:31:09,820 --> 00:31:12,400
the main automatic metric for summarization.

628
00:31:12,400 --> 00:31:17,695
So ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation.

629
00:31:17,695 --> 00:31:19,480
I'm not sure if that was the first thing they came up with

630
00:31:19,480 --> 00:31:21,790
or if they made it like that to match BLEU.

631
00:31:21,790 --> 00:31:24,610
Um, and here's the,

632
00:31:24,610 --> 00:31:26,050
here's the equation, uh,

633
00:31:26,050 --> 00:31:28,855
for, well, I suppose one of the ROUGE metrics.

634
00:31:28,855 --> 00:31:31,210
I'll tell you more about what that means later and you can

635
00:31:31,210 --> 00:31:34,105
read more in the original paper which is linked at the bottom.

636
00:31:34,105 --> 00:31:38,095
So, uh, the overall idea is that ROUGE is actually pretty similar to BLEU.

637
00:31:38,095 --> 00:31:40,015
It's based on n-gram overlap.

638
00:31:40,015 --> 00:31:45,655
So, some main differences with BLEU are ROUGE doesn't have a brevity penalty.

639
00:31:45,655 --> 00:31:47,230
Um, I'll talk more about that in a minute.

640
00:31:47,230 --> 00:31:52,195
Uh, the other big one is that ROUGE is based on recall while BLEU is based on precision.

641
00:31:52,195 --> 00:31:53,440
So you can see it's there in the title.

642
00:31:53,440 --> 00:31:57,115
[NOISE] Um, so, if you think about this a little bit,

643
00:31:57,115 --> 00:32:02,245
I think you can say arguably precision is more important for machine translation.

644
00:32:02,245 --> 00:32:09,130
That is, you only want to generate text that appears in one of your reference, uh,

645
00:32:09,130 --> 00:32:12,520
translations, and then to avoid taking

646
00:32:12,520 --> 00:32:14,770
a really conservative strategy where you only generate

647
00:32:14,770 --> 00:32:17,545
really safe things in a really short translation.

648
00:32:17,545 --> 00:32:20,035
That's why you add the brevity penalty to make sure

649
00:32:20,035 --> 00:32:23,035
that [NOISE] it tries to write something long enough.

650
00:32:23,035 --> 00:32:24,640
And then by contrast,

651
00:32:24,640 --> 00:32:26,290
recall is more important for

652
00:32:26,290 --> 00:32:30,265
summarization because you want to include all the information,

653
00:32:30,265 --> 00:32:33,190
the info- the important information in your summary, right?

654
00:32:33,190 --> 00:32:36,490
So the information that's in the reference summary is,

655
00:32:36,490 --> 00:32:38,080
uh, assumed to be the important information.

656
00:32:38,080 --> 00:32:40,240
So recall means that you captured all of that.

657
00:32:40,240 --> 00:32:42,460
Um, and I suppose i- if you assume that you have

658
00:32:42,460 --> 00:32:45,040
a maximum length constraint for your summarization system,

659
00:32:45,040 --> 00:32:47,950
then those two kind of give a trade-off, right?

660
00:32:47,950 --> 00:32:52,720
Where you want to include all the information but you can't be too long as a summary.

661
00:32:52,720 --> 00:32:55,435
So I think that's the kind of justification why you have

662
00:32:55,435 --> 00:32:58,150
recall and precision for these two different tasks.

663
00:32:58,150 --> 00:33:01,480
However, confusingly, often an F1,

664
00:33:01,480 --> 00:33:03,910
that is combination of precision and recall version of

665
00:33:03,910 --> 00:33:06,940
ROUGE is reported anyway in the summarization literature.

666
00:33:06,940 --> 00:33:09,490
And to be honest, I'm not entirely sure why this is, uh,

667
00:33:09,490 --> 00:33:11,140
maybe it's because of the lack of,

668
00:33:11,140 --> 00:33:13,495
uh, explicit max length constraint.

669
00:33:13,495 --> 00:33:17,815
Um, anyway, I, I tried to search that but I couldn't find an answer.

670
00:33:17,815 --> 00:33:21,100
So here's some more information on ROUGE.

671
00:33:21,100 --> 00:33:22,840
Um, if you remember,

672
00:33:22,840 --> 00:33:24,940
BLEU is reported as a single number, right?

673
00:33:24,940 --> 00:33:26,980
BLEU is just a single number and it is

674
00:33:26,980 --> 00:33:30,640
a combination of the precisions for the different n-grams

675
00:33:30,640 --> 00:33:32,950
which is usually 1-4 whereas

676
00:33:32,950 --> 00:33:36,910
ROUGE scores are usually reported separately for each n-gram.

677
00:33:36,910 --> 00:33:42,250
So, the most commonly reported ROUGE scores are ROUGE-1, ROUGE-2 and ROUGE-L.

678
00:33:42,250 --> 00:33:47,365
So, ROUGE one, not to be confused with Rogue One: A Star Wars Story.

679
00:33:47,365 --> 00:33:49,060
Um, I feel like since that film came out,

680
00:33:49,060 --> 00:33:51,610
I see so many people mistyping this, and I think it's related.

681
00:33:51,610 --> 00:33:54,730
Um, so, ROUGE-1 is, uh,

682
00:33:54,730 --> 00:33:57,295
based on unigram overlap,

683
00:33:57,295 --> 00:34:01,015
um, [NOISE] and ROUGE-2 based on bigram overlap.

684
00:34:01,015 --> 00:34:03,310
It's kind of an analogy to BLEU really except,

685
00:34:03,310 --> 00:34:05,245
uh, recall-based, not precision-based.

686
00:34:05,245 --> 00:34:10,450
The more interesting one is ROUGE-L which is longest common subsequence overlap.

687
00:34:10,450 --> 00:34:14,590
Um, so, the idea here is that you are interested not only in, uh,

688
00:34:14,590 --> 00:34:16,855
particular n-grams matching up but in,

689
00:34:16,855 --> 00:34:18,310
you know, how many, uh, how,

690
00:34:18,310 --> 00:34:23,240
how long a sequence of words can you find that appear in both.

691
00:34:23,520 --> 00:34:26,635
So you can, uh, read more about these metrics

692
00:34:26,635 --> 00:34:29,065
in the paper that was linked on the previous page.

693
00:34:29,065 --> 00:34:31,495
And another really important thing to note is there's [NOISE] now

694
00:34:31,495 --> 00:34:35,200
a convenient Python implementation of ROUGE, and um,

695
00:34:35,200 --> 00:34:38,155
maybe it is not apparent why that's exciting,

696
00:34:38,155 --> 00:34:40,420
but it's actually pretty exciting because for a long time,

697
00:34:40,420 --> 00:34:42,480
there was just this Perl script, um,

698
00:34:42,480 --> 00:34:46,365
that was quite hard to run and quite hard to set up and understand.

699
00:34:46,365 --> 00:34:49,440
So um, someone out there has been a hero and has, uh,

700
00:34:49,440 --> 00:34:52,290
implemented a pure Python version of ROUGE and checked that it

701
00:34:52,290 --> 00:34:55,320
really does match up to the Perl script that people were using before.

702
00:34:55,320 --> 00:34:58,890
So if any of you are using ROUGE or doing summarization for your projects, uh,

703
00:34:58,890 --> 00:35:00,075
make sure that you, uh,

704
00:35:00,075 --> 00:35:02,530
go use that because it will probably save you some time.

705
00:35:02,530 --> 00:35:06,085
[NOISE] Okay.

706
00:35:06,085 --> 00:35:08,020
So we're gonna re- return to ROUGE a little bit later.

707
00:35:08,020 --> 00:35:10,210
Um, I know that in assignment 4 you thought about

708
00:35:10,210 --> 00:35:12,790
the shortcomings of BLEU as a metric and um,

709
00:35:12,790 --> 00:35:16,555
for sure ROUGE has some short- shortcomings as well as a metric for summarization.

710
00:35:16,555 --> 00:35:18,920
Um, we're gonna come back to that later.

711
00:35:19,080 --> 00:35:23,230
Okay. So, we're gonna move on to neural approaches for summarization.

712
00:35:23,230 --> 00:35:27,969
[NOISE] So uh, going back to 2015,

713
00:35:27,969 --> 00:35:30,310
I don't have another dramatic reenactment, I'm afraid.

714
00:35:30,310 --> 00:35:32,710
[NOISE] Um, Rush et al.

715
00:35:32,710 --> 00:35:35,590
published the first seq2seq summarization paper.

716
00:35:35,590 --> 00:35:39,070
[NOISE] So uh, they were viewing this as,

717
00:35:39,070 --> 00:35:41,395
you know, NMT has recently been super successful,

718
00:35:41,395 --> 00:35:44,500
why don't we view abstractive summarization as a translation task and

719
00:35:44,500 --> 00:35:48,565
therefore apply standard translation seq2seq methods to it.

720
00:35:48,565 --> 00:35:51,910
So that's exactly what they did and they applied,

721
00:35:51,910 --> 00:35:53,500
uh, a standard attention model,

722
00:35:53,500 --> 00:35:58,000
and then they did a pretty good job at, uh, Gigaword summarization.

723
00:35:58,000 --> 00:35:59,620
That's the one where you're, um,

724
00:35:59,620 --> 00:36:03,130
converting from the first sentence of the news article to the headline.

725
00:36:03,130 --> 00:36:05,575
So it's kind of like, uh, sentence compression.

726
00:36:05,575 --> 00:36:10,570
So crucially, this is kind of the same order of magnitude of length as NMT, right?

727
00:36:10,570 --> 00:36:13,810
Because NMT is sentence to sentence and this is kind of sentence to sentence,

728
00:36:13,810 --> 00:36:15,805
maybe at most two sentence two sentence.

729
00:36:15,805 --> 00:36:18,310
So this works pretty well and you can get pretty decent, um,

730
00:36:18,310 --> 00:36:20,920
headline generation or sentence compression using this kind of method.

731
00:36:20,920 --> 00:36:23,515
[NOISE] Okay.

732
00:36:23,515 --> 00:36:25,510
So after that, since 2015,

733
00:36:25,510 --> 00:36:29,380
there have been lots more developments in neural abstractive summarization.

734
00:36:29,380 --> 00:36:31,435
And you can kind of um,

735
00:36:31,435 --> 00:36:33,865
group together these developments in,

736
00:36:33,865 --> 00:36:35,440
uh, a collection of themes.

737
00:36:35,440 --> 00:36:38,020
So one theme is make it easier to copy.

738
00:36:38,020 --> 00:36:41,050
Uh, this seems pretty obvious because in summarization, you know,

739
00:36:41,050 --> 00:36:44,035
you're gonna want to copy every, quite a few words and even phrases,

740
00:36:44,035 --> 00:36:46,615
but don't copy too much.

741
00:36:46,615 --> 00:36:48,130
Uh, that's the other thing is that if you make it

742
00:36:48,130 --> 00:36:49,630
too easy to copy, then you copy too much.

743
00:36:49,630 --> 00:36:52,600
So, then there's other research showing how to prevent too much copying.

744
00:36:52,600 --> 00:36:58,135
[NOISE] Uh, the next thing is some kind of hierarchical or multi-level attention.

745
00:36:58,135 --> 00:36:59,470
So as I just showed,

746
00:36:59,470 --> 00:37:01,690
the attention has been pretty key to, um,

747
00:37:01,690 --> 00:37:04,000
abstractive neural summarization so far.

748
00:37:04,000 --> 00:37:05,605
So there's been some work looking at, you know,

749
00:37:05,605 --> 00:37:08,890
can we kind of make this attention work at a more kind of high-level,

750
00:37:08,890 --> 00:37:12,100
low-level cost fine version so

751
00:37:12,100 --> 00:37:16,030
that we can kind of maybe do our selection at the high-level and at low-level.

752
00:37:16,030 --> 00:37:18,985
Another thing which is kind of related is having

753
00:37:18,985 --> 00:37:21,700
some more kind of global content selection.

754
00:37:21,700 --> 00:37:23,515
So if you remember when we were talking about the,

755
00:37:23,515 --> 00:37:26,020
the pipelines pre-neural summarization,

756
00:37:26,020 --> 00:37:28,435
they had these different content selection algorithms.

757
00:37:28,435 --> 00:37:30,250
And I think you can say that,

758
00:37:30,250 --> 00:37:32,110
um, kind of naive attention,

759
00:37:32,110 --> 00:37:34,630
attention seq2seq is not necessarily

760
00:37:34,630 --> 00:37:37,495
the best way to do content selection for summarization,

761
00:37:37,495 --> 00:37:40,885
maybe you want a more kind of global strategy where you choose what's important.

762
00:37:40,885 --> 00:37:44,049
It's not so apparent here when you're doing this small-scale summarization,

763
00:37:44,049 --> 00:37:45,430
but if you imagine that you're summarizing

764
00:37:45,430 --> 00:37:48,294
a whole news article and you're choosing which information,

765
00:37:48,294 --> 00:37:50,455
kind of deciding on each decoder step,

766
00:37:50,455 --> 00:37:53,170
what to choose doesn't seem like the most global strategy.

767
00:37:53,170 --> 00:37:56,005
Er, what else have we got?

768
00:37:56,005 --> 00:37:59,410
Uh, there's using, uh, Reinforcement Learning to directly maximize

769
00:37:59,410 --> 00:38:01,300
ROUGE or other discrete goals you might

770
00:38:01,300 --> 00:38:03,820
care about such as maybe the length of the summary.

771
00:38:03,820 --> 00:38:07,495
Um, and I say discrete here because ROUGE is a non-differentiable,

772
00:38:07,495 --> 00:38:09,640
uh, function of your generated outputs.

773
00:38:09,640 --> 00:38:12,160
There's no, you know, easy way to differentiably

774
00:38:12,160 --> 00:38:14,200
learn that during training in the usual way.

775
00:38:14,200 --> 00:38:20,170
Uh, my last point on this list is the kind of theme of

776
00:38:20,170 --> 00:38:24,040
resurrecting pre-neural ideas such as those graph algorithms that I mentioned

777
00:38:24,040 --> 00:38:25,960
earlier and working them into

778
00:38:25,960 --> 00:38:32,005
these new seq2seq abstractive neural systems and I'm sure there is more as well.

779
00:38:32,005 --> 00:38:34,150
So, I'm gonna show you a few of these, um,

780
00:38:34,150 --> 00:38:37,660
especially because even if you're not particularly interested in summarization,

781
00:38:37,660 --> 00:38:40,930
a lot of the ideas that we're gonna explore here are actually kind of applicable

782
00:38:40,930 --> 00:38:45,260
to other areas of NLG or just other areas of NLP deep learning.

783
00:38:45,300 --> 00:38:48,700
So, the first thing on the list is making it easier to copy,

784
00:38:48,700 --> 00:38:50,875
which seems like probably the first thing you want to fix,

785
00:38:50,875 --> 00:38:53,335
if you've just got basic seq2seq with attention.

786
00:38:53,335 --> 00:38:55,795
So, um, a copy mechanism,

787
00:38:55,795 --> 00:38:58,900
which can exist outside of summarization.

788
00:38:58,900 --> 00:39:03,160
The reason, why you want this is that basic seq2seq with attention,

789
00:39:03,160 --> 00:39:05,590
they're good at writing fluent output, as we know,

790
00:39:05,590 --> 00:39:09,835
but they are pretty bad at copying over details like rare words correctly.

791
00:39:09,835 --> 00:39:13,210
So a copy mechanism is just the kind of sensible idea of saying,

792
00:39:13,210 --> 00:39:17,950
um, let's have an explicit mechanism to just copy over words.

793
00:39:17,950 --> 00:39:20,140
So for example, you could use

794
00:39:20,140 --> 00:39:25,375
the attention distribution to- to kind of select what you're going to copy.

795
00:39:25,375 --> 00:39:28,890
Um, so, if you are allowing both copying

796
00:39:28,890 --> 00:39:32,235
over words and generating words in the usual way with your language model,

797
00:39:32,235 --> 00:39:37,220
then now you've got a kind of hybrid extractive/abstractive approach to summarization.

798
00:39:37,220 --> 00:39:40,360
So, there are several papers, which are- which propose

799
00:39:40,360 --> 00:39:43,330
some kind of copy mechanism variants and I think,

800
00:39:43,330 --> 00:39:45,040
the reason why there is multiple is because there's

801
00:39:45,040 --> 00:39:48,730
kind of a few different choices you can make about how to implement this,

802
00:39:48,730 --> 00:39:53,380
and that means that there's a few different versions of how to implement copy mechanism.

803
00:39:53,380 --> 00:39:56,155
So, uh, yeah, there are several papers here which you can look at.

804
00:39:56,155 --> 00:39:58,690
I'm going to show you a diagram from a paper that um,

805
00:39:58,690 --> 00:40:01,150
I did a few years ago with Chris.

806
00:40:01,150 --> 00:40:04,915
So, this is just one example of how you can do a copying mechanism.

807
00:40:04,915 --> 00:40:06,505
So, the - the way we did it,

808
00:40:06,505 --> 00:40:08,485
is we said that on each decoder step,

809
00:40:08,485 --> 00:40:11,590
you're going to calculate this probability Pgen and that's

810
00:40:11,590 --> 00:40:15,370
the probability of generating the next word rather than copying it,

811
00:40:15,370 --> 00:40:19,090
and the idea is that this is computed based on your current kind of context,

812
00:40:19,090 --> 00:40:20,935
your current decoder hidden state.

813
00:40:20,935 --> 00:40:22,585
So, then once you've done that,

814
00:40:22,585 --> 00:40:24,790
then the idea is you've got your attention distribution as

815
00:40:24,790 --> 00:40:27,280
normal and you've got your kind of output,

816
00:40:27,280 --> 00:40:31,360
you know, generation distribution as normal and you're going to use this Pgen,

817
00:40:31,360 --> 00:40:32,545
which is just a scalar.

818
00:40:32,545 --> 00:40:35,049
You can use that to kind of, uh, combine,

819
00:40:35,049 --> 00:40:38,005
mix together these two probability distributions.

820
00:40:38,005 --> 00:40:40,120
So, what this equation is telling you,

821
00:40:40,120 --> 00:40:41,410
is that saying that the uh,

822
00:40:41,410 --> 00:40:44,230
final output distribution for uh,

823
00:40:44,230 --> 00:40:45,595
what word is gonna come next,

824
00:40:45,595 --> 00:40:47,080
it's kind of saying, you know,

825
00:40:47,080 --> 00:40:48,685
it is the probability of generating

826
00:40:48,685 --> 00:40:51,895
times your probability distribution of what you would generate

827
00:40:51,895 --> 00:40:54,279
but then also the probability of copying

828
00:40:54,279 --> 00:40:57,220
and then also what you're attending to at that time.

829
00:40:57,220 --> 00:41:01,555
So, the, the main thing is, you're using attention as your copying mechanism.

830
00:41:01,555 --> 00:41:03,610
So, attention is kind of doing double-duty here.

831
00:41:03,610 --> 00:41:07,885
It's both uh, being useful for the generator to,

832
00:41:07,885 --> 00:41:10,000
you know, uh, maybe choose to rephrase things

833
00:41:10,000 --> 00:41:12,460
but it is also being useful as a copying mechanism.

834
00:41:12,460 --> 00:41:15,430
And I think that's one of the several things that these different papers do differently.

835
00:41:15,430 --> 00:41:18,940
I think, I've seen a paper that maybe has like two separate uh,

836
00:41:18,940 --> 00:41:21,685
attention distributions, one for the copying and one for the attending.

837
00:41:21,685 --> 00:41:24,460
Um, other choices you can make differently are for example,

838
00:41:24,460 --> 00:41:27,430
D1 Pgen to be this kind of soft thing that's between zero and

839
00:41:27,430 --> 00:41:30,730
one or do you want it to be a hard thing that has to be either zero or one.

840
00:41:30,730 --> 00:41:33,970
Um, you can also make decisions about like

841
00:41:33,970 --> 00:41:37,000
do you want the Pgen to have supervision during training?

842
00:41:37,000 --> 00:41:40,165
Do you want to kind of annotate your data set saying these things are copied, things,

843
00:41:40,165 --> 00:41:43,540
these things are not, or do you want to just like learn it end-to-end?

844
00:41:43,540 --> 00:41:46,075
So there's multiple ways you can do this and um,

845
00:41:46,075 --> 00:41:48,980
this has now become pretty, pretty standard.

846
00:41:50,100 --> 00:41:52,990
Okay, so copy mechanism seems like,

847
00:41:52,990 --> 00:41:55,960
seems like a sensible idea but there's a big problem with them,

848
00:41:55,960 --> 00:41:58,330
which is what I mentioned earlier and that problem is,

849
00:41:58,330 --> 00:41:59,665
that they copy too much.

850
00:41:59,665 --> 00:42:03,309
Um, so, when you- when you run these kind of systems on summarization,

851
00:42:03,309 --> 00:42:05,530
you find that they end up copying a lot of

852
00:42:05,530 --> 00:42:08,860
long phrases and sometimes even whole sentences and uh,

853
00:42:08,860 --> 00:42:11,860
unfortunately your dream of having an abstractive summarization system,

854
00:42:11,860 --> 00:42:13,795
isn't going to work out because your, um,

855
00:42:13,795 --> 00:42:16,510
you know, copy augmented seq2seq system has just

856
00:42:16,510 --> 00:42:20,035
collapsed into a mostly extractive system, which is unfortunate.

857
00:42:20,035 --> 00:42:22,060
Another problem with these uh,

858
00:42:22,060 --> 00:42:25,165
copy mechanism models is that they are bad at

859
00:42:25,165 --> 00:42:28,600
overall content selection especially if the input document is long,

860
00:42:28,600 --> 00:42:30,250
and this is what I was hinting at earlier.

861
00:42:30,250 --> 00:42:33,580
Um, let's suppose, that you are summarizing something that's quite

862
00:42:33,580 --> 00:42:37,090
long like a news article that's hundreds of words long and you,

863
00:42:37,090 --> 00:42:38,995
you want to write a several sentence summary.

864
00:42:38,995 --> 00:42:41,575
It doesn't seem like the kind of smartest choice to

865
00:42:41,575 --> 00:42:44,350
on every step of writing your several sentence summary,

866
00:42:44,350 --> 00:42:46,390
but you're choosing again what to attend to,

867
00:42:46,390 --> 00:42:48,325
what to select, what to summarize.

868
00:42:48,325 --> 00:42:52,795
It seems better to kind of make a global decision at the beginning and then summarize.

869
00:42:52,795 --> 00:42:56,560
So, yeah, the problem is, there's no overall strategy for selecting the contents.

870
00:42:56,560 --> 00:43:03,825
So, uh, here's a paper that I found. Nope, not yet.

871
00:43:03,825 --> 00:43:08,450
Okay. So, how might you do better content selection for neural summarization?

872
00:43:08,450 --> 00:43:12,010
So, if you remember in this pre-neural summarization we looked at,

873
00:43:12,010 --> 00:43:14,890
you had completely separate stages in the pipeline, right?

874
00:43:14,890 --> 00:43:16,870
You had the content selection stage and you had

875
00:43:16,870 --> 00:43:20,260
a surface realization that is the text generation stage.

876
00:43:20,260 --> 00:43:22,750
But in our seq2seq attention systems,

877
00:43:22,750 --> 00:43:25,240
these two stages are just completely mixed together, right?

878
00:43:25,240 --> 00:43:28,780
You're doing your step-by-step surface realization that is text generation,

879
00:43:28,780 --> 00:43:31,735
and then on each of those, you're also doing content selection.

880
00:43:31,735 --> 00:43:35,305
So, yeah, this doesn't make sense.

881
00:43:35,305 --> 00:43:37,510
So, I found a paper, which is,

882
00:43:37,510 --> 00:43:39,745
uh, published I think last year,

883
00:43:39,745 --> 00:43:42,160
which gives a quite nice kind of

884
00:43:42,160 --> 00:43:47,050
simple solution to this problem and it's called bottom-up summarization.

885
00:43:47,050 --> 00:43:51,715
So, in this paper if you look at the- if you look at the figure,

886
00:43:51,715 --> 00:43:53,260
uh, the main idea is pretty simple.

887
00:43:53,260 --> 00:43:57,370
It says that, first you're going to have a content selection stage and this is

888
00:43:57,370 --> 00:44:01,990
just uh, thought of as a neural sequence tagging model problem, right?

889
00:44:01,990 --> 00:44:04,660
You run through your source documents and

890
00:44:04,660 --> 00:44:07,615
you kind of tag every word as include or don't include.

891
00:44:07,615 --> 00:44:09,790
So, you're just kinda deciding like what seems important,

892
00:44:09,790 --> 00:44:11,680
what seems like it should make it into the summary and what

893
00:44:11,680 --> 00:44:15,625
doesn't and then the bottom-up attention stage says that,

894
00:44:15,625 --> 00:44:18,010
now you'll seq2seq with an attention system,

895
00:44:18,010 --> 00:44:19,945
which is gonna generate the summary.

896
00:44:19,945 --> 00:44:21,610
Are you're gonna kind of apply a mask?

897
00:44:21,610 --> 00:44:23,125
You know, apply a hard constraint that says,

898
00:44:23,125 --> 00:44:26,905
that you can't attend to words that were tagged don't-include.

899
00:44:26,905 --> 00:44:30,595
So, this turns out to be pretty simple but effective um,

900
00:44:30,595 --> 00:44:34,090
because it's a better overall content selection strategy because by doing

901
00:44:34,090 --> 00:44:38,800
this first content selection stage by sequence-tagging you're kind of just,

902
00:44:38,800 --> 00:44:42,730
just doing the selection thing without also at the same time doing the generation thing,

903
00:44:42,730 --> 00:44:44,800
which I think turns out to be a better way to make

904
00:44:44,800 --> 00:44:47,815
better decisions about what to include and then separately,

905
00:44:47,815 --> 00:44:49,900
this also means as a great side effect,

906
00:44:49,900 --> 00:44:53,500
you have less copying of long sequences in the generation model.

907
00:44:53,500 --> 00:44:56,830
Um, because if you are not allowed to attend to things,

908
00:44:56,830 --> 00:44:58,225
which you shouldn't be including,

909
00:44:58,225 --> 00:45:01,960
then it's kind of hard to copy a really long sequence, right?

910
00:45:01,960 --> 00:45:05,320
Like if you want to copy a whole sentence but the sentence has

911
00:45:05,320 --> 00:45:08,980
plenty of don't include words in it,

912
00:45:08,980 --> 00:45:11,635
then you can't really copy a long sequence, you have to break it up.

913
00:45:11,635 --> 00:45:12,970
So, what the model ends up doing,

914
00:45:12,970 --> 00:45:14,320
is it kind of has to skip,

915
00:45:14,320 --> 00:45:17,110
skip around the parts that is meant to include and then it's forced to

916
00:45:17,110 --> 00:45:20,650
be more abstractive to put the parts together. Yep.

917
00:45:20,650 --> 00:45:25,510
How did they backpropagate the masking decision because it seems like-

918
00:45:25,510 --> 00:45:28,720
Because during training [inaudible] masking decision.

919
00:45:28,720 --> 00:45:32,035
Yeah, I think it might be trained separately.

920
00:45:32,035 --> 00:45:33,610
I mean, you can go and check the paper.

921
00:45:33,610 --> 00:45:35,890
I've, I've read a lot of papers in the last days, I can't quite remember.

922
00:45:35,890 --> 00:45:37,990
I think, it might be trained separately but they might

923
00:45:37,990 --> 00:45:40,660
have tried training it together but it didn't work as well.

924
00:45:40,660 --> 00:45:42,860
I am not sure. You can check it out.

925
00:45:43,200 --> 00:45:48,745
Okay. So, another paper I want to tell you about is a paper which uh,

926
00:45:48,745 --> 00:45:53,965
used reinforcement learning to directly maximize ROUGE for neural summarization.

927
00:45:53,965 --> 00:45:56,275
So this was a paper from two years ago.

928
00:45:56,275 --> 00:45:58,360
And the main idea is that they can use RL to

929
00:45:58,360 --> 00:46:01,870
directly optimize in this case ROUGE-L, the metric.

930
00:46:01,870 --> 00:46:06,010
So by contrast, the standard maximum likelihood of training that is

931
00:46:06,010 --> 00:46:07,840
the training objective we've been talking about for

932
00:46:07,840 --> 00:46:10,390
the whole class so far for language models uh,

933
00:46:10,390 --> 00:46:13,840
that can't directly optimize ROUGE-L because it's a non-differentiable function.

934
00:46:13,840 --> 00:46:16,870
So they uh, they use this RL technique

935
00:46:16,870 --> 00:46:21,820
to compute the ROUGE score during training and then uh,

936
00:46:21,820 --> 00:46:26,110
use a reinforcement learning to backprop to the model.

937
00:46:26,110 --> 00:46:33,220
So, the interesting finding from this paper is that if they just used the RL objective,

938
00:46:33,220 --> 00:46:36,040
then they do indeed get higher ROUGE scores.

939
00:46:36,040 --> 00:46:38,470
So they can successfully optimize

940
00:46:38,470 --> 00:46:40,240
this ROUGE-L metric that they were aiming to

941
00:46:40,240 --> 00:46:42,760
optimize but the problem is that when you do that,

942
00:46:42,760 --> 00:46:44,725
you get lower human judgment scores.

943
00:46:44,725 --> 00:46:47,050
So, on the right we're seeing that the RL only model has

944
00:46:47,050 --> 00:46:51,775
actually pretty pretty bad readability relevance human judgment scores.

945
00:46:51,775 --> 00:46:57,235
It's worse than just the maximum likelihood supervised training system.

946
00:46:57,235 --> 00:47:00,685
So, this is a quote from their blog post that says,

947
00:47:00,685 --> 00:47:02,950
"We have observed that our models with the highest ROUGE scores

948
00:47:02,950 --> 00:47:05,335
also generated barely readable summaries."

949
00:47:05,335 --> 00:47:06,760
So, this is- this is,

950
00:47:06,760 --> 00:47:08,140
um, I suppose a problem, right?

951
00:47:08,140 --> 00:47:11,170
If you try to directly optimize for the metric,

952
00:47:11,170 --> 00:47:13,450
then you might start finding that you're kind of gaming

953
00:47:13,450 --> 00:47:16,680
the metric and not optimizing for the true task, right,

954
00:47:16,680 --> 00:47:20,550
because we know, just as we know that BLEU was not really a perfect analogy to

955
00:47:20,550 --> 00:47:22,530
actual translation quality so is ROUGE

956
00:47:22,530 --> 00:47:26,255
not a perfect analogy to uh, summarization quality.

957
00:47:26,255 --> 00:47:28,660
But they did do something cool, which is that they found that if

958
00:47:28,660 --> 00:47:31,419
you combine the two objectives,

959
00:47:31,419 --> 00:47:33,025
so they kind of, uh, you know,

960
00:47:33,025 --> 00:47:36,910
predict the language model sequence objective and then they also like produce

961
00:47:36,910 --> 00:47:41,305
an overall summary that gets a high ROUGE score objective and you combine them together,

962
00:47:41,305 --> 00:47:45,370
then you can get a better human uh, judgment score,

963
00:47:45,370 --> 00:47:48,220
which in the end is the closest thing we have to uh,

964
00:47:48,220 --> 00:47:49,930
a measure of actual summarization quality.

965
00:47:49,930 --> 00:47:54,340
[NOISE] Okay.

966
00:47:54,340 --> 00:47:57,280
So, I'm gonna move on to uh, dialogue,

967
00:47:57,280 --> 00:48:01,750
which is um, a different NLG, kind of family of tasks.

968
00:48:01,750 --> 00:48:05,590
Uh, so, really dialogue encompasses a really large variety of settings.

969
00:48:05,590 --> 00:48:06,700
And we are not going to cover them all,

970
00:48:06,700 --> 00:48:08,800
but here is a kind of overview of all the different kinds

971
00:48:08,800 --> 00:48:11,185
of tasks that people might mean, when they say dialogue.

972
00:48:11,185 --> 00:48:15,490
Um, so, there's task-oriented dialogue and this kind of refers to any setting,

973
00:48:15,490 --> 00:48:18,205
where you're trying to kind of get something done in the conversation.

974
00:48:18,205 --> 00:48:19,690
So, if for example, you've got kind of

975
00:48:19,690 --> 00:48:23,590
assistive tasks where it's assumed that you have, you know, maybe the uh,

976
00:48:23,590 --> 00:48:27,040
the dialogue agent is trying to help a human user to do

977
00:48:27,040 --> 00:48:30,700
something like maybe giving customer service or recommendations,

978
00:48:30,700 --> 00:48:32,890
answering questions, helping a user,

979
00:48:32,890 --> 00:48:35,950
you know, accomplish a task like buying or booking something.

980
00:48:35,950 --> 00:48:38,350
Uh, these are the kinds of tasks, which the virtual systems on

981
00:48:38,350 --> 00:48:41,740
your phone can do or can kind of do.

982
00:48:41,740 --> 00:48:46,585
Um, another family of task-oriented dialogue tasks are cooperative tasks.

983
00:48:46,585 --> 00:48:49,150
So, this is kind of anything where you've  got two agents who are

984
00:48:49,150 --> 00:48:52,120
trying to solve a task together via dialogue.

985
00:48:52,120 --> 00:48:54,715
Um, and the opposite of that would be adversarial.

986
00:48:54,715 --> 00:48:58,600
So anything where you have two agents who are trying to compete in a task and that uh,

987
00:48:58,600 --> 00:49:01,400
competition is conducted through dialogue.

988
00:49:02,340 --> 00:49:08,950
[NOISE] So uh, the opposite to task-oriented dialogue is, uh, social dialogue.

989
00:49:08,950 --> 00:49:13,600
So that's something where there is no explicit task other than to, I suppose socialize.

990
00:49:13,600 --> 00:49:16,105
So chit-chat dialogue, um,

991
00:49:16,105 --> 00:49:20,200
is just dialogue where you're just doing it for social fun or for company.

992
00:49:20,200 --> 00:49:24,910
Um, I've also seen some work on kind of like therapy or mental well-being dialogue,

993
00:49:24,910 --> 00:49:26,740
I'm not sure if this should go in task or social,

994
00:49:26,740 --> 00:49:28,105
it's kind of a mix, uh,

995
00:49:28,105 --> 00:49:30,580
but I suppose these are the ones where the goal is to

996
00:49:30,580 --> 00:49:34,285
maybe offer kind of emotional support to the human user.

997
00:49:34,285 --> 00:49:40,030
Um, so as a very kind of brief overview of how,

998
00:49:40,030 --> 00:49:42,070
uh, the deep learning, uh,

999
00:49:42,070 --> 00:49:45,070
renaissance has kind of changed dialog research, um,

1000
00:49:45,070 --> 00:49:48,595
I think you can say that in kind of pre-deep learning,

1001
00:49:48,595 --> 00:49:50,620
uh, the difficulty of open-ended,

1002
00:49:50,620 --> 00:49:53,830
free-form natural language generation, meant that, uh,

1003
00:49:53,830 --> 00:49:55,405
dialogue systems were often,

1004
00:49:55,405 --> 00:49:58,360
uh, not doing free-form NLG.

1005
00:49:58,360 --> 00:50:00,730
They might use predefined templates meaning that you have

1006
00:50:00,730 --> 00:50:03,775
a template where you just fill in some slots with the content, uh,

1007
00:50:03,775 --> 00:50:06,700
or maybe you retrieve an appropriate response from

1008
00:50:06,700 --> 00:50:09,625
a corpus of responses that you have in order to find,

1009
00:50:09,625 --> 00:50:11,380
you know, an appropriate response for the user.

1010
00:50:11,380 --> 00:50:13,330
And these are by no means simple systems,

1011
00:50:13,330 --> 00:50:16,180
they had some very complex things going on like deciding, you know,

1012
00:50:16,180 --> 00:50:19,570
what their dialogue state is and what template you should use and so on and the-

1013
00:50:19,570 --> 00:50:23,905
all the natural language understanding components of understanding the context so far.

1014
00:50:23,905 --> 00:50:26,455
But, uh, one effect that,

1015
00:50:26,455 --> 00:50:28,825
that deep learning had is that, uh,

1016
00:50:28,825 --> 00:50:31,915
since again kind of 2015 which is when NMT, uh,

1017
00:50:31,915 --> 00:50:34,615
became standard, there's been, uh,

1018
00:50:34,615 --> 00:50:38,440
just like summarization, lots of papers applying seq2seq methods to dialogue.

1019
00:50:38,440 --> 00:50:43,435
And this has kind of led to a renewed interest in open-ended, free-form dialogue systems.

1020
00:50:43,435 --> 00:50:45,760
So uh, if you wanna have a look at what did

1021
00:50:45,760 --> 00:50:48,130
those early seq2seq dialogue papers look like,

1022
00:50:48,130 --> 00:50:53,090
um, here's two kind of early ones like maybe the first ones to apply seq2seq.

1023
00:50:55,530 --> 00:51:00,400
Okay. So uh, people quickly applied seq2seq, uh,

1024
00:51:00,400 --> 00:51:03,160
NMT methods to dialogue but it quickly became

1025
00:51:03,160 --> 00:51:06,130
very apparent that this kind of naive application of

1026
00:51:06,130 --> 00:51:08,560
standard NMT methods has

1027
00:51:08,560 --> 00:51:13,915
some serious pervasive deficiencies when applied to a task like chitchat dialogue.

1028
00:51:13,915 --> 00:51:16,960
And this is even more true than it was for summarization.

1029
00:51:16,960 --> 00:51:21,145
So what are some examples of these serious pervas- pervasive deficiencies?

1030
00:51:21,145 --> 00:51:24,430
Uh, one would be genericness or boring responses,

1031
00:51:24,430 --> 00:51:26,710
and I'll go into more detail about these in a moment.

1032
00:51:26,710 --> 00:51:29,005
Another one is irrelevant responses.

1033
00:51:29,005 --> 00:51:30,175
So that's when, uh,

1034
00:51:30,175 --> 00:51:32,200
the dialogue agent kind of says something back

1035
00:51:32,200 --> 00:51:35,065
that's just kind of unrelated to what the user says.

1036
00:51:35,065 --> 00:51:36,700
Um, another one is repetition,

1037
00:51:36,700 --> 00:51:38,080
this is pretty basic but it,

1038
00:51:38,080 --> 00:51:39,640
uh, it happens a lot.

1039
00:51:39,640 --> 00:51:44,275
Um, so that's also repetition within the utterance and maybe repetition across utterances.

1040
00:51:44,275 --> 00:51:47,485
Ah, another difficulty is,

1041
00:51:47,485 --> 00:51:48,910
uh, kind of lack of context,

1042
00:51:48,910 --> 00:51:50,800
like not remembering the conversation history.

1043
00:51:50,800 --> 00:51:53,710
Obviously, if you do not condition on the whole conversation history,

1044
00:51:53,710 --> 00:51:57,190
there's no way your dialogue agent can use it but it is a challenge especially if you

1045
00:51:57,190 --> 00:52:01,315
have a very long dialogue history to figure out how to condition on it effectively.

1046
00:52:01,315 --> 00:52:04,060
Another problem is the lack of consistent persona.

1047
00:52:04,060 --> 00:52:05,380
So if you kind of, uh,

1048
00:52:05,380 --> 00:52:09,970
naively as in maybe those two papers that I referenced on the previous slide,

1049
00:52:09,970 --> 00:52:14,395
if you naively train a kind of standard seq2seq model to maybe take the, uh,

1050
00:52:14,395 --> 00:52:16,480
you know the user's last utterance and then say something back,

1051
00:52:16,480 --> 00:52:18,955
or maybe even the whole dialogue history and say something back.

1052
00:52:18,955 --> 00:52:22,675
Often your dialogue agent will have this completely inconsistent persona,

1053
00:52:22,675 --> 00:52:26,800
like one moment they will say that it lives in Europe and then it'll say it lives in,

1054
00:52:26,800 --> 00:52:29,770
I don't know, China or something and it just doesn't make sense.

1055
00:52:29,770 --> 00:52:31,915
So I'm gonna go through, uh,

1056
00:52:31,915 --> 00:52:34,810
some of these problems and give you a bit more detail on them.

1057
00:52:34,810 --> 00:52:37,870
So first, this irrelevant response problem.

1058
00:52:37,870 --> 00:52:40,960
So in a bit more detail, your problem is that seq2seq often

1059
00:52:40,960 --> 00:52:44,080
generates some response that's kind of unrelated to the user's utterance.

1060
00:52:44,080 --> 00:52:47,155
So it can be unrelated because it's simply generic,

1061
00:52:47,155 --> 00:52:49,150
which means that this is kind of like an overlap with

1062
00:52:49,150 --> 00:52:51,610
a generic response problem or it can be

1063
00:52:51,610 --> 00:52:54,160
kind of unrelated because the model's choosing to kind of change,

1064
00:52:54,160 --> 00:52:56,845
to change the subject to something unrelated.

1065
00:52:56,845 --> 00:52:59,020
So one solution of many, there,

1066
00:52:59,020 --> 00:53:00,880
there are a lot of different papers which, uh,

1067
00:53:00,880 --> 00:53:04,735
kind of attack this irrelevant response problem, uh, but just one,

1068
00:53:04,735 --> 00:53:07,015
one for example is, uh,

1069
00:53:07,015 --> 00:53:09,835
that you should tr- change the training objective.

1070
00:53:09,835 --> 00:53:12,760
So instead of trying to optimize, um,

1071
00:53:12,760 --> 00:53:15,520
mapping from input S to response T such that

1072
00:53:15,520 --> 00:53:18,625
you're maximizing the conditional probability of T given S,

1073
00:53:18,625 --> 00:53:22,435
instead you should maximize the maximum mutual information.

1074
00:53:22,435 --> 00:53:24,235
So that's why this is here.

1075
00:53:24,235 --> 00:53:26,530
So maximum mutual information, uh,

1076
00:53:26,530 --> 00:53:29,140
you can kind of rewrite the objective like this,

1077
00:53:29,140 --> 00:53:31,915
and if you want to see some more detail you can go look at this paper here.

1078
00:53:31,915 --> 00:53:35,830
But the idea is that you're trying to find your response T that kind of, uh,

1079
00:53:35,830 --> 00:53:38,680
maximizes this thing which is kind of like saying,

1080
00:53:38,680 --> 00:53:41,680
it needs to be probable given the inputs but

1081
00:53:41,680 --> 00:53:44,920
kind of like as a ratio of its probability in itself.

1082
00:53:44,920 --> 00:53:49,510
So if T is very high likelihood,

1083
00:53:49,510 --> 00:53:52,600
then it gets penalized and it's kind of like about the ratio

1084
00:53:52,600 --> 00:53:56,440
of the probability given the input and it's just the stand-alone probability.

1085
00:53:56,440 --> 00:53:59,650
So the idea is that this is meant to discourage, um,

1086
00:53:59,650 --> 00:54:04,240
just saying generic things that just have a high PT by themselves.

1087
00:54:04,240 --> 00:54:08,995
Um, so that's the irrelevant response problem.

1088
00:54:08,995 --> 00:54:10,780
And as I just hinted at, there's, uh,

1089
00:54:10,780 --> 00:54:12,040
definitely a strong link between

1090
00:54:12,040 --> 00:54:16,870
the irrelevant response problem and the kind of generic or boring response problem.

1091
00:54:16,870 --> 00:54:21,490
So to look at the genericness or boring response problem.

1092
00:54:21,490 --> 00:54:27,730
[NOISE] So I think

1093
00:54:27,730 --> 00:54:32,230
there are some pretty easy fixes that you can make to,

1094
00:54:32,230 --> 00:54:35,470
to a degree ameliorate the boring response problem.

1095
00:54:35,470 --> 00:54:38,410
Whether you're really getting to the heart of the issue is a different question.

1096
00:54:38,410 --> 00:54:42,310
But some kind of easy test-time fixes that you can certainly do are for example,

1097
00:54:42,310 --> 00:54:46,885
you can just directly up-rate, up-weight rare words during beam search.

1098
00:54:46,885 --> 00:54:49,675
So you can say, all rare words kind of get a boost to their, uh,

1099
00:54:49,675 --> 00:54:51,880
log probabilities and then now we're

1100
00:54:51,880 --> 00:54:54,220
more likely to produce them during beam search.

1101
00:54:54,220 --> 00:54:56,410
Another thing you could do is you could use for example,

1102
00:54:56,410 --> 00:55:00,535
a sampling decoding algorithm rather than beam search and we talked about that earlier,

1103
00:55:00,535 --> 00:55:02,350
um, or you could use, oh yeah,

1104
00:55:02,350 --> 00:55:04,195
you could use softmax temperature as well.

1105
00:55:04,195 --> 00:55:07,105
That's another thing. So those are

1106
00:55:07,105 --> 00:55:12,040
kind of test-time fixes and you could regard those as a kind of late intervention, right?

1107
00:55:12,040 --> 00:55:16,000
So an earlier intervention would be maybe training your model differently.

1108
00:55:16,000 --> 00:55:20,005
So I'm calling these kind of conditioning fixes because these fixes kind of relate to,

1109
00:55:20,005 --> 00:55:23,965
uh, conditioning your model on something that's gonna help it be less boring.

1110
00:55:23,965 --> 00:55:26,320
So one example is maybe you should condition

1111
00:55:26,320 --> 00:55:28,930
the decoder on some kind of additional context.

1112
00:55:28,930 --> 00:55:31,150
Uh, so for example, there's some work showing that, you know,

1113
00:55:31,150 --> 00:55:33,625
if you're doing chitchat dialogue, then maybe you should, uh,

1114
00:55:33,625 --> 00:55:36,280
go and sample some related words that are related to

1115
00:55:36,280 --> 00:55:38,710
what the user said and then just kind of attend to them when you

1116
00:55:38,710 --> 00:55:41,350
generate and then you're more likely to say something that's kind of content

1117
00:55:41,350 --> 00:55:44,080
full and interesting compared to the boring things you were saying before.

1118
00:55:44,080 --> 00:55:46,870
Ah, another option is you could train

1119
00:55:46,870 --> 00:55:50,770
a retrieve-and-refine model rather than a generate-from-scratch model.

1120
00:55:50,770 --> 00:55:53,440
So by retrieve-and-refine, I mean, uh,

1121
00:55:53,440 --> 00:55:55,825
you've- supposing you have some kind of corpus of,

1122
00:55:55,825 --> 00:55:57,400
of just general kind of utterances,

1123
00:55:57,400 --> 00:56:00,460
things that you could say and then maybe you sample one, uh,

1124
00:56:00,460 --> 00:56:01,855
from that test set,

1125
00:56:01,855 --> 00:56:03,775
th- the training set,

1126
00:56:03,775 --> 00:56:06,895
and then you edit it to fit the current situation.

1127
00:56:06,895 --> 00:56:10,630
So it turns out that this is a pretty strong method to produce

1128
00:56:10,630 --> 00:56:14,800
much more kind of diverse and human-like and interesting utterances, um,

1129
00:56:14,800 --> 00:56:19,555
because you can get all of that kind of fine grain detail from the sampled,

1130
00:56:19,555 --> 00:56:23,665
ah, utterance and then you edit it as necessary to fit your current situation.

1131
00:56:23,665 --> 00:56:26,740
So I mean, there are downsides to these kinds of methods like maybe it can be

1132
00:56:26,740 --> 00:56:30,145
hard to edit it to actually appropriately fit the situation,

1133
00:56:30,145 --> 00:56:32,410
um, but it's certainly a way to effectively get like

1134
00:56:32,410 --> 00:56:36,530
some more diversity and, um, interest in that.

1135
00:56:37,170 --> 00:56:40,810
So on the subject of the repetition problem,

1136
00:56:40,810 --> 00:56:43,105
that was another kind of major problem we noticed for,

1137
00:56:43,105 --> 00:56:45,790
um, applying seq2seq to, uh, chitchat.

1138
00:56:45,790 --> 00:56:48,970
Um, again, there are kind of simple solutions and more complex solutions.

1139
00:56:48,970 --> 00:56:52,150
Um, so a simple solution is you could just block repeating

1140
00:56:52,150 --> 00:56:55,915
n-grams during beam search and this is usually really quite effective.

1141
00:56:55,915 --> 00:56:57,595
And what I mean by that is, uh,

1142
00:56:57,595 --> 00:56:59,815
during beam search when you're kind of considering,

1143
00:56:59,815 --> 00:57:01,510
you know, what are my K hypotheses?

1144
00:57:01,510 --> 00:57:05,110
Which is just kind of the top K in the probability distribution, you say,

1145
00:57:05,110 --> 00:57:09,535
well, anything that would constitute a repeating n-gram just gets thrown out.

1146
00:57:09,535 --> 00:57:11,590
So when I say constitutes a repeating n-gram,

1147
00:57:11,590 --> 00:57:14,470
I mean if you did take that word,

1148
00:57:14,470 --> 00:57:19,630
would you now be creating a repeating let's say two-gram, bigram and, um,

1149
00:57:19,630 --> 00:57:23,500
if we're deciding that we're banning all repeating bigrams or trigrams or whatever,

1150
00:57:23,500 --> 00:57:26,620
then you essentially just have to check for every possible word that you might

1151
00:57:26,620 --> 00:57:30,700
be looking at in beam search and whether that would create a repeating n-gram.

1152
00:57:30,700 --> 00:57:32,440
So this works pretty well, I mean,

1153
00:57:32,440 --> 00:57:34,780
it's by no means a kind of principled solution, right?

1154
00:57:34,780 --> 00:57:37,495
If feels like we should kind of have a better way to learn not to repeat, um,

1155
00:57:37,495 --> 00:57:39,790
but as a kind of, uh,

1156
00:57:39,790 --> 00:57:42,535
effective hack, I think that works, that works pretty well.

1157
00:57:42,535 --> 00:57:44,830
So the more complex solutions are,

1158
00:57:44,830 --> 00:57:47,920
for example, you can train something called coverage mechanism.

1159
00:57:47,920 --> 00:57:50,530
Um, so in seq2seq, and this is mostly, uh,

1160
00:57:50,530 --> 00:57:53,800
inspired by the machine translation setting, uh,

1161
00:57:53,800 --> 00:57:56,440
a coverage mechanism is a kind of objective that prevents

1162
00:57:56,440 --> 00:57:58,630
the attention mechanism from attending to

1163
00:57:58,630 --> 00:58:01,810
the same words multiple times or too many times.

1164
00:58:01,810 --> 00:58:03,655
And the intuition here is that, uh,

1165
00:58:03,655 --> 00:58:06,595
maybe repetition is caused by repeated attention.

1166
00:58:06,595 --> 00:58:08,620
So if you attend to the same things many times,

1167
00:58:08,620 --> 00:58:09,970
then maybe you're gonna repeat,

1168
00:58:09,970 --> 00:58:11,605
you know, the same output many times.

1169
00:58:11,605 --> 00:58:13,690
So if you prevent the repeated attention,

1170
00:58:13,690 --> 00:58:15,280
you prevent the repeated output.

1171
00:58:15,280 --> 00:58:18,190
So this does work pretty well but it's definitely,

1172
00:58:18,190 --> 00:58:21,490
um, more of a complex thing to implement,

1173
00:58:21,490 --> 00:58:23,635
it's less convenient and,

1174
00:58:23,635 --> 00:58:25,120
um, I don't know,

1175
00:58:25,120 --> 00:58:28,075
in some settings, it does seem like the simple solution is,

1176
00:58:28,075 --> 00:58:29,530
uh, easier and works just as well.

1177
00:58:29,530 --> 00:58:32,740
Uh, so other complex solutions

1178
00:58:32,740 --> 00:58:35,800
might be you could define a training objective to discourage repetition.

1179
00:58:35,800 --> 00:58:38,320
Uh, this cou- you could try to, um,

1180
00:58:38,320 --> 00:58:41,125
define something differentiable but one of the,

1181
00:58:41,125 --> 00:58:45,600
the difficulties there is that because you're training with a teacher forcing, right?

1182
00:58:45,600 --> 00:58:47,055
Where you're always like looking at the,

1183
00:58:47,055 --> 00:58:48,435
the gold inputs so far,

1184
00:58:48,435 --> 00:58:50,700
then you never really do the thing where

1185
00:58:50,700 --> 00:58:53,010
you generate your own output and start repeating yourself.

1186
00:58:53,010 --> 00:58:55,845
So it's kind of hard to define the penalty in that situation.

1187
00:58:55,845 --> 00:58:58,350
So maybe this needs to be a kind of non-differentiable function.

1188
00:58:58,350 --> 00:59:00,255
So kind of like how,

1189
00:59:00,255 --> 00:59:03,745
um, the Paul et al paper was, uh,

1190
00:59:03,745 --> 00:59:06,250
optimizing for ROUGE, maybe we kind of, uh,

1191
00:59:06,250 --> 00:59:11,455
optimize for not repeating which is a discrete function of the input.

1192
00:59:11,455 --> 00:59:14,425
Uh, I'm going to skip ahead to storytelling.

1193
00:59:14,425 --> 00:59:16,195
So in storytelling, uh,

1194
00:59:16,195 --> 00:59:19,015
there's a lot of interesting neural storytelling work going on right now.

1195
00:59:19,015 --> 00:59:22,285
And most of it uses some kind of prompt to write a story.

1196
00:59:22,285 --> 00:59:24,610
So for example, uh,

1197
00:59:24,610 --> 00:59:28,115
writing a story given an image or given a writing prompt

1198
00:59:28,115 --> 00:59:32,715
or writing the next sentence of the story given the story so far.

1199
00:59:32,715 --> 00:59:37,645
So, uh, here's an example of generating a story from an image.

1200
00:59:37,645 --> 00:59:40,360
And what's interesting here is that we have this image which

1201
00:59:40,360 --> 00:59:42,940
is a picture of what appears to be an explosion and

1202
00:59:42,940 --> 00:59:44,740
then here you have

1203
00:59:44,740 --> 00:59:48,475
a story about the image but written in the style of Taylor Swift lyrics.

1204
00:59:48,475 --> 00:59:52,015
So it says, you have to be the only light bulb in the night sky I thought,

1205
00:59:52,015 --> 00:59:55,225
oh god, it's so dark out of me that I missed you, I promise.

1206
00:59:55,225 --> 00:59:58,690
And what's interesting here is that there wasn't any straightforward, supervised,

1207
00:59:58,690 --> 01:00:02,620
you know, image-captioning data set of explosions and Taylor Swift lyrics.

1208
01:00:02,620 --> 01:00:05,890
Um, they kind of learned this, uh, separately.

1209
01:00:05,890 --> 01:00:12,220
So how they did this is that they used a kind of common sentence encoding space.

1210
01:00:12,220 --> 01:00:15,160
So they used this particular kind of sentence encoding called

1211
01:00:15,160 --> 01:00:18,205
skip-thought vectors and then they trained,

1212
01:00:18,205 --> 01:00:21,880
um, this COCO image-captioning, uh,

1213
01:00:21,880 --> 01:00:24,790
system to go from the image to the encoding of

1214
01:00:24,790 --> 01:00:28,105
the sentence and then separately they also trained,

1215
01:00:28,105 --> 01:00:30,370
uh, a language model, a conditional language model to go from

1216
01:00:30,370 --> 01:00:33,010
the sentence-encoding to the Taylor Swift lyrics.

1217
01:00:33,010 --> 01:00:35,230
And then because you had this shared encoding space,

1218
01:00:35,230 --> 01:00:38,305
you can now put the two together and then go from the picture,

1219
01:00:38,305 --> 01:00:41,050
to the embedding, to the Taylor Swift style output,

1220
01:00:41,050 --> 01:00:43,790
which I think is pretty, pretty amazing.

1221
01:00:44,220 --> 01:00:46,600
Wow, I've really lost, lost track of the time.

1222
01:00:46,600 --> 01:00:48,250
So I, I think I have to hurry up quite a lot.

1223
01:00:48,250 --> 01:00:55,140
So, um, we've got some really impressive story,

1224
01:00:55,140 --> 01:00:57,900
generation systems, recently, um,

1225
01:00:57,900 --> 01:01:00,795
and this is an example of,

1226
01:01:00,795 --> 01:01:02,580
uh, a system which,

1227
01:01:02,580 --> 01:01:03,840
uh, prepares a new data set,

1228
01:01:03,840 --> 01:01:05,745
where you write a story given a prompt,

1229
01:01:05,745 --> 01:01:08,165
and they made this very impressive,

1230
01:01:08,165 --> 01:01:10,900
very beefed-up, uh, convolutional language model,

1231
01:01:10,900 --> 01:01:13,975
seq-to-seq system that generates the story given the input.

1232
01:01:13,975 --> 01:01:15,640
I'm not gonna go through all these details,

1233
01:01:15,640 --> 01:01:18,070
but I encourage you if you want to check out, uh,

1234
01:01:18,070 --> 01:01:20,950
what's the state of the art in story generation, you should check this out.

1235
01:01:20,950 --> 01:01:23,110
There's a lot of different interesting things going on with

1236
01:01:23,110 --> 01:01:25,780
very fancy attention and convolutions and so on,

1237
01:01:25,780 --> 01:01:29,695
and they managed to generate some really interesting, um, impressive stories.

1238
01:01:29,695 --> 01:01:31,749
So here, if you look at this example,

1239
01:01:31,749 --> 01:01:36,280
we've got some really interesting, um, kind of,

1240
01:01:36,280 --> 01:01:39,324
uh, story generation that's kind of diverse, it's non-generic,

1241
01:01:39,324 --> 01:01:41,320
it's stylistically dramatic which is good,

1242
01:01:41,320 --> 01:01:42,925
and is related to the prompts.

1243
01:01:42,925 --> 01:01:46,330
Um, but I think you can see here kind of the limits of what

1244
01:01:46,330 --> 01:01:49,855
the state of the art story generation system can do which is that- um,

1245
01:01:49,855 --> 01:01:51,715
although it's kind of in style,

1246
01:01:51,715 --> 01:01:54,625
it's mostly kind of atmospheric and descriptive.

1247
01:01:54,625 --> 01:01:56,140
It's not really moving the plot forward.

1248
01:01:56,140 --> 01:01:57,940
There's no kind of events here, right?

1249
01:01:57,940 --> 01:02:02,305
Um, so the problem is it gets even worse when you generate for longer.

1250
01:02:02,305 --> 01:02:04,180
When you generate a long, a long text,

1251
01:02:04,180 --> 01:02:08,755
then it will mostly just stay on the same idea without moving forward with new ideas.

1252
01:02:08,755 --> 01:02:11,500
Okay. So I'm gonna skip forward a lot and,

1253
01:02:11,500 --> 01:02:13,510
uh, sorry, ought to have planned better.

1254
01:02:13,510 --> 01:02:15,160
There's a lot of information here which you wanna check

1255
01:02:15,160 --> 01:02:17,335
out about poetry generation and other things.

1256
01:02:17,335 --> 01:02:19,690
I'm going to skip ahead because I want to get to

1257
01:02:19,690 --> 01:02:23,320
the NLG evaluation section because that's pretty important.

1258
01:02:23,320 --> 01:02:27,655
So, um, we've talked about Automatic Evaluation Metrics fr NLG,

1259
01:02:27,655 --> 01:02:30,760
and we know that these words overlap based metrics, such as BLEU,

1260
01:02:30,760 --> 01:02:32,155
and ROUGE, and METEOR, uh,

1261
01:02:32,155 --> 01:02:34,360
we know they're not ideal for machine translation.

1262
01:02:34,360 --> 01:02:37,780
Ah, they're kind of even worse for summarization mostly

1263
01:02:37,780 --> 01:02:41,770
because summarization is even more open-ended than machine translation.

1264
01:02:41,770 --> 01:02:44,170
And that means that having this kind of rigid notion,

1265
01:02:44,170 --> 01:02:45,835
if you've got to match the N-grams,

1266
01:02:45,835 --> 01:02:47,380
is even less useful.

1267
01:02:47,380 --> 01:02:49,930
And then for something even more open-ended like dialogue,

1268
01:02:49,930 --> 01:02:51,580
then it's just kind of a disaster.

1269
01:02:51,580 --> 01:02:54,220
It's not even a metric that gives you a good signal at all,

1270
01:02:54,220 --> 01:02:58,045
and this also applies to anything else open-ended, like story generation.

1271
01:02:58,045 --> 01:03:01,225
So it's been shown, and you can check out the paper at the bottom,

1272
01:03:01,225 --> 01:03:05,125
that word overlap metrics are just not a good fit for dialogue.

1273
01:03:05,125 --> 01:03:07,480
So the orange box is showing you, uh,

1274
01:03:07,480 --> 01:03:13,855
some plots of the correlation between human score on a dialog class and BLEU-2,

1275
01:03:13,855 --> 01:03:15,415
some variation of BLEU.

1276
01:03:15,415 --> 01:03:18,490
And the prob- the problem here is you're not seeing much of a correlation at all, right?

1277
01:03:18,490 --> 01:03:21,420
It seems that particularly on this dialogue setting, ah,

1278
01:03:21,420 --> 01:03:23,370
the correlation between the BLEU metric and

1279
01:03:23,370 --> 01:03:26,565
the human judgment of whether it's a good dialogue response is,

1280
01:03:26,565 --> 01:03:28,020
uh, the correlation is- I mean,

1281
01:03:28,020 --> 01:03:29,040
it looks kind of non-existent.

1282
01:03:29,040 --> 01:03:30,720
It's at least very weak.

1283
01:03:30,720 --> 01:03:35,120
So that's pretty unfortunate and there's some other papers that show much the same thing.

1284
01:03:35,120 --> 01:03:36,640
So you might think, "Well,

1285
01:03:36,640 --> 01:03:38,920
what other automatic metrics can we use?

1286
01:03:38,920 --> 01:03:40,600
"What about perplexity?

1287
01:03:40,600 --> 01:03:45,820
Um, so perplexity certainly captures how powerful your language model is,

1288
01:03:45,820 --> 01:03:48,085
but it doesn't tell you anything about generation.

1289
01:03:48,085 --> 01:03:51,970
So for example, if your deca- decoding algorithm is bad in some way,

1290
01:03:51,970 --> 01:03:54,700
then perplexity is not gonna tell you anything about that, right?

1291
01:03:54,700 --> 01:03:57,640
Because decoding is something you apply to your trained language model.

1292
01:03:57,640 --> 01:04:00,130
Perplexity can tell if you've got a strong language model or not,

1293
01:04:00,130 --> 01:04:01,840
but it's not gonna tell you, um,

1294
01:04:01,840 --> 01:04:04,165
necessarily how good your generation is.

1295
01:04:04,165 --> 01:04:07,330
So some other thoughts you might have about automatic evaluation are,

1296
01:04:07,330 --> 01:04:09,460
well, what about word embedding based metrics?

1297
01:04:09,460 --> 01:04:12,145
Uh, so the main idea with word embedding based metrics,

1298
01:04:12,145 --> 01:04:14,515
uh, you want to compute the similarity of the,

1299
01:04:14,515 --> 01:04:18,220
the word embeddings or maybe the average of the word embeddings across a sentence,

1300
01:04:18,220 --> 01:04:20,530
not just the overlap of the words themselves.

1301
01:04:20,530 --> 01:04:22,780
Um, so the idea is that rather than just being

1302
01:04:22,780 --> 01:04:25,510
very strict and saying only the exact same word counts,

1303
01:04:25,510 --> 01:04:28,885
you say, "Well, if the words are similar and in word embedding space, then they count."

1304
01:04:28,885 --> 01:04:31,900
So this is certainly more flexible, but unfortunately, uh,

1305
01:04:31,900 --> 01:04:34,360
the same paper I showed before shows that this doesn't

1306
01:04:34,360 --> 01:04:37,315
correlate well either with human judgments of quality,

1307
01:04:37,315 --> 01:04:39,955
at least for the- the dialogue task they are looking at.

1308
01:04:39,955 --> 01:04:43,285
So here, the middle column is showing the correlation between human,

1309
01:04:43,285 --> 01:04:47,515
judgments, and some kind of average of word embedding based metric.

1310
01:04:47,515 --> 01:04:49,540
So, um, yeah, that doesn't look great either,

1311
01:04:49,540 --> 01:04:51,400
not a great correlation.

1312
01:04:51,400 --> 01:04:54,970
So if we have no automatic metrics to adequately

1313
01:04:54,970 --> 01:04:58,435
capture overall quality for natural language generation,

1314
01:04:58,435 --> 01:05:00,460
um, what, what can we do instead?

1315
01:05:00,460 --> 01:05:02,590
So I think often the strategy is,

1316
01:05:02,590 --> 01:05:06,280
you end up defining some more kind of focused automatic metrics to

1317
01:05:06,280 --> 01:05:10,705
capture the particular aspects of the generated text that you might be interested in.

1318
01:05:10,705 --> 01:05:13,640
Um, so for example, you might be interested in, uh, fluency,

1319
01:05:13,640 --> 01:05:15,540
and you can compute that by just kind of running

1320
01:05:15,540 --> 01:05:18,734
a well-trained language model over your text and generating the probability,

1321
01:05:18,734 --> 01:05:23,505
and that's kind of a proxy for how well it's written, you know, good,  fluent, grammatical text.

1322
01:05:23,505 --> 01:05:28,000
Um, if you're particularly interested in maybe generating text in a particular style,

1323
01:05:28,000 --> 01:05:29,860
then you could ta- take a language model that's

1324
01:05:29,860 --> 01:05:32,185
trained on the corpus representing that style,

1325
01:05:32,185 --> 01:05:35,110
and now the probability tells you not only is it a good text,

1326
01:05:35,110 --> 01:05:36,685
but is it in the right style.

1327
01:05:36,685 --> 01:05:38,875
Um, there are some other things as well that are like,

1328
01:05:38,875 --> 01:05:41,185
you know, diversity, um,

1329
01:05:41,185 --> 01:05:43,900
and you can can that pretty easily by just having some statistics about,

1330
01:05:43,900 --> 01:05:45,940
you know, how much you're using rare words.

1331
01:05:45,940 --> 01:05:48,250
Um, relevance to input,

1332
01:05:48,250 --> 01:05:50,710
you can kind of compute a similarity score with the input,

1333
01:05:50,710 --> 01:05:52,555
and there are just some simple things like, you know,

1334
01:05:52,555 --> 01:05:55,480
length and repetition that you surely can count, and yes,

1335
01:05:55,480 --> 01:05:58,075
it doesn't tell you overall the overall quality,

1336
01:05:58,075 --> 01:06:00,535
but these things are worth measuring.

1337
01:06:00,535 --> 01:06:02,410
So I think my main point is that yes,

1338
01:06:02,410 --> 01:06:04,960
we have a really difficult situation with NLG evaluation.

1339
01:06:04,960 --> 01:06:06,400
There's no kind of overall metric.

1340
01:06:06,400 --> 01:06:08,860
Often, they capture this overall quality.

1341
01:06:08,860 --> 01:06:11,365
Um, but if you measure lots of these things,

1342
01:06:11,365 --> 01:06:16,075
then they certainly can help you track some important things that you should know.

1343
01:06:16,075 --> 01:06:21,895
So we talked about how automatic evaluation metrics for NLG are really tough.

1344
01:06:21,895 --> 01:06:23,710
So let's talk about human evaluation.

1345
01:06:23,710 --> 01:06:27,400
Uh, human judgments are regarded as the gold standard, right?

1346
01:06:27,400 --> 01:06:30,865
But we already know that human evaluation is slow and expensive,

1347
01:06:30,865 --> 01:06:34,165
uh, but are those the only problems with human eval?

1348
01:06:34,165 --> 01:06:36,910
Let's suppose that you do have access, uh, to,

1349
01:06:36,910 --> 01:06:40,060
let's say, the time or money you need to do human evaluations.

1350
01:06:40,060 --> 01:06:42,115
Um, does that solve all your problems?

1351
01:06:42,115 --> 01:06:43,480
Suppose you have unlimited human eval,

1352
01:06:43,480 --> 01:06:44,980
does that actually solve your problems?

1353
01:06:44,980 --> 01:06:47,590
And my answer is, uh, no.

1354
01:06:47,590 --> 01:06:49,630
And this is kinda from personal experience.

1355
01:06:49,630 --> 01:06:53,590
Um, conducting human evaluation in itself is very difficult to get right.

1356
01:06:53,590 --> 01:06:57,280
It's not easy at all, and this is partially because humans do a lot of weird things.

1357
01:06:57,280 --> 01:06:59,905
Humans, uh, unlike a metric, uh,

1358
01:06:59,905 --> 01:07:02,125
an automatic metric, they're inconsistent,

1359
01:07:02,125 --> 01:07:03,670
they could be illogical.

1360
01:07:03,670 --> 01:07:05,290
Sometimes, they just get bored of your task,

1361
01:07:05,290 --> 01:07:06,760
and they don't really pay attention anymore.

1362
01:07:06,760 --> 01:07:09,580
Uh, they can misinterpret the question you asked,

1363
01:07:09,580 --> 01:07:12,400
and sometimes they do things they can't really explain why they did it.

1364
01:07:12,400 --> 01:07:14,440
So, um, as a kind of case study of

1365
01:07:14,440 --> 01:07:16,540
this I'm going to tell you about, um,

1366
01:07:16,540 --> 01:07:18,010
a project I did where I was,

1367
01:07:18,010 --> 01:07:19,540
uh, building some chatbots,

1368
01:07:19,540 --> 01:07:23,485
and it turned out that the human evaluation was kind of the hardest part of the project.

1369
01:07:23,485 --> 01:07:26,230
So I was trying to build these chatbots for the Persona-Chat data

1370
01:07:26,230 --> 01:07:29,635
set and in particular investigating controllability.

1371
01:07:29,635 --> 01:07:32,875
So we're trying to control aspects of the generated texts such as, you know,

1372
01:07:32,875 --> 01:07:34,045
whether you repeat itself,

1373
01:07:34,045 --> 01:07:35,395
how generic you are,

1374
01:07:35,395 --> 01:07:37,615
kind of these same problems that we noted before.

1375
01:07:37,615 --> 01:07:40,180
So we built these models that control, you know,

1376
01:07:40,180 --> 01:07:42,085
specificity of what we're saying and

1377
01:07:42,085 --> 01:07:44,740
how related what we're saying is to what the user said.

1378
01:07:44,740 --> 01:07:46,090
So here you can see that,

1379
01:07:46,090 --> 01:07:48,880
you know, uh, our partner said something like, "Yes,

1380
01:07:48,880 --> 01:07:51,745
I'm studying law at the moment," and we can kind of control-

1381
01:07:51,745 --> 01:07:54,715
turn this control knob that makes us say something very generic like,

1382
01:07:54,715 --> 01:07:57,010
"Oh," and then like 20 dots or something

1383
01:07:57,010 --> 01:07:59,470
just completely bonkers that's just all the rare words you know.

1384
01:07:59,470 --> 01:08:01,510
And there's like a sweet- a sweet spot between what you say,

1385
01:08:01,510 --> 01:08:03,955
"That sounds like a lot of fun. How long have you been studying?"

1386
01:08:03,955 --> 01:08:06,955
And then similarly, we have a knob we can turn to,

1387
01:08:06,955 --> 01:08:11,260
uh, determine how semantically related what we say is to what, what they said.

1388
01:08:11,260 --> 01:08:13,540
So, um, you know, that's kind of interesting.

1389
01:08:13,540 --> 01:08:16,615
It's, it's a way to control the output of the, uh, NLG system.

1390
01:08:16,615 --> 01:08:19,525
But actually, I want to tell you about how the human evaluation was so difficult,

1391
01:08:19,525 --> 01:08:22,885
so we have these systems that we wanted to generate using human eval.

1392
01:08:22,885 --> 01:08:26,230
So the question is, how do you ask for the human quality judgments here?

1393
01:08:26,230 --> 01:08:29,800
Uh, you can ask kind of simple overall quality questions,

1394
01:08:29,800 --> 01:08:31,975
like, you know, how well does the conversation go?

1395
01:08:31,975 --> 01:08:33,670
Was- was the user engaging?

1396
01:08:33,670 --> 01:08:34,990
Um, or maybe comparative,

1397
01:08:34,990 --> 01:08:38,605
Which of these users gave the best response? Uh, questions like this.

1398
01:08:38,605 --> 01:08:40,330
And, you know, we tried a lot of them,

1399
01:08:40,330 --> 01:08:43,000
but there were just major problems with all of them.

1400
01:08:43,000 --> 01:08:46,960
Like, these questions are necessarily very subjective and also,

1401
01:08:46,960 --> 01:08:49,150
the different respondents have different expectations,

1402
01:08:49,150 --> 01:08:50,620
and this affects their judgments.

1403
01:08:50,620 --> 01:08:53,695
So for example, if you ask, do you think this user is a human or a bot?

1404
01:08:53,695 --> 01:08:55,645
Then, well, that depends entirely on

1405
01:08:55,645 --> 01:09:00,220
this respondents' knowledge of bots or opinion of bots and what they think they can do.

1406
01:09:00,220 --> 01:09:03,940
Another example is, you'd have kind of catastrophic misunderstanding of the question.

1407
01:09:03,940 --> 01:09:05,140
So for example, if we ask,

1408
01:09:05,140 --> 01:09:07,675
was this user- was this chatbot engaging?

1409
01:09:07,675 --> 01:09:09,475
Then someone responded saying, "Yup,

1410
01:09:09,475 --> 01:09:11,020
it was engaging because it always wrote back",

1411
01:09:11,020 --> 01:09:12,310
which clearly isn't what we meant.

1412
01:09:12,310 --> 01:09:14,605
We meant like are they an engaging conversation partner,

1413
01:09:14,605 --> 01:09:16,765
but they took a very literal assumption,

1414
01:09:16,765 --> 01:09:19,075
uh, of, of what engaging means.

1415
01:09:19,075 --> 01:09:22,975
So the problem here is that overall quality depends on many underlying factors,

1416
01:09:22,975 --> 01:09:25,390
and it's pretty hard to kind of find a single,

1417
01:09:25,390 --> 01:09:28,720
overall question that captures just overall quality.

1418
01:09:28,720 --> 01:09:31,030
So we ended up doing this, we ended up breaking this down

1419
01:09:31,030 --> 01:09:34,270
into lots more kind of factors of quality.

1420
01:09:34,270 --> 01:09:36,205
So, uh, the way we saw it is that,

1421
01:09:36,205 --> 01:09:39,820
you have maybe these kind of overall measures of quality of the chatbot,

1422
01:09:39,820 --> 01:09:41,380
such as how engaging was it,

1423
01:09:41,380 --> 01:09:43,135
how enjoyable was it to talk to,

1424
01:09:43,135 --> 01:09:45,685
and kind of maybe how convincing was it that it was human.

1425
01:09:45,685 --> 01:09:47,230
And then below those,

1426
01:09:47,230 --> 01:09:49,810
we kind of broke down as these more low level, uh,

1427
01:09:49,810 --> 01:09:51,685
components of quality such as,

1428
01:09:51,685 --> 01:09:53,290
you know, uh, were you interesting?

1429
01:09:53,290 --> 01:09:55,150
Were you li- showing that you were listening?

1430
01:09:55,150 --> 01:09:56,935
Were you asking enough questions and so on?

1431
01:09:56,935 --> 01:09:59,620
And then below that, we had these kind of controllable attributes which

1432
01:09:59,620 --> 01:10:02,470
were the knobs that we were turning and then the goal was to figure out,

1433
01:10:02,470 --> 01:10:04,525
um, how these things affected the output.

1434
01:10:04,525 --> 01:10:09,745
Um, so let's see.

1435
01:10:09,745 --> 01:10:13,120
Um, so we had a bunch of findings here, and I think,

1436
01:10:13,120 --> 01:10:16,440
maybe the ones which I will highlight were,

1437
01:10:16,440 --> 01:10:18,060
uh, these two kind of in the middle.

1438
01:10:18,060 --> 01:10:19,980
So the overall metric engagingness,

1439
01:10:19,980 --> 01:10:23,085
which means enjoyment, that was really easy to maximize.

1440
01:10:23,085 --> 01:10:24,420
It turned out, uh,

1441
01:10:24,420 --> 01:10:28,300
our bots managed to get near human performance in terms of engagingness.

1442
01:10:28,300 --> 01:10:30,730
Um, but the overall metric humanness,

1443
01:10:30,730 --> 01:10:32,440
that is the kind of Turing test metric,

1444
01:10:32,440 --> 01:10:34,405
that was not at all easy to maximize.

1445
01:10:34,405 --> 01:10:35,920
All of our bots were way,

1446
01:10:35,920 --> 01:10:38,020
way below humans in terms of humanness, right?

1447
01:10:38,020 --> 01:10:40,300
So we were not at all convincing of being human,

1448
01:10:40,300 --> 01:10:41,785
and this is kind of interesting, right?

1449
01:10:41,785 --> 01:10:44,035
Like, we were as enjoyable as talk to as humans,

1450
01:10:44,035 --> 01:10:46,630
but we were clearly not human, right?

1451
01:10:46,630 --> 01:10:50,245
So like, humanness is not the same thing as conversational quality.

1452
01:10:50,245 --> 01:10:52,615
And one of the interesting things we found in this,

1453
01:10:52,615 --> 01:10:55,390
um, study, where we not only evaluated our chatbots,

1454
01:10:55,390 --> 01:10:57,655
we also actually got humans to evaluate each other,

1455
01:10:57,655 --> 01:11:00,925
was that, um, humans are sub-optimal conversationalists.

1456
01:11:00,925 --> 01:11:05,170
Uh, they scored pretty poorly on interestingness, fluency, listening.

1457
01:11:05,170 --> 01:11:06,895
They didn't ask each other enough questions,

1458
01:11:06,895 --> 01:11:09,460
and this is kind of the reason why we managed to like approach

1459
01:11:09,460 --> 01:11:13,150
human performance in kind of enjoyableness to talk to you because we just,

1460
01:11:13,150 --> 01:11:16,500
for example, turned up the question asking knob, asked more questions,

1461
01:11:16,500 --> 01:11:19,875
and people responded really well to that because people like talking about themselves.

1462
01:11:19,875 --> 01:11:22,290
So, um, yeah.

1463
01:11:22,290 --> 01:11:23,610
I think this is kind of interesting, right?

1464
01:11:23,610 --> 01:11:26,820
Because it shows that there is no obvious just one question to ask, right?

1465
01:11:26,820 --> 01:11:27,990
Because if you just seemed, "Oh,

1466
01:11:27,990 --> 01:11:31,870
the one question to ask is clearly engagingness or it's clearly humanness,

1467
01:11:31,870 --> 01:11:35,365
then we would have gotten completely different reads on how well we were doing, right?

1468
01:11:35,365 --> 01:11:39,770
Whereas asking these multiple questions kind of gives you more of an overview.

1469
01:11:41,730 --> 01:11:46,780
I am going to skip this just because there's not a lot of time.

1470
01:11:46,780 --> 01:11:48,595
Okay. So, here's the final section.

1471
01:11:48,595 --> 01:11:51,670
Uh, this is my kind of wrap-up thoughts on NLG research,

1472
01:11:51,670 --> 01:11:54,340
the current trends and where we're going in the future.

1473
01:11:54,340 --> 01:11:59,020
So, here's kind of three exciting current trends to identify in NLG.

1474
01:11:59,020 --> 01:12:00,985
And of course your mileage may vary,

1475
01:12:00,985 --> 01:12:02,860
you might think that other things are more interesting.

1476
01:12:02,860 --> 01:12:05,110
So, uh, the ones which I was thinking about, are

1477
01:12:05,110 --> 01:12:08,635
firstly incorporating discrete latent variables into NLG.

1478
01:12:08,635 --> 01:12:11,140
Um, so, you should go check out

1479
01:12:11,140 --> 01:12:13,585
the slides I skipped over because there were some examples of this.

1480
01:12:13,585 --> 01:12:16,960
But the idea is that with some tasks such as for example

1481
01:12:16,960 --> 01:12:19,360
storytelling or task oriented dialogue

1482
01:12:19,360 --> 01:12:21,100
where you're trying to actually get something done.

1483
01:12:21,100 --> 01:12:22,810
Um, you probably want a more kind of

1484
01:12:22,810 --> 01:12:25,540
concrete hard notion of the things that you're talking about

1485
01:12:25,540 --> 01:12:30,160
like you know, entities and people and events and negotiation and so on.

1486
01:12:30,160 --> 01:12:33,805
So, uh, there's, there's mentioning what kind of modeling

1487
01:12:33,805 --> 01:12:38,860
these discrete latent variables inside these continuous, uh, NLG methods.

1488
01:12:38,860 --> 01:12:42,520
The second one is alternatives to strict left to right generation.

1489
01:12:42,520 --> 01:12:44,440
And I'm really sorry [LAUGHTER] I skipped over so many things.

1490
01:12:44,440 --> 01:12:47,020
Um, so, there's some interesting work recently in trying

1491
01:12:47,020 --> 01:12:49,810
to generate text in ways other than left to right.

1492
01:12:49,810 --> 01:12:51,160
So, for example there's some kind of

1493
01:12:51,160 --> 01:12:55,735
parallel generation stuff or maybe writing something and iteratively refining it, uh,

1494
01:12:55,735 --> 01:12:59,815
there's also the idea of kind of top-down generation, um, for

1495
01:12:59,815 --> 01:13:02,800
especially longer pieces of text like maybe tried to decide the contents

1496
01:13:02,800 --> 01:13:06,385
of each of the sentences separately before uh, writing the words.

1497
01:13:06,385 --> 01:13:08,620
And then a third one is like

1498
01:13:08,620 --> 01:13:11,530
alternatives to maximum likelihood training with teacher forcing.

1499
01:13:11,530 --> 01:13:14,320
So, to remind you, a maximum likelihood training with teacher forcing is

1500
01:13:14,320 --> 01:13:16,420
just the standard method of training

1501
01:13:16,420 --> 01:13:19,210
a language model that we've been telling you about in the class so far.

1502
01:13:19,210 --> 01:13:20,755
Um, so, you know,

1503
01:13:20,755 --> 01:13:23,200
there's some interesting work on looking at more kind of holistic,

1504
01:13:23,200 --> 01:13:25,735
um, sentence level rather than word level objectives.

1505
01:13:25,735 --> 01:13:27,550
Uh, so, unfortunately I ran out of time with

1506
01:13:27,550 --> 01:13:29,860
this slide, and I didn't have time to put the references in but I will

1507
01:13:29,860 --> 01:13:31,990
put the references in later and it

1508
01:13:31,990 --> 01:13:34,945
will be on the course website so you can go check them out later.

1509
01:13:34,945 --> 01:13:39,820
Okay. So, as a kind of overview, NLG research, where are we and where are we going?

1510
01:13:39,820 --> 01:13:41,950
Um, so my metaphor is I think that

1511
01:13:41,950 --> 01:13:46,210
about five years ago NLP and deep learning research was a kind of a Wild West.

1512
01:13:46,210 --> 01:13:50,770
Right? Like everything was new and um, we were unsure,

1513
01:13:50,770 --> 01:13:52,300
NLP research weren't sure what kind of what

1514
01:13:52,300 --> 01:13:55,660
the new research landscape was because uh, you know,

1515
01:13:55,660 --> 01:13:58,645
uh, neural methods kind of changed machine translation a lot,

1516
01:13:58,645 --> 01:14:01,675
looked like they might change other areas but it was uncertain how much.

1517
01:14:01,675 --> 01:14:04,690
Um, but these days you know five years later,

1518
01:14:04,690 --> 01:14:06,475
um, it's a lot less wild.

1519
01:14:06,475 --> 01:14:09,125
I'd say, you know things are settled down a lot kind of

1520
01:14:09,125 --> 01:14:13,140
standard practices have emerged and sure there's still a lot of things changing.

1521
01:14:13,140 --> 01:14:15,240
Um, but you know there's more people in the community,

1522
01:14:15,240 --> 01:14:16,500
there's more standard practices,

1523
01:14:16,500 --> 01:14:18,240
we have things like TensorFlow and PyTorch.

1524
01:14:18,240 --> 01:14:20,085
So, you don't have to take up gradients anymore.

1525
01:14:20,085 --> 01:14:22,665
So, I'd say things are a lot less wild now

1526
01:14:22,665 --> 01:14:26,370
but I would say NLG does seem to be one of the wildest parts

1527
01:14:26,370 --> 01:14:29,880
remaining and part of the reasons for that is because of

1528
01:14:29,880 --> 01:14:33,915
the lack of evaluation metrics that makes it so difficult to tell what we're doing.

1529
01:14:33,915 --> 01:14:37,260
It's, uh, quite difficult to identify like what are the main methods that are

1530
01:14:37,260 --> 01:14:41,810
working when we don't have any metrics that can clearly tell us what's going on.

1531
01:14:41,880 --> 01:14:44,710
So, another thing that I'm really glad to see is that

1532
01:14:44,710 --> 01:14:47,830
the neural NLG community is rapidly expanding.

1533
01:14:47,830 --> 01:14:51,040
Um, so, in the early years, uh,

1534
01:14:51,040 --> 01:14:55,390
people were mostly transferring successful NMT methods to various NLG tasks.

1535
01:14:55,390 --> 01:14:58,870
Uh, but now I'm seeing you know, increasingly more inventive NLG techniques

1536
01:14:58,870 --> 01:15:02,725
merging which is specific to the non-NMT generation settings.

1537
01:15:02,725 --> 01:15:05,845
Um, and again I urge you to go back into the slides that I skipped.

1538
01:15:05,845 --> 01:15:08,650
Um, so, I'm also saying there's increasingly more kind of

1539
01:15:08,650 --> 01:15:11,590
neural NLG workshops and competitions especially

1540
01:15:11,590 --> 01:15:14,470
focusing on open-ended NLG like those tasks that we

1541
01:15:14,470 --> 01:15:18,055
know are not well suited by the automatic metrics that work for NMT.

1542
01:15:18,055 --> 01:15:22,720
So, there's a neural generation workshop, a storytelling workshop uh,

1543
01:15:22,720 --> 01:15:26,470
and various challenges as well where people enter their for example, um,

1544
01:15:26,470 --> 01:15:28,870
conversational dialogue agents to be,

1545
01:15:28,870 --> 01:15:31,495
um, evaluated against each other.

1546
01:15:31,495 --> 01:15:33,520
So, I think that these different, um,

1547
01:15:33,520 --> 01:15:35,350
kind of community organizing workshops and

1548
01:15:35,350 --> 01:15:38,710
competitions are really doing a great job to kind of organize a community,

1549
01:15:38,710 --> 01:15:44,080
increase reproducibility and standard evaluate, standardized evaluation.

1550
01:15:44,080 --> 01:15:46,300
Um, so, this is great but I'd say

1551
01:15:46,300 --> 01:15:50,230
the biggest roadblock to progress is definitely still evaluation.

1552
01:15:50,230 --> 01:15:53,305
Okay. So, the last thing that I want to share with you

1553
01:15:53,305 --> 01:15:56,260
is eight things that I've learned from working in NLG.

1554
01:15:56,260 --> 01:15:58,930
So, the first one is the more open-ended the task,

1555
01:15:58,930 --> 01:16:00,535
the harder everything becomes.

1556
01:16:00,535 --> 01:16:03,655
Evaluation becomes harder, defining what you're doing becomes harder,

1557
01:16:03,655 --> 01:16:05,905
telling when you're doing a good job becomes harder.

1558
01:16:05,905 --> 01:16:09,130
So, for this reason constraints can sometimes make things more welcome.

1559
01:16:09,130 --> 01:16:15,370
So, if you decide to constrain your task then sometimes it's easier to, to complete it.

1560
01:16:15,370 --> 01:16:19,120
Uh, the next one is aiming for a specific improvement can

1561
01:16:19,120 --> 01:16:22,675
often be more manageable than aiming to improve overall generation quality.

1562
01:16:22,675 --> 01:16:25,285
So, for example, if you decide that you want to

1563
01:16:25,285 --> 01:16:27,865
well for example increase diversity for your model, like say

1564
01:16:27,865 --> 01:16:31,270
more interesting things that's an easier thing to achieve and measure than just

1565
01:16:31,270 --> 01:16:35,875
saying we want to do overall generation quality because of the evaluation problem.

1566
01:16:35,875 --> 01:16:40,285
The next one is if you're using your language model to do NLG,

1567
01:16:40,285 --> 01:16:44,860
then improving the language model that is getting better with perplexity will give you

1568
01:16:44,860 --> 01:16:46,960
probably better generation quality because you've got

1569
01:16:46,960 --> 01:16:51,220
a stronger language model but it's not the only way to improve generation quality,

1570
01:16:51,220 --> 01:16:53,065
as we talked about before, uh,

1571
01:16:53,065 --> 01:16:56,995
there's also other components that can affect generation apart from just language model,

1572
01:16:56,995 --> 01:17:00,340
and that's part of the problem is that that's not in the training objective.

1573
01:17:00,340 --> 01:17:03,655
Um, my next tip is that you should look at your output a lot,

1574
01:17:03,655 --> 01:17:07,150
partially because you don't have any single metric that can tell you what's going on.

1575
01:17:07,150 --> 01:17:10,270
It's pretty important to look at your output a lot to form your own opinions.

1576
01:17:10,270 --> 01:17:12,745
It can be time consuming but it's probably worth doing.

1577
01:17:12,745 --> 01:17:14,575
I ended up talking to these chatbots

1578
01:17:14,575 --> 01:17:17,440
a huge amount during the time that I was working on the project.

1579
01:17:17,440 --> 01:17:21,640
Okay. Almost done, so, five you need an automatic metric, even if it's imperfect.

1580
01:17:21,640 --> 01:17:23,050
So, I know you that already know this because we

1581
01:17:23,050 --> 01:17:25,135
wrote it all over the project instructions.

1582
01:17:25,135 --> 01:17:29,200
Uh, but I'd probably amend that to like maybe you need several automatic metrics.

1583
01:17:29,200 --> 01:17:30,760
I talked earlier about how you might track

1584
01:17:30,760 --> 01:17:33,445
multiple things to get an overall picture of what's going on,

1585
01:17:33,445 --> 01:17:36,100
I'd say the more open-ended your NLG task is,

1586
01:17:36,100 --> 01:17:39,175
the more likely you need probably several metrics.

1587
01:17:39,175 --> 01:17:43,195
If you do human eval, you want to make the questions as focused as possible.

1588
01:17:43,195 --> 01:17:45,100
So, as I found out the hard way if you

1589
01:17:45,100 --> 01:17:47,785
define the question as a very kind of overall vague thing,

1590
01:17:47,785 --> 01:17:49,885
then you're just opening yourself up to, um,

1591
01:17:49,885 --> 01:17:52,975
the respondents kind of misunderstanding you and, uh,

1592
01:17:52,975 --> 01:17:54,670
if they are doing that then it's actually not their fault,

1593
01:17:54,670 --> 01:17:57,475
it's your fault and you need to take your questions and that's what I learned.

1594
01:17:57,475 --> 01:17:59,860
Uh, next thing is reproducibility is

1595
01:17:59,860 --> 01:18:03,580
a huge problem in today's NLP and deep learning in general,

1596
01:18:03,580 --> 01:18:06,130
and the problem is only bigger in NLG,

1597
01:18:06,130 --> 01:18:08,380
I guess it's another way that it's still a wild west.

1598
01:18:08,380 --> 01:18:10,930
So, I'd say that, uh, it would be really great,

1599
01:18:10,930 --> 01:18:13,300
if everybody could publicly release all of

1600
01:18:13,300 --> 01:18:15,985
their generated output when they write NLG papers.

1601
01:18:15,985 --> 01:18:20,155
I think this is a great practice because if you released your generated outputs,

1602
01:18:20,155 --> 01:18:23,875
then if someone later let's say comes up with a great automatic metric,

1603
01:18:23,875 --> 01:18:27,985
then they can just grab your generated output and then compute the metric on that.

1604
01:18:27,985 --> 01:18:30,040
Whereas if he never released your output or you

1605
01:18:30,040 --> 01:18:32,470
released with some kind of imperfect metric number,

1606
01:18:32,470 --> 01:18:35,020
then future researchers have nothing to compare it against.

1607
01:18:35,020 --> 01:18:38,575
Uh, so lastly, my last thought

1608
01:18:38,575 --> 01:18:42,790
about working in NLG is that it can be very frustrating sometimes,

1609
01:18:42,790 --> 01:18:45,745
uh, because things can be difficult and it's hard to know when you're making progress.

1610
01:18:45,745 --> 01:18:48,805
But the upside is it can also be very funny.

1611
01:18:48,805 --> 01:18:52,740
So this my last slide, here are some bizarre conversations that I've had with my chatbot.

1612
01:18:52,740 --> 01:18:54,000
[LAUGHTER] Thanks.

1613
01:18:54,000 --> 01:19:37,000
[NOISE] [LAUGHTER] All right, thanks.

