1
00:00:04,850 --> 00:00:07,410
Okay. Hi everyone.

2
00:00:07,410 --> 00:00:11,430
Let's get started [NOISE] Okay.

3
00:00:11,430 --> 00:00:14,280
So, so for today's lecture,

4
00:00:14,280 --> 00:00:20,025
what we're gonna do is look at the topic of having Tree Recursive Neural Networks.

5
00:00:20,025 --> 00:00:22,110
I mean, this is actually, uh,

6
00:00:22,110 --> 00:00:26,660
a topic which I feel especially fond of and attached to,

7
00:00:26,660 --> 00:00:32,975
because actually when we started doing deep learning for NLP here at Stanford in 2010,

8
00:00:32,975 --> 00:00:37,940
really for the sort of period from 2010 to 2015,

9
00:00:37,940 --> 00:00:42,320
the dominant set of ideas that we were working on was this topic of how you

10
00:00:42,320 --> 00:00:46,945
could build a recur- recursive tree structure into neural networks.

11
00:00:46,945 --> 00:00:50,645
So in a way, it's kind of funny that I'm only getting to it now.

12
00:00:50,645 --> 00:00:53,420
I mean, there are sort of reasons for that,

13
00:00:53,420 --> 00:00:56,720
but I think there are a bunch of interesting ideas

14
00:00:56,720 --> 00:01:00,365
here which relate closely to linguistic structure,

15
00:01:00,365 --> 00:01:02,720
and so it's good stuff to have seen.

16
00:01:02,720 --> 00:01:04,625
But in practice, um,

17
00:01:04,625 --> 00:01:08,105
these ideas have proven kind of hard to scale

18
00:01:08,105 --> 00:01:11,870
and not necessarily to work better in practice than

19
00:01:11,870 --> 00:01:15,140
the kind of things that we've spent more time on

20
00:01:15,140 --> 00:01:19,100
meaning things like looking at LSTMs and looking at transformers,

21
00:01:19,100 --> 00:01:20,570
and things like that.

22
00:01:20,570 --> 00:01:25,860
And so that's kinda why we sort of shunted them towards the end of the curriculum.

23
00:01:25,860 --> 00:01:29,090
But I want to sort of say something about the motivations,

24
00:01:29,090 --> 00:01:31,190
and the ways you can build tree structures,

25
00:01:31,190 --> 00:01:34,220
and neural networks, and look at some of the possibilities,

26
00:01:34,220 --> 00:01:36,065
um, we explored um,

27
00:01:36,065 --> 00:01:38,170
in during this class.

28
00:01:38,170 --> 00:01:44,690
Um, another fact about this class is actually this is the last class I'm going to give.

29
00:01:44,690 --> 00:01:47,060
Um, so two more classes next week.

30
00:01:47,060 --> 00:01:49,320
Don't forget about next week, um,

31
00:01:49,320 --> 00:01:52,275
CS224N classes, um,

32
00:01:52,275 --> 00:01:54,220
but on Tuesday, um,

33
00:01:54,220 --> 00:01:57,280
we've gotten the final invited speaker, ,

34
00:01:57,280 --> 00:02:00,410
who's a great speaker and has tons of interesting stuff

35
00:02:00,410 --> 00:02:04,865
to say about fairness and ethics in NLP and AI.

36
00:02:04,865 --> 00:02:06,425
And then for the final lecture,

37
00:02:06,425 --> 00:02:09,650
one of my- another of my PhD students  is

38
00:02:09,650 --> 00:02:13,270
gonna give that and talk about some of the recent,

39
00:02:13,270 --> 00:02:16,965
what's been happening in deep learning in 2018, '19,

40
00:02:16,965 --> 00:02:20,975
of some of the sort of recent developments in NLP and deep learning. Um,

41
00:02:20,975 --> 00:02:24,875
so, um, let's- I'll say my farewells at the end of this one.

42
00:02:24,875 --> 00:02:28,730
Um, so hopefully, everyone has submitted, um,

43
00:02:28,730 --> 00:02:33,380
their, um, milestone for their final project.

44
00:02:33,380 --> 00:02:37,310
If you haven't, you should really begin your milestone in- um,

45
00:02:37,310 --> 00:02:40,640
you know, it's inevitable that somewhere around here,

46
00:02:40,640 --> 00:02:46,290
there start to be problems that people have the situation that nothing works,

47
00:02:46,290 --> 00:02:48,855
and everything is too slow, and you panic.

48
00:02:48,855 --> 00:02:52,095
Um, and, um, this happens.

49
00:02:52,095 --> 00:02:54,030
Um, I wish you luck, of course.

50
00:02:54,030 --> 00:02:55,900
I mean, what can you do about it?

51
00:02:55,900 --> 00:03:00,785
I mean, it can be really hard when you have things that don't work as to work out,

52
00:03:00,785 --> 00:03:02,255
why they don't work,

53
00:03:02,255 --> 00:03:03,800
and how to fix them.

54
00:03:03,800 --> 00:03:08,660
I mean, I think often the best thing to do is really to go back to something

55
00:03:08,660 --> 00:03:14,405
simple that you can get working and to work forward from there again.

56
00:03:14,405 --> 00:03:18,980
It also really helps to have really small data sets.

57
00:03:18,980 --> 00:03:23,540
I really recommend the strategy of sort of having a 10-item,

58
00:03:23,540 --> 00:03:28,510
or 20-item data set and checking that your model works perfectly,

59
00:03:28,510 --> 00:03:31,320
over-trains to 100 percent accuracy on that kind of data

60
00:03:31,320 --> 00:03:34,610
set saves you huge amounts of time,

61
00:03:34,610 --> 00:03:39,170
and it's sort of after you've gotten something simple working on a small amount of data,

62
00:03:39,170 --> 00:03:41,780
that's the right time to sort of then,

63
00:03:41,780 --> 00:03:44,600
um, expand forward again.

64
00:03:44,600 --> 00:03:47,330
Um, you should definitely always make sure that

65
00:03:47,330 --> 00:03:49,970
you can completely overfit on your training data set.

66
00:03:49,970 --> 00:03:51,180
That's sort of, um,

67
00:03:51,180 --> 00:03:52,370
not quite a proof,

68
00:03:52,370 --> 00:03:56,780
but it's at least a first good requirement for your model being implemented properly.

69
00:03:56,780 --> 00:04:01,130
Um, you, you know part of the trick of being

70
00:04:01,130 --> 00:04:03,920
a successful deep learning researcher is actually

71
00:04:03,920 --> 00:04:07,565
managing to get things done and not wasting a ton of time.

72
00:04:07,565 --> 00:04:10,300
And so it definitely always helps just to be, you know,

73
00:04:10,300 --> 00:04:12,975
plotting as you go along your training and

74
00:04:12,975 --> 00:04:16,160
dev errors so that you can sort of tell if things are working,

75
00:04:16,160 --> 00:04:17,900
or if things aren't working,

76
00:04:17,900 --> 00:04:20,930
and you should abandon and start again with a new experiment,

77
00:04:20,930 --> 00:04:25,505
tha- that just things like that save you hours and get you, uh, more done.

78
00:04:25,505 --> 00:04:27,350
And so then once things are working,

79
00:04:27,350 --> 00:04:30,080
there's sort of a whole bunch of things to make it work better.

80
00:04:30,080 --> 00:04:33,410
There's regularization with L2 and Dropout,

81
00:04:33,410 --> 00:04:36,969
there's time to do hyperparameter search,

82
00:04:36,969 --> 00:04:38,530
um, and, you know,

83
00:04:38,530 --> 00:04:42,620
often doing these things and make quite a lot of difference to what

84
00:04:42,620 --> 00:04:46,820
your final results are and so it's good to have time to do those things.

85
00:04:46,820 --> 00:04:48,680
But clearly, you want to get things, um,

86
00:04:48,680 --> 00:04:51,590
working first before you go on to that, um,

87
00:04:51,590 --> 00:04:54,380
and sort of really encourage people to still

88
00:04:54,380 --> 00:04:57,470
stop by in office hours if you've got any problems,

89
00:04:57,470 --> 00:05:00,530
and we'll try our best to help out here within

90
00:05:00,530 --> 00:05:05,205
the limitations of what we can do from just being hit cold with problems.

91
00:05:05,205 --> 00:05:08,240
Okay, um, yeah.

92
00:05:08,240 --> 00:05:13,595
So, I wanted to sort of just say some general remarks about, um,

93
00:05:13,595 --> 00:05:16,680
language and theories of language,

94
00:05:16,680 --> 00:05:22,790
um, that, in the context that motivate these tree recursive networks.

95
00:05:22,790 --> 00:05:27,485
Um, so this is an art installation at Carnegie Mellon University.

96
00:05:27,485 --> 00:05:29,150
And as an NLP person,

97
00:05:29,150 --> 00:05:31,700
I really love this art installation.

98
00:05:31,700 --> 00:05:36,805
Um, so we need better art installations around the Stanford School of Engineering.

99
00:05:36,805 --> 00:05:40,510
Um, so this is the bag-of-words art Installation.

100
00:05:40,510 --> 00:05:42,710
There's the bag with a lot of words in it.

101
00:05:42,710 --> 00:05:44,120
And you see down here,

102
00:05:44,120 --> 00:05:47,015
there were the stop words, the the, and the us,

103
00:05:47,015 --> 00:05:49,135
that had fallen out of the bag,

104
00:05:49,135 --> 00:05:52,175
and are represented on the ground as the stop words.

105
00:05:52,175 --> 00:05:55,730
Beautiful artwork, right? So, um,

106
00:05:55,730 --> 00:06:02,690
one of the interesting things that has been found about NLP models of language,

107
00:06:02,690 --> 00:06:04,670
and I think this is even more true in

108
00:06:04,670 --> 00:06:08,600
the deep learning world than it used to be previously is,

109
00:06:08,600 --> 00:06:11,510
boy, you can do a lot with bag-of-words models, right?

110
00:06:11,510 --> 00:06:15,920
That you can just often get a lot of power by saying,

111
00:06:15,920 --> 00:06:18,400
well, let's get our neural word vectors,

112
00:06:18,400 --> 00:06:20,670
we're gonna average them or max pool them,

113
00:06:20,670 --> 00:06:21,975
or something like this,

114
00:06:21,975 --> 00:06:23,580
and do nothing more,

115
00:06:23,580 --> 00:06:26,300
and that gives me a pretty good sentence representation or

116
00:06:26,300 --> 00:06:30,035
document representation that I could use in a classifier or something.

117
00:06:30,035 --> 00:06:33,830
And sometimes, you can do not much more than that and get even better.

118
00:06:33,830 --> 00:06:37,910
So people have done things like deep averaging networks where you're taking

119
00:06:37,910 --> 00:06:40,400
the output of a bag-of-words model and sort of

120
00:06:40,400 --> 00:06:43,475
feeding it through a couple more layers and improving things.

121
00:06:43,475 --> 00:06:47,300
So that is in complete distinction to

122
00:06:47,300 --> 00:06:51,560
what's been dominant in linguistics of looking at language structure.

123
00:06:51,560 --> 00:06:58,340
That typically in linguistics the emphasis has been on identifying kind of

124
00:06:58,340 --> 00:07:05,480
huge amounts of structure of linguistic utterances through very complex formalisms.

125
00:07:05,480 --> 00:07:10,455
I guess this is sort of a bit of a picture of a Chomsky minimalism syntactic tree,

126
00:07:10,455 --> 00:07:15,260
and the one up at the top is a bit of a picture of head-driven phrase structure grammar.

127
00:07:15,260 --> 00:07:18,035
Which was a theory that was predominantly, um,

128
00:07:18,035 --> 00:07:21,935
developed at Stanford in the '90s.

129
00:07:21,935 --> 00:07:25,070
Um, but sort of very complex data structures and

130
00:07:25,070 --> 00:07:28,895
articulated structures used to describe linguistics.

131
00:07:28,895 --> 00:07:32,645
And there's a huge gap between these two things.

132
00:07:32,645 --> 00:07:36,260
And you might think that, you know, surely,

133
00:07:36,260 --> 00:07:41,180
there's some good points in the middle where we have a certain amount of structure,

134
00:07:41,180 --> 00:07:43,460
and that's going to help us do what we want.

135
00:07:43,460 --> 00:07:46,525
And so in particular, um,

136
00:07:46,525 --> 00:07:49,745
that if we're wanting to semantically interpret language,

137
00:07:49,745 --> 00:07:53,300
it seems like we don't just want to have word vectors,

138
00:07:53,300 --> 00:07:56,120
we want to have meanings of bigger phrases.

139
00:07:56,120 --> 00:07:59,705
So here's the snowboarders leaping over a mogul,

140
00:07:59,705 --> 00:08:03,170
and a person on a snowboard jumps into the air.

141
00:08:03,170 --> 00:08:06,635
And what we'd like to be able to say is that the snowboarder

142
00:08:06,635 --> 00:08:10,790
means basically the same thing as a person on the snowboard.

143
00:08:10,790 --> 00:08:13,220
So we wanted to have these chunks of

144
00:08:13,220 --> 00:08:17,255
language which in linguistics will be constituent  phrases,

145
00:08:17,255 --> 00:08:19,430
and say that they have a meaning,

146
00:08:19,430 --> 00:08:21,830
and we'd like to be able to compare their meaning.

147
00:08:21,830 --> 00:08:26,870
Now, we've looked at at least one tool that allows us to have chunks of language, right?

148
00:08:26,870 --> 00:08:30,320
Because we looked at convolutional neural networks where you could take

149
00:08:30,320 --> 00:08:34,805
three words and make a representation of the convolutional neural network,

150
00:08:34,805 --> 00:08:38,090
but the fundamental difference is that in

151
00:08:38,090 --> 00:08:41,254
human languages you have these chunks that have meaning,

152
00:08:41,254 --> 00:08:43,055
that are of different sizes.

153
00:08:43,055 --> 00:08:45,935
So we'd like to say the snowboarder

154
00:08:45,935 --> 00:08:50,120
is pretty much semantically equivalent to a person on the snowboard,

155
00:08:50,120 --> 00:08:52,790
but the top one is two words long,

156
00:08:52,790 --> 00:08:55,295
and the bottom one is five words long.

157
00:08:55,295 --> 00:08:59,200
And so if we're gonna be able to do that, um,

158
00:08:59,200 --> 00:09:03,230
we somehow wanted to have these sort of constituent chunks and be

159
00:09:03,230 --> 00:09:07,070
able to work with and represent them in neural networks.

160
00:09:07,070 --> 00:09:08,630
And that's sort of, um,

161
00:09:08,630 --> 00:09:11,870
the central idea of what

162
00:09:11,870 --> 00:09:16,235
motivated some of the sort of tree structured neural networks that I'm about to show you.

163
00:09:16,235 --> 00:09:21,380
There's another related thing that you might wanna think about is, you know,

164
00:09:21,380 --> 00:09:23,240
a person on a snowboard,

165
00:09:23,240 --> 00:09:27,950
how do human beings manage to understand what that means?

166
00:09:27,950 --> 00:09:31,055
And then a person on a snowboard jumps into the air,

167
00:09:31,055 --> 00:09:35,655
how does people manage to understand what that means?

168
00:09:35,655 --> 00:09:41,030
And it sort of seems like the only possible answer to

169
00:09:41,030 --> 00:09:46,595
this is what's normally referred to as the principle of compositionality.

170
00:09:46,595 --> 00:09:49,040
That people know the word person,

171
00:09:49,040 --> 00:09:50,240
they know the word on,

172
00:09:50,240 --> 00:09:52,760
they know the word snowboard, therefore,

173
00:09:52,760 --> 00:09:55,880
they can work out what on a snowboard means, um,

174
00:09:55,880 --> 00:10:00,170
and they can work out what person on a snowboard means by knowing

175
00:10:00,170 --> 00:10:05,900
the meanings of components and putting them together into bigger pieces.

176
00:10:05,900 --> 00:10:08,295
There's a f- there's a famous,

177
00:10:08,295 --> 00:10:12,500
um, applied mathematician statistician, um,

178
00:10:12,500 --> 00:10:17,235
at Brown University, Stu Geman, and I guess the way he summarized this is,

179
00:10:17,235 --> 00:10:21,680
either the principle of compositionality is true, or God exists.

180
00:10:21,680 --> 00:10:24,285
Um, for [LAUGHTER] which he was, um,

181
00:10:24,285 --> 00:10:27,200
well you can take that as- as you want but, you know,

182
00:10:27,200 --> 00:10:30,480
um, I think what he meant was well, you know,

183
00:10:30,480 --> 00:10:32,880
you can just make these infinite number of

184
00:10:32,880 --> 00:10:36,085
infinitely long sentences and human beings understand them,

185
00:10:36,085 --> 00:10:39,365
that it just has to be that people can know about

186
00:10:39,365 --> 00:10:43,650
words and ways to combine meanings and-and make bigger meanings because,

187
00:10:43,650 --> 00:10:47,945
you know, how else could it possibly work that people could understand sentences.

188
00:10:47,945 --> 00:10:50,375
And so we want to be able to do that.

189
00:10:50,375 --> 00:10:54,664
We want to be able to work out semantic compositions of smaller elements,

190
00:10:54,664 --> 00:10:57,415
to work out the meanings of bigger pieces.

191
00:10:57,415 --> 00:11:01,200
And that this obviously isn't only a linguistic thing,

192
00:11:01,200 --> 00:11:05,050
compositionality, um, appears in other places as well, right.

193
00:11:05,050 --> 00:11:10,620
So, um, if you want to understand how some piece of machinery works,

194
00:11:10,620 --> 00:11:14,195
what you kind of wanna know is it has different sub-components.

195
00:11:14,195 --> 00:11:16,140
And if you can understand how

196
00:11:16,140 --> 00:11:19,370
the different sub-components work and how they're fitted together,

197
00:11:19,370 --> 00:11:24,605
um, then you might have some understanding of how the whole scene works.

198
00:11:24,605 --> 00:11:31,170
Um, and, um, compositionality seems to be wor- at work in vision as well.

199
00:11:31,170 --> 00:11:35,950
So here is a scene and again it seems like this scene has parts.

200
00:11:35,950 --> 00:11:38,650
So there are little parts that go together, right.

201
00:11:38,650 --> 00:11:41,725
So there are people that go together into a crowd of people,

202
00:11:41,725 --> 00:11:44,860
and there's a roof and a second floor and another bit of roof.

203
00:11:44,860 --> 00:11:48,850
and a first floor that go together into a picture of this church.

204
00:11:48,850 --> 00:11:54,275
And so this is also kind of a compositional scene in which pieces go together.

205
00:11:54,275 --> 00:11:58,695
So it sort of seems like certainly for language understanding,

206
00:11:58,695 --> 00:12:02,810
and then really for a lot of the other things that we use for intelligence,

207
00:12:02,810 --> 00:12:05,495
that we somehow need to be able to understand

208
00:12:05,495 --> 00:12:09,335
bigger things from knowing about smaller parts.

209
00:12:09,335 --> 00:12:14,870
Um, yeah, so computational- so the most- I mentioned this earlier,

210
00:12:14,870 --> 00:12:16,850
sometime the most famous, um,

211
00:12:16,850 --> 00:12:20,480
linguist is Noam Chomsky at MIT and,

212
00:12:20,480 --> 00:12:24,480
um, you know, really computational linguists,

213
00:12:24,480 --> 00:12:28,355
a lot of the time haven't been that friendly to, um,

214
00:12:28,355 --> 00:12:32,590
linguistics linguists and in particular some of Noam Chomsky's, um,

215
00:12:32,590 --> 00:12:36,250
theories of language because really he's never been

216
00:12:36,250 --> 00:12:39,970
sympathetic to the idea of machine learning.

217
00:12:39,970 --> 00:12:44,075
Or in general does some of the empirical ability to learn from data.

218
00:12:44,075 --> 00:12:46,020
He's sort of always been, um,

219
00:12:46,020 --> 00:12:48,575
[NOISE] wanting to refuse to that exists.

220
00:12:48,575 --> 00:12:51,470
But, um, if we nevertheless look for a little bit of,

221
00:12:51,470 --> 00:12:53,565
um, insight on that.

222
00:12:53,565 --> 00:12:58,115
Um, you know, this is a recent paper of Chomsky's with authors and that they're sort

223
00:12:58,115 --> 00:13:03,120
of trying to give a version of what is unique about human language.

224
00:13:03,120 --> 00:13:05,280
And essentially what they, um,

225
00:13:05,280 --> 00:13:07,985
zero in on is that well,

226
00:13:07,985 --> 00:13:09,910
if you're sort of looking at, you know,

227
00:13:09,910 --> 00:13:13,730
humans versus other fairly intelligent creatures.

228
00:13:13,730 --> 00:13:17,790
They suggest that the defining difference of human beings, um,

229
00:13:17,790 --> 00:13:22,095
is that they have this ability to model recursion.

230
00:13:22,095 --> 00:13:27,450
And so the- this paper argues that the- the singular distinction that allowed

231
00:13:27,450 --> 00:13:30,390
language to develop in human beings was that we

232
00:13:30,390 --> 00:13:33,555
could put together smaller parts to make bigger things,

233
00:13:33,555 --> 00:13:38,495
in a recursive process and that that was the sort of defining new ability.

234
00:13:38,495 --> 00:13:41,135
Um, not sure I- not sure I believe that or not,

235
00:13:41,135 --> 00:13:43,920
um, [LAUGHTER] you can decide what you think.

236
00:13:43,920 --> 00:13:46,325
But what I think, um,

237
00:13:46,325 --> 00:13:51,390
is certainly the case is that- it's just incontrovertible that

238
00:13:51,390 --> 00:13:56,990
the structure of human language sentences have these pieces,

239
00:13:56,990 --> 00:14:01,260
um, constituents that then form together hierarchically or

240
00:14:01,260 --> 00:14:05,555
recursively into bigger pieces as you go up in the tree.

241
00:14:05,555 --> 00:14:11,190
And in particular you get this recursion where you get a little noun phrase meat,

242
00:14:11,190 --> 00:14:15,375
which then appears in a bigger noun phrase like spaghetti with meat.

243
00:14:15,375 --> 00:14:17,640
And you can repeat that several times,

244
00:14:17,640 --> 00:14:19,530
giving you a recursive structure.

245
00:14:19,530 --> 00:14:22,710
And I have an example of that in blue up at the top.

246
00:14:22,710 --> 00:14:25,180
So the person standing next to the man from

247
00:14:25,180 --> 00:14:28,035
the company that purchased the firm that you used to work at,

248
00:14:28,035 --> 00:14:32,675
um, that whole thing is a big noun phrase.

249
00:14:32,675 --> 00:14:36,280
Um, but inside that there's a noun phrase,

250
00:14:36,280 --> 00:14:39,780
the man from the company that purchased the firm that you used to work at,

251
00:14:39,780 --> 00:14:41,880
which is another big noun phrase.

252
00:14:41,880 --> 00:14:43,820
And well inside that, um,

253
00:14:43,820 --> 00:14:47,260
there are smaller noun phrases like,

254
00:14:47,260 --> 00:14:49,890
the company that purchased the firm you used to work at.

255
00:14:49,890 --> 00:14:53,190
But, you know, it's still got inside that noun phrases like,

256
00:14:53,190 --> 00:14:55,055
the firm that you used to work at.

257
00:14:55,055 --> 00:14:57,660
And actually even that's got it inside,

258
00:14:57,660 --> 00:14:59,365
the smaller noun phrase,

259
00:14:59,365 --> 00:15:02,035
which is just the word you.

260
00:15:02,035 --> 00:15:06,945
So an individual pronoun is also a noun phrase.

261
00:15:06,945 --> 00:15:11,000
Um, so just kind of structuring of

262
00:15:11,000 --> 00:15:13,565
language where you get this sort of

263
00:15:13,565 --> 00:15:16,895
hierarchical structure and the same kind of things inside them.

264
00:15:16,895 --> 00:15:20,465
I think that's just sort of totally, totally correct.

265
00:15:20,465 --> 00:15:23,130
Um, the- the claim then that,

266
00:15:23,130 --> 00:15:26,210
you know, our language is recursive, I mean,

267
00:15:26,210 --> 00:15:29,615
in a formal sense is not quite clear that that's,

268
00:15:29,615 --> 00:15:33,365
uh, it's a clear thing.

269
00:15:33,365 --> 00:15:36,850
And that's the reason- to say something is recursive,

270
00:15:36,850 --> 00:15:39,500
it has to repeat out to infinity, right.

271
00:15:39,500 --> 00:15:43,180
So as soon as you put any bound on something,

272
00:15:43,180 --> 00:15:48,819
and you say, "Look that's a noun phrase you just gave me with five levels of nesting."

273
00:15:48,819 --> 00:15:52,615
That's pretty implausible that someone is going to say that.

274
00:15:52,615 --> 00:15:54,630
And so as soon as you sort of,

275
00:15:54,630 --> 00:15:56,570
um, want to make an argument like,

276
00:15:56,570 --> 00:15:57,960
okay even if they said that,

277
00:15:57,960 --> 00:16:01,110
no one is going to say a noun phrase with 10 levels of nesting.

278
00:16:01,110 --> 00:16:04,340
And if you put some hard limit on it like that, um,

279
00:16:04,340 --> 00:16:08,970
then in some sense it's not truly recursive because it doesn't go out to infinity.

280
00:16:08,970 --> 00:16:10,290
Um, but, you know,

281
00:16:10,290 --> 00:16:12,280
regardless what you think about that,

282
00:16:12,280 --> 00:16:16,090
that doesn't negate the basic argument that you get this hierarchical

283
00:16:16,090 --> 00:16:19,900
structuring with the same kinds of things like noun phrases,

284
00:16:19,900 --> 00:16:26,780
sentences, verb phrases, appearing inside each other in a way that has no clear bound.

285
00:16:26,780 --> 00:16:30,200
Like to the extent that I show you a complex sentence,

286
00:16:30,200 --> 00:16:35,430
you can say I can make that an even bigger, more complex sentence by putting it inside,

287
00:16:35,430 --> 00:16:38,315
you said to me that, and then saying,

288
00:16:38,315 --> 00:16:39,940
um, my sentence, right.

289
00:16:39,940 --> 00:16:44,880
So that's the sense in which it does appear to be a recursive generative process,

290
00:16:44,880 --> 00:16:50,510
even though practically there are limits to how complex sentences people say.

291
00:16:50,510 --> 00:16:53,625
And so that's the kind of structure that gets

292
00:16:53,625 --> 00:16:57,605
captured in these constituency, um, structure trees.

293
00:16:57,605 --> 00:17:02,780
So before the early time when we talked about parsing and you guys did some of it,

294
00:17:02,780 --> 00:17:05,365
I emphasized dependency parsing.

295
00:17:05,365 --> 00:17:08,220
Um, but the other kind of parsing which is actually

296
00:17:08,220 --> 00:17:12,035
the kind that the models I'm going to talk about today was using,

297
00:17:12,035 --> 00:17:15,605
was this idea of what's often called constituency

298
00:17:15,605 --> 00:17:19,790
parsing or linguists often call it phrase structure grammars,

299
00:17:19,790 --> 00:17:24,635
um, or in sort of computer science formal language theory.

300
00:17:24,635 --> 00:17:27,185
These are context-free grammars, where, um,

301
00:17:27,185 --> 00:17:29,525
we're having, um, these,

302
00:17:29,525 --> 00:17:32,129
um, non-terminals like noun phrase,

303
00:17:32,129 --> 00:17:35,105
and verb phrase, and that's inside another noun phrases,

304
00:17:35,105 --> 00:17:36,715
it's inside another verb phrase,

305
00:17:36,715 --> 00:17:38,879
which is inside more verb phrases,

306
00:17:38,879 --> 00:17:40,775
heading up the sentence.

307
00:17:40,775 --> 00:17:43,895
And so these are our constituency grammars.

308
00:17:43,895 --> 00:17:47,585
And when we've occasionally mentioned the Penn Treebank tree,

309
00:17:47,585 --> 00:17:52,495
this was kind of an original Penn Treebank tree which is basically, uh,

310
00:17:52,495 --> 00:17:54,005
phrase structure grammar like,

311
00:17:54,005 --> 00:17:56,705
this with sort of various extra annotations,

312
00:17:56,705 --> 00:17:58,850
um, put on the nodes.

313
00:17:58,850 --> 00:18:04,775
Okay, so what did seem- what- what do you- to capture some of these properties,

314
00:18:04,775 --> 00:18:07,925
it seems like we'd like to have a neural model

315
00:18:07,925 --> 00:18:11,405
that can make use of some of this same kind of tree structure.

316
00:18:11,405 --> 00:18:17,720
And so what we'd like to do for working out semantic similarity of constituents,

317
00:18:17,720 --> 00:18:20,255
is we want to not only have

318
00:18:20,255 --> 00:18:25,185
a word vector space like we started off with right at the beginning of the quarter,

319
00:18:25,185 --> 00:18:30,065
but we'd like to be able to take bigger constituents like noun phrases,

320
00:18:30,065 --> 00:18:31,565
the country of my birth,

321
00:18:31,565 --> 00:18:33,475
and the place where I was born,

322
00:18:33,475 --> 00:18:35,535
and also give them a meaning.

323
00:18:35,535 --> 00:18:39,370
And so it seems like what we'd like to do is have a method of

324
00:18:39,370 --> 00:18:44,140
computing the meaning of any phrase in a compositional manner,

325
00:18:44,140 --> 00:18:47,320
such that the end result is also that

326
00:18:47,320 --> 00:18:52,410
these phrases could be stuck inside our vector space models.

327
00:18:52,410 --> 00:18:56,300
So we're still going to stick with our vector space semantics of phrases,

328
00:18:56,300 --> 00:18:59,625
and we wanna comp- compute the meanings of phrases.

329
00:18:59,625 --> 00:19:01,530
And so then the question is,

330
00:19:01,530 --> 00:19:04,715
how could we go about doing that?

331
00:19:04,715 --> 00:19:07,865
And well answer number one is we're gonna use the principle of

332
00:19:07,865 --> 00:19:11,375
compositionality since we're sure it's right,

333
00:19:11,375 --> 00:19:15,740
and so, well, what the principle of compositionality essentially says,

334
00:19:15,740 --> 00:19:20,170
if you want to work out the meaning- or here it says of a sentence.

335
00:19:20,170 --> 00:19:24,340
But the meaning of any phrase, any constituent is you're going to

336
00:19:24,340 --> 00:19:29,050
build it by knowing the meanings of its words,

337
00:19:29,050 --> 00:19:31,565
and then having rules that combine these meanings.

338
00:19:31,565 --> 00:19:34,280
So starting off with the country of my birth,

339
00:19:34,280 --> 00:19:37,310
I should be able to calculate a meaning of my birth,

340
00:19:37,310 --> 00:19:39,105
and meaning of the country,

341
00:19:39,105 --> 00:19:43,540
and meaning of of the- my birth and then a meaning of the country of my birth.

342
00:19:43,540 --> 00:19:47,680
So we'd have meaning composition rules which will let us calculate

343
00:19:47,680 --> 00:19:52,520
meanings upwards for larger constituents or sentences.

344
00:19:52,520 --> 00:19:57,290
Um, so that seems kind of the right thing to do.

345
00:19:57,290 --> 00:20:00,140
And so then the question is well, can we, um,

346
00:20:00,140 --> 00:20:04,470
then build a model of how to do that?

347
00:20:04,470 --> 00:20:08,630
Well, here's sort of a straightforward way of doing this, okay.

348
00:20:08,630 --> 00:20:16,105
So we- we have word vectors for the words that we've calculated.

349
00:20:16,105 --> 00:20:20,405
And what we'd like to do is work out, um-

350
00:20:20,405 --> 00:20:23,625
Then a meaning representation of this sentence.

351
00:20:23,625 --> 00:20:26,680
And at this point we sort of have two things to do.

352
00:20:26,680 --> 00:20:31,590
We have parsing to do of working out what's the right structure of the sentence,

353
00:20:31,590 --> 00:20:35,250
and then we have meaning computation to do of

354
00:20:35,250 --> 00:20:39,515
working out what is the meaning representation of this sentence.

355
00:20:39,515 --> 00:20:42,900
Um, so for parsing we'd sort of be building,

356
00:20:42,900 --> 00:20:45,280
sort of noun phrase, prepositional phrase,

357
00:20:45,280 --> 00:20:48,120
verb phrase, sentence kind of units, um,

358
00:20:48,120 --> 00:20:49,895
to get "the cat sat on the mat",

359
00:20:49,895 --> 00:20:51,380
and then will, what,

360
00:20:51,380 --> 00:20:52,935
we, if we had that,

361
00:20:52,935 --> 00:20:57,220
we could then run some kind of meaning computation program,

362
00:20:57,220 --> 00:20:59,195
and give us sort of a vector space,

363
00:20:59,195 --> 00:21:01,310
um, meaning of these sentences.

364
00:21:01,310 --> 00:21:02,970
So that's kind of what we want,

365
00:21:02,970 --> 00:21:04,270
is to do both of those,

366
00:21:04,270 --> 00:21:07,355
and in a little bit I'll show you an example of the kind

367
00:21:07,355 --> 00:21:10,660
of one way that you go about approaching that.

368
00:21:10,660 --> 00:21:13,085
But before I do that, just sort of stepping back for

369
00:21:13,085 --> 00:21:15,935
a moment as to what's different here, right?

370
00:21:15,935 --> 00:21:18,395
That here we had our

371
00:21:18,395 --> 00:21:21,630
recurrent neural network which in some sense has been

372
00:21:21,630 --> 00:21:25,125
our workhorse tool in this class up to now,

373
00:21:25,125 --> 00:21:26,465
and it gives you,

374
00:21:26,465 --> 00:21:30,605
it gives you a representation of the meaning of the country of my birth sort of,

375
00:21:30,605 --> 00:21:32,920
you could either say that's the meaning of,

376
00:21:32,920 --> 00:21:34,740
um, the country of my birth,

377
00:21:34,740 --> 00:21:36,980
or we talked about other tricks like,

378
00:21:36,980 --> 00:21:39,979
doing max pooling across all of these,

379
00:21:39,979 --> 00:21:42,820
or you could have a separate node out here,

380
00:21:42,820 --> 00:21:44,620
which so does attention over these.

381
00:21:44,620 --> 00:21:49,240
So it does give you a sort of representation, um,

382
00:21:49,240 --> 00:21:50,950
of the meaning of this,

383
00:21:50,950 --> 00:21:54,785
of any, um, sub-sequence of words as well.

384
00:21:54,785 --> 00:21:57,410
Um, but they, they're sort of different, right?

385
00:21:57,410 --> 00:21:59,290
That this what, the top,

386
00:21:59,290 --> 00:22:01,435
the tree recursive neural network,

387
00:22:01,435 --> 00:22:07,775
it requires a sentence or any kind of phrase to have a tree structure.

388
00:22:07,775 --> 00:22:10,390
So we know what its component parts are,

389
00:22:10,390 --> 00:22:14,935
but then we're working out meaning representations

390
00:22:14,935 --> 00:22:20,800
for the phrase that is sensitive to what its syntactic structure is,

391
00:22:20,800 --> 00:22:24,215
that how the words go together to build phrases.

392
00:22:24,215 --> 00:22:27,875
Whereas for the recurrent neural network we're

393
00:22:27,875 --> 00:22:31,549
just in an oblivious way running a sequence model along,

394
00:22:31,549 --> 00:22:33,500
and say and compute things,

395
00:22:33,500 --> 00:22:35,120
and in the obvious,

396
00:22:35,120 --> 00:22:38,390
it doesn't in any obvious way give a meaning representation of,

397
00:22:38,390 --> 00:22:41,970
of my birth, or my birth contained inside that.

398
00:22:41,970 --> 00:22:46,240
We sort of only have a meaning representation for the whole sequence,

399
00:22:46,240 --> 00:22:48,680
whereas if we're doing things this way, um,

400
00:22:48,680 --> 00:22:54,705
we do have meaning representations for the different meaningful parts of the sentence.

401
00:22:54,705 --> 00:22:58,730
Okay. That makes sense of what we're trying to do?

402
00:22:59,120 --> 00:23:01,625
Okay. So how could we do,

403
00:23:01,625 --> 00:23:03,240
go about doing that?

404
00:23:03,240 --> 00:23:08,385
Um, well, the idea of how we could go about doing that is,

405
00:23:08,385 --> 00:23:09,859
if we work bottom-up,

406
00:23:09,859 --> 00:23:14,245
at the very bottom we have word vectors,

407
00:23:14,245 --> 00:23:19,890
and so we want to recursively compute the meaning of bigger constituents.

408
00:23:19,890 --> 00:23:25,280
So if we wanted to compute the meaning of "on the mat" what we can do is say,

409
00:23:25,280 --> 00:23:30,135
well, we have, already have a meaning representation of, on and mat.

410
00:23:30,135 --> 00:23:33,910
So if we could feed those into a neural network, because that's our

411
00:23:33,910 --> 00:23:37,820
one tool, we could maybe get out of it two things.

412
00:23:37,820 --> 00:23:42,055
We could get out of it a goodness score.

413
00:23:42,055 --> 00:23:44,420
So this is what we're going to use for parsing.

414
00:23:44,420 --> 00:23:49,450
We're going to say, "Do you belie- do you believe you can put together "on" and the

415
00:23:49,450 --> 00:23:55,055
"mat" to form a good constituent that's part of a parse tree?

416
00:23:55,055 --> 00:23:58,055
And this will be a big positive number if the answer is true,

417
00:23:58,055 --> 00:23:59,865
and negative if it's not true,

418
00:23:59,865 --> 00:24:03,305
and then we have a meaning composition device,

419
00:24:03,305 --> 00:24:05,270
which says, "Okay, um,

420
00:24:05,270 --> 00:24:07,480
if you put together these two things,

421
00:24:07,480 --> 00:24:11,965
what would be the meaning representation of what we put together?"

422
00:24:11,965 --> 00:24:16,200
And so this is the first model that we explored which

423
00:24:16,200 --> 00:24:19,660
was doing this in a pretty simple way, right?

424
00:24:19,660 --> 00:24:22,750
So here was our meaning composition, um,

425
00:24:22,750 --> 00:24:27,665
device that we concatenated the two vectors of the constituents,

426
00:24:27,665 --> 00:24:30,230
we multiply them by a matrix, add a

427
00:24:30,230 --> 00:24:31,840
bias as usual,

428
00:24:31,840 --> 00:24:33,955
put it through a tan h. Uh,

429
00:24:33,955 --> 00:24:35,345
this work is old enough,

430
00:24:35,345 --> 00:24:36,815
it's sort of before things, like,

431
00:24:36,815 --> 00:24:38,110
ReLUs became popular,

432
00:24:38,110 --> 00:24:41,735
but maybe it's better to have a tan h anyway, um, fit more like,

433
00:24:41,735 --> 00:24:43,235
a recurrent neural network,

434
00:24:43,235 --> 00:24:47,580
and so this was our meaning composition that gave the meaning of the parent.

435
00:24:47,580 --> 00:24:52,305
And then to the side, what the score of it was as to whether this was a good phrase,

436
00:24:52,305 --> 00:24:55,370
we were taking that parent vector representation,

437
00:24:55,370 --> 00:24:58,949
and multiplying it by another vector,

438
00:24:58,949 --> 00:25:01,990
and that was giving us out a number.

439
00:25:02,100 --> 00:25:06,180
Um, if you think about it a bit while we're doing this,

440
00:25:06,180 --> 00:25:10,699
you might think that this isn't quite a perfect model of meaning composition,

441
00:25:10,699 --> 00:25:14,800
and later on in the class I'll talk about some more complex models,

442
00:25:14,800 --> 00:25:17,880
um, that we then started to explore.

443
00:25:17,880 --> 00:25:21,815
Um, but this is sort of enough to get us going,

444
00:25:21,815 --> 00:25:24,520
and this gave us a way of building

445
00:25:24,520 --> 00:25:29,805
a recursive neural network parser which both found parsers,

446
00:25:29,805 --> 00:25:33,530
and worked out a meaning representation for them.

447
00:25:33,530 --> 00:25:37,565
And so the way we did this was in the simplest possible way really,

448
00:25:37,565 --> 00:25:39,580
which was to have a greedy parser.

449
00:25:39,580 --> 00:25:42,620
So if we start off with the "cat sat on the mat",

450
00:25:42,620 --> 00:25:43,960
what we could do is say,

451
00:25:43,960 --> 00:25:47,040
well, maybe you should join "the" and "cat" together.

452
00:25:47,040 --> 00:25:48,220
Let's try that.

453
00:25:48,220 --> 00:25:49,865
Run it through our neural network,

454
00:25:49,865 --> 00:25:53,315
it'll get a score and a meaning representation,

455
00:25:53,315 --> 00:25:55,565
and while we could try doing that for "cat" and

456
00:25:55,565 --> 00:25:58,635
"sat" we could try doing it for "sat" and "on".

457
00:25:58,635 --> 00:26:03,170
We could try doing it for "on" and "the" we could try doing it for "the" and "mat".

458
00:26:03,170 --> 00:26:06,665
And then at this point we'd say, okay, well the,

459
00:26:06,665 --> 00:26:12,725
the best phrase that we can make combining these word vectors is the one for "the cat".

460
00:26:12,725 --> 00:26:14,770
So let's just commit to that one,

461
00:26:14,770 --> 00:26:17,360
and it has this semantic representation,

462
00:26:17,360 --> 00:26:20,825
and at this point we can essentially repeat.

463
00:26:20,825 --> 00:26:25,190
Now, all the work we did over there we can just reuse because nothing has changed,

464
00:26:25,190 --> 00:26:28,850
but we can also consider now joining the "cat"

465
00:26:28,850 --> 00:26:32,965
as a constituent with "sat" and get a score for that.

466
00:26:32,965 --> 00:26:34,930
And so at this point we decide, okay,

467
00:26:34,930 --> 00:26:37,140
the mat is the best constituent to build,

468
00:26:37,140 --> 00:26:41,455
commit to that, calculate a meaning representation for "on the mat".

469
00:26:41,455 --> 00:26:44,025
That looks good, commit to that,

470
00:26:44,025 --> 00:26:46,080
and kind of keep on chugging up,

471
00:26:46,080 --> 00:26:51,200
and so we've got a mechanism for sort of choosing a parse of a sentence in a,

472
00:26:51,200 --> 00:26:52,655
in a greedy manner.

473
00:26:52,655 --> 00:26:55,300
But, you know, when we looked at the dependency parsing,

474
00:26:55,300 --> 00:26:57,230
we're also doing that greedily, right?

475
00:26:57,230 --> 00:27:00,955
Um, and coming up with a meaning representation.

476
00:27:00,955 --> 00:27:06,105
Okay. So that was our first model of having a tree recursive neural network,

477
00:27:06,105 --> 00:27:07,630
and using it for parsing.

478
00:27:07,630 --> 00:27:12,630
Um, there are a few more details here,

479
00:27:12,630 --> 00:27:16,200
some of which probably aren't super,

480
00:27:16,200 --> 00:27:18,335
um, important at this point, right?

481
00:27:18,335 --> 00:27:22,790
So we could score a tree by summing the scores at each node,

482
00:27:22,790 --> 00:27:25,290
um, for working out,

483
00:27:25,290 --> 00:27:27,730
for the optimization we were working out,

484
00:27:27,730 --> 00:27:33,545
we're using this kind of max-margin loss that we've looked at in other places.

485
00:27:33,545 --> 00:27:37,780
Um, the simplest way to do things is completely greedily.

486
00:27:37,780 --> 00:27:41,825
You just, um, find the best local decision at each point,

487
00:27:41,825 --> 00:27:42,845
and make that structure,

488
00:27:42,845 --> 00:27:43,985
and keep on going.

489
00:27:43,985 --> 00:27:45,610
But if you wanna do things a bit better,

490
00:27:45,610 --> 00:27:47,100
and we explored this,

491
00:27:47,100 --> 00:27:48,750
um, you could say,

492
00:27:48,750 --> 00:27:50,910
um, we could do beam search.

493
00:27:50,910 --> 00:27:54,020
We could explore out several good ways of merging,

494
00:27:54,020 --> 00:27:59,480
and then decide later higher up the tree as to which was the best way, um, to merge.

495
00:27:59,480 --> 00:28:03,155
Um, we haven't talked about it in this class,

496
00:28:03,155 --> 00:28:05,240
but just to mention, um,

497
00:28:05,240 --> 00:28:08,490
something in case people have seen it is, um,

498
00:28:08,490 --> 00:28:12,545
traditional constituency parsing where you have symbols here,

499
00:28:12,545 --> 00:28:14,405
like, NP or VP.

500
00:28:14,405 --> 00:28:19,240
Um, there exist efficient dynamic programming algorithms where you can

501
00:28:19,240 --> 00:28:24,660
find the optimal parse of a sentence in polynomial time.

502
00:28:24,660 --> 00:28:26,190
So in, in cubic time.

503
00:28:26,190 --> 00:28:29,375
So if you have a regular context-free grammar, and well,

504
00:28:29,375 --> 00:28:32,615
so regular probabilistic context-free grammar, um,

505
00:28:32,615 --> 00:28:35,010
and if you want to know what is the best parse of

506
00:28:35,010 --> 00:28:38,385
the sentence according to the probabilistic context-free grammar,

507
00:28:38,385 --> 00:28:43,265
you can write a cubic time dynamic programming algorithm and you can find it.

508
00:28:43,265 --> 00:28:47,560
That's good. And in the old days of CS224N,

509
00:28:47,560 --> 00:28:51,605
um, before neural networks we used to have everyone do that.

510
00:28:51,605 --> 00:28:57,240
The, the most, the most brain-breaking assignment of the old CS224N

511
00:28:57,240 --> 00:29:02,380
was writing this dynamic program to do context-free grammar parsing of a sentence.

512
00:29:02,380 --> 00:29:05,620
Um, the slightly sad fact is,

513
00:29:05,620 --> 00:29:08,810
once you go to these kind of neural network representations,

514
00:29:08,810 --> 00:29:12,975
you can't write clever dynamic programming algorithms anymore,

515
00:29:12,975 --> 00:29:18,094
because clever dynamic programming algorithms only work when you have symbols

516
00:29:18,094 --> 00:29:23,580
from a reasonably small set for your non-terminals because if that's the case,

517
00:29:23,580 --> 00:29:26,110
you can, you kind of have collisions, right?

518
00:29:26,110 --> 00:29:29,075
You have lots of ways of parsing stuff lower down,

519
00:29:29,075 --> 00:29:30,760
which kind of, uh,

520
00:29:30,760 --> 00:29:33,485
turn out to be different ways to make a noun phrase,

521
00:29:33,485 --> 00:29:35,850
or different ways to make a prepositional phrase,

522
00:29:35,850 --> 00:29:38,825
and therefore you can save work with dynamic programming.

523
00:29:38,825 --> 00:29:40,645
If you've got a model like this,

524
00:29:40,645 --> 00:29:44,490
since everything that you build is going through layers of neural network,

525
00:29:44,490 --> 00:29:47,745
and you've got a meaning representation, some high-dimensional vector,

526
00:29:47,745 --> 00:29:49,760
things are never going to collide,

527
00:29:49,760 --> 00:29:53,070
and so you can never save work by doing dynamic programming.

528
00:29:53,070 --> 00:29:58,520
And so, um, you're either doing exponential work to explore out everything,

529
00:29:58,520 --> 00:30:03,950
or else you're using some kind of beam to explore a bunch of likely stuff.

530
00:30:04,090 --> 00:30:09,260
Yeah. Um, we actually also applied this,

531
00:30:09,260 --> 00:30:11,915
um, to vision at the same time.

532
00:30:11,915 --> 00:30:14,330
So it wasn't just sort of completely

533
00:30:14,330 --> 00:30:17,300
a vague motivation of, um,

534
00:30:17,300 --> 00:30:23,255
visual scenes have parts that we actually started exploring that well you could take, um,

535
00:30:23,255 --> 00:30:27,590
these pieces of scenes and then work out, um,

536
00:30:27,590 --> 00:30:32,885
representations for scenes using a similar form of compositionality.

537
00:30:32,885 --> 00:30:35,405
And so in particular,

538
00:30:35,405 --> 00:30:40,715
um, there was sort of this dataset that was being used for, um,

539
00:30:40,715 --> 00:30:43,894
multi-class segmentation in vision,

540
00:30:43,894 --> 00:30:48,830
where you start off with very small patches and then you wanna combine them

541
00:30:48,830 --> 00:30:51,050
up into parts of a scene of sort of

542
00:30:51,050 --> 00:30:54,170
recognizing which part of the picture was the building,

543
00:30:54,170 --> 00:30:58,025
the sky, the road, various other classes.

544
00:30:58,025 --> 00:31:02,435
And we were actually at the time able to do this really rather well, um,

545
00:31:02,435 --> 00:31:06,140
using one of these tree recursive structured neural networks better

546
00:31:06,140 --> 00:31:11,435
than preceding work in vision had done in the late 2000s decade.

547
00:31:11,435 --> 00:31:17,225
Okay. So how can we- how can we build neural networks,

548
00:31:17,225 --> 00:31:19,550
um, that do this kind of stuff?

549
00:31:19,550 --> 00:31:25,730
Um, so when- when we started off exploring these tree structured neural networks, um,

550
00:31:25,730 --> 00:31:29,480
we thought that this was a cool original idea and no one

551
00:31:29,480 --> 00:31:33,305
had worked on tree structured neural networks successfully before.

552
00:31:33,305 --> 00:31:39,560
Um, but it turned out we were wrong, that there were a couple of Germans in the mid-1990s,

553
00:31:39,560 --> 00:31:44,765
um, had actually started looking at tree structured neural networks and had worked out,

554
00:31:44,765 --> 00:31:45,950
um, the math of them.

555
00:31:45,950 --> 00:31:49,535
So corresponding to the backpropagation through time algorithm,

556
00:31:49,535 --> 00:31:52,865
um, that Abby talked about when we were doing recurrent neural networks.

557
00:31:52,865 --> 00:31:55,310
They worked out the tree structured case which they

558
00:31:55,310 --> 00:31:58,310
called backpropagation, um, through structure.

559
00:31:58,310 --> 00:32:02,810
Um, there are several slides on this in

560
00:32:02,810 --> 00:32:07,505
the slides but I think I'm gonna sort of skip them.

561
00:32:07,505 --> 00:32:08,765
If anyone wants to look at them,

562
00:32:08,765 --> 00:32:10,610
they're on the web and you can look at them.

563
00:32:10,610 --> 00:32:14,720
I mean, there isn't actually anything that's new.

564
00:32:14,720 --> 00:32:17,420
So if you remember with- with

565
00:32:17,420 --> 00:32:21,725
bad scarring or something that was early lectures of this class of working out,

566
00:32:21,725 --> 00:32:26,450
um, the derivatives of neural networks and how it worked with recurrent neural networks.

567
00:32:26,450 --> 00:32:28,130
It's sort of the same, right.

568
00:32:28,130 --> 00:32:32,585
You have this recurrent matrix at different levels of tree structure.

569
00:32:32,585 --> 00:32:37,220
You're summing the derivatives of everywhere it turns up.

570
00:32:37,220 --> 00:32:40,370
The only difference is sort of because we now have tree structure,

571
00:32:40,370 --> 00:32:43,040
you're sort of splitting things downwards.

572
00:32:43,040 --> 00:32:45,410
Um, so yes.

573
00:32:45,410 --> 00:32:48,560
So forward prop we kind of compute it forwards.

574
00:32:48,560 --> 00:32:51,245
And then when we're doing back prop,

575
00:32:51,245 --> 00:32:55,910
when we've had the backward propagation we have the error signal coming from above.

576
00:32:55,910 --> 00:32:58,025
We then, um, combine it,

577
00:32:58,025 --> 00:33:00,530
um, with the calculations at this node.

578
00:33:00,530 --> 00:33:03,350
And then we're sort of sending it back in a tree structure

579
00:33:03,350 --> 00:33:06,845
down to each of the branches underneath us.

580
00:33:06,845 --> 00:33:11,960
So that was our first version of things and we got some decent results.

581
00:33:11,960 --> 00:33:16,985
We got this good vision results that I showed you and it sort of seemed to do,

582
00:33:16,985 --> 00:33:19,490
um, some good for, um,

583
00:33:19,490 --> 00:33:23,030
language both for parsing and doing- We had

584
00:33:23,030 --> 00:33:27,410
some results I haven't actually included here of sort of doing paraphrase,

585
00:33:27,410 --> 00:33:33,545
um, judgment between sentences and it- it modeled things, um, fairly well.

586
00:33:33,545 --> 00:33:37,760
But once we started thinking about it more it seemed like

587
00:33:37,760 --> 00:33:41,780
that very simple neural net function couldn't possibly

588
00:33:41,780 --> 00:33:46,955
compute the kind of meanings that we wanted to compute for sentence meanings.

589
00:33:46,955 --> 00:33:49,760
And so we then sort of set about trying to come up with

590
00:33:49,760 --> 00:33:52,970
some more complex ways of working out kind

591
00:33:52,970 --> 00:33:56,180
of meaning composition functions and nodes that

592
00:33:56,180 --> 00:33:59,630
could then be used to build a better neural network.

593
00:33:59,630 --> 00:34:04,520
And sort of some- some of the essence of that is on this slide.

594
00:34:04,520 --> 00:34:07,280
But, you know, for the first version we just

595
00:34:07,280 --> 00:34:10,130
didn't have enough complexity of neural network, frankly, right?

596
00:34:10,130 --> 00:34:13,730
So when we had two constituents we concatenated

597
00:34:13,730 --> 00:34:19,040
them and multiply that by a weight, uh, weight matrix.

598
00:34:19,040 --> 00:34:21,590
Um, and that was sort of essentially all we had.

599
00:34:21,590 --> 00:34:27,740
And, um, as I hope you've gotten more of a sense of in this class.

600
00:34:27,740 --> 00:34:31,610
If you just concatenate and multiply by a weight matrix,

601
00:34:31,610 --> 00:34:36,200
you're not actually modeling the interaction between these two vectors, right.

602
00:34:36,200 --> 00:34:39,500
Because you can think of this weight matrix as just sort of being

603
00:34:39,500 --> 00:34:43,490
divided in two and half of it multiplies this vector,

604
00:34:43,490 --> 00:34:45,605
and half of it multiplies this vector.

605
00:34:45,605 --> 00:34:49,685
So the meanings of these two things don't act on each other.

606
00:34:49,685 --> 00:34:52,280
And so somehow you have to make your neural network,

607
00:34:52,280 --> 00:34:54,620
um, more complex than that.

608
00:34:54,620 --> 00:34:59,944
But the other way in which this seemed too simple is in the first model,

609
00:34:59,944 --> 00:35:04,505
we had just one weight matrix which we use for everything.

610
00:35:04,505 --> 00:35:08,120
And, ah, at least if you're a linguist and you're

611
00:35:08,120 --> 00:35:12,050
thinking about the structure of language you might start thinking of well,

612
00:35:12,050 --> 00:35:14,630
wait a minute, sometimes you're gonna be putting

613
00:35:14,630 --> 00:35:17,645
together a verb and an object noun phrase.

614
00:35:17,645 --> 00:35:19,490
Um, hit the ball.

615
00:35:19,490 --> 00:35:24,125
Sometimes you're gonna be putting together an article and a noun, uh, ball.

616
00:35:24,125 --> 00:35:29,150
Sometimes you're gonna be doing adjectival modification blue ball.

617
00:35:29,150 --> 00:35:32,705
These things are very different in their semantics.

618
00:35:32,705 --> 00:35:36,980
Can it really be the case that you can just have one weight matrix that is

619
00:35:36,980 --> 00:35:41,645
this universal composition function for putting together the meaning of phrases?

620
00:35:41,645 --> 00:35:43,280
Could that possibly work?

621
00:35:43,280 --> 00:35:45,020
And you sort of might suspect,

622
00:35:45,020 --> 00:35:46,820
um, it doesn't work.

623
00:35:46,820 --> 00:35:49,355
Um, and so I'm gonna go on and, um,

624
00:35:49,355 --> 00:35:53,150
show, um, some of those different things.

625
00:35:53,150 --> 00:35:57,965
But really, um, before I show the different things,

626
00:35:57,965 --> 00:36:02,810
um, I'm gonna show one more version that's sort of related to the first thing,

627
00:36:02,810 --> 00:36:07,355
which actually gave a pretty successful and good parser,

628
00:36:07,355 --> 00:36:10,250
um, for doing, um,

629
00:36:10,250 --> 00:36:14,195
context-free style constituency parsing.

630
00:36:14,195 --> 00:36:21,950
And so this was another way of getting away from the parsing being completely greedy.

631
00:36:21,950 --> 00:36:26,900
Um, which was to actually split apart the two parts of g. We

632
00:36:26,900 --> 00:36:31,880
have to come up with a tree structure for our sentence from,

633
00:36:31,880 --> 00:36:34,805
'Let's compute the meaning of the sentence'.

634
00:36:34,805 --> 00:36:37,280
And so the thinking was, well,

635
00:36:37,280 --> 00:36:42,965
in terms of deciding what's a good tree structure for a sentence,

636
00:36:42,965 --> 00:36:47,015
that's actually something you can do pretty well with the symbolic grammar.

637
00:36:47,015 --> 00:36:49,940
But the problems with symbolic grammars aren't

638
00:36:49,940 --> 00:36:53,270
that they can't put tree structures over sentences.

639
00:36:53,270 --> 00:36:55,790
The problems you have with those grammars is that,

640
00:36:55,790 --> 00:36:59,390
they can't compute meaning representation and they're not

641
00:36:59,390 --> 00:37:03,710
very good at choosing between alternative tree structures.

642
00:37:03,710 --> 00:37:07,175
But we can divide up the two parts.

643
00:37:07,175 --> 00:37:08,750
So what we can do is say, well,

644
00:37:08,750 --> 00:37:13,175
let's just use a regular probabilistic Context-Free Grammar

645
00:37:13,175 --> 00:37:16,580
to generate possible tree structures for sentences.

646
00:37:16,580 --> 00:37:19,205
We can generate a k best list and say,

647
00:37:19,205 --> 00:37:21,110
what are the 50 best, um,

648
00:37:21,110 --> 00:37:24,230
context-free grammar structures for this sentence?

649
00:37:24,230 --> 00:37:28,775
And that's something we can do very efficiently with dynamic programming algorithms.

650
00:37:28,775 --> 00:37:33,125
And then we can work out a neural net,

651
00:37:33,125 --> 00:37:38,105
um, that will work out the meaning representation of the sentence.

652
00:37:38,105 --> 00:37:41,675
Um, and so that led to this, um,

653
00:37:41,675 --> 00:37:46,670
what's called syntactically untied recursive neural network.

654
00:37:46,670 --> 00:37:51,260
Um, so essentially what this is saying is that we

655
00:37:51,260 --> 00:37:55,925
ha- for each node and the sentence it's got a category,

656
00:37:55,925 --> 00:37:58,940
um, of a symbolic context-free grammar.

657
00:37:58,940 --> 00:38:02,630
So they're category A and B and C. So when

658
00:38:02,630 --> 00:38:06,500
we put things together we'll be able to say, okay.

659
00:38:06,500 --> 00:38:10,430
We've got a rule that says, um,

660
00:38:10,430 --> 00:38:12,620
X goes to BC,

661
00:38:12,620 --> 00:38:15,365
so that licenses this node here.

662
00:38:15,365 --> 00:38:18,800
So that part of the parsing is symbolic.

663
00:38:18,800 --> 00:38:21,050
Then- then we want to, um,

664
00:38:21,050 --> 00:38:24,110
work out the meaning of this phrase.

665
00:38:24,110 --> 00:38:27,770
Um, and well, the second problem I talked about

666
00:38:27,770 --> 00:38:32,075
was surely just having a one way of doing composition

667
00:38:32,075 --> 00:38:35,990
is expecting a lot too much to be able to have

668
00:38:35,990 --> 00:38:40,535
sort of verb and object versus adjective and noun composed the same way.

669
00:38:40,535 --> 00:38:42,980
So we have this idea of well,

670
00:38:42,980 --> 00:38:46,190
since we now know about the syntactic categories of

671
00:38:46,190 --> 00:38:51,215
the children that we maybe know that this is an adjective and this is a noun.

672
00:38:51,215 --> 00:38:55,010
What we could do is have different weight matrices for

673
00:38:55,010 --> 00:38:58,895
composition depending on what the categories are.

674
00:38:58,895 --> 00:39:01,790
So rather than where before there was

675
00:39:01,790 --> 00:39:07,325
just this one universal weight matrix which was meant to do all meaning composition.

676
00:39:07,325 --> 00:39:08,810
Here we can have,

677
00:39:08,810 --> 00:39:11,840
this is the weight matrix for combining together

678
00:39:11,840 --> 00:39:15,439
the meanings of an adjective and a noun and it will compute,

679
00:39:15,439 --> 00:39:17,810
um, the meaning of this constituent.

680
00:39:17,810 --> 00:39:21,590
But then we'll have a different weight matrix for combining

681
00:39:21,590 --> 00:39:27,870
together the meanings of a determiner and a noun phrase or something like that.

682
00:39:30,090 --> 00:39:34,450
Okay. Um, yes.

683
00:39:34,450 --> 00:39:37,360
So I sort of always said this one I guess,

684
00:39:37,360 --> 00:39:40,825
um, we wanted to be able to do things quickly.

685
00:39:40,825 --> 00:39:44,830
And so our solution to be able to do that is we sort of

686
00:39:44,830 --> 00:39:49,525
used a probabilistic context-free grammar to find likely parses,

687
00:39:49,525 --> 00:39:55,120
um, and then only worked out our meaning for ones that were, um, quite probable.

688
00:39:55,120 --> 00:39:59,050
And so we call this result a compositional vector grammar which was

689
00:39:59,050 --> 00:40:03,745
a combination of a PCFG and a tree recursive neural network.

690
00:40:03,745 --> 00:40:07,440
Um, and yeah.

691
00:40:07,440 --> 00:40:11,010
So, um, essentially at the time,

692
00:40:11,010 --> 00:40:14,235
this actually gave a pretty good constituency parser.

693
00:40:14,235 --> 00:40:16,845
So there are sort of lots of results here.

694
00:40:16,845 --> 00:40:20,040
The top ones are kind of our classic older, um,

695
00:40:20,040 --> 00:40:25,285
Stanford Parser which is a PCFG,  the kind of parsers that people had built.

696
00:40:25,285 --> 00:40:32,080
This is our compositional vector grammar that the time of this being done in 2013,

697
00:40:32,080 --> 00:40:34,690
it wasn't the very best parser available.

698
00:40:34,690 --> 00:40:38,155
There had been some better work by Eugene Charniak at Brown.

699
00:40:38,155 --> 00:40:41,980
But we actually had a pretty good parser coming out of that system.

700
00:40:41,980 --> 00:40:46,495
But what was perhaps a bit more interesting was we,

701
00:40:46,495 --> 00:40:50,845
we didn't only have a parser that was meant to give the right parse trees.

702
00:40:50,845 --> 00:40:54,850
We are also computing meaning representations of nodes.

703
00:40:54,850 --> 00:40:58,210
And as a kind of a consequence of that,

704
00:40:58,210 --> 00:41:02,140
you can look at not only meaning representations of nodes.

705
00:41:02,140 --> 00:41:06,355
You could learn about the weight matrices that these models were learning,

706
00:41:06,355 --> 00:41:08,980
um, when they combine together meanings.

707
00:41:08,980 --> 00:41:13,600
So remember we had these sort of category-specific W matrices,

708
00:41:13,600 --> 00:41:17,455
that were going together with the children to work out the meaning.

709
00:41:17,455 --> 00:41:20,875
Um, so these are a little bit hard to interpret.

710
00:41:20,875 --> 00:41:24,130
But the deal is, when we load these matrices,

711
00:41:24,130 --> 00:41:27,310
we initialize them as a pair of diagonal matrices.

712
00:41:27,310 --> 00:41:32,125
So these are sort of two by one rectangular matrices because there are two children.

713
00:41:32,125 --> 00:41:35,335
Um, so half of it is, um,

714
00:41:35,335 --> 00:41:37,090
multiplying the left child,

715
00:41:37,090 --> 00:41:39,535
the other half is multiplying the right child.

716
00:41:39,535 --> 00:41:45,520
And we initialize them as sort of like a compi- two identity matrices next to each

717
00:41:45,520 --> 00:41:48,895
other which would give us the sort of default semantics

718
00:41:48,895 --> 00:41:52,840
of just averaging until something different was learned in the,

719
00:41:52,840 --> 00:41:55,690
in the, in the weight vectors.

720
00:41:55,690 --> 00:42:02,070
And to the extent that sort of nothing interesting has been learned by the model,

721
00:42:02,070 --> 00:42:08,325
you'll get yellow along the diagonal and this sort of sky blue in the rest of the field.

722
00:42:08,325 --> 00:42:11,100
And to the extent that it's learned something

723
00:42:11,100 --> 00:42:14,355
interesting to take out of the semantics of a child,

724
00:42:14,355 --> 00:42:17,770
you will then start to see reds and oranges on the diagonal,

725
00:42:17,770 --> 00:42:22,465
and dark blues and greens and stuff in the rest of the field.

726
00:42:22,465 --> 00:42:26,200
So what you find is that if you train this model,

727
00:42:26,200 --> 00:42:34,225
it's learning about which children of a phrase are actually the important ones.

728
00:42:34,225 --> 00:42:36,370
Um, so these ones are saying that if you're

729
00:42:36,370 --> 00:42:39,310
combining together a noun phrase and the coordination,

730
00:42:39,310 --> 00:42:41,830
so something like "the cat and",

731
00:42:41,830 --> 00:42:45,055
that most of the semantics have to be found in "the cat"

732
00:42:45,055 --> 00:42:48,760
and not much of the semantics is going to be found in "and".

733
00:42:48,760 --> 00:42:52,825
Whereas if you are combining together a possessive pronoun,

734
00:42:52,825 --> 00:42:55,045
something like her or his,

735
00:42:55,045 --> 00:42:58,615
um, with a noun phrase inside it like,

736
00:42:58,615 --> 00:43:02,425
um, her tabby cat or something like that.

737
00:43:02,425 --> 00:43:06,595
Then most of the meaning is to be found inside the tabby cat constituent.

738
00:43:06,595 --> 00:43:10,555
So it's actually learning where the important semantics of sentences is.

739
00:43:10,555 --> 00:43:18,460
Um, and there're lots of examples of that. Um, yeah.

740
00:43:18,460 --> 00:43:21,850
This one sort of- so this one shows a variety of

741
00:43:21,850 --> 00:43:26,575
modification structures where adjectives or adverbs,

742
00:43:26,575 --> 00:43:31,330
um, modify either a noun phrase or an adjective phrase or

743
00:43:31,330 --> 00:43:35,995
just a single adjective is multiplying a noun phrase.

744
00:43:35,995 --> 00:43:40,120
And the thing that you seem to notice is that there are particular dimensions which are

745
00:43:40,120 --> 00:43:44,200
kind of capturing sort of modification meanings.

746
00:43:44,200 --> 00:43:50,395
So dimension 6 and dimension 11 is sort of showing up in these different,

747
00:43:50,395 --> 00:43:53,830
um, combinations here, as sort of capturing meaning components.

748
00:43:53,830 --> 00:43:55,645
So that was kind of neat.

749
00:43:55,645 --> 00:44:00,430
And so this slightly more complex model actually worked pretty

750
00:44:00,430 --> 00:44:05,035
well at capturing a meaning of phrases and sentences.

751
00:44:05,035 --> 00:44:07,105
So in this test here,

752
00:44:07,105 --> 00:44:11,920
we were giving it- the system a test sentence and saying well,

753
00:44:11,920 --> 00:44:17,785
what are the other- what are sentences that are most similar in meaning,

754
00:44:17,785 --> 00:44:22,120
nearest to paraphrases in our corpus for this sentence?

755
00:44:22,120 --> 00:44:25,990
So if all the figures are adjusted for seasonal variations,

756
00:44:25,990 --> 00:44:29,650
the two most similar other sentences in the corpus were,

757
00:44:29,650 --> 00:44:32,995
all the numbers are adjusted for seasonal vet fluctuation.

758
00:44:32,995 --> 00:44:34,270
That's a pretty easy one.

759
00:44:34,270 --> 00:44:37,960
Or all the figures are adjusted to remove usual seasonal patterns.

760
00:44:37,960 --> 00:44:40,240
So that seems to be working pretty well.

761
00:44:40,240 --> 00:44:43,000
"Knight-Ridder wouldn't a comment on the author,

762
00:44:43,000 --> 00:44:46,360
Harsco declined to say what country placed the order."

763
00:44:46,360 --> 00:44:48,640
The semantics there are a bit more different but it

764
00:44:48,640 --> 00:44:51,490
seems like it is capturing something similar.

765
00:44:51,490 --> 00:44:53,860
"Um, Coastal wouldn't disclose the terms."

766
00:44:53,860 --> 00:44:55,630
That's kind of a really interesting one,

767
00:44:55,630 --> 00:45:00,010
because that one is actually very similar in meaning but it's expressed in

768
00:45:00,010 --> 00:45:05,215
a very different way in terms of the words and the syntactic structure that are used.

769
00:45:05,215 --> 00:45:09,820
Okay, so that was progress because now

770
00:45:09,820 --> 00:45:14,815
we could have different matrices for different constituent types.

771
00:45:14,815 --> 00:45:22,015
Um, but there's still some reason to think that we didn't have enough power,

772
00:45:22,015 --> 00:45:28,225
and that was we're still at heart using this very simple compositional structure

773
00:45:28,225 --> 00:45:34,960
where we're just concatenating the two children's vectors and multiplying it by a matrix.

774
00:45:34,960 --> 00:45:37,885
So that means the two words, um,

775
00:45:37,885 --> 00:45:41,470
didn't interact with each other in terms of their meaning.

776
00:45:41,470 --> 00:45:49,450
Um, but, um, it seems like we want to have them interact in their meaning, right?

777
00:45:49,450 --> 00:45:54,100
So in particular if you if you think about

778
00:45:54,100 --> 00:45:59,305
human languages and the kind of things that people look at in linguistic semantics,

779
00:45:59,305 --> 00:46:04,465
you get words that appear to be kind of modifiers or operators.

780
00:46:04,465 --> 00:46:06,970
So the word very,

781
00:46:06,970 --> 00:46:09,580
sort of doesn't mean much by itself.

782
00:46:09,580 --> 00:46:14,980
I mean it means something like strengthening or more so or something like that,

783
00:46:14,980 --> 00:46:18,160
but you know, it doesn't really have a meaning, right?

784
00:46:18,160 --> 00:46:20,050
It doesn't have any denotation.

785
00:46:20,050 --> 00:46:22,360
You can't show me very things, right?

786
00:46:22,360 --> 00:46:25,135
You can show me chairs and pens and, um,

787
00:46:25,135 --> 00:46:27,910
children but you can't show me very things,

788
00:46:27,910 --> 00:46:32,695
that the meaning of very seems to be that something comes after it, good.

789
00:46:32,695 --> 00:46:39,580
And this has a sort of an operator meaning of increase on the scale of this thing,

790
00:46:39,580 --> 00:46:42,490
and it can increase on the scale in either direction.

791
00:46:42,490 --> 00:46:45,115
You can have very good or very bad.

792
00:46:45,115 --> 00:46:49,990
So if we want to capture that kind of semantics,

793
00:46:49,990 --> 00:46:53,425
it seems like we can't capture that kind of semantics by just

794
00:46:53,425 --> 00:46:58,330
concatenating two vectors and multiplying them by a matrix.

795
00:46:58,330 --> 00:47:04,300
It seems like what we really want to say is very is gonna grab

796
00:47:04,300 --> 00:47:06,880
hold of the meaning of good and

797
00:47:06,880 --> 00:47:10,915
modify it in some ways to produce a new meaning for very good.

798
00:47:10,915 --> 00:47:15,130
And indeed, that's the kind of approach that's typically,

799
00:47:15,130 --> 00:47:17,980
um, been done in linguistic semantics.

800
00:47:17,980 --> 00:47:20,530
So in linguistic theories of semantic,

801
00:47:20,530 --> 00:47:22,120
you would normally say, okay,

802
00:47:22,120 --> 00:47:23,620
good has a meaning,

803
00:47:23,620 --> 00:47:29,530
very is a function that takes in the meaning of good and returns a meaning of very good.

804
00:47:29,530 --> 00:47:31,825
And so we wanted to have, um,

805
00:47:31,825 --> 00:47:35,050
a way of putting that into a neural network.

806
00:47:35,050 --> 00:47:40,750
And so to try and come up with a new composition function as to how to do that.

807
00:47:40,750 --> 00:47:44,530
And there are various ways that you could think about

808
00:47:44,530 --> 00:47:48,340
doing that and other people have had a couple of different attempts.

809
00:47:48,340 --> 00:47:52,915
But essentially what was in our head is well,

810
00:47:52,915 --> 00:47:55,120
we have word vectors,

811
00:47:55,120 --> 00:48:01,990
and if we want to say that very takes the meaning of good and returns a new meaning,

812
00:48:01,990 --> 00:48:05,410
the kind of obvious thing to do is to say very

813
00:48:05,410 --> 00:48:08,830
has a matrix attached to it because then we could use the,

814
00:48:08,830 --> 00:48:13,990
the very matrix and multiply it by the good vector and we get a new,

815
00:48:13,990 --> 00:48:16,360
um, vector coming out.

816
00:48:16,360 --> 00:48:19,045
And so then, well,

817
00:48:19,045 --> 00:48:21,310
the problem is, uh,

818
00:48:21,310 --> 00:48:25,810
which- then which words have vectors and which words have matrices?

819
00:48:25,810 --> 00:48:27,700
And that's kind of, um,

820
00:48:27,700 --> 00:48:30,190
hard to know the answer to.

821
00:48:30,190 --> 00:48:32,485
I mean, in particular, um,

822
00:48:32,485 --> 00:48:35,875
words that act as operators can,

823
00:48:35,875 --> 00:48:39,550
um, often themselves be modified.

824
00:48:39,550 --> 00:48:44,305
Um, and so, um, that you know,

825
00:48:44,305 --> 00:48:49,680
good can also- good also is a operator, right?

826
00:48:49,680 --> 00:48:52,740
So that from a sort of a person,

827
00:48:52,740 --> 00:48:56,115
you can have a good person and that's sort of also an operator,

828
00:48:56,115 --> 00:48:58,290
and very is modifying that good.

829
00:48:58,290 --> 00:49:03,460
So the idea we came up with is let's not try and predetermine all of this.

830
00:49:03,460 --> 00:49:07,630
Why don't we say that every word and every phrase has

831
00:49:07,630 --> 00:49:12,385
connected to it both matrix and vector.

832
00:49:12,385 --> 00:49:15,175
So here's our very good movie.

833
00:49:15,175 --> 00:49:16,645
So for each word,

834
00:49:16,645 --> 00:49:19,945
we have a vector meaning and it has a matrix meaning,

835
00:49:19,945 --> 00:49:23,530
and then as we start to build up phrases like very good,

836
00:49:23,530 --> 00:49:28,795
they're also going to have a vector meaning and a matrix meaning.

837
00:49:28,795 --> 00:49:32,635
And so what we proposed was,

838
00:49:32,635 --> 00:49:34,390
um, so first of all,

839
00:49:34,390 --> 00:49:37,015
we, we would like to be able, um,

840
00:49:37,015 --> 00:49:40,765
to calculate, um, the vector meanings.

841
00:49:40,765 --> 00:49:47,005
So to work out the vector meaning of a phrase like very good.

842
00:49:47,005 --> 00:49:49,540
Each word has a matrix meaning.

843
00:49:49,540 --> 00:49:53,680
And so we're going to combine their opposing matrix and vector meaning.

844
00:49:53,680 --> 00:49:56,860
So we're going to take the matrix meaning of

845
00:49:56,860 --> 00:50:00,610
good and multiply it by the vector meaning of very.

846
00:50:00,610 --> 00:50:03,910
And we're going to take the matrix meaning of very and

847
00:50:03,910 --> 00:50:07,525
multiply it by the vector meaning of good.

848
00:50:07,525 --> 00:50:11,185
And so we're going to have both of those two things.

849
00:50:11,185 --> 00:50:17,620
And then we're going to have a neural network layer like before that combine those together.

850
00:50:17,620 --> 00:50:19,540
And so that's sort of in the red box.

851
00:50:19,540 --> 00:50:22,045
Then those two things were concatenated,

852
00:50:22,045 --> 00:50:25,840
and put through the kind of neural network layer we had before to give us

853
00:50:25,840 --> 00:50:30,235
a final vector meaning on this, for the phrase.

854
00:50:30,235 --> 00:50:34,675
And then we also needed a matrix meaning for the phrase.

855
00:50:34,675 --> 00:50:38,395
And so for the matrix meaning for the phrase, um.

856
00:50:38,395 --> 00:50:44,185
We did this kind of simple model which maybe actually wasn't very good which was to say,

857
00:50:44,185 --> 00:50:50,260
let's just concatenate the two matrices of the- um,

858
00:50:50,260 --> 00:50:53,560
the constituents, multiply them by

859
00:50:53,560 --> 00:50:57,280
another matrix and that's then going to give us a matrix,

860
00:50:57,280 --> 00:50:59,920
um, version of the parent node.

861
00:50:59,920 --> 00:51:05,605
And so this was gave us our new more compo- more powerful composition procedure.

862
00:51:05,605 --> 00:51:11,620
Um, this did seem like it could do some kind of good things that captured,

863
00:51:11,620 --> 00:51:17,980
uh, uh, sort of operator semantics where one word modified the meaning of another word.

864
00:51:17,980 --> 00:51:25,610
Um, so here's a kind of a neat thing that we were able to do with this.

865
00:51:25,620 --> 00:51:30,040
Um, that we are wanting to be able to work out

866
00:51:30,040 --> 00:51:34,330
the semantics of an operator modifying another word.

867
00:51:34,330 --> 00:51:40,450
So unbelievably annoying, unbelievably awesome, unbelievably sad.

868
00:51:40,450 --> 00:51:44,260
Um, not annoying, not awesome, not sad.

869
00:51:44,260 --> 00:51:48,340
[NOISE] And so this was contrasting,

870
00:51:48,340 --> 00:51:54,205
our, um, old model versus the new model.

871
00:51:54,205 --> 00:51:58,345
And this scale is a scale of positive to negative.

872
00:51:58,345 --> 00:52:03,210
So this is completely negative to completely positive, all right?

873
00:52:03,210 --> 00:52:06,750
And so the kind of contrast you get,

874
00:52:06,750 --> 00:52:09,910
uh, that for, um,

875
00:52:09,910 --> 00:52:15,290
not annoying that the simple model thought that this was pretty negative,

876
00:52:15,290 --> 00:52:19,070
whereas the new model thinks this is pretty neutral in meaning,

877
00:52:19,070 --> 00:52:22,615
and that seems to be reasonably correct.

878
00:52:22,615 --> 00:52:25,210
Um, but not sad,

879
00:52:25,210 --> 00:52:31,180
that means it's a little bit positive and both models were trying to capt- capture that,

880
00:52:31,180 --> 00:52:34,775
that- you know, the results here are a little bit ambivalent,

881
00:52:34,775 --> 00:52:36,970
but- but it sort of seems that they sort of go a

882
00:52:36,970 --> 00:52:40,105
little bit in the direction of what we want. Yes.

883
00:52:40,105 --> 00:52:42,640
What is the ground truth in the "not sad" example?

884
00:52:42,640 --> 00:52:45,745
Oh, yeah. So this ground truth

885
00:52:45,745 --> 00:52:49,510
was- we actually asked a whole bunch of human beings to say,

886
00:52:49,510 --> 00:52:53,215
um, rate the [LAUGHTER] meaning of not sad,

887
00:52:53,215 --> 00:52:55,210
on this scale of 1 to 10.

888
00:52:55,210 --> 00:52:58,390
Maybe this wasn't a very good clear task because as you can see it,

889
00:52:58,390 --> 00:53:01,630
bounced around a lot [LAUGHTER] as to,

890
00:53:01,630 --> 00:53:05,350
um, what kind of ratings we were getting for things.

891
00:53:05,350 --> 00:53:08,230
But yeah, that was actually kind of getting human judgments.

892
00:53:08,230 --> 00:53:13,464
Um, we also then use this,

893
00:53:13,464 --> 00:53:15,220
um, model to say, "Well,

894
00:53:15,220 --> 00:53:18,910
could we do, um, semantic classification tasks?"

895
00:53:18,910 --> 00:53:24,965
So if we wanted to understand relations between different noun phrases,

896
00:53:24,965 --> 00:53:28,255
so this was a dataset where, um,

897
00:53:28,255 --> 00:53:32,695
there were relations marked between two noun phrases.

898
00:53:32,695 --> 00:53:37,390
My apartment has a pretty large kitchen that that was seen as an example of

899
00:53:37,390 --> 00:53:43,840
a component-whole, a part of relationship between the two noun phrases,

900
00:53:43,840 --> 00:53:49,210
and there were other relationships between different kinds of noun phrases.

901
00:53:49,210 --> 00:53:52,240
So if it was the movie showed wars, um,

902
00:53:52,240 --> 00:53:54,535
that that was then a message topic,

903
00:53:54,535 --> 00:53:59,455
so there's some communication medium that contains some topic relationship.

904
00:53:59,455 --> 00:54:01,930
And so we were using this kind of

905
00:54:01,930 --> 00:54:05,860
neural network to sort of build our meaning representations and

906
00:54:05,860 --> 00:54:06,940
then putting them through

907
00:54:06,940 --> 00:54:12,395
another neural network layer as a classifier to see how well we did.

908
00:54:12,395 --> 00:54:15,580
And so we got some sort of fairly good results on that.

909
00:54:15,580 --> 00:54:18,970
So this was a dataset that people had worked on with

910
00:54:18,970 --> 00:54:24,070
traditional NLP systems of different kinds of machine learning methods.

911
00:54:24,070 --> 00:54:26,170
But in some sense, you know,

912
00:54:26,170 --> 00:54:29,710
what we were interested in was we seem to be making progress in having

913
00:54:29,710 --> 00:54:32,440
a better semantic composition system that

914
00:54:32,440 --> 00:54:37,180
our old recursive neural network was getting about 75 percent,

915
00:54:37,180 --> 00:54:40,255
and then our new one was getting about 79 percent,

916
00:54:40,255 --> 00:54:45,280
which we could sort of push up further by putting more features into our system.

917
00:54:45,280 --> 00:54:47,770
So that was progress,

918
00:54:47,770 --> 00:54:50,815
um, but we didn't stop there.

919
00:54:50,815 --> 00:54:55,375
We kept on trying to come up with better ways of doing things.

920
00:54:55,375 --> 00:54:59,660
And so even though things worked fairly well here,

921
00:54:59,660 --> 00:55:07,500
it sort of seemed like this way of doing matrices wasn't necessarily very good.

922
00:55:07,500 --> 00:55:09,675
It sort of had two problems.

923
00:55:09,675 --> 00:55:15,910
One problem was it introduced a humongous number of parameters because,

924
00:55:15,910 --> 00:55:19,630
you know, for just about everything that we've done, otherwise,

925
00:55:19,630 --> 00:55:22,360
words have had a vector and well,

926
00:55:22,360 --> 00:55:28,090
maybe sometimes we use quite high dimensional vectors like 1,024,

927
00:55:28,090 --> 00:55:31,900
um, [NOISE] but, you know, that's a relatively modest number of parameters.

928
00:55:31,900 --> 00:55:35,305
Whereas once we introduce this matrix here,

929
00:55:35,305 --> 00:55:40,540
we've got that number of squared additional parameters for every word.

930
00:55:40,540 --> 00:55:43,300
And essentially because of that number of

931
00:55:43,300 --> 00:55:46,690
parameters to be able to compute this model at all,

932
00:55:46,690 --> 00:55:49,180
we were making the vector size small.

933
00:55:49,180 --> 00:55:51,250
So what we were actually using was that these were

934
00:55:51,250 --> 00:55:55,570
just 25-dimensional vectors so that the 25 squared,

935
00:55:55,570 --> 00:56:01,495
625, still safe, sort of decently within the range in which we could compute.

936
00:56:01,495 --> 00:56:03,775
So that was the first problem.

937
00:56:03,775 --> 00:56:05,485
The second problem is,

938
00:56:05,485 --> 00:56:08,620
we didn't really have very good ways of

939
00:56:08,620 --> 00:56:13,210
sort of building up the matrix meaning of bigger phrases.

940
00:56:13,210 --> 00:56:14,350
I mean, you know,

941
00:56:14,350 --> 00:56:17,830
this sort of seems something simple we could do but it didn't,

942
00:56:17,830 --> 00:56:21,970
you know, feel a very good way of getting a matrix meaning of a phrase.

943
00:56:21,970 --> 00:56:24,760
So we sort of wanted to come up with some other way of doing

944
00:56:24,760 --> 00:56:28,375
things that could fix both of those problems.

945
00:56:28,375 --> 00:56:33,940
And then, that led into work on recursive neural tensor networks.

946
00:56:33,940 --> 00:56:39,415
Um, and there's a kind of a nice idea here of these neural tensors,

947
00:56:39,415 --> 00:56:44,590
which is an idea that's actually been used in other places including, um,

948
00:56:44,590 --> 00:56:49,210
work on sort of putting vector embeddings of knowledge graphs and so on,

949
00:56:49,210 --> 00:56:51,550
which is a kind of a bit of a nice idea.

950
00:56:51,550 --> 00:56:55,570
So I wanted to sort of show a bit of how this model works.

951
00:56:55,570 --> 00:56:58,930
Um, and but just to say, first,

952
00:56:58,930 --> 00:57:03,670
a place where we applied this model was on the problem of sentiment analysis.

953
00:57:03,670 --> 00:57:08,650
Now, I think the term sentiment analysis has come up a few times as something you can

954
00:57:08,650 --> 00:57:13,720
do and actually which I then mentioned in the last, um, lecture.

955
00:57:13,720 --> 00:57:17,320
But I think we've never really talked for five minutes, um,

956
00:57:17,320 --> 00:57:19,450
in this class on sentiment analysis,

957
00:57:19,450 --> 00:57:22,645
so I'll, um, give you this as an example of that.

958
00:57:22,645 --> 00:57:24,910
Um, sentiment analysis has actually been

959
00:57:24,910 --> 00:57:30,940
a really common and important application in natural language processing.

960
00:57:30,940 --> 00:57:34,705
Um, you're looking at a piece of text and you're sort of saying,

961
00:57:34,705 --> 00:57:37,810
"Is it, um, positive or negative?"

962
00:57:37,810 --> 00:57:42,160
Um, and that's just something that's very useful for lots of, um,

963
00:57:42,160 --> 00:57:46,955
commercial applications of looking at product reviews or doing brand,

964
00:57:46,955 --> 00:57:51,530
um, awareness and things like that of sort of looking at sentiment connected to things.

965
00:57:51,530 --> 00:57:55,540
And to some extent doing sentiment analysis is easy, right?

966
00:57:55,540 --> 00:57:57,220
That you can kind of say,

967
00:57:57,220 --> 00:57:58,840
"Well, look at a piece of text.

968
00:57:58,840 --> 00:58:00,700
If you see words like loved,

969
00:58:00,700 --> 00:58:03,265
great, impressed, marvelous, then it's positive.

970
00:58:03,265 --> 00:58:04,570
It's a positive review.

971
00:58:04,570 --> 00:58:06,880
And if it's saying, bad and awful,

972
00:58:06,880 --> 00:58:08,440
then it's a negative review."

973
00:58:08,440 --> 00:58:13,420
And to some extent that's the baseline of sentiment analysis that you can use

974
00:58:13,420 --> 00:58:18,805
just either selected word features or all words in a bag of words.

975
00:58:18,805 --> 00:58:20,035
And if you do that,

976
00:58:20,035 --> 00:58:22,780
you don't actually do that badly,

977
00:58:22,780 --> 00:58:25,150
um, in sentiment analysis.

978
00:58:25,150 --> 00:58:26,650
If you have longer documents,

979
00:58:26,650 --> 00:58:31,135
just looking at bags of words can give you 90 percent in sentiment analysis.

980
00:58:31,135 --> 00:58:32,335
But on the other hand,

981
00:58:32,335 --> 00:58:35,140
things often do get trickier, right?

982
00:58:35,140 --> 00:58:38,020
So, um, this is from Rotten Tomatoes.

983
00:58:38,020 --> 00:58:40,450
With this cast and the subject matter,

984
00:58:40,450 --> 00:58:43,480
the movie should have been funnier and more entertaining.

985
00:58:43,480 --> 00:58:47,004
And if you sort of pretend you're a bag of words model,

986
00:58:47,004 --> 00:58:52,840
the only words in this that are sort of clearly sentiment-laden words, uh,

987
00:58:52,840 --> 00:58:57,969
entertaining and funnier, and both of those are pretty positive words,

988
00:58:57,969 --> 00:59:04,615
um, but it's fairly obvious that this actually is meant to be a bad review of the movie.

989
00:59:04,615 --> 00:59:07,360
And so well, how are we meant to know that?

990
00:59:07,360 --> 00:59:11,320
Well, it sort of seems again like what we have to do is meaning composition.

991
00:59:11,320 --> 00:59:15,310
We have to get sort of phrases like "should have been

992
00:59:15,310 --> 00:59:21,070
funnier," and then realized that that's actually a negative meaning for a phrase.

993
00:59:21,070 --> 00:59:25,690
And so we wanted to explore how we could look at those sort of meanings for

994
00:59:25,690 --> 00:59:33,310
phrases and explore building up those meanings as doing meaning composition over trees.

995
00:59:33,310 --> 00:59:36,400
Um, so the first thing we did, um,

996
00:59:36,400 --> 00:59:42,490
was we built a treebank of sentiment trees where we got people to rate sentiment.

997
00:59:42,490 --> 00:59:45,910
And so this led to the Stanford Sentiment Treebank,

998
00:59:45,910 --> 00:59:49,675
which is still a dataset you often see used in, um,

999
00:59:49,675 --> 00:59:54,280
various of evaluations with a whole bunch of datasets. Indeed,

1000
00:59:54,280 --> 00:59:57,175
it showed up in decaNLP last week.

1001
00:59:57,175 --> 01:00:00,545
Um, so what we were doing in this was taking,

1002
01:00:00,545 --> 01:00:06,265
um, sentences which were Rotten Tomatoes sentences from movies.

1003
01:00:06,265 --> 01:00:13,450
We were parsing them to give tree structure and then we were asking Mechanical Turkers to

1004
01:00:13,450 --> 01:00:16,745
rate the different phra- the different words and

1005
01:00:16,745 --> 01:00:21,460
phrases on a sentiment scale of very positive to very negative.

1006
01:00:21,460 --> 01:00:25,660
So lots of stuff is white because it's just not sentiment-laden, right?

1007
01:00:25,660 --> 01:00:27,575
There's words that are the,

1008
01:00:27,575 --> 01:00:29,710
and there's phrases like the movie and

1009
01:00:29,710 --> 01:00:33,325
the movie was- which don't really have any sentiment,

1010
01:00:33,325 --> 01:00:37,180
but then you have pieces that are sort of very positives pieces of

1011
01:00:37,180 --> 01:00:42,025
tree and negative pieces of tree that are then shown in the blue and the red.

1012
01:00:42,025 --> 01:00:45,265
And- so typically in sentiment datasets,

1013
01:00:45,265 --> 01:00:49,720
people have only labeled the entire sentence to say,

1014
01:00:49,720 --> 01:00:53,140
"This is a positive sentence or a very positive sentence.

1015
01:00:53,140 --> 01:00:55,840
This is a negative sentence or a very negative sentence."

1016
01:00:55,840 --> 01:01:01,810
Crucially, what we were doing differently here is every phrase in the sentence

1017
01:01:01,810 --> 01:01:08,170
according to our tree structure was being given a label for its positivity or negativity.

1018
01:01:08,170 --> 01:01:11,080
Um, and perhaps not surprisingly,

1019
01:01:11,080 --> 01:01:14,990
just the fact that you have a lot more annotations like that, um,

1020
01:01:14,990 --> 01:01:20,400
just improves the behavior of classifiers because you kind of can

1021
01:01:20,400 --> 01:01:26,735
do better attribution of which words in a sentence are positive or negative. Um.

1022
01:01:26,735 --> 01:01:32,810
So, these were um were results of sort of preceding models.

1023
01:01:32,810 --> 01:01:39,650
So the green is a Naive Bayes model except it not only uses individual words,

1024
01:01:39,650 --> 01:01:41,630
but it uses pairs of words.

1025
01:01:41,630 --> 01:01:45,590
It turns out if you're building a traditional classifier and you

1026
01:01:45,590 --> 01:01:49,940
wanna do sentiment analysis as opposed to something like topic classification,

1027
01:01:49,940 --> 01:01:54,500
you get a lot better results if you also use word pair features.

1028
01:01:54,500 --> 01:01:58,850
And that's because it does a baby bit of um composition for you.

1029
01:01:58,850 --> 01:02:01,659
You don't only have features for not an interesting,

1030
01:02:01,659 --> 01:02:03,610
but you can have a feature for not

1031
01:02:03,610 --> 01:02:07,030
interesting and that lets you model a certain amount of stuff.

1032
01:02:07,030 --> 01:02:10,630
Um, and then these are our older generations of neural networks,

1033
01:02:10,630 --> 01:02:14,935
our ori- original tree structured neural network and our matrix vector one.

1034
01:02:14,935 --> 01:02:19,070
And so simply having- for these sort of fixed models,

1035
01:02:19,070 --> 01:02:23,810
simply having the richer supervision that comes from our new treebank,

1036
01:02:23,810 --> 01:02:26,330
it's sort of moved up the performance of every model.

1037
01:02:26,330 --> 01:02:29,450
So even um, for just the um,

1038
01:02:29,450 --> 01:02:33,695
Naive Bayes model's performances going up about four percent um,

1039
01:02:33,695 --> 01:02:35,940
because of the fact um,

1040
01:02:35,940 --> 01:02:38,405
that it now knows more about which

1041
01:02:38,405 --> 01:02:42,005
particular words are positive or negative in the sentences.

1042
01:02:42,005 --> 01:02:47,075
Um, but still none of these performances are really great.

1043
01:02:47,075 --> 01:02:53,120
Um, so we still thought that well can we build better models of how to do this.

1044
01:02:53,120 --> 01:02:56,390
Um, in particular, if you look at sentences with

1045
01:02:56,390 --> 01:02:59,450
sort of various kinds of negation you know,

1046
01:02:59,450 --> 01:03:01,505
things like should've been funnier,

1047
01:03:01,505 --> 01:03:06,170
these models in general still couldn't capture the right meanings for them.

1048
01:03:06,170 --> 01:03:11,600
And so that led into our fourth model of how to do this,

1049
01:03:11,600 --> 01:03:15,965
which is this idea of recursive neural tensor networks.

1050
01:03:15,965 --> 01:03:22,550
Um, and so what we wanted to be able to do is go back to just having um,

1051
01:03:22,550 --> 01:03:26,660
meanings of words be vectors,

1052
01:03:26,660 --> 01:03:30,335
but nevertheless despite that to be able to

1053
01:03:30,335 --> 01:03:34,355
have a meaningful phrase where the two vectors um,

1054
01:03:34,355 --> 01:03:36,140
acted on each other.

1055
01:03:36,140 --> 01:03:39,245
And well, you know, this kind of,

1056
01:03:39,245 --> 01:03:41,810
this is the picture of what we did when we were

1057
01:03:41,810 --> 01:03:44,615
doing attention in a bi-linear way, right?

1058
01:03:44,615 --> 01:03:46,625
We had vectors for two words.

1059
01:03:46,625 --> 01:03:50,330
We stuck a matrix in between and we used

1060
01:03:50,330 --> 01:03:54,775
that and gave an attention and got an attention score out.

1061
01:03:54,775 --> 01:03:59,245
So that let these two vectors interact with each other,

1062
01:03:59,245 --> 01:04:02,635
but it only produced one number as the output.

1063
01:04:02,635 --> 01:04:04,630
But there's a way to fix that,

1064
01:04:04,630 --> 01:04:10,295
which is to say well rather than having a matrix here,

1065
01:04:10,295 --> 01:04:14,555
what we could stick here is a three-dimensional cube,

1066
01:04:14,555 --> 01:04:19,220
which physicists and deep learning people now call a tensor, right?

1067
01:04:19,220 --> 01:04:22,550
So a tensor is just higher multi-dimensional array um,

1068
01:04:22,550 --> 01:04:24,560
in computer science terms.

1069
01:04:24,560 --> 01:04:28,595
Um, so if we sort of made that a tensor,

1070
01:04:28,595 --> 01:04:33,035
you know, it's like we have sort of multiple layers of matrix here.

1071
01:04:33,035 --> 01:04:38,795
And so the end result of that is we get one number here and one number here.

1072
01:04:38,795 --> 01:04:42,979
So in total, we get out a size two vector,

1073
01:04:42,979 --> 01:04:46,355
which is all we need in my baby example where

1074
01:04:46,355 --> 01:04:50,300
baby examples, where we only have these two component vectors for words.

1075
01:04:50,300 --> 01:04:52,250
But in general, we have a tensor with

1076
01:04:52,250 --> 01:04:55,835
the extra mention dimension of the size of our word vector.

1077
01:04:55,835 --> 01:04:58,985
And so therefore, we will get a word vector, w hat,

1078
01:04:58,985 --> 01:05:03,800
we will get a phrase vector out from the composition that's the same size of

1079
01:05:03,800 --> 01:05:06,980
the input vectors and will allow them to

1080
01:05:06,980 --> 01:05:12,450
interact with each other in working out the meaning of the entire thing.

1081
01:05:12,910 --> 01:05:17,390
Okay. Um, all right.

1082
01:05:17,390 --> 01:05:19,220
So at that point um,

1083
01:05:19,220 --> 01:05:23,250
we use the resulting vectors um um,

1084
01:05:24,610 --> 01:05:28,265
so we had our neural tensor network.

1085
01:05:28,265 --> 01:05:33,695
We actually combined it together with the sort of previous kind of layer we used to have,

1086
01:05:33,695 --> 01:05:37,310
our sort of first RNN, maybe you  didn't need to do this,

1087
01:05:37,310 --> 01:05:39,485
but we just decided to add that in as well,

1088
01:05:39,485 --> 01:05:42,470
put things through a nonlinearity and that was then

1089
01:05:42,470 --> 01:05:45,770
giving us our new representation of phrases.

1090
01:05:45,770 --> 01:05:49,190
We built that up the tree and then at the end,

1091
01:05:49,190 --> 01:05:53,120
we could classify the meaning of any phrase um,

1092
01:05:53,120 --> 01:05:56,900
in the same kind of way with softmax regression and we could

1093
01:05:56,900 --> 01:06:00,585
train these weights with gradient descent to predict sentiment.

1094
01:06:00,585 --> 01:06:03,910
And so this actually worked pretty nicely.

1095
01:06:03,910 --> 01:06:05,245
I mean in particular,

1096
01:06:05,245 --> 01:06:09,820
it didn't so really work any better with just the sentence labels.

1097
01:06:09,820 --> 01:06:13,194
But if we train the model with our treebank,

1098
01:06:13,194 --> 01:06:15,820
we could then get a kind of- of whatever

1099
01:06:15,820 --> 01:06:18,700
that is about another couple of percent in performance,

1100
01:06:18,700 --> 01:06:20,575
and so that seemed good.

1101
01:06:20,575 --> 01:06:21,880
And so in particular,

1102
01:06:21,880 --> 01:06:26,920
it seemed to do a much better job of actually understanding meaning composition.

1103
01:06:26,920 --> 01:06:32,095
So here's the kind of sentence where you have there are slow and repetitive parts,

1104
01:06:32,095 --> 01:06:35,245
but it has just enough spice to keep it interesting.

1105
01:06:35,245 --> 01:06:38,470
And the model seen here is pretty good at understanding.

1106
01:06:38,470 --> 01:06:41,400
Okay, this part of the sentence is negative,

1107
01:06:41,400 --> 01:06:43,880
this part of the sentence is positive,

1108
01:06:43,880 --> 01:06:46,310
and actually when you stick the two halves together,

1109
01:06:46,310 --> 01:06:50,450
the end result is a sentence that's positive in meaning.

1110
01:06:50,450 --> 01:06:54,170
But focusing in a little bit more what seems

1111
01:06:54,170 --> 01:06:58,610
like it's especially good was for the first time this actually

1112
01:06:58,610 --> 01:07:02,270
did seem like it could do a better job of

1113
01:07:02,270 --> 01:07:07,220
working out sort of what happens when you do things like negation.

1114
01:07:07,220 --> 01:07:11,975
So here we have it's just incredibly dull and it's definitely not dull.

1115
01:07:11,975 --> 01:07:14,000
So if it's definitely not dull,

1116
01:07:14,000 --> 01:07:16,130
that's actually means it's good, right?

1117
01:07:16,130 --> 01:07:20,480
Can we work out the meaning of, it's definitely not dull?

1118
01:07:20,480 --> 01:07:25,610
And so um, these, this is sort of

1119
01:07:25,610 --> 01:07:31,505
showing sort of what happens when you have a negative,

1120
01:07:31,505 --> 01:07:34,790
a negative sentence that's further negated.

1121
01:07:34,790 --> 01:07:39,890
So if you go from um,

1122
01:07:39,890 --> 01:07:42,305
so if you sort of do

1123
01:07:42,305 --> 01:07:48,710
a annex- negation of a negative thing should become moderately positive, right?

1124
01:07:48,710 --> 01:07:54,065
So that if you have dull is negative and if you say not dull,

1125
01:07:54,065 --> 01:07:56,105
it doesn't mean it's fantastic,

1126
01:07:56,105 --> 01:07:58,400
but it means it's moderately positive.

1127
01:07:58,400 --> 01:08:04,070
And so for either a kind of Naive Bayes model or our preceding models,

1128
01:08:04,070 --> 01:08:09,755
they weren't capable of capturing that of sort of going from dull to not dull your,

1129
01:08:09,755 --> 01:08:14,135
your meaning computation did not come out any more positive.

1130
01:08:14,135 --> 01:08:17,000
Whereas this sort of neural tensor network was

1131
01:08:17,000 --> 01:08:22,470
capturing the fact that not dull meant that it was reasonably good.

1132
01:08:22,750 --> 01:08:26,960
So that was progress. Um, yes.

1133
01:08:26,960 --> 01:08:31,460
So I think that's as much as I'll um show you really now about applying

1134
01:08:31,460 --> 01:08:37,590
these tree structured neural networks um, to natural language.

1135
01:08:37,810 --> 01:08:43,190
Um, you know, I think the summary I sort of said at the beginning um is that I

1136
01:08:43,190 --> 01:08:48,275
think you know, they're kind of interesting ideas and linguistic connections here.

1137
01:08:48,275 --> 01:08:52,325
I mean, for various reasons,

1138
01:08:52,325 --> 01:08:55,100
these ideas haven't been um,

1139
01:08:55,100 --> 01:08:59,570
pursued a ton in recent years of natural language processing.

1140
01:08:59,570 --> 01:09:04,610
You know, one is in all honesty people have found that um,

1141
01:09:04,610 --> 01:09:08,090
once you have high dimensional vectors

1142
01:09:08,090 --> 01:09:11,480
in things like the kind of sequence models that we've looked at,

1143
01:09:11,480 --> 01:09:15,980
whether it's meaning things like the sort of LSTM models or any of

1144
01:09:15,980 --> 01:09:21,200
the more recent contextual language models, those work incredibly well um,

1145
01:09:21,200 --> 01:09:24,890
and it's not, it's not clear that overall these models work better.

1146
01:09:24,890 --> 01:09:28,399
The second reason is sort of a computational reason,

1147
01:09:28,399 --> 01:09:35,495
which is um, GPUs work great when you're doing uniform computation.

1148
01:09:35,495 --> 01:09:40,130
And the beauty of having something like a sequence model is that there's uh,

1149
01:09:40,130 --> 01:09:43,595
there's just one determinant computation you are doing

1150
01:09:43,595 --> 01:09:47,180
along the sequence or in the convolutional neural network,

1151
01:09:47,180 --> 01:09:49,010
there's one determinant um,

1152
01:09:49,010 --> 01:09:51,530
computation you're doing up um,

1153
01:09:51,530 --> 01:09:54,170
through your convolutional layers and therefore,

1154
01:09:54,170 --> 01:09:58,805
things can be represented and computed efficiently on a GPU.

1155
01:09:58,805 --> 01:10:03,020
The huge problem with these kind of models was what computations you are

1156
01:10:03,020 --> 01:10:07,475
going to do depended on which structure you are assigning to the sentence,

1157
01:10:07,475 --> 01:10:12,110
and every sentence was going to have a different structure, and so therefore,

1158
01:10:12,110 --> 01:10:15,200
there was no way to batch the computations over a group of

1159
01:10:15,200 --> 01:10:18,860
sentences and have the same computations being done for

1160
01:10:18,860 --> 01:10:22,310
different sentences, it sort of undermined the ability

1161
01:10:22,310 --> 01:10:26,365
to sort of efficiently build these models in the large.

1162
01:10:26,365 --> 01:10:31,145
The thing I thought I'd just sort of say a moment about at the end.

1163
01:10:31,145 --> 01:10:36,195
Um, the funny thing is that although these haven't been used much for,

1164
01:10:36,195 --> 01:10:39,250
um, language in the last few years, um,

1165
01:10:39,250 --> 01:10:45,650
that they've actually had some use and found different applications in different places,

1166
01:10:45,650 --> 01:10:48,215
um, which is just sort of seen kind of cute.

1167
01:10:48,215 --> 01:10:52,850
Um, so this is actually an application from physics.

1168
01:10:52,850 --> 01:10:56,020
Um, and I think I'm going to just have to read this and

1169
01:10:56,020 --> 01:10:58,890
so I have no idea what half the words mean.

1170
01:10:58,890 --> 01:11:04,295
Um, but, um, what it says is by far the most common structures seen in collisions at

1171
01:11:04,295 --> 01:11:10,295
the Large Hadron Collider are collimated sprays of energetic hadrons referred to as jets.

1172
01:11:10,295 --> 01:11:14,135
These jets are produced from the fragmentation and hadronization of

1173
01:11:14,135 --> 01:11:19,200
quarks and gluons as described by quantum chromodynamics.

1174
01:11:19,200 --> 01:11:21,420
Anyone knows what that means?

1175
01:11:21,420 --> 01:11:23,550
Um, I hope you're following along here.

1176
01:11:23,550 --> 01:11:26,970
Um, one compelling physics challenge is to search for

1177
01:11:26,970 --> 01:11:32,000
highly boosted standard model particles decaying hadronically.

1178
01:11:32,000 --> 01:11:36,935
Unfortunately there's a large background from jets produced by more mundane,

1179
01:11:36,935 --> 01:11:41,090
um, QCD, that's quantum chromodynamics processes.

1180
01:11:41,090 --> 01:11:44,610
In this work, we propose instead a solution for

1181
01:11:44,610 --> 01:11:48,215
jet classification based on an analogy between

1182
01:11:48,215 --> 01:11:52,470
quantum chromodynamics and natural languages as inspired by

1183
01:11:52,470 --> 01:11:56,775
several works from natural language, um, processing.

1184
01:11:56,775 --> 01:11:59,520
Much like a sentence is composed of words

1185
01:11:59,520 --> 01:12:02,865
following a syntactic structure organized as a parse tree,

1186
01:12:02,865 --> 01:12:08,050
a jet is also composed of 4-momenta following a structure dictated by

1187
01:12:08,050 --> 01:12:09,760
QCD and organized via

1188
01:12:09,760 --> 01:12:14,100
the clustering history of a sequential co- combination jet algorithm.

1189
01:12:14,100 --> 01:12:18,005
Um, so anyway um, yeah with these jets you see they're getting

1190
01:12:18,005 --> 01:12:23,794
a tree structure over them and they're using the tree recursive neural network,

1191
01:12:23,794 --> 01:12:25,100
um, to model it.

1192
01:12:25,100 --> 01:12:31,435
Um, well that's a little bit far afield but to show you just one more example, um,

1193
01:12:31,435 --> 01:12:35,320
that another place where these models have actually being quite

1194
01:12:35,320 --> 01:12:39,840
useful is for doing things in programming languages.

1195
01:12:39,840 --> 01:12:42,020
And I think in part this,

1196
01:12:42,020 --> 01:12:46,545
this is because the application is easier in programming languages.

1197
01:12:46,545 --> 01:12:51,150
So unlike in natural language where we have this uncertainty as to what is

1198
01:12:51,150 --> 01:12:55,775
the correct parse tree because there's a lot of ambiguity in natural language,

1199
01:12:55,775 --> 01:12:58,295
in programming languages, um,

1200
01:12:58,295 --> 01:13:01,175
the parse trees are actually pretty determinant.

1201
01:13:01,175 --> 01:13:07,560
Um, so a group of people at Berkeley, Dawn Song and her students have worked on doing

1202
01:13:07,560 --> 01:13:10,870
programming language translation by building

1203
01:13:10,870 --> 01:13:14,490
tree recursive neural network encoder-decoders.

1204
01:13:14,490 --> 01:13:17,375
So that you're building up a tree structured

1205
01:13:17,375 --> 01:13:22,120
neural network representation of a program in one language.

1206
01:13:22,120 --> 01:13:26,345
This is a CoffeeScript program and then you're wanting to build a tree to

1207
01:13:26,345 --> 01:13:31,760
tree model which is then translating that to a program in a different language.

1208
01:13:31,760 --> 01:13:35,150
And they've been able to do that and get good results.

1209
01:13:35,150 --> 01:13:38,760
Um, I was too lazy to retype this table.

1210
01:13:38,760 --> 01:13:40,610
So this is probably a bit,

1211
01:13:40,610 --> 01:13:42,010
bit hard to read.

1212
01:13:42,010 --> 01:13:46,320
But what's it's contrasting is for a number of programs this is the sort of

1213
01:13:46,320 --> 01:13:51,650
CoffeeScript to JavaScript, um, um, translation.

1214
01:13:51,650 --> 01:13:54,980
They're comparing using tree to tree models.

1215
01:13:54,980 --> 01:13:57,130
Um, and then using sequence to sequence

1216
01:13:57,130 --> 01:14:00,120
models and then they tried both other combinations,

1217
01:14:00,120 --> 01:14:03,345
sequence to tree and tree to sequence.

1218
01:14:03,345 --> 01:14:06,215
Um, and what they find is you can get the best

1219
01:14:06,215 --> 01:14:10,175
results with the tree to tree neural network models.

1220
01:14:10,175 --> 01:14:13,665
And in particular these tree to tree models are

1221
01:14:13,665 --> 01:14:17,345
augmented with attention so they have attention like we talked about

1222
01:14:17,345 --> 01:14:21,490
the sequence to sequence models where you're then being able to do attention back to

1223
01:14:21,490 --> 01:14:26,990
nodes in the tree structure which is a pretty natural way of doing translation.

1224
01:14:26,990 --> 01:14:31,320
And indeed what these results show is if you don't have- that's right these results

1225
01:14:31,320 --> 01:14:36,035
show is if you don't have the attention operation it doesn't work at all.

1226
01:14:36,035 --> 01:14:37,680
It's too difficult, um,

1227
01:14:37,680 --> 01:14:39,060
to get things, um,

1228
01:14:39,060 --> 01:14:41,050
sort of done if you've just sort of trying to create

1229
01:14:41,050 --> 01:14:45,810
a single tree representation and then say generate the tra- the translation from that.

1230
01:14:45,810 --> 01:14:48,245
But if you can do it with this sort of putting attention

1231
01:14:48,245 --> 01:14:51,735
into the different modes, um, that's great.

1232
01:14:51,735 --> 01:14:56,385
Um, you might- If you know what CoffeeScript is you might, um,

1233
01:14:56,385 --> 01:14:58,740
feel like wait that's cheating slightly because

1234
01:14:58,740 --> 01:15:02,125
CoffeeScript is a bit too similar to JavaScript.

1235
01:15:02,125 --> 01:15:03,725
Um, but they've also, um,

1236
01:15:03,725 --> 01:15:05,310
done it in other languages.

1237
01:15:05,310 --> 01:15:11,090
So this is going between Java and C# and this is a sort of

1238
01:15:11,090 --> 01:15:14,490
handwritten Java to C# converter that you can

1239
01:15:14,490 --> 01:15:18,820
download from GitHub if you want but it doesn't actually work that well.

1240
01:15:18,820 --> 01:15:20,570
Um, and they're able to show,

1241
01:15:20,570 --> 01:15:23,120
the- they're able to build a far better, um,

1242
01:15:23,120 --> 01:15:25,580
Java to C# translator,

1243
01:15:25,580 --> 01:15:28,080
um, doing that.

1244
01:15:28,080 --> 01:15:30,390
Um, so that's actually kind of cool.

1245
01:15:30,390 --> 01:15:33,110
And it's good to know that tree structured recursive neural networks

1246
01:15:33,110 --> 01:15:34,905
are good for some things.

1247
01:15:34,905 --> 01:15:37,820
Um, so I'm pleased to see work like this.

1248
01:15:37,820 --> 01:15:41,135
Okay. I'm, I'm, just about done but I thought,

1249
01:15:41,135 --> 01:15:43,515
um, before, um, finishing,

1250
01:15:43,515 --> 01:15:47,135
I'd just mention one other [NOISE] thing which is sort of nothing to do

1251
01:15:47,135 --> 01:15:50,850
with natural language processing precisely but it's about AI.

1252
01:15:50,850 --> 01:15:54,570
Um, but I wanted to sort of put in a little bit of advertisement.

1253
01:15:54,570 --> 01:15:57,335
Um, that's something that a number of us have been

1254
01:15:57,335 --> 01:16:00,855
working on very hard for the last year or so,

1255
01:16:00,855 --> 01:16:06,710
is developing, um, a new Stanford Institute for Human-Centered Artificial Intelligence.

1256
01:16:06,710 --> 01:16:11,880
And actually the launch of this institute is going to be on Monday of exam week,

1257
01:16:11,880 --> 01:16:16,365
just when you're maximally concentrating on things such as this.

1258
01:16:16,365 --> 01:16:19,680
Um, but our hope is that we can have a lot of

1259
01:16:19,680 --> 01:16:23,495
new activity around artificial intelligence,

1260
01:16:23,495 --> 01:16:27,510
taking a much broader perspective to artificial intelligence, um,

1261
01:16:27,510 --> 01:16:34,770
which is centrally viewing it from the viewpoint of humans and working out, um,

1262
01:16:34,770 --> 01:16:38,080
I'll- exploring a much broader range of issues that

1263
01:16:38,080 --> 01:16:41,490
embrace a lot of the interests of the rest of the university whether

1264
01:16:41,490 --> 01:16:45,100
it's the social sciences and humanities, or also variously

1265
01:16:45,100 --> 01:16:48,945
in professional schools like the law school and the business school.

1266
01:16:48,945 --> 01:16:52,760
Um, so let's just quickly say a minute about that.

1267
01:16:52,760 --> 01:16:58,980
Um, that the, the sort of motivating idea is that sort of for most of my life sort

1268
01:16:58,980 --> 01:17:01,270
of AI seemed like a kind of

1269
01:17:01,270 --> 01:17:03,670
a fun intellectual quest as

1270
01:17:03,670 --> 01:17:06,420
to whether you could write bits of software that did anything,

1271
01:17:06,420 --> 01:17:10,245
um, halfway intelligent but that's clearly not what's going to be,

1272
01:17:10,245 --> 01:17:12,820
what's happening for the next 25 years.

1273
01:17:12,820 --> 01:17:14,830
That we're now at this point in which

1274
01:17:14,830 --> 01:17:19,680
artificial intelligence systems are being unleashed on society.

1275
01:17:19,680 --> 01:17:23,730
Um, and well hopefully they do some good things but as we've

1276
01:17:23,730 --> 01:17:26,120
increasingly been seeing there are lots of

1277
01:17:26,120 --> 01:17:29,070
also lots of opportunities for them to do bad things.

1278
01:17:29,070 --> 01:17:32,510
And even if we're not imagining Terminator scenarios,

1279
01:17:32,510 --> 01:17:35,530
there are just lots of places where people are using

1280
01:17:35,530 --> 01:17:39,545
machine learning and AI algorithms for making decisions.

1281
01:17:39,545 --> 01:17:42,570
Some of the worst ones are things like sentencing guidelines in

1282
01:17:42,570 --> 01:17:46,950
courts where you have very biased algorithms making bad decisions and

1283
01:17:46,950 --> 01:17:51,425
people are starting to become a lot more aware of the issues and so

1284
01:17:51,425 --> 01:17:54,075
effectively we are wanting to have this institute sort of

1285
01:17:54,075 --> 01:17:56,940
embracing a lot of the work of social scientists,

1286
01:17:56,940 --> 01:18:01,420
the ethicists and other people to actually explore how to have an AI

1287
01:18:01,420 --> 01:18:06,030
that's really improving human lives rather than having the opposite effect.

1288
01:18:06,030 --> 01:18:07,860
And so the three themes,

1289
01:18:07,860 --> 01:18:09,390
um, that we're, um,

1290
01:18:09,390 --> 01:18:15,870
mainly emphasizing for this institute is the first one in the top left is

1291
01:18:15,870 --> 01:18:19,329
developing AI technologies but we're particularly

1292
01:18:19,329 --> 01:18:22,770
interested in making linkages back to human intelligence.

1293
01:18:22,770 --> 01:18:25,575
So cognitive science and neuroscience

1294
01:18:25,575 --> 01:18:28,925
that when a lot of the early formative work in AI was

1295
01:18:28,925 --> 01:18:31,410
done including all of

1296
01:18:31,410 --> 01:18:35,330
the early work in neural networks like the development of back propagation,

1297
01:18:35,330 --> 01:18:38,400
it was actually largely done in the context of cognitive science.

1298
01:18:38,400 --> 01:18:42,090
Right? And that was sort of a linkage that tended to get lost in

1299
01:18:42,090 --> 01:18:47,140
the '90s and 2000s statistical machine learning emphasis.

1300
01:18:47,140 --> 01:18:49,030
And I think it would be good to renew that.

1301
01:18:49,030 --> 01:18:51,240
Um, the top right, um,

1302
01:18:51,240 --> 01:18:53,910
there's paying much more attention to

1303
01:18:53,910 --> 01:18:59,310
the human and societal impact of AI and so this is looking at legal issues,

1304
01:18:59,310 --> 01:19:01,920
economic issues, labor forces,

1305
01:19:01,920 --> 01:19:05,905
ethics, um, green power, politics, whatever you are.

1306
01:19:05,905 --> 01:19:09,520
But then down at the bottom is something where it seems like

1307
01:19:09,520 --> 01:19:13,725
there's just kind of enormous opportunities to do more which is,

1308
01:19:13,725 --> 01:19:18,825
um, how can we build technology that actually augments human lives.

1309
01:19:18,825 --> 01:19:25,860
Like to some extent here tech- we've got technology with AI augmenting human lives.

1310
01:19:25,860 --> 01:19:29,200
So all of your cell phones have speech recognition in them now.

1311
01:19:29,200 --> 01:19:31,235
So you know that's AI, um,

1312
01:19:31,235 --> 01:19:34,055
that can augment, um, your human lives.

1313
01:19:34,055 --> 01:19:37,205
But there's a sense of which not very much of

1314
01:19:37,205 --> 01:19:43,055
artificial intelligence has actually been put into the service of augmenting human lives.

1315
01:19:43,055 --> 01:19:45,840
Like most of what a cell phone has on it is still

1316
01:19:45,840 --> 01:19:48,160
sort of clever and cute stuff done by

1317
01:19:48,160 --> 01:19:51,270
HCI people and designers which is very nice a lot of

1318
01:19:51,270 --> 01:19:55,470
the time when you're using your map program or something but we don't really have

1319
01:19:55,470 --> 01:20:00,455
much AI inside these devices helping to make people's lives better.

1320
01:20:00,455 --> 01:20:05,880
And so we're hoping not only for individuals when applications like health care,

1321
01:20:05,880 --> 01:20:08,240
um, to be doing much more of sort of putting

1322
01:20:08,240 --> 01:20:12,420
artificial intelligence into human-centered applications.

1323
01:20:12,420 --> 01:20:14,755
Um, anyway that's my brief advertisement.

1324
01:20:14,755 --> 01:20:17,840
Um, look it out for this while you're not studying for your exams.

1325
01:20:17,840 --> 01:20:20,700
And I think there'll be sort of lots of opportunities, um,

1326
01:20:20,700 --> 01:20:24,315
for students and others to be getting more involved in this in the coming months.

1327
01:20:24,315 --> 01:20:26,290
Okay. Thank you very much.

1328
01:20:26,290 --> 01:20:37,290
Um, and I will see you later, um, at the poster session.

1329
01:20:37,290 --> 01:20:37,400
[APPLAUSE].

